{
  "abstractText": "In this paper I argue that the search for explainable models and interpretable decisions in AI must be reformulated in terms of the broader project of offering a pragmatic and naturalistic account of understanding in AI. Intuitively, the purpose of providing an explanation of a model or a decision is to make it understandable to its stakeholders. But without a previous grasp of what it means to say that an agent understands a model or a decision, the explanatory strategies will lack a well-defined goal. Aside from providing a clearer objective for XAI, focusing on understanding also allows us to relax the factivity condition on explanation, which is impossible to fulfill in many machine learning models, and to focus instead on the pragmatic conditions that determine the best fit between a model and the methods and devices deployed to understand it. After an examination of the different types of understanding discussed in the philosophical and psychological literature, I conclude that interpretative or approximation models not only provide the best way to achieve the objectual understanding of a machine learning model, but are also a necessary condition to achieve post-hoc interpretability. This conclusion is partly based on the shortcomings of the purely functionalist approach to post-hoc interpretability that seems to be predominant in most recent literature.",
  "authors": [],
  "id": "SP:ccc2c3e2584c46900cfa666ce704b703886bf97c",
  "references": [
    {
      "authors": [
        "P. Achinstein"
      ],
      "title": "The nature of explanation",
      "venue": "New York: Oxford University Press.",
      "year": 1983
    },
    {
      "authors": [
        "H. Allahyari",
        "N. Lavesson"
      ],
      "title": "User-oriented assessment of classification model understandability",
      "venue": "Proceedings of the 11th Scandinavian Conference on Artificial Intelligence",
      "year": 2011
    },
    {
      "authors": [
        "J.A. Carter",
        "E.C. Gordon"
      ],
      "title": "Objectual understanding, factivity and belief",
      "year": 2016
    },
    {
      "authors": [
        "R. Caruana",
        "H. Kangarloo",
        "J.D.N. Dionisio",
        "U. Sinha",
        "D. Johnson"
      ],
      "title": "Case-based explanations of non-case-based learning methods",
      "venue": "In Proceedings of the AMIA Symposium (p. 212)",
      "year": 1999
    },
    {
      "authors": [
        "C. Castelfranchi",
        "Tan",
        "Y-H"
      ],
      "title": "Trust and deception in virtual societies (pp. 157-168)",
      "year": 2001
    },
    {
      "authors": [
        "C. Darwin"
      ],
      "title": "Letter to Henslow, May 1860",
      "venue": "In F. Darwin (Ed.), More letters of Charles Darwin,",
      "year": 1860
    },
    {
      "authors": [
        "M.M. De Graaf",
        "B.F. Malle"
      ],
      "title": "How people explain action (and Autonomous Intelligent Systems should too)",
      "venue": "In AAAI Fall Symposium on Artificial Intelligence for Human-Robot Interaction (pp",
      "year": 2017
    },
    {
      "authors": [
        "H.W. de Regt",
        "D. Dieks"
      ],
      "title": "A contextual approach to scientific understanding",
      "year": 2005
    },
    {
      "authors": [
        "H.W. de Regt",
        "S. Leonelli",
        "K. Eigner"
      ],
      "title": "Scientific understanding: Philosophical perspectives",
      "year": 2009
    },
    {
      "authors": [
        "F. Doshi-Velez",
        "B. Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608",
      "year": 2017
    },
    {
      "authors": [
        "U. Ehsan",
        "B. Harrison",
        "L. Chan",
        "M.O. Riedl"
      ],
      "title": "Rationalization: A neural machine translation approach to generating natural language explanations",
      "venue": "In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 81-87)",
      "year": 2018
    },
    {
      "authors": [
        "C.Z. Elgin"
      ],
      "title": "True enough",
      "venue": "Philosophical Issues, 14, 113-131",
      "year": 2004
    },
    {
      "authors": [
        "C.Z. Elgin"
      ],
      "title": "Understanding and the facts",
      "venue": "Philosophical Studies,132, 33\u201342.",
      "year": 2007
    },
    {
      "authors": [
        "C.Z. Elgin"
      ],
      "title": "Exemplification, idealization, and scientific understanding",
      "venue": "M. Su\u00e1rez (Ed.), Fictions in science: Philosophical essays on modelling and idealization (pp. 7790). London: Routledge.",
      "year": 2008
    },
    {
      "authors": [
        "C.Z. Elgin"
      ],
      "title": "True enough",
      "venue": "Cambridge: MIT Press.",
      "year": 2017
    },
    {
      "authors": [
        "Falcone R",
        "C. Castelfranchi"
      ],
      "title": "Social trust: A cognitive approach",
      "year": 2001
    },
    {
      "authors": [
        "A.A. Freitas"
      ],
      "title": "Comprehensible classification models: a position paper",
      "venue": "ACM SIGKDD explorations newsletter,",
      "year": 2014
    },
    {
      "authors": [
        "J. F\u00fcrnkranz",
        "T. Kliegr",
        "H. Paulheim"
      ],
      "title": "On cognitive preferences and the plausibility of rule-based models",
      "venue": "arXiv preprint arXiv:1803.01316",
      "year": 2018
    },
    {
      "authors": [
        "L.H. Gilpin",
        "D. Bau",
        "B.Z. Yuan",
        "A. Bajwa",
        "M. Specter",
        "L. Kagal"
      ],
      "title": "Explaining explanation. An overview of interpretability of machine learning. arXiv preprint arXiv:1806.00069v3",
      "year": 2019
    },
    {
      "authors": [
        "J. Greco"
      ],
      "title": "Achieving knowledge",
      "venue": "Cambridge: Cambridge University Press.",
      "year": 2010
    },
    {
      "authors": [
        "J. Greco"
      ],
      "title": "Intellectual virtues and their place in philosophy",
      "venue": "C. J\u00e4ger & W. L\u00f6ffler (Eds.), Epistemology: Contexts, values, disagreement: Proceedings of the 34th International Wittgenstein Symposium (pp. 117-130). Heusenstamm: Ontos.",
      "year": 2012
    },
    {
      "authors": [
        "S.R. Grimm"
      ],
      "title": "Is understanding a species of knowledge",
      "venue": "British Journal for the Philosophy of Science,",
      "year": 2006
    },
    {
      "authors": [
        "S.R. Grimm"
      ],
      "title": "Understanding",
      "venue": "S. Bernecker & D. Pritchard (Eds.), The Routledge companion to epistemology (pp. 84-94). New York: Routledge.",
      "year": 2011
    },
    {
      "authors": [
        "S.R. Grimm"
      ],
      "title": "Understanding as knowledge of causes",
      "venue": "A. Fairweather (Ed.), Virtue epistemology naturalized: Bridges between virtue epistemology and philosophy of science. Dordrecht: Springer.",
      "year": 2014
    },
    {
      "authors": [
        "S.R. Grimm"
      ],
      "title": "Making sense of the world: New essays on the philosophy of understanding",
      "year": 2018
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "F. Turini",
        "F. Giannotti",
        "D. Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM Computing Surveys (CSUR),",
      "year": 2018
    },
    {
      "authors": [
        "C.G. Hempel"
      ],
      "title": "Aspects of scientific explanation",
      "venue": "New York: The Free Press.",
      "year": 1965
    },
    {
      "authors": [
        "J. Huysmans",
        "K. Dejaeger",
        "C. Mues",
        "J. Vanthienen",
        "B. Baesens"
      ],
      "title": "An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models",
      "venue": "Decision Support Systems,",
      "year": 2011
    },
    {
      "authors": [
        "D. Kelemen"
      ],
      "title": "Functions, goals, and intentions: Children\u2019s teleological reasoning about objects",
      "venue": "Trends in Cognitive Science, 12, 461-468.",
      "year": 1999
    },
    {
      "authors": [
        "K. Khalifa"
      ],
      "title": "Inaugurating understanding or repackaging explanation",
      "venue": "Philosophy of Science,",
      "year": 2012
    },
    {
      "authors": [
        "B. Kim"
      ],
      "title": "Interactive and interpretable machine learning models for human machine collaboration",
      "venue": "PhD thesis, Massachusetts Institute of Technology.",
      "year": 2015
    },
    {
      "authors": [
        "T. Kliegr",
        "\u0160. Bahn\u00edk",
        "J. F\u00fcrnkranz"
      ],
      "title": "A review of possible effects of cognitive biases on interpretation of rule-based machine learning models. arXiv preprint arXiv:1804.02969",
      "year": 2018
    },
    {
      "authors": [
        "S. Krening",
        "B. Harrison",
        "K. Feigh",
        "C. Isbell",
        "M. Riedl",
        "A. Thomaz"
      ],
      "title": "Learning from explanations using sentiment and advice in RL",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems,",
      "year": 2016
    },
    {
      "authors": [
        "J. Kvanvig"
      ],
      "title": "The value of knowledge and the pursuit of understanding",
      "venue": "New York: Cambridge University Press.",
      "year": 2003
    },
    {
      "authors": [
        "J. Kvanvig"
      ],
      "title": "Response to critics",
      "venue": "A. Haddock, A. Millar, & D. Pritchard (Eds.), Epistemic value (pp. 339\u2013351). New York: Oxford University Press.",
      "year": 2009
    },
    {
      "authors": [
        "I. Lage",
        "E. Chen",
        "J. He",
        "M. Narayanan",
        "B. Kim",
        "S. Gershman",
        "F. Doshi-Velez"
      ],
      "title": "An Evaluation of the Human-Interpretability of Explanation",
      "venue": "arXiv preprint arXiv:1902.00006",
      "year": 2019
    },
    {
      "authors": [
        "S. Lapuschkin",
        "S. W\u00e4ldchen",
        "A. Binder",
        "G. Montavon",
        "W. Samek",
        "K.R. M\u00fcller"
      ],
      "title": "Unmasking Clever Hans predictors and assessing what machines really learn",
      "venue": "Nature communications,",
      "year": 2019
    },
    {
      "authors": [
        "B. Lepri",
        "N. Oliver",
        "E. Letouz\u00e9",
        "A. Pentland",
        "P. Vinck"
      ],
      "title": "Fair, transparent, and accountable algorithmic decision-making processes: The premise, the proposed solutions, and the open challenges",
      "venue": "Philosophy & Technology,",
      "year": 2017
    },
    {
      "authors": [
        "D.K. Lewis"
      ],
      "title": "Causal explanation",
      "venue": "Philosophical papers, vol. II (pp. 214-240). New York: Oxford University Press.",
      "year": 1986
    },
    {
      "authors": [
        "P. Lipton"
      ],
      "title": "Understanding without explanation",
      "venue": "H. W. de Regt, S. Leonelli, & K. Eigner (Eds.), Scientific understanding: Philosophical perspectives (pp. 43-63). Pittsburgh: University of Pittsburgh Press.",
      "year": 2009
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "arXiv preprint arXiv:1606.03490.",
      "year": 2016
    },
    {
      "authors": [
        "T. Lombrozo",
        "N.Z. Gwynne"
      ],
      "title": "Explanation and inference: Mechanistic",
      "year": 2014
    },
    {
      "authors": [
        "T. Miller",
        "P. Howe",
        "L. Sonenberg"
      ],
      "title": "Explainable AI: Beware of inmates",
      "venue": "Artificial Intelligence,",
      "year": 2017
    },
    {
      "authors": [
        "C. Russell",
        "S. Wachter"
      ],
      "title": "Explaining explanations in AI",
      "year": 2019
    },
    {
      "authors": [
        "A. 237-252. P\u00e1ez"
      ],
      "title": "Explanations in K. An analysis of explanation as a belief revision",
      "year": 2006
    },
    {
      "authors": [],
      "title": "Artificial explanations: The epistemological interpretation of explanation",
      "year": 2009
    },
    {
      "authors": [
        "M. Pazzani"
      ],
      "title": "Knowledge discovery from data",
      "venue": "AI. Synthese,",
      "year": 2000
    },
    {
      "authors": [
        "R. Piltaver",
        "M. Lu\u0161trek",
        "M. Gams",
        "S. Martin\u010di\u0107-Ip\u0161i\u0107"
      ],
      "title": "What makes classification",
      "year": 2016
    },
    {
      "authors": [
        "D. Pritchard"
      ],
      "title": "Knowing the answer, Understanding and epistemic value",
      "venue": "Grazer Philosophische Studien, 77, 325-339.",
      "year": 2008
    },
    {
      "authors": [
        "D. Pritchard"
      ],
      "title": "Knowledge and understanding",
      "venue": "A. Fairweather (Ed.), Virtue scientia: Bridges between virtue epistemology and philosophy of science (pp. 315-328). Dordrecht: Springer.",
      "year": 2014
    },
    {
      "authors": [
        "J. Quinonero-Candela",
        "M. Sugiyama",
        "A. Schwaighofer",
        "N.D. Lawrence"
      ],
      "title": "Dataset shift in machine learning",
      "year": 2009
    },
    {
      "authors": [
        "J. Reiss"
      ],
      "title": "The explanation paradox",
      "venue": "Journal of Economic Methodology, 19, 43-62.",
      "year": 2012
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should I trust you?\u201d: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144)",
      "year": 2016
    },
    {
      "authors": [
        "W.C. Salmon"
      ],
      "title": "Statistical explanation",
      "venue": "W. C. Salmon (Ed.), Statistical explanation and statistical relevance. Pittsburgh: Pittsburgh University Press.",
      "year": 1971
    },
    {
      "authors": [
        "W.C. Salmon"
      ],
      "title": "Scientific explanation and the causal structure of the world",
      "venue": "Princeton: Princeton University Press.",
      "year": 1984
    },
    {
      "authors": [
        "W. Samek",
        "T. Wiegand",
        "K.R. M\u00fcller"
      ],
      "title": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. arXiv preprint arXiv:1708.08296",
      "year": 2017
    },
    {
      "authors": [
        "M. Strevens"
      ],
      "title": "No understanding without explanation",
      "venue": "Studies in the History and Philosophy of Science, 44, 510-515.",
      "year": 2013
    },
    {
      "authors": [
        "D. Wilkenfeld"
      ],
      "title": "Understanding as representation manipulability",
      "venue": "Synthese, 190, 997\u2013 1016.",
      "year": 2013
    },
    {
      "authors": [
        "J. Woodward"
      ],
      "title": "Making things happen",
      "venue": "A theory of causal explanation. New York: Oxford University Press.",
      "year": 2003
    },
    {
      "authors": [
        "L. Zagzebski"
      ],
      "title": "On epistemology",
      "venue": "Belmont: Wadsworth.",
      "year": 2009
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "European Conference on Computer Vision ECCV",
      "year": 2014
    }
  ],
  "sections": [
    {
      "heading": "1. Introduction",
      "text": "The main goal of Explainable Artificial Intelligence (XAI) has been variously described as a search for explainability, transparency and interpretability, for ways of validating the decision process of an opaque AI system and generating trust in the model and its predictive performance.1 All of these goals remain underspecified in the literature and there are numerous proposals about which attributes make models interpretable. Instead of analyzing\n1 For a survey of recent characterizations of the goals of XAI, see Lipton (2016), Doshi-Velez & Kim (2017), Samek et al. (2017), and Gilpin et al. (2019).\nexplainable, interpretable, trustworthy models and decisions2 in AI must be reformulated in terms of the broader project of offering an account of understanding in AI. Intuitively, the purpose of providing an explanation or an interpretation of a model or a decision is to make it understandable or comprehensible to its stakeholders. But without a previous grasp of what it means to say that a human agent understands a decision or a model, the explanatory or interpretative strategies will lack a well-defined theoretical and practical goal. This paper provides a characterization of the theoretical goal of XAI by offering an analysis of human understanding in the context of machine learning in general, and of black box models in particular.\nIn recent years, there has been an increased interest in the notion of understanding\namong epistemologists (Pritchard 2014, Grimm 2018) and philosophers of science (de Regt et al. 2009). The interest in this notion has several sources. In epistemology, several authors realized that the conceptual analysis of understanding differs significantly from the traditional analysis of knowledge. In particular, unlike knowledge, understanding need not be factive: not all the information on the basis of which a phenomenon is understood must be true. Understanding is also an epistemic achievement that some authors regard as more valuable than mere knowledge. It also seems to be immune to Gettier cases, it is transparent to the epistemic agent, and it has internalist conditions of success. In sum, understanding and knowledge seem to be entirely different concepts and it is implausible to conceive the former simply as a species of the latter.3\nIn the philosophy of science, the first philosophers of explanation (Hempel 1965;\nSalmon 1984) regarded the understanding provided by a scientific explanation as a pragmatic and psychological by-product that falls beyond the ken of a proper philosophical theory. In their view, once we have developed an adequate account of explanation, any remaining\n2 I will use decision as the general term to encompass outputs from AI models, such as predictions, categorizations, action selection, etc. 3 Needless to say, each of these differences has been the subject of great philosophical controversy. I am simply reporting some of the reasons that have been stated in the literature to motivate the analysis of understanding as an independent concept.\nperspective. A recent interest in the role of models, simulations, and idealizations in science, and a closer examination of actual scientific practice, has revealed that scientific understanding can be achieved without the use of traditionally-defined scientific explanations, and that the simple possession of explanatory knowledge is often not sufficient for the working scientist\u2019s understanding of a phenomenon. Scientific understanding thus seems to be a topic worth investigating in its own right.\nThere are many aspects of this literature that are germane to XAI. Here I will only\nfocus on two main issues. The first one regards the relationship between explanation and understanding in the context of opaque machine learning models. While many authors defend the idea that there is no understanding without explanation, the impossibility of finding explanations, in the traditional sense of the term, for black box machine learning models should lead us to question the inseparability of these two concepts in the context of AI. The literature suggests alternative paths to achieve understanding, and it is worth investigating how these paths can be fruitfully adapted to understand opaque models and decisions.\nThe second issue regards the nature of understanding itself. Are understanding the\ndecision of a model and understanding the model that produced that decision two states that demand different accounts or can they be reduced to the same underlying cognitive processes and abilities? I will argue that post-hoc interpretability and model transparency correspond to different levels of the same type of understanding. There is, however, a different kind of understanding that stems from the functional or instrumental analysis of machine learning models. I will argue that functional understanding falls short in many respects of the stated goals of XAI.\nIt should be noted that the notion of opacity in machine learning is itself in need of\nfurther specification. There are many types of machine learning models that are purposely designed as black boxes (e.g. deep neural networks and Support Vector Machines). Other methods, such as rule lists, linear regressions, simple na\u00efve Bayes classifiers, and decision trees are often interpretable, but not always. \u201cSufficiently high dimensional models, unwieldy rule lists, and deep decision trees could all be considered less transparent than comparatively compact neural networks\u201d (Lipton 2016, p. 5). Other relatively simple models will be opaque\nsimplify the object of analysis, in this paper I will only focus on the extreme case of models that are unambiguously designed as black boxes. Most of the results of this analysis can then be naturally extended to models and methods that are opaque only in certain cases or to specific stakeholders.\nFinally, given the variety of purposes of black box machine learning models, and the\ndifferences in background knowledge and interests of their stakeholders, there is no reason to believe that a single interpretative strategy will be equally successful in all cases. Designing interpretative models and tools will inevitably require taking into account the psychological and pragmatic aspects of explanation. The study of the cognitive aspects of interpretative models is in its infancy. It follows from the general outlook that I present in this paper that this area of research should receive more attention in the coming years.\nThe essay is organized as follows. The next section examines the question of whether\nthere are independent theoretical reasons to appeal to the notion of understanding in XAI or whether it will be sufficient, as the traditionalists claim, to develop the best possible account of explanation in AI and let understanding naturally emerge from it. I will argue that the connection between explanation and understanding in AI is not comparable to that same connection in the natural and social sciences. The disconnection arises from the impossibility, in most cases, to offer an explanation that fulfills the factivity condition. This will lead, in section 3, to a discussion about alternative paths to understanding that are not based on traditional explanations. I show how these alternative paths are exemplified in some recent attempts to find adequate methods and devices to understand opaque models and decisions. In section 4, I analyze the types of understanding that emerge from these different avenues to understanding. This will require introducing a distinction between understanding-why, which prima facie is the type of understanding involved in post-hoc interpretability, and objectual understanding, which requires grasping the inner workings of a complex system such as an AI model. This section also addresses the functional understanding of AI systems. Using evidence from psychology, it will be possible to offer a nuanced analysis of the interconnections between these three possible ways of characterizing understanding in AI."
    },
    {
      "heading": "2. Why not settle for AI-explanations?",
      "text": "A great number of philosophers of science have argued that understanding is inextricably linked to explanation. For Salmon, a defender of the ontic conception of explanation, \u201cunderstanding results from our ability to fashion scientific explanations\u201d (1984, p. 259). In more recent times, Strevens has staunchly defended the idea that \u201cexplanation is essentially involved in scientific understanding\u201d (2013, p. 510). Perhaps the strongest claim in this direction is made by Khalifa, who defends the reductionist thesis that \u201cany philosophically relevant ideas about scientific understanding can be captured by philosophical ideas about the epistemology of scientific explanation without loss\u201d (2012, p. 17). In the context of XAI, this thesis4 implies that understanding an AI model or decision is simply a question of finding an adequate explanation for it. But the implication holds only if scientific explanations and AI-explanations share a sufficient number of essential characteristics to be considered two species of the same genus. If they are, our task will be to find in AI-explanations the same features that enable scientific explanations to generate understanding. However, in this section I will argue that explanations in the present stage of AI are incommensurable with the types of explanations discussed in the philosophy of science.\nMy first task will be to clarify what I mean by an AI-explanation. The notion of\nexplanation in what is often referred to as \u201cGood Old-Fashioned AI\u201d (GOFAI), that is, in symbolic, logic-based AI models, differs significantly from the present task of explaining opaque machine learning models. The function of an explanation in a logic-based system, either monotonic or nonmonotonic, is to support the addition of an input to a belief set or a database. For example, an update request \u201cinsert (\u03c6)\u201d can be achieved by finding some formula consistent with the database such that the union of the set of ground facts in the database and the formula yields \u03c6 as a logical consequence. In previous work (P\u00e1ez 2009) I argued that this abductive task is at odds with the way in which explanation has historically been understood in the philosophy of science. I refer the reader to the paper for the relevant\n4 Here I will not evaluate the merits of this thesis in the philosophy of science. For a discussion, see the collection edited by De Regt, Leonelli and Eigner (2009).\nas it is being used in the field of computational intelligence in recent times.\nAn important difference between explanation in logic-based models and in current\nmachine learning models is that the explanandum is entirely different. In the former, as just mentioned, the goal is to justify an input. In the latter, it is to explain an output, generally a decision, or to provide explanatory information about the workings of the model that generated that output. The explanandum of an AI-explanation as it is currently conceived is thus similar to the outcome of a scientific experiment, or to the structure of a physical or social system. A natural scientist and the stakeholder of a machine learning model would thus be searching for explanations for similar objects. But that is as far as the similarities go. In what follows I will present three fundamental reasons why it is misguided to make our understanding of machine learning models dependent on establishing an account of AIexplanations, even if we were to accept the claim that scientific understanding depends on devising bona fide scientific explanations.\nThe first reason has to do with truth. An essential feature of explanations in science\nis their factivity (Hempel 1965), i.e., both the explanans and the explanandum must be true.5 If one denies the factivity of explanation, the claim goes, one cannot avoid the conclusion that the Ptolemaic theory, the phlogiston theory, or the caloric theory, provided bona fide scientific explanations. An explanation of x must reveal, depending on which theory of explanation one adopts, either the true causal structure of x or the natural laws that determine x or its relationship with factors that make x more or less probable.6 All objectivist theories of explanation assume that researchers have epistemic access either to the inner workings of x or to the complete7 causal or probabilistic context that determines the properties of the\n5 More precisely, the explanans-statement and the explanandum-statement must be true. If one holds, following Lewis (1986) and Woodward (2003), that the relata of the explanation relation are particulars, i.e., things or events, the claim amounts to saying that the things or events occurring in both the explanans and the explanandum position exist or occur. 6 This list is not meant to be exhaustive and it excludes pragmatic theories of explanation such as the ones defended by Achinstein (1983) and van Fraassen (1980). I have argued elsewhere (P\u00e1ez 2006) that these theories offer an account of explanation that lacks any sort of objectivity. 7 Salmon\u2019s (1971) reference class rule, for example, requires the probabilistic (causal) context of a single event to be complete to avoid any epistemic relativity.\nexplanatory information about x.\nThis kind of epistemic access is blocked in the case of opaque AI models. A general\nknowledge of the structure of a deep neural network will be insufficient to explain, in this traditional sense, either a specific decision or the actual computation that was made to generate it. Many types of black box models, like deep neural networks, are stochastic (nondeterministic). Randomness is introduced in data selection, training, and evaluation to help the learning algorithm be more robust and accurate.8 Examining the training set and all the weights, biases and structure of the network will not allow us to understand its specific decisions, and its predictive failures and successes cannot be traced back to particular causal paths in its hidden layers. To be sure, it is possible to give a true explanation of the general design and purpose of a black box model, but such an explanation will not be sufficient to explain specific decisions or to generate trust in the model.\nOne of the main virtues of replacing explanation by understanding as the focus of\nanalysis in XAI is that the factivity condition need not be satisfied. According to so-called moderate factivists (Kvanvig 2009b; Mizrahi 2012; Carter & Gordon 2016), not all the information on the basis of which something is understood must be true, only the central propositions. Other philosophers go even further and reject the factivity condition altogether.9 Elgin\u2019s (2007) discussion of the role of models and idealizations allows that our understanding of some aspects of reality may be literally false. Far from being an unfortunate expedient, idealizations and models are an essential and ineliminable component of our scientific understanding of the world; she calls them \u201cfelicitous falsehoods\u201d (2004, p. 116). In section 3 I will explore Elgin\u2019s idea in the context of our understanding of opaque models. I will argue that although the methods and artifacts used to understand an intelligent system and its decisions are not, and perhaps cannot be, entirely faithful to the model, this does not tell against them. On the contrary, they can afford indirect epistemic access to matters of fact that are otherwise humanly impossible to discern.\n8 I am grateful to an anonymous reviewer for pointing this out. 9 The relaxation of the factivity condition is often defended in the context of objectual understanding, but it remains controversial in the case of understanding why. I return to this distinction in section 4.\nimportance of taking into account the specific context, background knowledge, and interests of end-users and stakeholders of opaque models.10 In any field it is possible to establish a distinction between different levels of expertise and different levels of understanding depending on the depth of a person\u2019s knowledge of a phenomenon. In the sciences, it is expected that the novice will become an expert by acquiring the required knowledge and skills. More importantly, scientific experts will be able to master the best possible explanations of the phenomena within their field of study. This situation is not replicated in the case of machine learning. The medical doctor or the parole officer who makes use of a black box model is not supposed to acquire the level of expertise of a computer scientist, and their respective level of understanding of any explanatory model of the opaque system will remain incomparable. This seems to be an element that has not always been kept in mind in XAI. Many AI researchers build explanatory models for themselves, rather than for the intended users, a phenomenon that Miller et al. (2017) refer to as \u201cthe inmates running the asylum\u201d (p. 36). The alternative they propose, and which I fully endorse, is to incorporate results from psychology and philosophy to XAI.11 It is necessary to explore a naturalistic approach to the way in which context and background knowledge mold an agent\u2019s understanding of an interpretative model. Existing theories of how people formulate questions and how they select and evaluate answers should also inform the discussion (Miller 2019).\nA third advantage of focusing on the pragmatic elements of interpretative models is\nthat we can obtain a better grasp of the relationship between explanation and trust. When using machine learning in high-stakes contexts such as medical diagnosis or parole decisions it is necessary to trust the individual decisions generated by the model. Several authors have argued that post-hoc interpretability, i.e., an explanation of the decision, is a necessary condition for trust (Kim 2015, Ribeiro et al. 2016). Additionally, of course, the system must\n10 De Graaf and Malle (2017) have also emphasized the importance of these pragmatic factors: \u201cThe success of an explanation therefore depends on several critical audience factors\u2014assumptions, knowledge, and interests that an audience has when decoding the explanation\u201d (p. 19). 11 See also De Graaf & Malle (2017), Miller (2019), and Mittelstadt et al. (2019).\nthat an opaque model has consistently shown a high degree of predictive accuracy and a user has been given a clear post-hoc explanation of its behavior. The user has the best possible understanding of the system, taking into account, of course, the epistemic limitations mentioned above. But predictive reliability and a post-hoc explanation are not sufficient to generate trust. Trust does not depend exclusively on epistemic factors; it also depends on the interests, goals, resources, and degree of risk aversion of the stakeholders. Trust involves a decision to accept an output and act upon it. Different agents bound by different contextual factors will make different decisions on the basis of the same information. I will leave open the question of whether classical decision theory can provide an adequate analysis of trust in AI systems.12 But the important lesson to draw from the multidimensional character of trust is that there is no simple correlation between explanation and trust, and that an adequate analysis of trust requires taking into account contextual factors that can foster or hinder it.\nThe reasons I have presented in this section recommend abandoning the traditional\n\u201cexplanationist\u201d path according to which understanding can only be obtained via an explanation in any of the guises it has adopted in the philosophy of science. The next section will offer alternative ways to achieve understanding."
    },
    {
      "heading": "3. Alternative Paths to Understanding",
      "text": "Abandoning the necessary connection between explanation and understanding opens up several avenues of research that can lead to understanding the workings and decisions of opaque models. Implicit causal knowledge, analogical reasoning, and exemplars are obvious alternative paths to understanding. But so are models, idealizations, simulations, and thought experiments, which play important roles in scientific understanding despite being literally false representations of their objects. In a similar vein, the methods and devices used to make black box models understandable need not be propositionally-stated explanations and they need not be truthful representations of the models. I will begin by presenting a few examples\n12 See Falcone & Castelfranchi (2001) for a critique of the use of decision theory to understand trust in virtual environments.\nbefore moving to a discussion of how similar devices can be used, and have been used, in understanding AI models.\nMany philosophers, beginning with Aristotle and continuing with the defenders of\ncausal explanations, have argued that understanding-why is simply knowledge of causes (Salmon 1984; Lewis 1986; Greco 2010; Grimm 2006, 2014).13 Naturally, causal explanations are the prime providers of knowledge of causes. But causal knowledge does not come exclusively from explanation. As Lipton points out, \u201cmuch of empirical inquiry consists in activities\u2013physical and intellectual\u2013that generate causal information, activities such as observation, experimentation, manipulation, and inference. And these activities are distinct from the activity of giving and receiving explanations\u201d (2009, p. 44). To be sure, the causal information generated by these activities can be given a propositional representation and can thus be transformed into explicit causal explanations. But Lipton argues that in many cases such activities generate causal information that remains as tacit knowledge, allowing us to perform epistemic and practical tasks. Such tacit causal knowledge comes primarily from images and physical models. An orrery or a video, for example, can provide better understanding of retrograde planetary motion than an explanation stated in propositional form. A subject might even be able to understand retrograde motion without being able to articulate such an explanation.\nDirect manipulation or tinkering of a causal system is an even more obvious source\nof implicit causal knowledge. Adjusting a lever, a button or an input variable and observing its effects on other parts of a system is a way of beginning to understand how the system works. Manipulation also provides modal information about the possible states of a system. In fact, the ability to manipulate a system into new desired states should be seen as a sign of understanding. In other words, understanding requires the ability to think counterfactually (de Regt & Dieks 2005; Wilkenfeld 2013).\n13 Philosophers of science are much more inclined to accept this view than epistemologists, who have fiercely resisted it. See, for example, Zagzebski (2001), Kvanvig (2003), Elgin (2004), and Pritchard (2014). I do not have space to discuss the issue here, but from the text it should be clear that I side with the epistemologists.\nConsider analogical reasoning. Darwin (1860/1903) used an analogy between the domestic selection of animals and natural selection to argue for the latter. Although it is incomparable in many respects, artificial selection illuminates how the mechanism would work in a larger class (Lipton 2009, p. 51). Exemplification is another important avenue towards understanding. The examples in a logic textbook can show a student how the rules of natural deduction work. Her initial understanding of the rules will be tied to the examples, but it will gradually drift away as her ability to use the rules in new situations improves. When an item serves as an example, \u201cit functions as a symbol that makes reference to some of the properties, patterns, or relations it instantiates\u201d (Elgin 2017, p. 184). It can only display some of these features, downplaying or ignoring others. As the complexity of the item increases, the decision to emphasize or underscore some salient features over others will be determined by pragmatic reasons, such as the intended audience and use of the example.\nThe use of non-propositional representations such as diagrams, graphs, and maps\npresent another clear case of understanding without explanation. A subway map is never a faithful representation of the real train network. It alters the distance between stations and the exact location of the tunnels in order to make the network easy to understand, but it must include the correct number of lines, stations and intersections to be useful at all. It must be sufficiently accurate without being too accurate.\nFinally, models and idealizations play a similar role in science (Potochnik 2017).\nThey simplify complex phenomena and sometimes the same phenomenon is represented by multiple, seemingly incongruous models. They afford epistemic access to features of the object that are otherwise difficult or impossible to discern. Models are not supposed to accurately represent the facts, but they must be objective. Models have to denote in some sense the facts they model. They \u201care representations of the things that they denote\u201d (Elgin 2008, p. 77). The general relation between scientific models and their objects is a thorny issue that deserves a more detailed discussion than the one I can provide here, but one important aspect that must be noted is that the adequate level of \u201cfit\u201d between a model and its object is a pragmatic question. Many models are, in Elgin\u2019s apt phrase, \u201ctrue enough\u201d of the phenomenon they denote:\nThis may be because the models are approximately true, or because they diverge from truth in irrelevant respects, or because the range of cases for which they are not true is a range of cases we do not care about, as for example when the model is inaccurate at the limit. Where a model is true enough, we do not go wrong if we think of the phenomena as displaying the features exemplified in the model. Obviously, whether such a representation is true enough is a contextual question. A representation that is true enough for some purposes, or in some respects is not true enough for or in others (2008, p. 85).\nApplications of all of the approaches mentioned above can be found in the XAI\nliterature. It is important to bear in mind that many authors in the field refer to these alternative paths to understanding as \u201cexplanations,\u201d a usage that threatens to trivialize the term. If whatever makes an opaque model or its decisions better understood is called an explanation, the term ceases to have any definitive meaning. My argument throughout the paper has only focused on the notion of explanation as it has been traditionally understood in the philosophy of science and epistemology (e.g., causal models, covering-law models, probabilistic approaches, etc.). It is in this sense that there are alternative sources of understanding.\nIt is customary to distinguish between two different goals in XAI: understanding a\ndecision, often called post-hoc interpretability, and understanding how the model functions, i.e., making the model transparent (Lipton 2016; Lepri et al. 2017; Mittelstadt et al. 2019). Exemplifications, analogies, and causal manipulation are often used in the former, while the use of models is more common in the latter. I will present some examples of the use of these techniques, and in the next section I will examine the kind of understanding they provide. The ultimate question I will try to answer is whether transparency and post-hoc interpretability aim at different types of understanding.\nThe attempts to make a model transparent can focus on the model as a whole\n(simulatability), on its parameters (decomposability), or on its algorithms (algorithmic transparency) (Lipton 2016). A complete understanding of the model would thus allow a user to repeat (simulate) the computation process with a full understanding of the algorithm and\nchallenges, but my interest here is in the use of interpretative devices to provide an overall understanding of opaque models, i.e., models that are not designed to be fully understood. The most common way to make a black-box model as understandable as possible is through the use of proxy or interpretative models (Guidotti et al. 2018). Many of these models provide coarse approximations of how the system behaves over a restricted domain. The two most widely used classes of models are linear or gradient-based approximations, and decision trees (Mittelstadt et al. 2019). For the interpretative model to be useful, a user must know \u201cover which domain a model is reliable and accurate, where it breaks down, and where its behavior is uncertain. If the recipient of a local approximation does not understand its limitations, at best it is not comprehensible, and at worst misleading\u201d (Mittelstadt et al. 2019, p. 281). Oversimplified or misleading models also incur the risk of being perceived as deceitful, thereby undermining the user\u2019s trust in the original model. Thus, the first desideratum of interpretative models is that they must be as faithful to the original model as possible and absolutely transparent about their limitations.\nMittelstadt et al. (2019) argue that XAI should not focus on developing interpretative\nmodels because they are akin to scientific models, and therefore very different from \u201cthe types of scientific and \u2018everyday\u2019 explanations considered in philosophy, cognitive science, and psychology\u201d (p. 279). My view is exactly the opposite. Since the notion of explanation discussed in the philosophy of science is inapplicable in the context of opaque machine learning models, and since I do not want to settle for a purely subjective sense of explanation, XAI should adopt any other methods and devices that provide objective understanding. Scientific models, suitably adapted to the intended users, offer an indirect14 path towards an objective understanding of a phenomenon. We should therefore see the parallel between scientific models and interpretative models in a positive light.\nThe fidelity desideratum for interpretative models has to be balanced against the\ndesideratum of comprehensibility. There are very few empirical studies about which kinds\n14 A direct understanding of a phenomenon would be factive, based on a literal description of the explanatory elements involved. It is in this sense that models offer an indirect path towards objective understanding.\npresent evidence that single-hit decision tables perform better than binary decision trees, propositional rules, and oblique rules in terms of accuracy, response time, and answer confidence for a set of problem-solving tasks involving credit scoring. This study is of limited use because it was done with extremely simple representation formats and the only participants were 51 graduate business students.15 It is necessary to undertake similar studies that also include linear regressions, simple na\u00efve Bayes classifiers, and random forests.16 These interpretative models also have to be tested on a more diverse population with different levels of expertise (Doshi-Velez & Kim 2017). These types of empirical studies are essential for the purposes of XAI, and they have to be complemented with psychological studies of the formal and contextual factors that enhance understanding. As noted by Pazzani (2000), there is little understanding of the factors that foster or hinder interpretability in these cases, and of whether users prefer, for example, visualizations over textual representations.\nThe appropriateness of an interpretative model thus depends on three factors:\nobtaining the right fit between the interpretative model and the black box model in terms of accuracy and reliability, providing sufficient information about its limitations, and achieving an acceptable degree of comprehensibility for the intended user. While there may be some identifiable, permanent features of interpretative models that facilitate understanding, the choice of the best proxy method or artifact will also depend on who the intended users of the original system are. Their background knowledge, their levels of expertise, and the time\n15 In Allahyari and Lavesson (2011), 100 non-expert users were asked to compare the understandability of decision trees and rule lists. The former method was deemed more understandable. Freitas (2014) examines the pros and cons of decision trees, classification rules, decision tables, nearest neighbors, and Bayesian network classifiers with respect to their interpretability, and discusses how to improve the comprehensibility of classification models in general. More recently, F\u00fcrnkranz et al. (2018) performed an experiment with 390 participants to question the idea that the likeliness that a user will accept a logical model such as rule sets as an explanation for a decision is determined by the simplicity of the model. Lage et al. (2019) also explore the complexities of rule sets to find features that make them more interpretable, while Piltaver et al. (2016) undertake a similar analysis in the case of classification trees. Another important aspect of this empirical line of research is the study of cognitive biases in the understanding of interpretable models. Kliegr et al. (2018) study the possible effects of biases on symbolic machine learning models. 16 As noted in the Introduction, none of these methods is intrinsically interpretable.\nentirely neglected in the literature; not a single method reviewed by Guidotti et al. (2018) presents real experiments about the time required to understand an interpretative model.\nTurning very briefly to post-hoc interpretability, we find in the literature several\ninterpretative devices to understand a decision. In many cases, a sensitivity analysis provides a local, feature-specific, linear approximation of the model\u2019s response. The result of the analysis consists of a list, a table, or a graphical representation of the main features that influenced a decision and their relative importance. Often, such devices allow a certain degree of causal manipulation that brings out feature interactions. This is the basis of the LIME model proposed by Ribeiro et al. (2016), a technique to offer functional explanations of the decisions of any machine learning classifier. To understand the behavior of the underlying model, the input is perturbed to see how the decisions change without worrying about the actual computation that produced it. The user can ask counterfactual questions about local changes and see the results in an intuitive way. Saliency maps offer a similar functional understanding of the model. A network is repeatedly tested with portions of the input occluded to create a map showing which parts of the data actually have influence on the network output (Zeiler & Fergus 2014; Lapuschkin et al. 2019).\nCaruana et al. (1999) argue that analogies and exemplars (prototypes) are a useful\nheuristic device. A model can report, for every new decision, other examples in the training set that the model considers to be most similar. The authors seek to use this method in clinical contexts, where doctors often refer to case-studies to justify a course of action. The basic assumption made by case-based methods, such as k-nearest neighbor, is that similar inputs correlate with similar outputs. The methods look for the case in the training set, the prototype, that is most similar in terms of input features to the case under consideration.\nAnother commonly used method, especially in interactions with autonomous agents,\nis to provide natural language explanations of a decision (McAuley & Leskovec 2013; Krening et al. 2016). These explanations state information about the most important features in a decision and come closer than any other method to the causal explanations used in science and everyday life. The difference, once again, is that these \u201cexplanations\u201d are not factive, regardless of how plausible they appear. Eshan et al. (2018) even suggest that textual\nare times when humans may not have full conscious access to reasons for their behavior and consequently may not give explanations that literally reveal how a decision was made. In these situations, it is more likely that humans create plausible explanations on the spot when pressed. However, we accept human-generated rationalizations as providing some lay insight into the mind of the other\u201d (p. 81).\nA common feature of many post-hoc interpretations is that they are model-agnostic.\nThey do not even attempt to open the black box and they offer only a functional approach to the problem of explaining a decision. The cognitive achievement reached by the use of these devices seems to differ in great measure from the understanding provided by an interpretative model. In the last section of the paper I will tackle the question of whether it is possible to characterize different types of understanding in AI."
    },
    {
      "heading": "4. Types of Understanding in AI",
      "text": "On the basis of the methods described in the previous section, it is tempting to divide the understanding they provide into two different types. The first one would be associated with post-hoc interpretability. This type is often called understanding-why, and in this case its object will be a specific decision of a model. In contrast, transparency seems to generate an objectual understanding of a model. The distinction between these two types of understanding has been widely discussed in epistemology. The question I will examine in the beginning of this section is whether this epistemological distinction can be defended in the present context.\nEpistemologists establish a distinction between understanding why something is the\ncase, and understanding an object, a system or a body of knowledge (Kvanvig, 2003). It seems straightforward to say that the goal of transparency in machine learning can be understood in terms of objectual understanding. Consider the various ways in which this type of understanding has been described: According to Zagzebski, understanding \u201cinvolves grasping relations of parts to other parts and perhaps the relation of parts to a whole\u201d (2009, p. 144). For Grimm, the target of objectual understanding is a \u201csystem or structure \u2026 that has parts or elements that depend upon one another in various ways\u201d (2011, p. 86). And\nThe interpretative models that we considered in the previous section all provide the kind of understanding described by these authors.\nUnderstanding why p, on the other hand, is not equivalent to simply knowing why p.\nSuppose the only thing a person knows about global warming is that it is caused, to a large extent, by an increase in the concentration of greenhouse gases. This is a claim the person has heard repeatedly in serious media outlets and scientific TV shows, but he has never stopped to think about the causal mechanisms involved. The person knows why the earth is warming, but this information is insufficient to understand why it is warming. The person lacks, for example, the ability to answer a wide range of questions of the type what-if-thingshad-been-different (Woodward 2003, p. 221). What would happen to global temperatures if all human activity were to cease? What would be the effect on global warming of a massive volcanic eruption similar in scale to the eruption of Krakatoa in 1883? These are the kind of counterfactual scenarios commonly studied in climate research and modelling, which the common person is unable to understand. A complete understanding of global warming also involves the ability to make probability estimates of future scenarios based on current data.\nNotice that the ability to answer counterfactual questions and to make predictions\ndepends to a large extent on an objectual understanding of the larger body of knowledge to which the specific object of understanding belongs. Without a basic understanding of the structure, chemistry, and behavior of the earth\u2019s atmosphere, for example, a person will not be able to answer counterfactual questions or deliver probability estimates about global warming. It follows, as Grimm (2011) convincingly argues, that understanding-why is a variety of objectual understanding, but at a local level, and that there is no genuine distinction between the two types of understanding. The implication for machine learning is that understanding a decision requires some degree of objectual understanding of the model. Mittelstadt et al. (2019) seem to reach a similar conclusion: [A]t the moment, XAI generally avoids the challenges of testing and validating\napproximation models, or fully characterizing their domain. If these elements are well understood by the individual, models can offer more information than an\naccurately maps onto the phenomena we are interested in, it can be used to answer \u2018what if\u2019 questions, for example \u201cWhat would the outcome be if the data looked like this instead?\u201d and to search for contrastive explanations, for example \u201cHow could I alter the data to get outcome X?\u201d (p. 282).17\nIt is true that some of the post-hoc interpretability devices described in the previous\nsection allow stakeholders to manipulate the parameters and observe the different decisions generated thereby. But this is not genuine counterfactual reasoning. By tinkering with the parameters, the stakeholders can only form functional generalizations with a very weak inductive base. True counterfactual reasoning is purely theoretical, based on knowledge about how the model works. Thus, if we take the ability to think counterfactually about a phenomenon as a sign that the agent understands it, as suggested by de Regt and Dieks (2005), understanding the decisions of a model requires some degree of objectual understanding.\nThere is, nonetheless, an important difference between the two types of understanding\nunder consideration. Virtually all epistemologists regard understanding-why as factive, while allowing that objectual understanding might not be entirely so. Pritchard, for example, gives the following example to show that understanding-why is factive: \u201cSuppose that I believe that my house has burned down because of an act of vandalism, when it was in fact caused by faulty wiring. Do I understand why my house burned down? Clearly not\u201d (2008, p. 8). In other words, according to Pritchard, without a true causal explanation there can be no understanding-why. But changing the example can debilitate the intuitions that support this conclusion. Suppose an engineer is investigating the collapse of a bridge and uses Newtonian\n17 A terminological clarification is in order. Mittelstadt et al. (2019) and other researchers in XAI use the phrase \u201ccontrastive explanations\u201d to refer to counterfactuals. But these are two very different things. In philosophy, an explanation is contrastive if it answers the question \u201cWhy p rather than q?\u201d instead of just \u201cWhy p?\u201d In either case the explanation provided must be factual. To turn it into a counterfactual situation, the question must be changed to: \u201cWhat changes in the world would have brought about q instead of p?\u201d And the answer will be a hypothetical or counterfactual statement, not an explanation.\ntheory, but it can hardly be argued that the engineer is a priori barred from understanding why the bridge collapsed. Or suppose an economist successfully explains a sudden rise in inflation using a macroeconomic model that, again, cannot be literally true (Reiss 2012). It thus seems that the factivity of understanding-why can only be defended in simple scenarios where a complete analysis of the relevant causal variables can be provided, but as soon as the context requires the use of theoretical tools such as idealizations and models, it becomes highly doubtful.\nMachine learning is precisely this kind of context. The use of arbitrary black-box\nfunctions to make decisions in machine learning makes it impossible to reach the causal knowledge necessary to provide a true causal explanation. The functions may be extremely complex and have an internal state composed of millions of interdependent values. Machine learning is the kind of context in which one can say that, in principle, it is impossible to satisfy the factivity condition for understanding-why.\nWe thus have an argument to the effect that understanding-why and objectual\nunderstanding in machine learning cannot be entirely independent of each other, but rather, that the former is a localized variety of the latter. And we have an argument against the claim that understanding-why is always factive, which was supposed to be the most important property that distinguished both types of understanding. So even if prima facie the devices and methods used to provide transparency and post-hoc interpretability are different, it is safe to say, on the one hand, that understanding-why and objectual understanding are two different species of the same genus, and on the other, that there is no essential difference between them in terms of truth.\nThere is, however, a third way of characterizing understanding in AI. Psychologists\ndistinguish between the functional and the mechanistic understanding of an event. The former \u201crelies on an appreciation for functions, goals, and purpose\u201d while the latter \u201crelies on an appreciation of parts, processes, and proximate causal mechanisms\u201d (Lombrozo & Wilkenfeld forthcoming, p. 1). For example, an alarm clock beeps because the circuit connecting the buzzer to a power source has been completed (mechanical understanding) and because its owner has set it to wake her up at a specific time (functional understanding).\nevent while being insensitive to mechanistic information. Lombrozo and Gwynne (2014) have shown that properties that are understood functionally, as opposed to mechanistically, are more likely to be generalized on the basis of shared functions. This means that a functional, as opposed to a mechanistic understanding of the relation between an input and an output will make it easier for a user to inductively conclude that similar inputs produce similar decisions. There is also evidence that functional reasoning may be psychologically privileged in the sense that it is often favored and seems to be less cognitively demanding than mechanistic reasoning. Humans are \u201cpromiscuously teleological,\u201d to use Kelemen\u2019s (1999) apt description. Finally, Lombrozo and Wilkenfeld also argue that functional and mechanistic understanding differ with regard to normative considerations. A functional understanding of a property of an object tells us what it is supposed to do, while understanding the mechanism that causes that property lacks this normative element. Functional understanding thus seems to be a different kind of understanding altogether, compared to objectual understanding and understanding-why. It involves different content, it supports different functions, and it has a distinctive phenomenology.\nIf we take the decision of an opaque model as our object of understanding, a\nmechanistic understanding of it is equivalent to the local objectual understanding of the model, as I have argued above. Its functional understanding, on the other hand, would focus on the purpose of the model and the relation between its features and decisions. Functional reasoning about black box models allows for a more mechanism-independent form of reasoning. Aiming at this type of understanding will be appealing to those who want to offer model-neutral interpretability devices and focus only on covariations between inputs and decisions.\nHowever, it seems to me that aiming for functional understanding in XAI is, to a\ncertain extent, to give up on the project of explaining why an AI model does what it does. It is to embrace the black box and trust it as one trusts a reliable oracle without understanding its mysterious ways. Less metaphorically, reliability by itself cannot usher trust because of the dataset shift problem (Quinonero-Candela et al. 2009). To have confidence that the model is really capturing the correct patterns in the target domain, and not just patterns valid in past\nobjectual understanding of the model.18 Unfortunately, most solutions to the dataset shift problem focus only on accuracy ignoring model comprehensibility issues (Freitas 2014). Furthermore, methods designed to enhance the functional understanding of a model are also more likely to be tailored to user preferences and expectations, and thus prone to oversimplification and bias. Although the understanding and trust sought by XAI should always take into account a model\u2019s stakeholders, it should not pursue these goals by offering misleadingly simple functional explanations that can derive in unjustified or dangerous actions (Gilpin et al. 2019). Finally, a purely functional understanding of a model would also impede legal accountability and public responsibility for the decisions of the model. Guilt for an unexpected decision with harmful or detrimental consequences to the user cannot be decided if the only information available is the previous predictive accuracy of the model. It is necessary to understand why the model produced the unexpected result, that is, to have a local objectual understanding of it.\nIn sum, in this section I have argued that both transparency and post-hoc\ninterpretability should be seen as more or less encompassing varieties of objectual understanding, and that the kind of understanding provided by the functional approach to a model offers an understanding of a different and more limited kind. In my view, it is the former kind that should interest researchers in XAI."
    },
    {
      "heading": "5. Conclusion",
      "text": "In this paper I have argued that the term \u2018explanation\u2019, as it is currently used in XAI, has no definitive meaning and shares none of the properties that have been traditionally attributed to explanations in epistemology and the philosophy of science. My suggestion has been to\n18 To be sure, there are many scenarios where both the owner and the user (but not the developer) of the model will be satisfied with its accurate decisions without feeling the need to have an objectual understand of it. Think of the books recommended by Amazon or the movies suggested by Netflix using the simple rule: \u201cIf you liked x, you might like y.\u201d As I argued in section 2, the relation between understanding and trust is always mediated by the interests, goals, resources, and degree of risk aversion of stakeholders. In these cases, the cost-benefit relation makes it unnecessary to make the additional effort of looking for mechanisms.\nuncertain, to the study of the mental state that XAI researchers are aiming at, namely, an objective understanding of opaque machine learning models and their decisions. I have argued that the use of interpretative models is the best avenue available to obtain understanding, both in terms of transparency (understanding how the model works) and posthoc interpretability (understanding a decision of the model). The current approaches to the latter rely on a purely functional understanding of models; however, leaving the black box entirely untouched seems to belie the purpose of XAI. It must be admitted that interpretative models can provide false assurances of comprehensibility. The task ahead for XAI is thus to fulfill the double desiderata of finding the right fit between the interpretative and the black box model, and to design interpretative models and devices that are easily understood by the intended users. This latter task must be guided by an empirical investigation of the features of interpretative models that make them easier to understand to users with different backgrounds and levels of expertise. One of the possible areas of research is the comparative study of the complexity of rule sets, decision tables and trees, nearest neighbors, and Bayesian network classifiers with respect to their interpretability. XAI can also benefit from interdisciplinary work with designers to create user-friendly, accessible, and engaging interpretative tools and interfaces, in the same spirit as the legal design movement. Finally, an important aspect of this empirical line of research is the study of cognitive biases in the interpretation of models, especially in the context of autonomous systems with human-like interfaces."
    }
  ],
  "year": 2020
}
