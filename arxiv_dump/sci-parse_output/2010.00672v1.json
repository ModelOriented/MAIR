{"abstractText": "Sam Sattarzadeh,1 Mahesh Sudhakar,1 Anthony Lem,2 Shervin Mehryar,1 K. N. Plataniotis,1 Jongseong Jang,3 Hyunwoo Kim, 3 Yeonjeong Jeong,3 Sangmin Lee,3 Kyunghoon Bae3 1The Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto 2Division of Engineering Science, University of Toronto 3AI Division, LG Sciencepark sam.sattarzadeh, mahesh.sudhakar@mail.utoronto.ca; j.jang, eugenehw.kim@lgsp.co.kr Abstract", "authors": [{"affiliations": [], "name": "Sam Sattarzadeh"}, {"affiliations": [], "name": "Mahesh Sudhakar"}, {"affiliations": [], "name": "Anthony Lem"}, {"affiliations": [], "name": "Shervin Mehryar"}, {"affiliations": [], "name": "K. N. Plataniotis"}, {"affiliations": [], "name": "Jongseong Jang"}, {"affiliations": [], "name": "Hyunwoo Kim"}, {"affiliations": [], "name": "Yeonjeong Jeong"}, {"affiliations": [], "name": "Sangmin Lee"}, {"affiliations": [], "name": "Kyunghoon Bae"}, {"affiliations": [], "name": "Edward S. Rogers"}], "id": "SP:42c39bcc9295ff8bf08833535d791e730fff813d", "references": [{"authors": ["J. Adebayo", "J. Gilmer", "M. Muelly", "I. Goodfellow", "M. Hardt", "B. Kim"], "title": "Sanity checks for saliency maps", "venue": "Advances in Neural Information Processing Systems, 9505\u2013 9515.", "year": 2018}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.-R. M\u00fcller", "W. Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PloS one 10(7): e0130140.", "year": 2015}, {"authors": ["A. Barredo Arrieta", "N. Diaz Rodriguez", "J. Del Ser", "A. Bennetot", "S. Tabik", "A. Barbado Gonz\u00e1lez", "S. Garcia", "S. GilLopez", "D. Molina", "V.R. Benjamins", "R. Chatila", "F. Herrera"], "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges", "year": 2019}, {"authors": ["A. Chattopadhyay", "A. Sarkar", "P. Howlader", "V.N. Balasubramanian"], "title": "Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks", "venue": "arXiv preprint arXiv:1710.11063 .", "year": 2017}, {"authors": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "title": "The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results", "venue": "URL http://www.pascal-network.org/challenges/ VOC/voc2007/workshop/index.html.", "year": 2007}, {"authors": ["R. Fong", "M. Patrick", "A. Vedaldi"], "title": "Understanding deep networks via extremal perturbations and smooth masks", "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2950\u20132958.", "year": 2019}, {"authors": ["R. Fu", "Q. Hu", "X. Dong", "Y. Guo", "Y. Gao", "B. Li"], "title": "Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs", "venue": "arXiv preprint arXiv:2008.02312 .", "year": 2020}, {"authors": ["R.R. Hoffman", "S.T. Mueller", "G. Klein", "J. Litman"], "title": "Metrics for Explainable AI: Challenges and Prospects", "venue": "CoRR abs/1812.04608. URL http://arxiv.org/abs/ 1812.04608.", "year": 2018}, {"authors": ["G. Huang", "Z. Liu", "K.Q. Weinberger"], "title": "Densely Connected Convolutional Networks", "venue": "CoRR abs/1608.06993. URL http://arxiv.org/abs/1608.06993.", "year": 2016}, {"authors": ["T. Lin", "M. Maire", "S.J. Belongie", "L.D. Bourdev", "R.B. Girshick", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "title": "Microsoft COCO: Common objects in context", "venue": "arXiv 2014. arXiv preprint arXiv:1405.0312 .", "year": 2014}, {"authors": ["Z.C. Lipton"], "title": "The Mythos of Model Interpretability", "venue": "CoRR abs/1606.03490. URL http://arxiv.org/abs/1606. 03490.", "year": 2016}, {"authors": ["F. Meng", "K. Huang", "H. Li", "Q. Wu"], "title": "Class Activation Map Generation by Representative Class Selection and Multi-Layer Feature Fusion", "venue": "arXiv preprint arXiv:1901.07683 .", "year": 2019}, {"authors": ["W.-J. Nam", "S. Gur", "J. Choi", "L. Wolf", "S.-W. Lee"], "title": "Relative Attributing Propagation: Interpreting the Comparative Contributions of Individual Units in Deep Neural Networks", "venue": "AAAI, 2501\u20132508.", "year": 2020}, {"authors": ["D. Omeiza", "S. Speakman", "C. Cintas", "K. Weldermariam"], "title": "Smooth grad-cam++: An enhanced inference level visualization technique for deep convolutional neural network models", "venue": "arXiv preprint arXiv:1908.01224 .", "year": 2019}, {"authors": ["N. Otsu"], "title": "A Threshold Selection Method from GrayLevel Histograms", "venue": "IEEE Transactions on Systems, Man, and Cybernetics 9(1): 62\u201366.", "year": 1979}, {"authors": ["PAO Severstal."], "title": "Severstal: Steel Defect Detection on Kaggle Challenge", "venue": "URL https://www.kaggle.com/c/ severstal-steel-defect-detection.", "year": 2019}, {"authors": ["V. Petsiuk", "A. Das", "K. Saenko"], "title": "RISE: Randomized Input Sampling for Explanation of Black-box Models", "venue": "Proceedings of the British Machine Vision Conference (BMVC).", "year": 2018}, {"authors": ["Ramaswamy", "H. G"], "title": "Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradientfree Localization", "venue": "In The IEEE Winter Conference on Applications of Computer Vision,", "year": 2020}, {"authors": ["S.-A. Rebuffi", "R. Fong", "X. Ji", "A. Vedaldi"], "title": "There and Back Again: Revisiting Backpropagation Saliency Methods", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8839\u20138848.", "year": 2020}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier", "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016,", "year": 2016}, {"authors": ["M. Sandler", "A.G. Howard", "M. Zhu", "A. Zhmoginov", "L. Chen"], "title": "Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation", "venue": "CoRR abs/1801.04381. URL http://arxiv.org/abs/ 1801.04381.", "year": 2018}, {"authors": ["K. Schulz", "L. Sixt", "F. Tombari", "T. Landgraf"], "title": "Restricting the flow: Information bottlenecks for attribution", "venue": "arXiv preprint arXiv:2001.00396 .", "year": 2020}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra"], "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "venue": "Proceedings of the IEEE international conference on computer vision, 618\u2013626.", "year": 2017}, {"authors": ["L. Shen", "Q. Ma", "S. Li"], "title": "End-to-end time series imputation via residual short paths", "venue": "Asian Conference on Machine Learning, 248\u2013263.", "year": 2018}, {"authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "venue": "arXiv preprint arXiv:1312.6034 .", "year": 2013}, {"authors": ["D. Smilkov", "N. Thorat", "B. Kim", "F. Vi\u00e9gas", "M. Wattenberg"], "title": "Smoothgrad: Removing noise by adding noise", "venue": "arXiv 2017. arXiv preprint arXiv:1706.03825 .", "year": 2017}, {"authors": ["S. Srinivas", "F. Fleuret"], "title": "Full-gradient representation for neural network visualization", "venue": "Advances in Neural Information Processing Systems, 4126\u20134135.", "year": 2019}, {"authors": ["M. Sundararajan", "A. Taly", "Q. Yan"], "title": "Axiomatic attribution for deep networks", "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume 70, 3319\u20133328. JMLR. org.", "year": 2017}, {"authors": ["M. Tan", "Q.V. Le"], "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "venue": "CoRR abs/1905.11946. URL http://arxiv.org/abs/1905.11946.", "year": 2019}, {"authors": ["A. Veit", "M.J. Wilber", "S. Belongie"], "title": "Residual networks behave like ensembles of relatively shallow networks", "venue": "Advances in neural information processing systems, 550\u2013 558.", "year": 2016}, {"authors": ["H. Wang", "Z. Wang", "M. Du", "F. Yang", "Z. Zhang", "S. Ding", "P. Mardziel", "X. Hu"], "title": "Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks", "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 24\u201325.", "year": 2020}, {"authors": ["M.D. Zeiler", "R. Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "European conference on computer vision, 818\u2013833. Springer.", "year": 2014}, {"authors": ["J. Zhang", "S.A. Bargal", "Z. Lin", "J. Brandt", "X. Shen", "S. Sclaroff"], "title": "Top-Down Neural Attention by Excitation Backprop", "venue": "Int. J. Comput. Vision 126(10): 1084\u20131102. ISSN 0920-5691. doi:10.1007/s11263-017-1059-x. URL https: //doi.org/10.1007/s11263-017-1059-x.", "year": 2018}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Learning deep features for discriminative localization", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2921\u20132929.", "year": 2016}, {"authors": ["B. Zoph", "Q.V. Le"], "title": "Neural architecture search with reinforcement learning", "venue": "arXiv preprint arXiv:1611.01578 .", "year": 2016}, {"authors": ["Adebayo"], "title": "2018), sanity checks on explanation methods can be conducted either by randomizing the model\u2019s parameters or retraining the model with the same training data, but with random labels", "year": 2018}], "sections": [{"text": "Explaining Convolutional Neural Networks through Attribution-Based Input Sampling and Block-Wise Feature Aggregation\nSam Sattarzadeh,1 Mahesh Sudhakar,1 Anthony Lem,2 Shervin Mehryar,1 K. N. Plataniotis,1 Jongseong Jang,3 Hyunwoo Kim, 3\nYeonjeong Jeong,3 Sangmin Lee,3 Kyunghoon Bae3 1The Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto\n2Division of Engineering Science, University of Toronto 3AI Division, LG Sciencepark\nsam.sattarzadeh, mahesh.sudhakar@mail.utoronto.ca; j.jang, eugenehw.kim@lgsp.co.kr"}, {"heading": "Abstract", "text": "As an emerging field in Machine Learning, Explainable AI (XAI) has been offering remarkable performance in interpreting the decisions made by Convolutional Neural Networks (CNNs). To achieve visual explanations for CNNs, methods based on class activation mapping and randomized input sampling have gained great popularity. However, the attribution methods based on these techniques provide lowerresolution and blurry explanation maps that limit their explanation power. To circumvent this issue, visualization based on various layers is sought. In this work, we collect visualization maps from multiple layers of the model based on an attribution-based input sampling technique and aggregate them to reach a fine-grained and complete explanation. We also propose a layer selection strategy that applies to the whole family of CNN-based models, based on which our extraction framework is applied to visualize the last layers of each convolutional block of the model. Moreover, we perform an empirical analysis of the efficacy of derived lower-level information to enhance the represented attributions. Comprehensive experiments conducted on shallow and deep models trained on natural and industrial datasets, using both groundtruth and model-truth based evaluation metrics validate our proposed algorithm by meeting or outperforming the stateof-the-art methods in terms of explanation ability and visual quality, demonstrating that our method shows stability regardless of the size of objects or instances to be explained."}, {"heading": "Introduction", "text": "Deep Neural models based on Convolutional Neural Networks (CNNs) have rendered inspiring breakthroughs in a wide variety of computer vision tasks. However, the lack of interpretability hurdles the understanding of decisions made by these models. This diminishes the trust consumers have for CNNs and limits the interactions between users and systems established based on such models. Explainable AI (XAI) attempts to interpret these cumbersome models (Hoffman et al. 2018). The offered interpretation ability has put XAI in the center of interest in various fields, especially where any single false prediction can cause severe consequences (e.g. healthcare) or where regulations force automotive decision-making systems to provide users with explanations (e.g. criminal justice) (Lipton 2016).\nInput Image\nScore-CAM RISE SISEGrad-CAM\nHorse 0.9956\nPerson 0.0021\nFigure 1: Comparison of conventional XAI methods with SISE (our proposed) to demonstrate SISE\u2019s ability to generate class discriminative explanations on a ResNet-50 model.\nThis work particularly addresses the problem of Visual Explanation, which is a branch of post-hoc XAI. This field aims to visualize the behavior of models trained for image recognition tasks (Barredo Arrieta et al. 2019). The outcome of the methods in this field is a heatmap in the same size as the input image named \u201cexplanation map\u201d that represents the evidence leading the model to make a decision.\nPrior works on visual explainable AI can be broadly categorized into \u2018approximation-based\u2019 (Ribeiro, Singh, and Guestrin 2016), \u2018backpropagation-based\u2019, \u2018perturbation-based\u2019, and \u2018CAM-based\u2019 methodologies. In backpropagation-based methods, only the local attributions are represented, making them unable to measure global sensitivity. This drawback is addressed by image perturbation techniques used in recent works such as RISE (Petsiuk, Das, and Saenko 2018) and Score-CAM (Wang et al. 2020). However, feedforwarding several perturbed images in these works makes them very slow. On the other hand, the explanation maps produced by CAM-based methods suffer from low resolution since these maps are formed by combining the feature maps in the last convolutional layer of CNN-based models that significantly lack spatial information regarding the captured attributions.\nIn this work, we delve deeper into providing a solution for interpreting CNN-based models by analyzing multiple layers of the network. Our solution concentrates on mutual utilization of features represented inside a CNN in different semantic levels that yields to achieving class discriminabil-\nar X\niv :2\n01 0.\n00 67\n2v 1\n[ cs\n.C V\n] 1\nO ct\n2 02\n0\nity and spatial resolution simultaneously. Inheriting productive ideas from the aforementioned types of approaches, we formulate an explanation method consisting of four phases. In the first three phases, visualization maps from multiple layers of a CNN are obtained and then combined via a fusion module to form a unique explanation heatmap in the last phase. The main contributions of our work can be summarized as follows:\n\u2022 We introduce a novel XAI algorithm that offers both spatial resolution and explanation completeness in its output explanation map by 1) using multiple layers from the \u201cintermediate blocks\u201d of the target CNN, 2) selecting crucial feature maps from the outputs of the layers, 3) employing an attribution-based technique for input sampling to visualize the perspective of each layer, and 4) applying a feature aggregation step to reach refined explanation maps.\n\u2022 We conducted thorough experiments on various models trained on an object detection dataset and an industrial classification dataset. To justify our method, we evaluated multiple metrics to compare our proposed algorithm with other conventional approaches. In this way, we show that the information between layers can be correctly combined to improve the visual explainability of its inference."}, {"heading": "Related Work", "text": "Backpropagation-based methods In general, calculating the gradient of a model\u2019s confidence score to the input features or the hidden neurons is the basis of this type of explanation algorithms. The earliest backpropagation-based (a.k.a. gradient-based) methods operate by computing the sensitivity of the model\u2019s confidence score to each of the input features directly (Simonyan, Vedaldi, and Zisserman 2013; Zeiler and Fergus 2014). To develop such methods, in some preceding pieces of literature like IntegratedGradient (Sundararajan, Taly, and Yan 2017) and SmoothGrad (Smilkov et al. 2017), backpropagation-based equations are adapted to take account for global sensitivity of the model\u2019s output to input features. Some gradient-based approaches such as LRP (Bach et al. 2015) and RAP (Nam et al. 2020) modify backpropagation rules to assign scores to the input features denoting the relevance or irrelevance of the input features to the model\u2019s prediction. Moreover, algorithms such as FullGrad (Srinivas and Fleuret 2019) and Excitation Backpropagation (Zhang et al. 2018) run by aggregating gradient information from several layers of the network.\nPerturbation-based methods Several visual explanation methods probe the model\u2019s behavior using perturbed copies of the input. In general, various strategies can be chosen to perform input sampling. Few of these approaches like RISE (Petsiuk, Das, and Saenko 2018) proposed random perturbation techniques to yield strong approximations of explanations. In Extremal Perturbation (Fong, Patrick, and Vedaldi 2019), an optimization problem is formulated to optimize a smooth perturbation mask maximizing the model\u2019s output confidence score. The noticeable property of most of the perturbation-based methods is that they treat the model like a \u201cblack-box\u201d instead of a \u201cwhite-box\u201d.\nCAM-based methods Based on the Class Activation Mapping (CAM) (Zhou et al. 2016) method, an extensive research effort was put to blend high-level features extracted by CNNs in a unique explanation map. CAM-based methods operate in three phases: 1) feeding the model with the input image, 2) scoring the feature maps in the last convolutional layer, and 3) combining the feature maps using the computed values as weight factors. Grad-CAM (Selvaraju et al. 2017) and Grad-CAM++ (Chattopadhyay et al. 2017) utilize backpropagation in the second phase which hurdles global sensitivity information from flowing to the explanation map due to gradient issues. By addressing this limitation, gradient-free algorithms such as Ablation-CAM (Ramaswamy et al. 2020), Smooth Grad-CAM++ (Omeiza et al. 2019), and Score-CAM (Wang et al. 2020) have been developed.\nDespite the strength of CAM-based methods in capturing the features extracted in CNNs, the lack of localization information in coarse high-level feature maps limits the performance of such methods by producing blurry explanations. Also, upsampling low-dimension feature maps to the size of input images causes disorientation in the location of captured features in some cases. Some recent works (Meng et al. 2019; Rebuffi et al. 2020) addressed these limitations by aggregating visualization maps obtained from multiple layers to achieve a fair trade-off between spatial resolution and class-distinctiveness of the features involved in forming explanation maps."}, {"heading": "Methodology", "text": "Our proposed algorithm is motivated by \u2018perturbationbased\u2019 methods that attempt to interpret the model\u2019s prediction by employing input sampling techniques. These methods have shown a great faithfulness in rationally inferring the predictions of models. However, they show instability as their output depends on random sampling (RISE) or random initialization for optimizing a perturbation mask (Extremal perturbation). They also produce different explanation maps each time and have an excessive runtime when attempting to get generalized results.\nTo address such limitations while enhancing their strength, we propose a CNN-specific algorithm that improves their fidelity and plausibility (in the view of reasoning) with adaptive runtime for practical usage. We term our algorithm as Semantic Input Sampling for Explanation (SISE). To claim such a reform, we replace the randomized input sampling technique in RISE with a sampling technique that relies on the feature maps derived from various layers of the model. We call this procedure attribution-based input sampling and we show that it provides a high-resolution perspective of the model in multiple semantic levels, in turn with restricting the applicability of SISE to CNNs.\nAs sketched in Fig. 2, SISE consists of four phases. In the first phase, multiple layers of the model are selected and a set of corresponding output feature maps are extracted. In the second phase, for each set of feature maps, a subset containing the most important feature maps are sampled with a backward pass. The selected feature maps are then\npost-processed to create sets of perturbation masks to be utilized in the third phase for attribution-based input sampling and are termed as attribution masks. These procedures in the first three phases are applied to the last layers of each convolutional block in the network, and their output is a 2- dimensional saliency map named visualization map. The details of this method as depicted in Fig. 3 and our intuition for the layer selection policy are discussed more analytically in this section. Such obtained visualization maps are aggregated in the last phase to reach the final explanation map.\nAttribution-Based Input Sampling Assume \u03a8 : I \u2192 R be a trained model that outputs a confidence score for a given input image, where I is the space of RGB images I = {I|I : \u039b \u2192 R3}, and \u039b = {1, ...,H}\u00d7 {1, ...,W} is the set of locations (pixels) in the image. Given any model and image, the goal of an explanation algorithm is to reach a unified explanation map SI,\u03a8(\u03bb), that assigns an \u201cimportance value\u201d to each location in the image (\u03bb \u2208 \u039b).\nIn RISE, the confidence scores observed for the copies of an image masked with a set of binary masks (M : \u039b \u2192 {0, 1}) are used to generate the explanation map by,\nSI,\u03a8(\u03bb) = EM [\u03a8(I m)|m(\u03bb) = 1] (1)\nwhere I m denotes a masked image obtained by pointwise multiplication between the input image and a mask m \u2208M . The representation of equation 1 can be modified to be generalized for sets of smooth masks (M : \u039b \u2192 [0, 1]). Hence, we reformat equation 1 as:\nSI,\u03a8(\u03bb) = EM [\u03a8(I m) \u00b7 Cm(\u03bb)] (2)\nwhere the term Cm(\u03bb) indicates the contribution amount of each pixel in the masked image. Setting the contribution indicator as Cm(\u03bb) = m(\u03bb), makes equation 2 equivalent to equation 1. We normalize these scores according to the size of perturbation masks to decrease the assigned reward to the background pixels when a high score is reached for a mask with too many activated pixels. Thus, we define this term as:\nCm(\u03bb) = m(\u03bb)\u2211 \u03bb\u2208\u039bm(\u03bb)\n(3)\nThis way of Cm(\u03bb) formulation increases the concentration on smaller features, particularly when multiple objects (either from the same instance or different instances) are present in an input image."}, {"heading": "Feature Map Selection", "text": "As discussed, we utilize attribution masks instead of the random masks as in RISE. Initially, we feed the model with an input image to derive sets of feature maps from various layers of the model. Then, we sample the most deterministic feature maps among each of these sets and post-process them to obtain sets of attribution masks. These masks are utilized for performing attribution-based input sampling.\nLet l be a selected layer containing N feature maps that are 2-dimensional matrices represented as A(l)k (k = {1, ..., N}) and the space of locations in these feature maps be denoted as \u039b(l). These feature maps are collected in the first phase of SISE by probing the feature extractor units of the model, and a similar strategy is also utilized in (Wang et al. 2020). The feature maps are formed in these units independently from the classifier part of the model. Thus, using the whole set of feature maps does not reflect the outlook of the classifier units of the model.\nTo identify and reject the class-indiscriminative feature maps, we partially backpropagate the signal to the selected layer to score the gradient of model\u2019s confidence score to each of the feature maps. These gradient scores are represented as follows:\n\u03b1 (l) k = \u2211 \u03bb(l)\u2208\u039b(l) \u2202\u03a8(I) \u2202A (l) k (\u03bb (l)) (4)\n\u03b2(l) = max k\u2208{1,...,N} (\u03b1 (l) k ) (5)\nThe feature maps with corresponding non-positive gradient scores - \u03b1(l)k , tend to contain features related to other classes rather than the class of interest. Terming such feature maps as \u2018negative-gradient\u2019, we define the set of attribution masks obtained from the \u2018positive-gradient\u2019 feature maps,M (l)d , as:\nM (l) d = {\u2126(A (l) k )|k \u2208 {1, ..., N}, \u03b1 (l) k < \u00b5\u00d7 \u03b2 (l)} (6)\nwhere \u00b5 is a parameter that is 0 by default to discard negative-gradient feature maps while retaining only the positive-gradients. Furthermore, the function \u2126(.) denotes the post-processing function converting feature maps to attribution masks. This function is a \u2018bilinear interpolation\u2019 which upsamples the feature maps to match the size of the input image, followed by a linear transformation that normalizes the values in the mask in the range [0, 1]. A visual comparison of attribution masks and random masks in Fig. 4 emphasizes such advantages discussed."}, {"heading": "Block-Wise Feature Explanation", "text": "As SISE extracts the feature maps from multiple layers in its first phase, we here define the most crucial layers for explicating the model\u2019s decisions. The intention is to reach a complete understanding of the model by visualizing the minimum number of layers.\nRegardless of the specification of their architecture, all types of CNNs consist of convolutional blocks connected via pooling layers that aid the network to justify the presence of semantic instances. Each convolutional block is formed by cascading multiple layers, which may vary from a simple convolutional filter to more complex structures (e.g. bottleneck or MBConv layers). However, the dimensions of their input and output signal are the same. In a convolutional block, assuming the number of layers to be L, each ith layer can be represented with the function fi(.), where i = {1, ..., L}. Denoting the input to each ith layer as yi, the whole block can be mathematically described as F (y1) = fL(yL). For plain CNNs (e.g. VGG, GoogleNet), the output of each convolutional block can be simply represented with the equation below:\nF (y1) = fL(fL\u22121(...(f1(y1))) (7) After the emergence of residual networks that utilize skipconnection layers to propagate the signals through a convolutional block in the families as ResNet models, DenseNet, EfficientNet (Tan and Le 2019; Huang, Liu, and Weinberger 2016; Sandler et al. 2018), and the models whose architecture are adaptively learned (Zoph and Le 2016), it is debated that these neural networks can be represented with a more complicated view. These types of networks can be viewed by the unraveled perspective, as presented in (Veit, Wilber, and Belongie 2016). Based on this perspective which is shown in Fig. 5, the connection between the input and output is formulated as follows:\nyi+1 = fi(yi) + yi (8)\nand hence,\nF (y1) = y1 +f1(y1)+ ...+fL(y1 + ...+fL\u22121(yL\u22121)) (9)\nThe unraveled architecture as in Fig. 5 is comprehensive enough to be generalized even to shallower CNN-based models that lack skip-connection layers. For plain networks, the layer functions fi can be decomposed to an identity function I and a residual function gi as follows:\nfi(yi) = I(yi) + gi(yi) (10)\nSuch a decomposition, yields to a similar equation form as equation 8, and consequently, equation 9.\nyi+1 = gi(yi) + yi (11)\nIt can be inferred from the unraveled view that while feeding the model with an input, signals might not pass through all convolutional layers as they may skip some layers using the skip-connection and be propagated to the next layers directly. However, this is not the case for pooling layers. Considering they change the dimensions of the signals, equation 10 cannot be applied to such layers. To prove this hypothesis, an experiment was conducted in (Veit, Wilber, and Belongie 2016), where the corresponding test errors are reported for removing a layer individually from a residual network. It was observed that a significant degradation in test performance is recorded only when the pooling layers are removed.\nBased on this hypothesis and result, most of the data in each model can be collected by probing the pooling layers. Thus, by visualizing these layers, it is possible to track the way features are propagated through convolutional blocks. Therefore, for all given CNNs, we select the inputs of the pooling layers to be visualized in the first three phases of SISE and pass their corresponding visualization maps to the fusion block to perform a block-wise feature aggregation."}, {"heading": "Fusion Module", "text": "In the fourth phase of SISE, the flow of features from lowlevel to high-level blocks are tracked. The inputs to the fusion module are the visualization layers obtained from the third phase of SISE after scoring attribution masks by merging equations 2 and 6 as:\nV (lb) I,\u03a8 (\u03bb) = EM(lb)d [\u03a8(I m) \u00b7 Cm(\u03bb)] (12)\nwhere lb \u2208 {1, ..., B} and V (lb)I,\u03a8 denote the ith pooling layer and the visualization map reached for that layer respectively,\nand B refers to the number of convolutional blocks in the model. The output of this module is a 2-dimensional explanation map, which is the output of SISE.\nThe proposed fusion module is designed with cascaded fusion blocks. In each fusion block, the feature information from the visualization maps representing explanations for two consecutive blocks is collected using an \u201caddition\u201d block. Then, the features that are absent in the latter visualization map are removed from the collective information by masking the output of the addition block with a binary mask indicating the activated regions in the latter visualization map. To reach the binary mask, we apply an adaptive threshold to the latter visualization map, determined by Otsu\u2019s method (Otsu 1979). By cascading fusion blocks as in Fig. 6, the features determining the model\u2019s prediction are represented in a more fine-grained manner while the inexplicit features are discarded."}, {"heading": "Experiments", "text": "We verify the performance of our method on shallow and deep CNNs, including VGG16, ResNet-50, and ResNet-101 architectures. To conduct the corresponding experiments, we employed PASCAL VOC 2007 (Everingham et al. 2007) and Severstal (PAO Severstal 2019) datasets. The former is a popular object detection dataset containing 4952 test images belonging to 20 object classes. As images with many small object occurrences and multiple instances of different classes are prevalent in this dataset, it is a hard task for an XAI algorithm to perform well on the whole dataset. The latter is an industrial steel defect detection dataset created for anomaly detection and steel defect segmentation problems. We reformatted it into a defect classification dataset instead, containing 11505 test images from 5 different classes, including one normal class and four different classes of defects. Class imbalance, intraclass variation, and interclass similarity are the main challenges of this recast dataset."}, {"heading": "Experimental Setup", "text": "Experiments conducted on the PASCAL VOC 2007 dataset are evaluated on its test set with a VGG16 and a ResNet50 model from the TorchRay library (Fong, Patrick, and Vedaldi 2019), trained by (Zhang et al. 2018). The top1 accuracies of the models on the test set were 87.18%\nand 87.96%, while the top-5 accuracies were 93.29% and 93.09% respectively. On the other hand, for conducting experiments on Severstal, we trained a ResNet-101 model (with a test accuracy of 86.58%) on the recast dataset to assess the performance of the proposed method in the task of visual defect inspection. To recast the Severstal dataset for classification, the train and test images were cropped into patches of size 256 \u00d7 256. For the evaluations presented in the results sections, a balanced subset of 1381 test images belonging to defect classes labeled as 1, 2, 3, and 4 is chosen. For evaluating our method, we have implemented SISE on Keras and set the parameter \u00b5 to its default value, 0."}, {"heading": "Qualitative Results", "text": "Based on explanation quality, we have compared SISE with other state-of-the-art methods on sample images from the Pascal dataset in Fig. 7 and Severstal dataset in Fig. 8. Images with both normal-sized and small object instances are shown along with their corresponding confidence scores. Moreover, Figs. 1 and 9 with images of multiple objects from different classes depict the superior ability of SISE in discriminating the explanations of various classes in comparison with other methods and RISE in particular. More qualitative results are attached in the technical appendix."}, {"heading": "Quantitative Results", "text": "Quantitative analysis includes evaluation results categorized into \u2018ground truth-based\u2019 and \u2018model truth-based\u2019 metrics. The former is used to justify the model by assessing the extent to which the algorithm satisfies the users by providing visually superior explanations, while the latter is used to analyze the model behavior by assessing the faithfulness of the algorithm and its correctness in capturing the attributions in line with the model\u2019s prediction procedure. The reported results of RISE and Extremal Perturbation in Table 1 are averaged on three runs, and the corresponding metrics employed are discussed below.\nGround truth-based Metrics: The state-of-the-art explanation algorithms are compared with SISE based on three distinct ground-truth based metrics, to justify the visual quality of the explanation maps generated by our method. Denoting the ground-truth mask as G and the achieved explanation map as S, the evaluation metrics used are:\nEnergy-Based Pointing Game (EBPG) (Wang et al. 2020) measures the precision and denoising ability of an XAI algorithm. It extends the traditional Pointing Game metric by considering all pixels for evaluation. EBPG measures the fraction of energy captured in the resultant explanation map S by considering the ground truth area G, as EBPG = ||S G||1||S||1 .\nmIoU analyses the localization ability and meaningfulness of the attributions captured in an explanation map. In our experiments, we select the top 20% pixels highlighted in each explanation map S and compute mean intersection over union with their corresponding ground-truth masks.\nBounding box (Bbox) (Schulz et al. 2020) is taken into account as a size-adaptive variant of mIoU. Considering N as the number of ground truth pixels in G, Bbox score is\ncalculated by selecting the top N pixels in S and evaluating the corresponding fraction captured over G.\nModel truth-based metrics: To evaluate the correlation between the representations of our method and the model\u2019s predictions, model-truth based metrics are employed to compare SISE with the other state-of-the-art methods. As the main objective of visual explanation algorithms is to envision the perspective of the model for its predictions, these metrics are considered with higher importance.\nDrop% and Increase%, as introduced in (Chattopadhyay et al. 2017) and later modified by (Ramaswamy et al. 2020; Fu et al. 2020), can be interpreted as an indicator of the positive attributions missed and the negative attribution discarded from the explanation map respectively. Given a model \u03a8(.), an input image Ii from a dataset containing K images, and an explanation map S(Ii), the Drop/Increase %\nmetric selects the most important pixels in S(Ii) to measure their contribution towards the model\u2019s prediction. A threshold function T (.) is applied on S(Ii) to select the top 15% pixels that are then extracted from Ii using pointwise multiplication and fed to the model. The confidence scores on such perturbed images are then compared with the original score, according to the equations Drop% = 1 K \u2211K i=1 max(0,\u03a8(Ii)\u2212\u03a8(Ii T (Ii))) \u03a8(Ii)\n\u00d7100 and Increase% =\u2211K i=1 sign(\u03a8(Ii T (Ii))\u2212\u03a8(Ii))."}, {"heading": "Discussion", "text": "The experimental results as in Figs. 1, 7, 8, and 9 demonstrate the ability of SISE in producing high-resolution, noise-free, and precise explanation maps. This claim is further supported by justifying our method via ground truthbased evaluation metrics as in Table 1. Moreover, model truth-based metrics in Tables 1 and 2 prove the superior ability of SISE in highlighting the evidence, based on which the model makes a prediction. Similar to the CAM-based meth-\nods, the output of the last convolutional block plays the most critical role in our method. However, by considering the intermediate layers based on the proposed block-wise layer selection strategy, the advantageous properties of SISE are enhanced. Furthermore, utilizing attribution-based input sampling instead of a randomized sampling procedure, ignoring the unrelated feature maps, and modifying the linear combination step allows our method to improve the clarity and completeness of its explanation map dramatically.\nComplexity Evaluation In addition to performance evaluations, we carried out a runtime test to compare the complexity of the methods. It was conducted on a Tesla T4 GPU with 16GB of memory and performed on a ResNet50 model. Reported runtimes were averaged over 100 trials using a random image from the PASCAL VOC 2007 test set for each trial. Grad-CAM and Grad-CAM++ are the fastest methods, requiring 19 and 20 milliseconds respectively to generate an explanation map. On the other hand, Extremal Perturbation is the slowest method, requiring 78.37 seconds since it optimizes thousands of variables (pixels of the explanation map). In comparison with RISE, which has a runtime of 26.08 seconds, our method runs in 9.21 seconds.\nAblation Study While RISE uses a constant number of 4000 randomly generated masks, our approach initially ex-\ntracts 3904 feature maps from the ResNet-50 model and then employs subsets of them as attribution masks. Setting the parameter \u00b5 to 0, SISE selects around 1900 of those masks that have positive gradients towards the chosen class id and operates in around 9.21 seconds. To analyze the effect of reducing the number of attribution masks on the performance of SISE, an ablation study is carried. By changing \u00b5 to 0.3, a scanty variation in the boundary of explanation maps can be noticed while the runtime is reduced to 2.18 seconds. This shows that ignoring feature maps with low gradient values does not affect the explanation ability of SISE considerably, since they tend to be assigned low scores in the third phase of SISE anyway. By further increasing \u00b5 to 0.5, a slight decline in evaluation metrics is recorded with a runtime of just 0.65 seconds.\nA more detailed analysis of the effect of \u00b5 on various evaluation metrics along with an extensive discussion of our algorithm and additional results on MS COCO 2014 dataset (Lin et al. 2014) are provided in the technical appendix."}, {"heading": "Conclusion", "text": "In this work, we propose SISE, a novel explanation algorithm that is specialized to the family of CNN-based models. SISE generates explanations by aggregating visualization maps obtained from the output of convolutional blocks reached through attribution-based input sampling. Qualitative results show that our method can output high resolution and noise-free explanation maps. These properties offered by our method are emphasized by quantitative analysis using ground truth-based metrics. Moreover, model truth-based metrics demonstrate that our proposed algorithm also outperforms other state-of-the-art methods in providing complete and precise visual explanations. Our experiments reveal that mutual utilization of features captured in final and intermediate layers of the model aids in producing explanation maps that not only accurately locate object instances, but also reach a greater portion of attributions leading the model to make a decision."}, {"heading": "Technical Appendix", "text": ""}, {"heading": "Datasets", "text": "Experiments are conducted on three different datasets: MS COCO 2014 (Lin et al. 2014), PASCAL VOC 2007 (Everingham et al. 2007), and Severstal (PAO Severstal 2019). The first two datasets are \u201cnatural image\u201d object detection datasets, while the last one is an \u201cindustrial\u201d steel defect detection dataset. They are discussed more in detail in the following subsections."}, {"heading": "MS COCO 2014 and PASCAL VOC 2007 Datasets", "text": "The MS COCO 2014 dataset features 80 different object classes, each one of a common object. All experimental results are performed on the validation set, which has 40,504 images. The PASCAL VOC 2007 dataset features 20 object classes, and all experimental results for this dataset are performed on its test set, which has 4,952 images. Both datasets are created for object detection and segmentation purposes and contain images with multiple object classes, and images with multiple object instances, making these datasets challenging for XAI algorithms to perform well on."}, {"heading": "Severstal Dataset", "text": "To extend the analysis of the influence of XAI algorithms beyond natural images, the Severstal steel defect detection dataset was chosen. It was originally hosted on Kaggle as a \u201cdetection\u201d task, which we then converted to a \u201cclassification\u201d task. The original dataset has 12,568 train images under one normal class labeled \u201c0\u201d, and four defective classes numbered 1 through 4. Each image may contain no defect, or one defect, or two and more defects from different classes in it. The ground truth annotations for the segments (masks) are provided in a CSV file, with a single row entry for each class of defect present within each image. The row entries provide the locations of defects, with some entries having several non-contiguous defect locations available."}, {"heading": "Class 0 Class 1 Class 2 Class 3 Class 4", "text": "The original images were long strips of steel sheets with dimensions 1600 \u00d7 256 pixels. To convert the dataset for our purpose, every training image was cropped (without any overlap) with an initial offset of 32 pixels into 6 individual images of dimensions 256 \u00d7 256 pixels. The few empty (black) images that tended to be located along the sides of the original long strip images were discarded, along with images that had multiple types of defects. This re-formulation left a highly-imbalanced dataset with 5 distinct classes - 0, 1, 2, 3, and 4. Class 0 contains images with no defects, whereas"}, {"heading": "Severstal: Steel Defect Detection", "text": ""}, {"heading": "Class Training Test set Totalset", "text": "the other four classes have images with only that specific defect group. Fig. 10 shows sample images from each class of the recast dataset. The image per class distribution is provided in Table 3. The training split is 70% of the data, and the test is the remaining 30%. From the training data, 20% is used for validation. The experimental results and qualitative figures of the Severstal dataset are conducted on a subset of the test set using all of the images from classes 1, 2, and 4, and using 500 images from class 3."}, {"heading": "Models", "text": ""}, {"heading": "VGG16 and ResNet-50", "text": "The top-1 accuracies of the VGG16 and ResNet-50 models (loaded from the TorchRay library (Fong, Patrick, and Vedaldi 2019)) on the test set of the PASCAL VOC 2007 dataset were 56.56 percent and 57.08 percent respectively out of a maximum top-1 accuracy of 64.88 percent, while the top-5 accuracies were 93.29 percent and 93.09 percent respectively out of a maximum top-5 accuracy of 99.99 percent. The top-1 accuracies of the VGG16 and ResNet-50 on the validation set of the MS COCO 2014 dataset were 29.62 percent and 30.25 percent respectively out of a maximum top-1 accuracy of 34.43 percent, while the top-5 accuracies were 69.01 percent and 70.27 percent respectively out of a maximum top-5 accuracy of 93.28 percent."}, {"heading": "ResNet-101", "text": "A ResNet-101 model was trained on the recast Severstal dataset using a Stochastic Gradient Descent (SGD) optimizer along with a categorical cross-entropy loss function. The model is trained for 40 epochs with an initial learning rate of 0.1, which is dropped by half every 5 epochs. Considering the high data imbalance among the classes, the top1 accuracy of the ResNet-101 model on the test set of the recast Severstal dataset was 86.58 percent, while the top-3 accuracy was 99.60 percent. Table 5 shows the normalized confusion matrix of this model."}, {"heading": "Evaluation", "text": "In addition to the quantitative evaluation results shared on the main paper, the results of both ground-truth based and model-truth based metrics on the MS COCO 2014 dataset are attached in Table 4. Similar to our earlier results, SISE outperforms other conventional XAI methods in most cases.\nThe MS COCO 2014 data set is more challenging for the explanation algorithms than the PASCAL VOC 2007 dataset because of\n\u2022 the higher number of object instances \u2022 the presence of more extra small objects \u2022 the presence of more objects either from the same or\ndifferent classes in each image (on average) \u2022 the lower classification accuracy of the models\ntrained on it (as provided in TorchRay library).\nHowever, the results depicted in Table 4 and Figs. 17 and 18 emphasizes the superior ability of SISE in providing satisfying, high-resolution, and complete explanation maps that provide a precise visual analysis of the model\u2019s predictions and perspective.\nThe benchmark results reported on the Pascal VOC 2007 and MS COCO 2014 datasets are calculated for all groundtruth labels in the test images. For example, if a chosen input image has both \u201cdog\u201d and \u201ccat\u201d object instances, then explanations are collected for both class ids and accounted for in overall performance. SISE\u2019s ability to generate class discriminative explanations is represented in this manner. As discussed in the main manuscript, SISE chooses pooling layers to collect feature maps, which are later combined in\nthe fusion module. The experiments on the Severstal dataset were performed for only the ground-truth labels, as each test image has exactly one class id associated with it.\nA detailed qualitative analysis of SISE explanations compared with other state-of-the-art XAI algorithms on the discussed models on Pascal VOC 2007 and recast Severstal datasets are shown in Figs. 14, 15 and 16 respectively. Figs. 17 and 18 show a similar comparative analysis on MS COCO 2014 dataset."}, {"heading": "Ablation Study", "text": "As stated in the main manuscript, in the second phase of SISE, each set of feature maps is valuated by backpropagating the signal from the output of the model to the layer from which the feature maps are derived. In this stage, after normalizing the backpropagation-based scores, a threshold \u00b5 is applied to each set, so that the feature maps passing the threshold are converted to attribution masks and utilized in the next steps, while the others are discarded. Some of these feature maps do not contain signals that lead the model to make a firm prediction since they represent the attributions related to the instances of the other classes (rather than\nthe class of interest). These feature maps are expected to be identified by reaching zero or negative backpropagationbased scores. Getting rid of them by setting the threshold parameter \u00b5 to 0 (\u00b5 is defined in the main manuscript) will improve our method, not only by increasing its speed but also by enabling us to analyze the model\u2019s decision making process more precisely.\nBy increasing the threshold parameter \u00b5, a trade-off be-\ntween performance and speed is reached. When this parameter is slightly increased, SISE will discard feature maps with low positive backpropagation-based scores, which is expected not to make a considerable impact on the output explanation map. The higher the parameter \u00b5 is though, the more deterministic feature maps are discarded, causing more degradation in SISE\u2019s performance.\nTo verify these interpretations, we have conducted an ablation analysis on the PASCAL VOC 2007 test set. As stated in the main manuscript, the model truth-based metrics (Drop% and Increase%) are the most important metrics revealing the sensitivity of SISE\u2019s performance with respect to its threshold parameter. According to our results as depicted in Table 6 and Fig. 12, the ground truth-based results also follow approximately the same trend for the effect of \u00b5 variation. Consequently, our results show that by adjusting this hyper-parameter, a dramatic increase in SISE\u2019s speed is gained in turn with a slight compromise in its explanation ability.\nSince the behavior of our method concerning this hyperparameter does not depend on the model and the dataset employed, it can be consistently fine-tuned, based on the requirements of the end-user."}, {"heading": "Sanity Check", "text": "In addition to the comprehensive quantitative experiments presented in the main manuscript and this appendix, we also verified the sensitivity of our explanation algorithm to the model\u2019s parameters, illustrating that our method adequately\nexplains the relationship between the input and output that the model reaches. As introduced by (Adebayo et al. 2018), sanity checks on explanation methods can be conducted either by randomizing the model\u2019s parameters or retraining the model with the same training data, but with random labels. In this work, we performed sanity checks on our method by randomizing the parameters of the model. To do so, we have randomized the weight and bias parameters on the VGG16 trained on PASCAL VOC 2007 dataset provided by (Fong, Patrick, and Vedaldi 2019). Fig. 11 represents the results of sanity checks for some input images. The layers for which the parameters to be randomized are selected in a top to bottom manner, as specified in the figure. Each row shows the effect on the output explanation maps for an image when we perturb the parameters in more layers. According to this figure, SISE shows alterations in explanation maps, while dealing with highly perturbed models. Hence, SISE passes our sanity check.\nTo access SISE\u2019s explanation beyond a few evaluation metrics, another sanity check was performed. Fig. 13 attached shows such experimentation where an untrained VGG16 model was directly compared with our Pascal VOC dataset trained VGG16 model. SISE doesn\u2019t generate quality explanations from the untrained model, insisting that our method not just provide \u201cfeatured regions\u201d obtained through convolutional operations, but depict the actual \u201cattributed regions\u201d affecting the model\u2019s decision."}, {"heading": "Complexity Evaluation", "text": "A runtime test was conducted to compare the complexity of the different XAI methods with SISE, timing how long it took for each algorithm to generate an explanation map. It was performed with a Tesla T4 GPU with 16GB of memory on both a VGG16 and ResNet-50 model and attached as Table 7.\nReported runtimes were averaged over 100 trials using a random image from the PASCAL VOC 2007 test set for each trial. Grad-CAM and Grad-CAM++ are the fastest methods when applied to both models. This is expected as they operate using only one main forward pass and one backward pass. Our method, SISE, is not the fastest, and the main bottleneck in its runtime is the number of feature maps extracted and used from the CNN. This is addressed by adjusting \u00b5, as discussed in the \u2018Ablation Study\u2019 section."}], "title": "Explaining Convolutional Neural Networks through Attribution-Based Input Sampling and Block-Wise Feature Aggregation", "year": 2020}