{"abstractText": "Machine Learning models become increasingly proficient in complex tasks. However, even for experts in the field, it can be difficult to understand what the model learned. This hampers trust and acceptance, and it obstructs the possibility to correct the model. There is therefore a need for transparency of machine learning models. The development of transparent classification models has received much attention, but there are few developments for achieving transparent Reinforcement Learning (RL) models. In this study we propose a method that enables a RL agent to explain its behavior in terms of the expected consequences of state transitions and outcomes. First, we define a translation of states and actions to a description that is easier to understand for human users. Second, we developed a procedure that enables the agent to obtain the consequences of a single action, as well as its entire policy. The method calculates contrasts between the consequences of the user\u2019s query-derived policy, and of the learned policy of the agent. Third, a format for generating explanations was constructed. A pilot survey study was conducted to explore preferences of users for different explanation properties. Results indicate that human users tend to favor explanations about policy rather than about single actions.", "authors": [{"affiliations": [], "name": "J. van der Waa"}, {"affiliations": [], "name": "J. van Diggelen"}, {"affiliations": [], "name": "K. van den Bosch"}, {"affiliations": [], "name": "M. Neerincx"}], "id": "SP:b618b5f212d8b93e8bb866a1e1ce18079946723b", "references": [{"authors": ["Abhijit Gosavi"], "title": "Reinforcement learning: A tutorial survey and recent advances", "venue": "INFORMS Journal on Computing, 21(2):178\u2013192,", "year": 2009}, {"authors": ["Riccardo Guidotti", "Anna Monreale", "Franco Turini", "Dino Pedreschi", "Fosca Giannotti"], "title": "A survey of methods for explaining black box models", "venue": "arXiv preprint arXiv:1802.01933,", "year": 2018}, {"authors": ["Bradley Hayes", "Julie A Shah. Improving robot controller transparency through autonomous policy explanation"], "title": "In Proceedings of the 2017 acm/ieee international conference on human-robot interaction", "venue": "pages 303\u2013312. ACM,", "year": 2017}, {"authors": ["Daniel Hein", "Steffen Udluft", "Thomas A Runkler"], "title": "Interpretable policies for reinforcement learning by genetic programming", "venue": "arXiv preprint arXiv:1712.04170,", "year": 2017}, {"authors": ["Kulesza et al", "2015] Todd Kulesza", "Margaret Burnett", "Weng-Keen Wong", "Simone Stumpf"], "title": "Principles of explanatory debugging to personalize interactive machine learning", "venue": "In Proceedings of the 20th International Conference on Intelligent User Interfaces,", "year": 2015}, {"authors": ["Zachary C Lipton"], "title": "The mythos of model interpretability", "venue": "arXiv preprint arXiv:1606.03490,", "year": 2016}, {"authors": ["Scott Lundberg", "Su-In Lee"], "title": "An unexpected unity among methods for interpreting model predictions", "venue": "arXiv preprint arXiv:1611.07478,", "year": 2016}, {"authors": ["Tim Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "arXiv preprint arXiv:1706.07269,", "year": 2017}, {"authors": ["Nicolas Papernot", "Patrick McDaniel"], "title": "Deep k-nearest neighbors: Towards confident", "venue": "interpretable and robust deep learning. arXiv preprint arXiv:1803.04765,", "year": 2018}, {"authors": ["Ribeiro et al", "2016] Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["Wojciech Samek", "Gr\u00e9goire Montavon", "Alexander Binder", "Sebastian Lapuschkin", "KlausRobert M\u00fcller"], "title": "Interpreting the predictions of complex ml models by layer-wise relevance propagation", "venue": "arXiv preprint arXiv:1611.08191,", "year": 2016}, {"authors": ["Alexander A Sherstov", "Peter Stone"], "title": "Improving action selection in mdp\u2019s via knowledge transfer", "venue": "AAAI, volume 5, pages 1024\u20131029,", "year": 2005}, {"authors": ["Tianmin Shu", "Caiming Xiong", "Richard Socher"], "title": "Hierarchical and interpretable skill acquisition in multi-task reinforcement learning", "venue": "arXiv preprint arXiv:1712.07294,", "year": 2017}, {"authors": ["Abhinav Verma", "Vijayaraghavan Murali", "Rishabh Singh", "Pushmeet Kohli", "Swarat Chaudhuri"], "title": "Programmatically interpretable reinforcement learning", "venue": "arXiv preprint arXiv:1804.02477,", "year": 2018}, {"authors": ["Adrian Weller"], "title": "Challenges for transparency", "venue": "arXiv preprint arXiv:1708.01870,", "year": 2017}], "sections": [{"heading": "1 Introduction", "text": "Complex machine learning (ML) models such as Deep Neural Networks (DNNs) and Support Vector Machines (SVMs) perform very well on a wide range of tasks [Lundberg and Lee, 2016], but their outcomes are often are often difficult to understand by humans [Weller, 2017]. Moreover, machine learning models cannot explain how they achieved their results. Even for experts in the field, it can be very difficult to understand what the model actually learned [Samek et al., 2016]. To remedy this issue, the field of eXplainable Artificial Intelligence (XAI) studies how such complex but useful models can be made more understandable [?].\nAchieving transparency of ML models has multiple advantages [Weller, 2017]. For example, if a model designer knows why a model performs badly on some data, he or she can start a more informed process of resolving the performance issues [Kulesza et al., 2015; Papernot and McDaniel, 2018]. However, even if a model has high performance, the users (typically non-experts in ML) would still like to know why it came to a certain output [Miller, 2017]. Especially in high-risk domains such as defense and health care, inappropriate trust in the output may cause substantial risks and problems [Lipton, 2016; Ribeiro et al., 2016]. If a ML model fails to provide transparency, the user cannot safely rely on its outcomes, which hampers the model\u2019s applicability [Lipton, 2016]. If, however, a ML-model is able to explain its workings and outcomes satisfactorily to the user, then this would not only improve the user\u2019s trust; it would also be able to provide new insights to the user.\nFor the problem of classification, recent research has developed a number of promising methods that enable classification models to explain their output [Guidotti et al., 2018]. Several of these methods prove to be model-independent in some way, allowing them to be applied on any existing ML classification model. However, for Reinforcement Learning (RL) models, there are relatively few methods available [Verma et al., 2018; Shu et al., 2017; Hein et al., 2017]. The scarcity of methods that enable RL agents to explain their actions and policies towards humans severely hampers the practical applications of RL-models in this field. It also diminishes the, often highly rated, value of RL to Artificial Intelligence [Hein et al., 2017; Gosavi, 2009]. Take for example a simple agent within a grid world that needs to reach a goal position while evading another actor who could cause termination as well as evading other static terminal states. The RL agent cannot easily explain why it takes the route it has learned as it only knows numerical rewards and its coordinates in the grid. The agent has no grounded knowledge about the \u2019evil actor\u2019 that tries to prevent it from reaching its goal nor has it knowledge of how certain actions will effect such grounded concepts. These state features and rewards are what drives the agent but do not lend themselves well for an explanations as they may not be grounded concepts nor do they offer a reason why the agent behaves a certain way. ar X iv :1 80 7.\n08 70\n6v 1\n[ cs\n.L G\n] 2\n3 Ju\nl 2 01\nImportant pioneering work has been done by Hayes and Shah [Hayes and Shah, 2017]. They developed a method for eXplainable Reinforcement Learning (XRL) that can generate explanations about a learned policy in a way that is understandable to humans. Their method converts feature vectors to a list of predicates by using a set of binary classification models. This list of predicates is searched to find sub-sets that tend to co-occur with specific actions. The method provides information about which actions are performed when which state predicates are true. A method that uses the cooccurrence to generate explanations may be useful for small problems, but becomes less comprehensible in larger planning and control problems, because the overview of predicate and action combinations becomes too large. Also, the method addresses only what the agent does, and not why it acts as it does. In other words, the method presents the user with the correlations between states and the policy but it does not provide a motivation why that policy is used in terms of rewards, or state transitions.\nThis study proposes an approach to XRL that allows an agent to answer questions about its actions and policy in terms of their consequences. Other questions unique to RL are also possible, for example those that ask about the time it takes to obtain some goal or those about RL specific problems (loop behavior, lack of exploration or exploitation, etc.). However we believe that a non-expert in RL is mostly interested in the expected consequences of the agent\u2019s learned behavior and whether the agent finds these consequences good or bad. This information can be used as an argument why the agent behaves in some way. This would allow human users to gain insight in what information the agent can perceive from a state and which outcomes it expects from an action or state visit. Furthermore, to limit the amount of information of all consequences, our proposed method aims to support contrastive explanations [Miller, 2017]. Contrastive explanations are a way of answering causal \u2019why\u2019-questions. In such questions, two potential items, the fact and foil, are compared to each other in terms of their causal effects on the world. Contrastive questions come natural between humans and offer an intuitive way of gathering motivations about why one performs a certain action instead of another [Miller, 2017]. In our case we allow the user to formulate a question of why the learned policy \u03c0t (the \u2019fact\u2019) is used instead of some other policy \u03c0f (the \u2019foil) that is of interest to the user. Furthermore, our proposed method translates the set of states and actions in a set of more descriptive state classes C and action outcomes O similar to that of [Hayes and Shah, 2017]. This allows the user to query the agent in a more natural way as well as receive more informative explanations as both refer to the same concepts instead of plain features. The translation of state features to more high-level concepts and actions in specific states to outcomes, is also done in the proposed algorithm of [Sherstov and Stone, 2005]. The translation in this algorithm was used to facilitate transfer learning within a single action over multiple tasks and domains. In our method we used it to create a user-interpretable variant of the underlying Markov Decision Problem (MDP).\nFor the purpose of implementation and evaluation of our proposed method, we performed a pilot study. In this study,\na number of explanation examples were presented to participants to see which of their varying properties are preferred the most. One of the properties was to see whether the participants prefer explanations about the expected consequences of a single-action or the entire policy."}, {"heading": "2 Approach for consequence-based explanations", "text": "The underlying Markov Decision Problem (MDP) of a RL agent consists of the tuple \u3008S,A,R, T, \u03bb\u3009. Here, S and A are the set of states (described by a feature vector) and actions respectively, R : S \u00d7 A \u2192 R is the reward function and T : S \u00d7 A \u2192 Pr(S) the transition function that provides a probability distribution over states. Also, \u03bb is the discount factor that governs how much of future rewards are taken into account by the agent. This tuple provides the required information to derive the consequences of the learned policy \u03c0t or the foil policy \u03c0f from the user\u2019s question. As one can use the transition function T to sample the effects of both \u03c0t and \u03c0f . In the case T is not explicit, one may use a separate ML model to learn it in addition to the actual agent. Through this simulation, one constructs a Markov Chain of state visits under each policy \u03c0t and \u03c0f and can present the difference to the user.\nThrough the simulation of future states with T , information can be gathered about state consequences. In turn, from the agent itself the state or state-action values for simulated state visits can be obtained to develop an explanation in terms of rewards. However, the issue with this approach is that the state features and rewards may not be easy to understand for a user as it would consist of possibly low-level concepts and numerical reward values or expected returns. To mitigate this issue we can apply a translation of the states and actions to a set of predefined state concepts and outcomes. These concepts can be designed to be more descriptive and informative for the potential user. A way to do this translation is by training a set of binary classifiers to recognize each outcome or state concept from the state features and taken action, a similar approach to the one from [Hayes and Shah, 2017]. Their training can occur during the exploratory learning process of the agent. This translation allows us to use the above described method of simulating consequences and transform the state features and results of actions to more user-interpretable concepts."}, {"heading": "2.1 A user-interpretable MDP", "text": "The original set of states can be transformed to a more descriptive set C according to the function k : S \u2192 C. This is similar to the approach of [Hayes and Shah, 2017] where k consists of a number of classifiers. Also, rewards can be explained in terms of a set of action outcomes O according to t : C \u00d7 A \u2192 Pr(O). This provides the results of an action in some state in terms of the concepts O. For example, the outcomes that the developer had in mind when designing the reward function R. The transformation of states and actions in state classes and outcomes is adopted from the work of [Sherstov and Stone, 2005] where the transformations are used to allow for transfer learning in RL. Here however, we\nuse them as a translation towards a more user-interpretable representation of the actual MDP.\nThe result is the new MDP tuple \u3008S,A,R, T, \u03bb,C,O, t,k\u3009. An RL agent is still trained on S, A, R and T with \u03bb independent of the descriptive sets C and O and functions k and t. This makes the transformation independent of the RL algorithm used to train the agent. See Figure 1 for an overview of this approach.\nAs an example take the grid world illustrated in Figure 2 that shows an agent in a simple myopic navigation task. The states S are the (x, y) coordinates and the presence of a forest, monster or trap in adjacent tiles with A = Up,Down,Left,Right. R consists of a small transient penalty, a slightly larger penalty for tiles with a forest, a large penalty shared over all terminal states (traps or adjacent tiles to a monster) and a large positive reward for the finishing state. T is skewed towards the intended result with small probabilities for the other results if possible.\nThe state transformation k can consist out of a set of classifiers for the predicates whether the agent is next to a forest, a wall, a trap or monster, or in the forest. Applying k to some state s \u2208 S results in a Boolean vector c \u2208 C whose information can be used to construct an explanation in terms of the stated predicates. The similar outcome transformation t may predict the probability of the outcomes O given a state and action. In our example, O consists of whether the agent will be at the goal, in a trap, next to the monster or in the forest. Each outcome o can be flagged as being positive o+ or negative o\u2212 purely such that they can be presented differently in the eventual explanation.\nGiven the above transformations we can simulate the next state of a single action a with T or even the entire chain of\nactions and visited states given some policy \u03c0. These can then be transformed into state descriptions C and action outcomes O to form the basis of an explanation. As mentioned, humans usually ask for contrastive questions especially regarding their actions [Miller, 2017]. In the next section we propose a method of translating the foil in a contrastive question into a new policy."}, {"heading": "2.2 Contrastive questions translated into value functions", "text": "A contrastive question consists of a fact and a foil, and its answer describes the contrast between the two from the fact\u2019s perspective [Miller, 2017]. In our case, the fact consists of the entire learned policy \u03c0t, a single action from it at = \u03c0t(st) or any number of consecutive actions from \u03c0t. We propose a method of how one can obtain a foil policy \u03c0f based on the foil in the user\u2019s question. An example of such a question could be (framed within the case of Figure 2);\n\u201dWhy do you move up and then right (fact) instead of moving to the right until you hit a wall and then move up (foil)?\u201d\nThe foil policy \u03c0f is ultimately obtained by combining a state-action value function QI \u2013 that represents the user\u2019s preference for some actions according to his/her question \u2013 with the learned Qt to obtain Qf ;\nQf (s, a) = Qt(s, a) +QI(s, a), \u2200s, a \u2208 S,A (1)\nEach state-action value is of the form Q : S \u00d7A\u2192 R. QI only values the state-action pairs queried by the user. For instance, the QI of the above given user question can be based on the following reward scheme for all potentially simulated s \u2208 S; \u2022 The action a1f = \u2019Right\u2019 receives a reward such that Qf (s,Right) > Qt(s, \u03c0t(s))\n\u2022 If \u2019RightWall\u2019 \u2208 k(s) \u2022 Then the action a2f = \u2019Up\u2019 receives a reward such that Qf (\u00b7,Up) > Qt(\u00b7, \u03c0t(s)).\nGiven this reward scheme we can train QI and obtain Qf according to equation 1. The state-action values Qf can then be used to obtain the policy \u03c0f using the original action selection mechanism of the agent. This results in a policy that tries to follow the queried policy as best as it can. The advantage of having \u03c0f constructed from Qf is that the agent is allowed to learn a different action then those in the user\u2019s question as long as the reward is higher in the long run (more user defined actions can be performed). Also, it allows for the simulation of the actual expected behavior of the agent as it is still based on the agent\u2019s action selection mechanism. This would both not be the case if we simply forced the agent to do exactly what the user stated.\nThe construction of QI is done through simulation with the help of the transition model T . The rewards that are given during the simulation are selected with Equation 1 in mind, as they need to eventually compensate for the originally learned action based on Qt. Hence, the reward for each state and queried action is as follows;\nRI(si, af ) = \u03bbf \u03bb w(si, st) [R(si, af )\u2212R(si, at] (1 + ) (2) With at = \u03c0t(st) the originally learned action and w being\na distance based weight;\nw(si, st) = e \u2212 ( d(si,st) \u03c3 )2 (3)\nFirst, si with i \u2208 {t, t + 1, ..., t + n} is the i\u2019th state in the simulation starting with st. af is the current foil action governed by the conveyed policy by the user. The fact that af is taken as the only rewarding action each time, greatly reduces the time needed to construct QI . Next, w(si, st) is obtained from a Radial Basis Function (RBF) with a Gaussian kernel and distance function d. This RBF represents the exponential distance between our actual state st and the simulated state si. The Gaussian kernel is governed by the standard deviation \u03c3 and allows us to reduce the effects of QI as we get further from our actual state st. The ratio of discount factors \u03bbf\u03bb allows for the compensation between the discount factor \u03bb of the original agent and the potentially different factor \u03bbf for QI if we wish it to be more shortsighted. Finally, [R(si, af )\u2212R(si, at)] (1 + ) is the amount of reward that af needs such that QI(si, af ) > Q(si, at). With > 0 that determines how much more QI will prefer af over at.\nThe parameter n defines how many future state transitions we simulate and are used to retrieve QI . As a general rule\nn \u2265 3\u03c3 as at this point the Gaussian kernel will reduce the contribution of QI to near zero such that Qf will resemble Qt. Hence, by setting \u03c3 one can vary the number of states the foil policy should starting from st. Also, by setting the strength of how much each af should be preferred over at can be regulated. Finally, \u03bbf defines how shortsighted QI should be. If set to \u03bbf = 0, \u03c0f will force the agent to perform af as long as si is not to distant from st. If set to values near one, \u03c0f is allowed to take different actions as long as it results into more possibilities of performing af ."}, {"heading": "2.3 Generating explanations", "text": "At this point we have the user-interpretable MDP consisting of state concepts C and action outcomes O provided by their respective transformation function k and t. Also, we have a definition of RI that values the actions and/or states that are of interest by the user which can be used to train QI through simulation and obtain Qf according to Equation 1. This provides us with the basis of obtaining the information needed to construct an explanation.\nAs mentioned before, the explanations are based on simulating the effects with T of \u03c0t and that of \u03c0f (if defined by the user). We can call T on the previous state si\u22121 for some action \u03c0(si\u22121 to obtain si and repeat this until i == n. The result is a single sequence or trajectory of visited states and performed actions for any policy \u03c0 starting from st;\n\u03b3(st, \u03c0) = {(s0, a0), ..., (sn, an) | T, \u03c0} (4) If T is probabilistic, multiple simulations with the same policy and starting state may result in different trajectories. To obtain the most probable trajectory \u03b3(st, \u03c0)\u2217 we can take the transition from T with the highest probability. Otherwise a Markov chain could be constructed instead of a single trajectory.\nThe next step is to transform each state and action pair in \u03b3(st, \u03c0)\n\u2217 to the user-interpretable description with the functions k and t;\nPath(st, \u03c0) = {(c0, o0), ..., (cn, on)} , ci = k(si), oi = t(si, ai), (si, ai) \u2208 \u03b3(st, \u03c0)\u2217 (5)\nFrom Path(st, \u03c0t) an explanation can be constructed about the state the agent will most likely visit and the action outcomes it will obtain. For example with the use of the following template;\n\u201dFor the next n actions I will mostly perform a. During these actions, I will come across situations with \u2200c \u2208 Path(st, \u03c0t). This will cause me \u2200o+ \u2208 Path(st, \u03c0t) but also \u2200o\u2212 \u2208 Path(st, \u03c0t)\u201d.\nLet a here be the action most common in \u03b3(st, \u03c0t) and both o+ and o\u2212 the positive and negative action outcomes respectively. Since we have access to the entire simulation of \u03c0f , a wide variety of explanations is possible. For instance we could also focus on the less common actions;\n\u201dFor the next n actions I will perform a1 when in situations with \u2200c \u2208 Path(st, \u03c0t|\u03c0t = a1) and a2 when in situations with \u2200c \u2208 Path(st, \u03c0t|\u03c0t = a2). These actions prevent me from \u2200o+ \u2208 Path(st, \u03c0t) but also \u2200o\u2212 \u2208 Path(st, \u03c0t)\u201d.\nA contrastive explanation given some question from the user that describes the foil policy \u03c0f can be constructed in a similar manner but take the contrast. Given a foil we can focus on the differences between Path(st, \u03c0t) and Path(st, \u03c0f ). This can be obtained by taking the relative complement Path(st, \u03c0t)\\Path(st, \u03c0f ); the set of expected unique consequences when behaving according to \u03c0t and not \u03c0f . A more extensive explanation can be given by taking the symmetric difference Path(st, \u03c0t)4Path(st, \u03c0f ) to explain the unique differences between both policies."}, {"heading": "3 User study", "text": "The above proposed method allows an RL agent to explain and motivate its behavior in terms of expected states and outcomes. It also enables the construction of contrastive explanations where any policy can be compared to the learned policy. This contrastive explanation is based on differences in expected outcomes between the compared policies.\nWe performed a small user study in which 82 participants were shown a number of exemplar explanations about the case shown in figure 2. These explanations addressed either the single next action or the policy. Both explanations can be generated by the above method by adjusting the Radial Basis Function weighting scheme and/or the foil\u2019s discount factor. Also, some example explanations were contrastive with only the second best action or policy, while others provided all consequences. Contrasts were determined using the relative complement between fact and foil. Whether the learned action or policy was treated as the fact or foil, was also systematically manipulated in this study.\nWe presented the developed exemplar explanations in pairs to the participants and asked them to select the explanation that helped them most to understand the agent\u2019s behavior. Afterwards we asked which of the following properties they used to assess their preference: long versus short explanations; explanations with ample information versus little information; explanations addressing actions versus those that address strategies (policies); and explanations addressing shortterm consequences of actions versus explanations that address distant consequences of actions.\nThe results of the preferred factors are shown in Figure 3. This shows that the participants prefer explanations that address strategy and policy, and that provide ample information. We note here that, given the simple case from figure 2, participants may have considered an explanation addressing a single action only as trivial, because the optimal action was, in most cases, already evident to the user."}, {"heading": "4 Conclusion", "text": "We proposed a method for a reinforcement learning (RL) agent to generate explanations for its actions and strategies. The explanations are based on the expected consequences of its policy. These consequences were obtained through simulation according to a (learned) state transition model. Since state features and numerical rewards do not lend themselves easily for an explanation that is informative to humans, we developed a framework that translates states and actions into user-interpretable concepts and outcomes.\nWe also proposed a method for converting the foil, \u2013or policy of interest to the user\u2013, of a contrastive \u2019why\u2019-question about actions into a policy. This policy follows locally the user\u2019s query but gradually transgresses back towards the original learned policy. This policy favors the actions that are of interest to the user such that the agent tries to perform them as best as possible. How much these actions are favored compared to the originally learned action can be set with a single parameter.\nThrough running simulations for a given number steps of both the policy derived from the user\u2019s question and the actually learned policy, we were able to obtain expected consequences of each. From here, we were able to construct contrastive explanations: explanations addressing the consequences of the learned policy and what would be different if the derived policy would have been followed.\nAn online survey pilot study was conducted to explore which of several explanations are most preferred by human users. Results indicate that users prefer explanations about policies rather than about single actions.\nFuture work will focus on implementing the method on complex RL benchmarks to explore the scalability of this approach in realistic cases. This is important given the computational costs of simultaneously simulating the consequences of different policies in large state spaces. Also, we will explore more methods to construct our translation functions from states and actions to concepts and outcomes. A more extensive user study will be carried out to evaluate the instructional value of generated explanations in more detail, and to explore the relationship between explanations and users\u2019 trust in the agent\u2019s performance."}, {"heading": "Acknowledgments", "text": "We would like to thank the reviewers for their time and effort in improving this paper. Also, we are grateful for the funding from the RVO Man Machine Teaming research project that made this research possible."}], "title": "Contrastive explanations for reinforcement learning in terms of expected consequences", "year": 2018}