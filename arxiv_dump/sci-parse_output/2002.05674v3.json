{"abstractText": "Recently we see a rising number of methods in the field of eXplainable Artificial Intelligence. To our surprise, their development is driven by model developers rather than a study of needs for human end users. The analysis of needs, if done, takes the form of an A/B test rather than a study of open questions. To answer the question \u201cWhat would a human operator like to ask the ML model?\u201d we propose a conversational system explaining decisions of the predictive model. In this experiment, we developed a chatbot called dr_ant to talk about machine learning model trained to predict survival odds on Titanic. People can talk with dr_ant about different aspects of the model to understand the rationale behind its predictions. Having collected a corpus of 1000+ dialogues, we analyse the most common types of questions that users would like to ask. To our knowledge, it is the first study which uses a conversational system to collect the needs of human operators from the interactive and iterative dialogue explorations of a predictive model.", "authors": [{"affiliations": [], "name": "P. Biecek"}], "id": "SP:1bc4e84abc3c392626e0c01eb3ef0d9493206ca2", "references": [{"authors": ["V. Arya", "R.K.E. Bellamy", "P.Y. Chen", "A. Dhurandhar", "M. Hind", "S.C. Hoffman", "S. Houde", "Q.V. Liao", "R. Luss", "A. Mojsilovi", "S. Mourad", "P. Pedemonte", "R. Raghavendra", "J. Richards", "P. Sattigeri", "K. Shanmugam", "M. Singh", "K.R. Varshney", "D. Wei", "Y. Zhang"], "title": "One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques", "year": 2019}, {"authors": ["H. Baniecki", "P. Biecek"], "title": "modelStudio: Interactive Studio with Explanations for ML Predictive Models", "venue": "The Journal of Open Source Software (Nov 2019),", "year": 2019}, {"authors": ["P. Biecek"], "title": "DALEX: Explainers for Complex Predictive Models in R", "venue": "Journal of Machine Learning Research 19, 1\u20135", "year": 2018}, {"authors": ["P. Biecek", "T. Burzykowski"], "title": "Explanatory Model Analysis. Explore, Explain and Examine Predictive Models", "year": 2020}, {"authors": ["P. Biecek", "M. Kosinski"], "title": "archivist: An R package for managing, recording and restoring data analysis results", "venue": "Journal of Statistical Software 82(11), 1\u201328", "year": 2017}, {"authors": ["M. El-Assady", "W. Jentner", "R. Kehlbeck", "U. Schlegel", "R. Sevastjanova", "F. Sperrle", "T. Spinner", "D. Keim"], "title": "Towards xai: Structuring the processes of explanations", "year": 2019}, {"authors": ["L.H. Gilpin", "D. Bau", "B.Z. Yuan", "A. Bajwa", "M. Specter", "L. Kagal"], "title": "Explaining explanations: An approach to evaluating interpretability of machine learning", "year": 2018}, {"authors": ["A. Gosiewska", "P. Biecek"], "title": "Do Not Trust Additive Explanations", "venue": "arXiv e-prints", "year": 2019}, {"authors": ["B. Hoover", "H. Strobelt", "S. Gehrmann"], "title": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models", "year": 2019}, {"authors": ["S. Jentzsch", "S. Hhn", "N. Hochgeschwender"], "title": "Conversational interfaces for explainable ai: A human-centred approach", "year": 2019}, {"authors": ["M. Kuzba", "E. Baranowska", "P. Biecek"], "title": "pyCeterisParibus: explaining Machine Learning models with Ceteris Paribus Profiles in Python", "venue": "JOSS 4(37),", "year": 2019}, {"authors": ["I. Lage", "E. Chen", "J. He", "M. Narayanan", "B. Kim", "S. Gershman", "F. Doshi-Velez"], "title": "An evaluation of the human-interpretability of explanation", "venue": "http://arxiv", "year": 1902}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability (2016), http://arxiv.org", "year": 2016}, {"authors": ["S.M. Lundberg", "S.I. Lee"], "title": "A Unified Approach to Interpreting Model Predictions", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["P. Madumal", "T. Miller", "L. Sonenberg", "F. Vetere"], "title": "A grounded interaction protocol for explainable artificial intelligence", "venue": "AAMAS", "year": 2019}, {"authors": ["P. Madumal", "T. Miller", "F. Vetere", "L. Sonenberg"], "title": "Towards a grounded dialog model for explainable artificial intelligence (2018)", "year": 2018}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "What Would You Ask the Machine Learning Model?", "year": 2017}, {"authors": ["T. Miller", "P. Howe", "L. Sonenberg"], "title": "Explainable AI: beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences", "year": 2017}, {"authors": ["C. Molnar", "G. Casalicchio", "B. Bischl"], "title": "Quantifying Interpretability of Arbitrary Machine Learning Models Through Functional Decomposition", "venue": "arXiv e-prints", "year": 2019}, {"authors": ["S.T. Mueller", "R.R. Hoffman", "W.J. Clancey", "A. Emrey", "G. Klein"], "title": "Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI", "venue": "http://arxiv.org/abs/", "year": 1902}, {"authors": ["H. Nori", "S. Jenkins", "P. Koch", "R. Caruana"], "title": "InterpretML: A Unified Framework for Machine Learning Interpretability", "year": 1909}, {"authors": ["F. Pecune", "S. Murali", "V. Tsai", "Y. Matsuyama", "J. Cassell"], "title": "A model of social explanations for a conversational movie recommendation system", "year": 2019}, {"authors": ["R R Core Team"], "title": "A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria", "year": 2019}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why Should I Trust You?: Explaining the Predictions of Any Classifier", "year": 2016}, {"authors": ["M. Ribera", "\u00c0. Lapedriza"], "title": "Can we do better explanations? a proposal of usercentered explainable ai", "venue": "IUI Workshops", "year": 2019}, {"authors": ["A. Rydelek"], "title": "xai2cloud: Deploys An Explainer To The Cloud", "year": 2020}, {"authors": ["T. Scantamburlo", "A. Charlesworth", "N. Cristianini"], "title": "Machine decisions and human consequences", "year": 2018}, {"authors": ["K. Sokol", "P. Flach"], "title": "Conversational Explanations of Machine Learning Predictions Through Class-contrastive Counterfactual Statements", "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI18. pp. 5785\u20135786. International Joint Conferences on Artificial Intelligence Organization", "year": 2018}, {"authors": ["K. Sokol", "P. Flach"], "title": "Glass-box: Explaining ai decisions with counterfactual statements through conversation with a voice-enabled virtual assistant", "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,", "year": 2018}, {"authors": ["K. Sokol", "P. Flach"], "title": "One explanation does not fit all", "venue": "KI - Knstliche Intelligenz", "year": 2020}, {"authors": ["H.F. Tan", "K. Song", "M. Udell", "Y. Sun", "Y. Zhang"], "title": "Why should you trust my interpretation? Understanding uncertainty in LIME predictions", "year": 1904}, {"authors": ["R. Tomsett", "D. Braines", "D. Harborne", "A. Preece", "S. Chakraborty"], "title": "Interpretable to whom? a role-based model for analyzing interpretable machine learning systems", "year": 2018}, {"authors": ["C. Werner"], "title": "Explainable ai through rule-based interactive conversation", "venue": "EDBT/ICDT Workshops", "year": 2020}], "sections": [{"text": "Keywords: eXplainable Artificial Intelligence \u00b7 Iterative dialogue explanations \u00b7 Human-centred Machine Learning"}, {"heading": "1 Introduction", "text": "Machine Learning models are widely adopted in all areas of human life. As they often become critical parts of the automated systems, there is an increasing need for understanding their decisions and ability to interact with such systems. Hence, we are currently seeing the growth of the area of eXplainable Artificial Intelligence (XAI). For instance, Scantamburlo et al. [28] raise an issue of understanding machine decisions and their consequences on the example of computermade decisions in criminal justice. This example touches upon such features as fairness, equality, transparency and accountability. Ribera & Lapedriza [26] identify the following motivations for why to design and use explanations: system\nar X\niv :2\n00 2.\n05 67\n4v 3\n[ cs\n.C Y\n] 3\nverification, including bias detection; improvement of the system (debugging); learning from the system\u2019s distilled knowledge; compliance with legislation, e.g. \u201cRight to explanation\u201d set by EU; inform people affected by AI decisions.\nWe see the rising number of explanation methods, such as LIME [25] and SHAP [15] and XAI frameworks such as AIX360 [2], InterpretML [22], DALEX [4], modelStudio [3], exBERT [10] and many others. These systems require a systematic quality evaluation [8,21,13]. For instance, Tan et al. [32] describe the uncertainty of explanations and Molnar et al. [20] describe a way to quantify the interpretability of the model.\nThese methods and toolboxes are focused on the model developer perspective. Most popular methods like Partial Dependence Plots, LIME or SHAP are tools for a post-hoc model diagnostic rather than tools linked with the needs of end users. But it is important to design an explanation system for its addressee (explainee). Both form and content of the system should be adjusted to the end user. And while explainees might not have the AI expertise, explanations are often constructed by engineers and researchers for themselves [19], therefore limiting its usefulness for the other audience [17].\nAlso, both the form and the content of the explanations should differ depending on the explainee\u2019s background and role in the model lifecycle. Ribera & Lapedriza [26] describe three types of explainees: AI researchers and developers, domain experts and the lay audience. Tomsett et al. [33] introduce six groups: creators, operators, executors, decision-subjects, data-subjects and examiners. These roles are positioned differently in the pipeline. Users differ in the background and the goal of using the explanation system. They vary in the technical skills and the language they use. Finally, explanations should have a comprehensible form \u2013 textual, visual or multimodal. Explanation is a cognitive process and a social interaction [7]. Moreover, interactive exploration of the model allows to personalize the explanations presented to the explainee [31].\nArya et al. identify a space for interactive explanations in a tree-shaped taxonomy of XAI techniques [2]. However, AIX360 framework presented in this paper implements only static explanations. Similarly, most of the other toolkits and methods focus entirely on the static branch of the explanations taxonomy. Sokol & Flach [29] propose conversation using class-contrastive counterfactual statements. This idea is implemented as a conversational system for the credit score systems lay audience [30]. Pecune et al. describe conversational movie recommendation agent explaining its recommendations [23]. A rule-based, interactive and conversational agent for explainable AI is also proposed by Werner [35]. Madumal et al. propose an interaction protocol and identify components of an explanation dialogue [16]. Finally, Miller [18] claims that truly explainable agents will use interactivity and communication.\nTo address these problems we create an open-ended dialog based explanation system. We develop a chatbot allowing the explainee to interact with a predictive model and its explanations. We implement this particular system for the random forest model trained on Titanic dataset [1,5]. However, any model trained on this dataset can be plugged into this system. Also, this approach can be applied successfully to other datasets and much of the components can be reused.\nOur goal is twofold. Firstly, we create a working prototype of a conversational system for XAI. Secondly, we want to discover what questions people ask to understand the model. This exploration is enabled by the open-ended nature of the chatbot. It means that the user might ask any question even if the system is unable to give a satisfying answer for each of them.\nThere are engineering challenges of building a dialogue agent and the \u201cWizard of Oz\u201d proxy approach might be used as an alternative [31,11]. In this work however, we decide to build such a system. With this approach we obtain a working prototype and a scalable dialogue collection process.\nAs a result, we gain a better understanding of how to answer the explanatory needs of a human operator. With this knowledge, we will be able to create explanation systems tailored to explainee\u2019s needs by addressing their questions. It is in contrast to developing new methods blindly or according to the judgement of their developers.\nWe outline the scope and capabilities of a dialogue agent (Section 2). In Section 3, we illustrate the architecture of the entire system and describe each of the components. We also demonstrate the agent\u2019s work on the examples. Finally, in Section 4, we describe the experiment and analyze the collected dialogues."}, {"heading": "2 Dialogue system", "text": "This dialogue system is a multi-turn chatbot with the user initiative. It offers a conversation about the underlying random forest model trained on the wellknown Titanic dataset. We deliberately select a black box model with no direct interpretation together with a dataset and a problem that can be easily imagined for a wider audience. The dialogue system was built to understand and respond to several groups of queries:\n\u2013 Supplying data about the passenger, e.g. specifying age or gender. This step might be omitted by impersonating one of two predefined passengers with different model predictions. \u2013 Inference \u2013 telling users what are their chances of survival. Model imputes missing variables. \u2013 Visual explanations from the Explanatory Model Analysis toolbox [5]: Ceteris Paribus profiles [12] (addressing \u201cwhat-if\u201d questions) and Break Down plots [9] (presenting feature contributions). Note this is to offer a warm start into the system by answering some of the anticipated queries. However, the principal purpose is to explore what other types of questions might be asked. \u2013 Dialogue support queries, such as listing and describing available variables or restarting the conversation.\nThis system was firstly trained with an initial set of training sentences and intents. After the deployment of the chatbot, it was iteratively retrained based on the collected conversations. Those were used in two ways: 1) to add new intents, 2) to extend the training set with the actual user queries, especially those which were misclassified. The final version of the dialogue agent which is used in the experiment at Section 4 consists of 40 intents and 874 training sentences."}, {"heading": "3 Implementation", "text": "Explainee\nBlackbox model\nDialogue Management\nNLU\nNLG\nCeterisParibus\niBreakDown\nChatbot admin\nExplainers\nDialogue agent\nSlack\nWWW\nInterface Query\nResponse\nIntent, entities\nPrediction\nPrediction\nVisual & textual explanations\nContext, response handler\nRetrain agent\nFig. 1. Overview of the system architecture. Explainee uses the system to talk about the blackbox model. They interact with the system using one of the interfaces. The conversation is managed by the dialogue agent which is created and trained by the chatbot admin. To create a response system queries the blackbox model for its predictions and explainers for visual explanations.\nA top-level chatbot architecture is depicted in Figure 1. The system consists of several components:\n1. Explainee Human operator \u2013 addressee of the system. They chat about the blackbox model and its predictions. 2. Interface This dialogue agent might be deployed to various conversational platforms independently of the backend and each other. The only exception to that is rendering some of the graphical, rich messages. We used a custom web integration as a major surface. It communicates with the dialogue agent\u2019s engine sending requests with user queries and receiving text and graphical content. The frontend of the chatbot uses Vue.js and is based on dialogflow3 repos-\n3 https://github.com/mishushakov/dialogflow-web-v2\nitory. It provides a chat interface and renders rich messages, such as plots and suggestion buttons. This integration allows to have a voice conversation using the browser\u2019s speech recognition and speech synthesis capabilities. 3. Dialogue agent Chatbot\u2019s engine implemented using Dialogflow framework and Node.js fulfilment code run on Google Cloud Functions. \u2013 Natural Language Understanding (NLU)\nThe Natural Language Understanding component classifies query intent and extracts entities. This classifier uses the framework\u2019s builtin rulebased and Machine Learning algorithms. NLU module recognizes 40 intents such as posing a what-if question, asking about a variable or specifying its value. It was trained on 874 training sentences. Some of these sentences come from the initial subset of the collected conversations. Additionally, NLU module comes with 4 entities \u2013 one for capturing the name of the variable and 3 to extract values of the categorical variables \u2013 gender, class and the place of embarkment. For numerical features, a builtin numerical entity is utilized. See examples in Section 3.1. \u2013 Dialogue management It implements the state and context. Former is used to store the passenger\u2019s data and the latter to condition response on more than the last query. For example, when the user sends a query with a number it might be classified as age or fare specification depending on the current context. \u2013 Natural-language generation (NLG) Response generation system. To build a chatbot\u2019s utterance the dialogue agent might need to use the explanations or the predictions. For this, the NLG component will query explainers or the model correspondingly. Plots, images and suggestion buttons which are part of the chatbot response are rendered as rich messages on the front end. 4. Blackbox model A random forest model was trained to predict the chance of survival on Titanic4. The model was trained in R [24] and converted into REST api with the plumber package [34]. The random forest model was trained with default hyperparameters. Data preprocessing includes imputation of missing values. The performance of the model on the test dataset was AUC 0.84 and F1 score 0.73. 5. Explainers REST API exposing visual and textual model explanations from iBreakDown [9] and CeterisParibus [12] libraries. They explore the blackbox model to create an explanation. See the xai2cloud package [27] for more details. 6. Chatbot admin Human operator \u2013 developer of the system. They can manually retrain the system based on misclassified intents and misextracted entities. For instance, this dialogue agent was iteratively retrained based on the initial subset of the collected dialogues.\n4 You can download the model from the archivist [6] database with a following hook: archivist::aread(\"pbiecek/models/42d51\").\nThis architecture works for any predictive model and tabular data. Its components differ in how they can be transferred for other tasks and datasets5. The user interface is independent of the rest of the system. When a dataset is fixed, the model is interchangeable. However, the dialogue agent is handcrafted and depends on the dataset as well as explainers. Change in a dataset needs to be at least reflected in an update of the data-specific entities and intents. For instance, a new set of variables needs to be covered. It is also followed by modifying the training sentences for the NLU module and perhaps some changes in the generated utterances. Adding a new explainer might require adding a new intent. Usually, we want to capture the user queries, that can be addressed with a new explanation method."}, {"heading": "3.1 NLU examples", "text": "Natural-language understanding module is designed to guess an intent and extract relevant parameters/entities from a user query. Queries can be specified in a open format. Here are examples of NLU for three intents. Query: What If I had been older? Intent: ceteris paribus Entities: [variable: age]\nQuery I\u2019m 20 year old woman Intent: multi slot filling Entities: [age: 20, gender: female]\nQuery: Which feature is the most important? Intent: break down Entities: []\n5 The source code is available at https://github.com/ModelOriented/xaibot."}, {"heading": "3.2 Example dialogue", "text": "An excerpt from an example conversation is presented in Figure 2. The corresponding intent classification flow is highlighted in Figure 3."}, {"heading": "4 Results", "text": "The initial subset of the collected dialogues is used to improve the NLU module of the dialogue agent. As a next step, we conduct an experiment by sharing the chatbot in the Data Science community and analyzing the collected dialogues."}, {"heading": "4.1 Experiment setup", "text": "For this experiment, we work on data collected throughout 2 weeks. This is a subset of all collected dialogues, separate from the data used to train the NLU module. Narrowing the time scope of the experiment allows to describe the audience and ensure the coherence of the data. As a next step, we filter out conversations with totally irrelevant content and those with less than 3 user queries. Finally, we obtain 621 dialogues consisting of 5675 user queries in total. The average length equals 9.14, maximum 83 and median 7 queries. We see the histogram of conversations length in Figure 4. Note that by conversation length\nwe mean the number of user queries which is equal to the number of turns in the dialogue (user query, chatbot response).\nThe audience acquisition comes mostly from R and Data Science community. Users are instructed to explore the model and its explanations individually. However, they might come across a demonstration of the chatbot\u2019s capabilities potentially introducing a source of bias.\nWe describe the results of the study in the section 4.2 and we share the statistical details about the experiment audience in the section 4.3."}, {"heading": "4.2 Query types", "text": "We analyze the content of the dialogues. Similar user queries, when different only in the formulation, are manually grouped together. For each category, we calculate the number of conversations with at least one query of this type. Numbers of occurrences are presented in Table 1.\nNote that users were not prompted or hinted to ask any of these with an exception of the \u201cwhat do you know about me\u201d question. Moreover, the taxonomy defined here is independent of the intents recognized by the NLU module and is defined based on collected dialogues.\nHere is the list of the query types ordered decreasingly by the number of conversation they occur in.\n1. why \u2013 general explanation queries, typical examples of such are:\n\u2013 \u201cwhy?\u201d \u2013 \u201cexplain it to me\u201d\n\u2013 \u201chow was this calculated?\u201d \u2013 \u201cwhy is my chance so low?\u201d\n2. what-if \u2013 alternative scenario queries. Frequent examples: what if I\u2019m older?, what if I travelled in the 1st class?. Rarely, we see multi-variable questions such as: What if I\u2019m older and travel in a different class?. 3. what do you know about me \u2013 this is the only query hinted to the user using the suggestion button. When the user inputs their data manually it usually serves to understand what is yet missing. However, in the scenario when the explainee impersonates a movie character it also aids understanding which information about the user is possessed by the system. 4. EDA \u2013 a general category on Exploratory Data Analysis. All questions related to data rather than the model fall into this category. For instance, feature distribution, maximum values, plot histogram for the variable v, describe/summarize the data, is dataset imbalanced, how many women survived, dataset size etc. 5. feature importance \u2013 here we group all questions about the relevance, influence, importance or effect of the feature on the prediction. We see several subtypes of that query: \u2013 Which are the most important variable(s)? \u2013 Does gender influence the survival chance? \u2013 local importance \u2013 How does age influence my survival, What makes\nme more likely to survive? \u2013 global importance \u2013 How does age influence survival across all pas-\nsengers? 6. how to improve \u2013 actionable queries for maximizing the prediction, e.g.\nwhat should I do to survive, how can I increase my chances. 7. class comparison \u2013 comparison of the predictions across different values of\nthe categorical variable. It might be seen as a variant of the what-if question. Examples: which class has the highest survival chance, are men more likely to die than women. 8. who has the best score \u2013 here, we ask about the observations that maximize/minimize the prediction. Examples: who survived/died, who is most likely to survive. It is similar to how to improve question, but rather on a per example basis. 9. model-related \u2013 these are the queries related directly to the model, rather than its predictions. We see questions about the algorithm and the code. We also see users asking about metrics (accuracy, AUC), confusion matrix and confidence. However, these are observed just a few times. 10. contrastive \u2013 question about why predictions for two observations are different. We see it very rarely. However, more often we observe the implicit comparison as a follow-up question \u2013 for instance, what about other passengers, what about Jack. 11. plot interaction \u2013 follow-up queries to interact with the displayed visual content. Not observed. 12. similar observations \u2013 queries regarding \u201cneighbouring\u201d observations. For instance, what about people similar to me. Not observed.\nWe also see users creating alternative scenarios and comparing predictions for different observations manually, i.e. asking for prediction multiple times with different passenger information. Additionally, we observe explainees asking about other sensitive features, that are not included in the model, e.g. nationality, race or income. However, some of these, e.g. income, are strongly correlated with class and fare."}, {"heading": "4.3 Statistics of surveyed sample", "text": "We use Google Analytics to get insights into the audience of the experiment. Users are distributed across 59 countries with the top five (Poland, United States, United Kingdom, Germany and India, in this order) accounting for 63% of the users. Figure 5 presents demographics data on the subset of the audience (53%) for which this information is available."}, {"heading": "5 Conclusions and Future Work", "text": "Depending on the area of application, different needs are linked with the concept of interpretability [14,33]. And even for a single area of application, different actors may have different needs related to model interpretability [2].\nIn this paper, we presented a novel application of the dialogue system for conversational explanations of a predictive model. Detailed contributions are following (1) we presented a process based on a dialogue system allowing for effective collection of user expectations related to model interpretation, (2) we presented a xai-bot implementation for a binary classification model for Titanic data, (3) we conducted an analysis of the collected dialogues.\nWe conduct this experiment on the survival model for Titanic. However, our prior goal of this work is to understand user needs related to the model explanation, rather than improve this specific implementation. The knowledge we gain from this experiment will aid in designing the explanations for various models trained on tabular data. One example might be survival models for COVID-19 which are currently under large interest.\nConversational agent proved to work as a tool to explore and extract user needs related to the use of the Machine Learning models. This method allowed us to validate hypotheses and gather requirements for the XAI system on the example from the experiment. In this analysis, we identified several frequent patterns among user queries.\nConversational agent is also a promising, novel approach to XAI as a modelhuman interface. Users were given a tool for the interactive explanation of the model\u2019s predictions. In the future, such systems might be useful in bridging the gap between automated systems and their end users. An interesting and natural extension of this work would be to compare user queries for different explainee\u2019s groups in the system, e.g. model creators, operators, examiners and decisionsubjects. In particular, it would be interesting to collect needs from explainees with no domain knowledge in Machine Learning. Similarly, it is interesting to take advantage of the process introduced in this work to compare user needs across various areas of applications, e.g. legal, medical and financial. Additionally, based on the analysis of the collected dialogues we see two related areas that would benefit from the conversational human-model interaction \u2013 Exploratory Data Analysis and model fairness based on the queries about the sensitive and bias-prone features."}, {"heading": "Acknowledgments", "text": "We would like to thank 3 anonymous reviewers for their insightful comments and suggestions. Micha l Kuz\u0301ba was financially supported by the NCN Opus grant 2016/21/B/ST6/0217."}], "title": "What Would You Ask the Machine Learning Model? Identification of User Needs for Model Explanations Based on Human-Model Conversations", "year": 2020}