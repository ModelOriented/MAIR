{"abstractText": "Generative Adversarial Networks (GANs) became very popular for generation of realistically looking images. In this paper, we propose to use GANs to synthesize artificial financial data for research and benchmarking purposes. We test this approach on three American Express datasets, and show that properly trained GANs can replicate these datasets with high fidelity. For our experiments, we define a novel type of GAN, and suggest methods for data preprocessing that allow good training and testing performance of GANs. We also discuss methods for evaluating the quality of generated data, and their comparison with the original real data.", "authors": [{"affiliations": [], "name": "Dmitry Efimov"}, {"affiliations": [], "name": "Alexey Nefedov"}], "id": "SP:98f9b2329931223f4603786b029e780eedaa91bc", "references": [{"authors": ["J Deng"], "title": "ImageNet: A Large-Scale Hierarchical Image Database", "year": 2009}, {"authors": ["D. Dua", "E. Karra Taniskidou"], "title": "UCI Machine Learning Repository archive.ics.uci.edu/ml, University of California, Irvine, School of Information and Computer Sciences", "year": 2017}, {"authors": ["Dal Pozzolo", "Andrea"], "title": "Calibrating probability with undersampling for unbalanced classification", "year": 2015}, {"authors": ["Yeh", "I-Cheng", "Che-hui Lien"], "title": "The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients", "venue": "Expert Systems with Applications", "year": 2009}, {"authors": ["Goodfellow", "I.J"], "title": "Generative adversarial nets. NIPS\u201914", "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems,", "year": 2014}, {"authors": ["L. Perez", "J. Wang"], "title": "The Effectiveness of Data Augmentation in Image Classification using Deep Learning", "year": 2017}, {"authors": ["Shin", "H.-C"], "title": "Medical Image Synthesis for Data Augmentation and Anonymization", "venue": "Using Generative Adversarial Networks. arXiv.org,", "year": 2018}, {"authors": ["X Zhu"], "title": "Data Augmentation in Emotion Classification", "venue": "Using Generative Adversarial Networks. arXiv.org,", "year": 2017}, {"authors": ["P Zheng"], "title": "One-Class Adversarial Nets for Fraud Detection", "year": 2018}, {"authors": ["U Fiore"], "title": "Using generative adversarial networks for improving classification effectiveness in credit card fraud detection", "venue": "Information Sciences,", "year": 2017}, {"authors": ["A Kumar"], "title": "eCommerceGAN : A Generative Adversarial Network for E-commerce", "year": 2018}, {"authors": ["M. Mirza", "S. Osindero"], "title": "Conditional generative adversarial nets. arXiv.org", "year": 2014}, {"authors": ["G. Box", "Cox D"], "title": "An analysis of transformations", "venue": "Journal of the Royal Statistical Society, Series B", "year": 1964}, {"authors": ["D Love"], "title": "An Automated System for Data Attribute Anomaly Detection", "venue": "Proceedings of the KDD 2017: Workshop on Anomaly Detection in Finance,", "year": 2018}, {"authors": ["L. Maaten", "G. Hinton"], "title": "Visualizing Data using t-SNE", "venue": "Journal of Machine Learning Research,", "year": 2008}, {"authors": ["D.M.W. Powers"], "title": "Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness and Correlation", "venue": "Journal of Machine Learning Technologies,", "year": 2011}], "sections": [{"heading": "1 Introduction", "text": "Machine learning (ML) algorithms are ubiquitous in many practical domains, including the banking and financial industry, where some of their core applications are assessing creditworthiness of customers, offering customers optimal financial products, and identifying fraud. Measuring performance of these algorithms is critical to understanding their strengths and weaknesses. Comprehensive comparisons of algorithms require high quality datasets that can be used as standard benchmarks.\nAn integral part of the modern machine learning field is a corpus of publicly shared, high quality datasets from numerous domains, which are used to benchmark and validate ML algorithms developed by various researchers, academic groups and companies (see, for example, MNIST [1], ImageNet [2], Kaggle [3], University of Irvine Machine Learning Repository [4], etc. [5]). These datasets also play\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :2\n00 2.\n02 27\nan important role in advancing state of the field by greatly facilitating testing and development of novel ideas and methods.\nIn the long list of publicly available datasets, one can notice the shortage of datasets originating from the banking and financial industry, especially datasets associated with credit and fraud risk management operations. One of the key reasons for this is that making such data publicly available would be a violation of customers\u2019 privacy and trust.\nFor several available financial datasets (e.g. [6], [7]), this issue is resolved by some pre-treatment of data, for example by reducing data to principal components, or by taking only a small sample of data [8], [9]. However, these transformed or subsampled datasets may fail to capture some unique properties of financial data. For example, transactional data is usually large, highly structured, contains a wide range of both numerical (continuous and discrete) and categorical variables, variables with very skewed distributions, missing values, etc. Practical algorithms should be able to deal with such variables in a robust and efficient manner, and be scalable to the large size of this data. To develop state of the art ML methods, including methods for anomaly detection and model interpretation, ML researchers and practitioners need to have access to data that is as close to the real one as possible.\nIn this paper, we suggest using Generative Adversarial Networks (GANs) as a means to create synthetic, \u201cartificial\u201d financial data. We describe experiments with three American Express datasets used for risk modeling. We show that properly trained GANs can replicate these datasets with high fidelity. Specifically, we show that synthesized data follows the same distribution as the original data, and that ML models trained on synthesized data have the same performance as those trained on the original data. In our experiments, we use a novel type of GAN architecture combining conditional GAN and DRAGAN, which gives us better training convergence and testing performance.\nSince GAN-generated data does not originate from real customers, it could be made public as a benchmark dataset for financial applications. Customers\u2019 personal data and privacy would be protected using this approach.\nThis paper is structured as follows. Section 2 gives a brief introduction to GANs and introduces a novel type of GAN that we used in our experiments. In Section 3, we discuss data preprocessing methods that can help to improve training and testing performance of GAN. In Section 4, we discuss methods for evaluation of the generated datasets and illustrate that any improvements made using the generated datasets scale and generalize to the real dataset. We conclude by discussing future steps in our research and other potential applications of GANs in the financial industry."}, {"heading": "2 Data Generation with Generative Adversarial Networks", "text": "Generative Adversarial Network (GAN) is a type of neural network comprised of two connected networks, called generator and discriminator, which compete with each other during the training phase ([10]). The objective of the discriminator is to distinguish samples coming from a given training set (\u201creal data\u201d) from samples created by generator (\u201csynthesized data\u201d). The error of discriminator is fed to the generator, which learns to produce samples that are increasingly difficult for discriminator to distinguish from real ones.\nWhen training of GAN is complete, its generator can be used to synthesize data reproducing original, real data that was used for training. Since its introduction in 2014, GANs have mostly been used to generate realistically looking images in image classification and computer vision applications [11], [12], [13]. However, there has been an increasing number of GAN applications to non-image data, where the goal of GANs can be, for example, enhancing real training data with synthetic samples [14], [15], [16].\nIn our experiments, the primary goal of using GANs was to generate synthetic data that replicate distribution of original real data, and allow us to build ML models that perform on par with ML models built using original real data. We used three American Express datasets from three different use cases in risk modeling. Basic details of these datasets are given in Table 1. The datasets were chosen to represent some variety of data features, feature distributions, and type of classification problems. We omit further details about our use cases; however, they are not relevant in the scope of this paper.\nIn the following paragraphs, we are going to give basic definitions related to GANs, and introduce a new type of GAN that we used in our experiments.\nIn a classic (vanilla) GAN, generator network G takes a random noise vector z as input, and produces fake sample G\u03d5(z), where \u03d5 is a set of generator\u2019s parameters (Fig. 1 (left)). Discriminator network D takes in a sample x, which can be either real or fake, and computes probability D\u03b8(x), where \u03b8 is a set of discriminator\u2019s parameters. The goal of discriminator is to distinguish between real and fake (generated) samples:\nD\u03b8(x) = { 1, if x is a real sample, 0, if x is a generated sample (1)\nGAN training is performed by sequential minimization of discriminator\u2019s loss function J (D)(\u03d5, \u03b8) with respect to parameters \u03b8, where J (D)(\u03d5, \u03b8) is defined as\nJ (D)(\u03d5, \u03b8) = \u2212Ex logD\u03b8(x)\u2212 Ez log(1\u2212D\u03b8(G\u03d5(z))), (2)\nand minimization of generator\u2019s loss function J (G)(\u03d5, \u03b8) with respect to parameters \u03d5, where J (G)(\u03d5, \u03b8) is defined as\nJ (G)(\u03d5, \u03b8) = \u2212Ez logD\u03b8(G\u03d5(z)). (3)\nTraining of vanilla GAN may be unstable due to gradient exploding or gradient vanishing effects [17]. These effects can be even stronger in applications with non-image, structured data.\nTo overcome this problem, Deep Regret Analytic Generative Adversarial Networks (DRAGANs) were introduced by Kodali et al. in [18]. In DRAGAN, discriminator\u2019s loss is modified by adding a regularization term that prevents sharp gradients and improves convergence:\nJ (D) \u2217 (\u03d5, \u03b8) = J (D)(\u03d5, \u03b8) + \u03bb \u00b7 Ex,\u03b4\u223cNd(0,cI) [||\u2207xD\u03b8(x+ \u03b4)|| \u2212 k] 2 , (4)\nwhere \u03bb, c and k are hyperparameters. Fig. 2 shows the difference in convergence of regular GAN and DRAGAN on our data; the convergence of DRAGAN is much more stable.\nConditional GANs (CGANs) were introduced by Mirza et al. in [19] to better handle categorical features in training data. In CGAN, generator\u2019s input contains two parts: a random noise vector z and a dummy features vector y generated from categorical features (Fig. 1 (right)). Generator\u2019s output consists of vector x of numerical features only. Before feeding to the discriminator, vectors y and x are concatenated and equations (2) and (3) are modified as\nJ (G)(\u03d5, \u03b8) = \u2212Ez logD\u03b8(G\u03d5(z,y),y) (5)\nand J (D)(\u03d5, \u03b8) = \u2212Ex,y logD\u03b8(x,y)\u2212 Ez log(1\u2212D\u03b8(G\u03d5(z,y),y)) (6)\naccordingly.\nSince our data contained categorical features, we decided to use a combination of DRAGAN with CGAN architecture, which we called conditional DRAGAN (CDRAGAN). In CDRAGAN, we add regularization term from (4) to the discriminator loss (6) in order to avoid exploding gradients. Just like in non-conditional case, convergence of CDRAGAN has better stability than convergence of CGAN (Fig. 3)."}, {"heading": "3 Preprocessing Data for GAN Training", "text": "Preprocessing of data used in GAN training is one of the crucial steps that determines success or failure of building a GAN that can accurately reproduce original data. In the domain of computer\nvision, where GANs have been used the most since their introduction, all data features (image pixel values) are numerical, continuous, with similar range and distribution. In many other domains, however, data does not have these nice properties. In particular, financial data is usually comprised of features that have\n\u2022 different types (numerical continuous, numerical discrete, categorical) \u2022 different ranges \u2022 different distributions (including very skewed ones, where the most frequent value occurs in\nmore than 90% of samples) \u2022 missing values (including variables with up to 90% of missing values) \u2022 special values used to denote missing values, cap feature range on the left or on the right,\netc.\nThese data properties pose unique challenges for GAN training, which should be addressed to ensure good training and testing performance of GAN.\nFor our datasets, introduced in Section 2, we found that the following preprocessing steps allowed good training and subsequently good testing performance of GANs.\n1. One-hot encoding for categorical features. 2. Missing value indicator feature: a new feature that is equal to one in samples where the\noriginal feature has missing value, and zero \u2013 in all other samples. 3. Box-Cox transformation [20]. 4. Standard scaling or min-max scaling. 5. Imputing missing values."}, {"heading": "4 Evaluating data generated by GANs", "text": "When GANs are used to generate images, the quality of produced samples can be easily evaluated by visual observation: humans can easily determine if generated images look similar to the images from the training set, and are overall of good quality. For non-image data, there are no commonly accepted techniques for evaluating the quality of generated data.\nWe can start with checking whether histograms of generated features match those of real features. However, matching distributions of individual features and target variable do not guarantee that all existing interactions (relationships) between features, as well as between features and target variable were also replicated by the generator. Therefore, it is necessary to evaluate and compare overall distribution of generated and real data. Since relationship between features and target variable is the most important one for developing ML models, we can perform a separate test of this relationship by comparing supervised ML models trained on generated and real data. ML models with similar performance would be indicative of good replication of dependencies between features and target variable.\nTo summarize, we propose that a good generator should produce data that satisfy the following three criteria:\n1. Distributions of individual features in generated data match those in real data\n2. Overall distributions of generated and real data match each other\n3. Relationship between features and the target variable in real data is replicated in generated data\nIn the following subsections, we will show how we performed above tests for the data generated by our GANs."}, {"heading": "4.1 DataQC tool", "text": "DataQC [21] is an internal automated tool developed at American Express for data quality assessment. It allows users to evaluate similarities and differences between two provided datasets. The tool performs a comprehensive set of data quality tests to quickly highlight how one dataset is different from another. These tests include comparison of feature means, rates of missing values, uni- and multivariate distributions, and extreme values. The tool produces detailed findings and quantitive scores for all the tests that it runs.\nWe used DataQC to evaluate similarity between GAN-generated and real data. In particular, we used it to compare distributions of individual features in generated and real data. As an example, Fig. 4 shows histograms of four selected features in real and generated data for a GAN trained on Dataset C. For all other features we observed similar, very close match between real and generated distributions. This was the case for all three Datasets. We noted that our GANs were able to reproduce discrete distributions just as good as continues ones, and that for some continuous variables GANs tended to produce slightly smoothed versions of their distributions. We found that smoothing of distributions can be reduced by increasing the number of hidden layers in the generator, or by increasing the number of training iterations."}, {"heading": "4.2 Visualization with t-SNE algorithm", "text": "To compare overall distribution of real and generated data, we visualized the data using t-SNE algorithm [22]. t-SNE is a transductive algorithm: the model produced by this algorithm cannot be applied to out-of-sample data that was not used to build the model. Therefore, we combined the real and generated data together, obtained t-SNE representation for the combined data, and then split it into two parts corresponding to real and generated data. Fig. 5 shows t-SNE graphs for our experiment with Dataset C. It can be seen in the figure that t-SNE graph for generated data closely matches t-SNE graph for real data, reproducing most of its clusters and gaps with pretty good accuracy in shape and density."}, {"heading": "4.3 Supervised models trained on real and generated data", "text": "To test relationship between features and target variable in real and generated data, we compared two supervised ML models: one trained on real data, and another one trained on data produced by GAN (containing the same number of samples as the real data). To train the first model, we used the actual target variable from the real data. To train the second model, we used target variable generated by GAN along with other features. Both ML models used identical hyperparameters during the training phase. Diagram 1 illustrates the idea of this approach.\nTrained supervised models were validated on an out-of-sample real data with actual values of target variable. Area under the ROC curve (AUC, [23]) was used to compare performance of the two models. AUC scores were calculated for ground truth target values and predictions obtained from the models trained on real and generated datasets; the final AUC scores are presented in Table. 2. We found\nthese scores to be close enough to assume that our GAN model replicated the relationship between the target variables and the features with good accuracy.\nTraining data GAN Synthetic data\nSupervised algorithm Trained model 1\nTrained model 2\nOut-of-sample validation data Performance on real data\nPerformance on synthetic data\nDiagram 1. Supervised model test overview."}, {"heading": "5 Results and conclusions", "text": "The final results are presented in Table 2. The models trained on synthetic data show slightly worse performances on out-of-sample validation data but the scores are very close. Generated data can be considered as good approximation of the original data though the performance of the models trained on the original data is still better than on synthetic data. Overall, we conclude that GANs can learn and accurately reproduce intricate features distribution and relationships between features of real modeling data.\nWe will continue our research in the following two directions. First, we would like to explore possible causes for lower performance of the models trained on synthetic data. We will also investigate different preprocessing approaches and GAN architectures in order to compare, and see if the baseline performance can be achieved or improved on pure GAN-generated data. We will also work on theoretical justification of the proposed approach, e.g. addressing the following question: given two datasets original and synthetic, what are the sufficient criteria for synthetic data to produce the model with the same performance on original data?"}], "title": "Using Generative Adversarial Networks to Synthesize Artificial Financial Datasets", "year": 2020}