{"authors": [{"affiliations": [], "name": "DINO PEDRESCHI"}, {"affiliations": [], "name": "FOSCA GIANNOTTI"}, {"affiliations": [], "name": "RICCARDO GUIDOTTI"}, {"affiliations": [], "name": "Dino Pedreschi"}, {"affiliations": [], "name": "Fosca Giannotti"}, {"affiliations": [], "name": "Riccardo Guidotti"}, {"affiliations": [], "name": "Anna Monreale"}, {"affiliations": [], "name": "Luca Pappalardo"}, {"affiliations": [], "name": "Salvatore Ruggieri"}], "id": "SP:f033e9d9f81bc3d2d3d53b7749ccce16f4d358ae", "references": [{"authors": ["Robert Andrews", "Joachim Diederich", "Alan B. Tickle"], "title": "Survey and critique of techniques for extracting rules from trained artificial neural networks", "venue": "Knowl.-Based Syst. 8,", "year": 1995}, {"authors": ["M. Gethsiyal Augasta", "T. Kathirvalavakumar"], "title": "Reverse Engineering the Neural Networks for Rule Extraction in Classification Problems", "venue": "Neural Processing Letters 35,", "year": 2012}, {"authors": ["Solon Barocas", "Andrew D. Selbst"], "title": "Big Data\u2019s Disparate Impact", "venue": "California Law Review", "year": 2016}, {"authors": ["Francesco Bonchi", "Sara Hajian", "Bud Mishra", "Daniele Ramazzotti"], "title": "Exposing the probabilistic causal structure of discrimination", "venue": "International Journal of Data Science and Analytics", "year": 2017}, {"authors": ["Aylin Caliskan-Islam", "Joanna J Bryson", "Arvind Narayanan"], "title": "Semantics derived automatically from language corpora necessarily contain human biases", "year": 2016}, {"authors": ["Giulio Caravagna", "Alex Graudenzi", "Daniele Ramazzotti", "Rebeca Sanz-Pamplona", "Luca De Sano", "Giancarlo Mauri", "Victor Moreno", "Marco Antoniotti", "Bud Mishra"], "title": "Algorithmic methods to infer the evolutionary trajectories in cancer progression", "venue": "Proceedings of the National Academy of Sciences 113,", "year": 2016}, {"authors": ["Carolyn Carter", "Elizabeth Renuart", "Margot Saunders", "Chi Chi Wu"], "title": "The Credit Card Market and Regulation", "venue": "In Need of Repair. NC Banking Inst", "year": 2006}, {"authors": ["Mark Craven", "Jude W. Shavlik"], "title": "Extracting Tree-Structured Representations of Trained Networks", "venue": "In NIPS. MIT Press,", "year": 1995}, {"authors": ["David Danks", "Alex John London"], "title": "Regulating Autonomous Systems: Beyond Standards", "venue": "IEEE Intelligent Systems 32,", "year": 2017}, {"authors": ["Pedro Domingos"], "title": "Knowledge discovery via multiple models", "venue": "Intelligent Data Analysis", "year": 1998}, {"authors": ["Pavlos Eirinakis", "Salvatore Ruggieri", "K Subramani", "Piotr Wojciechowski"], "title": "A complexity perspective on entailment of parameterized linear constraints", "venue": "Constraints 17,", "year": 2012}, {"authors": ["Paolo Ferragina", "Ugo Scaiella"], "title": "Fast and Accurate Annotation of Short Texts with Wikipedia Pages", "venue": "IEEE Software 29,", "year": 2012}, {"authors": ["J.L. Fleiss", "B. Levin", "M.C. Paik"], "title": "Statistical Methods for Rates and Proportions", "year": 2003}, {"authors": ["S.R. Foster"], "title": "Causation in antidiscrimination law: Beyond intent versus impact", "venue": "Houston Law Review 41,", "year": 2004}, {"authors": ["Glenn Fung", "Sathyakama Sandilya", "R. Bharat Rao"], "title": "Rule extraction from linear support vector machines", "venue": "In KDD", "year": 2005}, {"authors": ["Johannes F\u00fcrnkranz", "Tom\u00e1s Kliegr"], "title": "A Brief Overview of Rule Learning. In Rule Technologies: Foundations, Tools, and Applications - 9th International Symposium, RuleML", "year": 2015}, {"authors": ["Sainyam Galhotra", "Yuriy Brun", "Alexandra Meliou"], "title": "Fairness testing: testing software for discrimination", "venue": "In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,", "year": 2017}, {"authors": ["Bryce Goodman", "Seth Flaxman"], "title": "EU regulations on algorithmic decision-making and a \u201cright to explanation", "venue": "In ICML workshop on Human Interpretability in Machine Learning (WHI 2016),", "year": 2016}, {"authors": ["Riccardo Guidotti", "Anna Monreale", "Franco Turini", "Dino Pedreschi", "Fosca Giannotti"], "title": "A Survey Of Methods For Explaining Black Box Models", "year": 2018}, {"authors": ["Satoshi Hara", "Kohei Hayashi"], "title": "Making tree ensembles interpretable", "venue": "arXiv preprint arXiv:1606.05390", "year": 2016}, {"authors": ["Andreas Henelius", "Kai Puolam\u00e4ki", "Henrik Bostr\u00f6m", "Lars Asker", "Panagiotis Papapetrou"], "title": "A peek into the black box: exploring classifiers by randomization. Data mining and knowledge discovery", "year": 2014}, {"authors": ["John K.C. Kingston"], "title": "Artificial Intelligence and Legal Liability", "venue": "In SGAI Conf. Springer,", "year": 2016}, {"authors": ["Samantha Kleinberg", "Bud Mishra"], "title": "The temporal logic of causal structures", "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence", "year": 2009}, {"authors": ["R Krishnan", "G Sivakumar", "P Bhattacharya"], "title": "Extracting decision trees from trained neural networks", "venue": "Pattern recognition 32,", "year": 1999}, {"authors": ["Himabindu Lakkaraju", "Stephen H. Bach", "Jure Leskovec"], "title": "Interpretable Decision Sets: A Joint Framework for Description and Prediction", "venue": "In KDD", "year": 2016}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke"], "title": "Intelligible models for classification and regression", "venue": "In KDD", "year": 2012}, {"authors": ["Stella Lowry", "Gordon Macpherson"], "title": "A blot on the profession", "venue": "British medical journal (Clinical research ed.)", "year": 1988}, {"authors": ["Gianclaudio Malgieri", "Giovanni Comand\u00e9"], "title": "Why a Right to Legibility of Automated Decision-Making Exists in the General Data Protection Regulation", "venue": "International Data Privacy Law 7,", "year": 2017}, {"authors": ["Dmitry M Malioutov", "Kush R Varshney", "Amin Emad", "Sanjeeb Dash"], "title": "Learning Interpretable Classification Rules with Boolean Compressed Sensing", "venue": "In Transparent Data Mining for Big and Small Data", "year": 2017}, {"authors": ["Frank Pasquale"], "title": "The black box society: The secret algorithms that control money and information", "year": 2015}, {"authors": ["Dino Pedreschi", "Salvatore Ruggieri", "Franco Turini"], "title": "Discrimination-aware data mining", "venue": "In KDD", "year": 2008}, {"authors": ["Jonas Peters", "Dominik Janzing", "Bernhard Sch\u00f6lkopf"], "title": "Elements of causal inference: foundations and learning algorithms", "year": 2017}, {"authors": ["Bilal Qureshi", "Faisal Kamiran", "Asim Karim", "Salvatore Ruggieri"], "title": "Causal Discrimination Discovery Through Propensity Score Analysis", "venue": "arXiv preprint arXiv:1608.03735", "year": 2016}, {"authors": ["Marco T\u00falio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "venue": "In KDD. ACM,", "year": 2016}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "In AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["Salvatore Ruggieri"], "title": "Learning from polyhedral sets", "year": 2013}, {"authors": ["Salvatore Ruggieri"], "title": "Learning from Polyhedral Sets", "venue": "IJCAI", "year": 2013}, {"authors": ["Salvatore Ruggieri", "Dino Pedreschi", "Franco Turini"], "title": "Data mining for discrimination discovery", "venue": "TKDD 4,", "year": 2010}, {"authors": ["Hui Fen Tan", "Giles Hooker", "Martin T Wells"], "title": "2016. Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable", "year": 2016}, {"authors": ["SandraWachter", "Brent Mittelstadt", "Luciano Floridi"], "title": "Why a right to explanation of automated decision-making does not exist in the general data protection regulation", "venue": "International Data Privacy Law 7,", "year": 2017}, {"authors": ["Sandra Wachter", "Brent Mittelstadt", "Chris Russell"], "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR", "venue": "Harvard Journal of Law & Technology,", "year": 2017}, {"authors": ["Kelvin Xu"], "title": "Show, attend and tell: Neural image caption generation with visual attention", "venue": "In ICML", "year": 2015}, {"authors": ["Xiaoxin Yin", "Jiawei Han"], "title": "CPAR: Classification based on predictive association rules", "venue": "In Proceedings of the 2003 SIAM International Conference on Data Mining", "year": 2003}, {"authors": ["Bolei Zhou", "Aditya Khosla", "\u00c0gata Lapedriza", "Aude Oliva", "Antonio Torralba"], "title": "Learning Deep Features for Discriminative Localization", "venue": "In CVPR. IEEE,", "year": 2016}], "sections": [{"text": "Open the Black Box Data-Driven Explanation of Black Box Decision Systems\nDINO PEDRESCHI, University of Pisa, Italy FOSCA GIANNOTTI, ISTI-CNR of Pisa, Italy RICCARDO GUIDOTTI, ISTI-CNR & University of Pisa, Italy ANNA MONREALE, University of Pisa, Italy LUCA PAPPALARDO, ISTI-CNR of Pisa, Italy SALVATORE RUGGIERI, University of Pisa, Italy FRANCO TURINI, University of Pisa, Italy\nACM Reference Format: Dino Pedreschi, Fosca Giannotti, Riccardo Guidotti, Anna Monreale, Luca Pappalardo, Salvatore Ruggieri, and Franco Turini. 2018. Open the Black Box Data-Driven Explanation of Black Box Decision Systems. 1, 1 (September 2018), 15 pages. https://doi.org/0000001.0000001"}, {"heading": "1 INTRODUCTION", "text": "The last decade has witnessed the rise of a black box society [33]. Ubiquitous obscure algorithms, often based on sophisticated machine learning models trained on (big) data, which predict behavioural traits of individuals, such as credit risk, health status, personality profile. Black boxes map user features into a class or a score without explaining why, because the decision model is either not comprehensible to stakeholders, or secret. This is worrying not only in terms of the lack of transparency, but also due to the possible biases hidden in the algorithms. Machine learning (ML) constructs predictive models and decision-making systems based on (possibly big) data, i.e., the digital traces of human activities (opinions, movements, lifestyles, etc.). Consequently, these models may reflect human biases and prejudices, as well as collection artifacts, possibly leading to unfair or simply wrong decisions. Many controversial cases have already highlighted that delegating decision-making to black box algorithms is critical in many sensitive domains, including crime prediction, personality scoring, image classification, personal assistance, and more (see box \u201cThe danger of black boxes\".) The EU General Data Protection Regulation (GDPR), entered into force in Europe on 25 May 2018, introduces a right of explanation for individuals to obtain \u201cmeaningful information of the logic involved\u201d when automated decision making takes place with \u201clegal effects\u201d on individuals \u201cor\nAuthors\u2019 addresses: Dino Pedreschi, University of Pisa, Italy, dino.pedreschi@unipi.it; Fosca Giannotti, ISTI-CNR of Pisa, Italy, fosca.giannotti@isti.cnr.it; Riccardo Guidotti, ISTI-CNR & University of Pisa, Italy, riccardo.guidotti@isti.cnr.it; Anna Monreale, University of Pisa, Italy, anna.monreale@unipi.it; Luca Pappalardo, ISTI-CNR of Pisa, Italy, luca.pappalardo@ unipi.it; Salvatore Ruggieri, University of Pisa, Italy, salvatore.ruggieri@unipi.it; Franco Turini, University of Pisa, Italy, franco.turini@unipi.it.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2018 Association for Computing Machinery. XXXX-XXXX/2018/9-ART $15.00 https://doi.org/0000001.0000001\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nar X\niv :1\n80 6.\n09 93\n6v 1\n[ cs\n.A I]\n2 6\nJu n\n20 18\nsimilarly significantly affect[ing]\u201d them1. Without an enabling technology capable of explaining the logic of black boxes, this right will either remain \u201cdead letter\u201d, or will just outlaw many such systems [19, 31, 43]. Through sophisticated machine learning models trained on massive datasets, we risk creating and using automated decision systems that we do not really understand. This impacts not only information ethics, but also accountability, safety and industrial liability [9, 24, 27]. Companies increasingly market services and products by embedding machine learning components, often in safety-critical industries such as self-driving cars, robotic assistants, domotic IoT systems, and personalized medicine. Another inherent risk of these components is the possibility of inadvertently making wrong decisions, learned from artifacts or spurious correlations in the training data, such as recognizing an object in a picture by the properties of the background or lighting, due to a systematic bias in training data collection. How can companies trust their products without understanding and validating the underlying rationale of their machine learning components? An explanation technology would be of immense help to companies for creating safer, more trustable products, and better managing any possible liability they may have. Likewise, the use of machine learning models in scientific research, for example in medicine, biology, socio-economic sciences, requires an explanation not only for trust and acceptance of results, but also for the very sake of the openness of scientific discovery and the progress of research. Explanation is at the heart of a responsible, open data science, across multiple industry sectors and scientific disciplines.\nDespite the soaring recent body of research on interpretable ML and explainable AI, a practical, widely applicable technology for black box explanation has not emerged yet. The challenge is hard, as explanations should be sound and complete in statistical and causal terms, and yet comprehensible to multiple stakeholders such as the users subject to the decisions, the developers of the automated decision system, researchers, data scientists and policy makers, authorities and auditors, including regulation and competition commissions, civil rights societies, etc. Stakeholders should be empowered to reason on explanations, to understand how the automated decision-making system works on the basis of the inputs provided by the user; what are the most critical features; whether the system adopts latent features; how a specific decision is taken and on the basis of what rationale/reasons; how the user could get a better decision in the future.\nAfter a succinct, high-level perspective on the booming field of explainable, interpretable machine learning, we focus on the open challenge of how to construct meaningful explanations of black boxes, and delineate a novel research direction suggested by a few recent methods for local explanations, i.e., methods to explain why a certain specific case has received its own classification outcome. Starting from these methods, including our own, we propose a new, local-first explanation framework: expressive logic rule languages for inferring local explanations, together with bottomup generalization methods to aggregate an exhaustive collection of local explanations into a global one, which optimizes jointly both simplicity and fidelity in mimicking a black box. We argue that the local-first approach has the potential to advance the state of art significantly, opening the door to a wide variety of alternative technical solutions.\n1http://ec.europa.eu/justice/data-protection/\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nThe danger of black boxes. Delegating decisions to black boxes may be critical, as the following cases illustrate.\n\u2022 The COMPAS score is a predictive model for the \u201crisk of crime recidivism\u201d, a proprietary secret of Northpointe, Inc. Journalists at propublica.org have shown that the model has a strong ethnic bias: blacks who did not reoffend were classified as high risk twice as much as whites who did not reoffenda. \u2022 Three major US credit bureaus, Experian, TransUnion, and Equifax, providing credit scoring for millions of individuals, are often discordant. In a study of 500,000 records, 29% of consumers received credit scores that differed by at least fifty points between credit bureaus, a difference that may mean tens of thousands of dollars over the life of a mortgage [7]. So much variability suggests that the three scoring systems either have a different (undisclosed) bias or are highly arbitrary. \u2022 During the 1970s and 1980s, St. George\u2019s Hospital Medical School in London used a computer program for screening job applicants, based on information from applicants\u2019 forms. The program was found to unfairly discriminate against women and ethnic minorities, inferred from surnames and place of birth, lowering their chances of being selected for interview [30]. This shows that automated discrimination is not new and is not necessarily due to machine learning. \u2022 In [37] the authors show how an accurate but untrustworthy classifier may result from an accidental artifact in the training data. In a task that involved discriminating between wolves and husky dogs in a dataset of images, the resulting deep learning model is shown to classify a wolf based solely on the presence of snow in the background of the picture. \u2022 A study at Princeton [5] shows how text/web corpora contain human biases, such as names that were associated with whites were found to be significantly more associated with pleasant than unpleasant terms, compared to names associated with black people. Therefore, models trained on text data for sentiment or opinion mining have a strong chance of inheriting the prejudices reflected in the data. \u2022 In 2016, Amazon.com used software to determine the areas of the US which it would offer free same-day delivery tob. It turned out that the software inadvertently prevented minority neighbourhoods from participating in the program, often when every surrounding neighbourhood was allowed.\nLearning from historical data recording human decision making may lead to the discovery of prejudices that are endemic in reality [3, 34], and to assign to such practices the status of general rules, maybe unconsciously, as these rules can be deeply hidden within the learned classifier. Today, we are warned about a rising \u201cblack box society\u201d [33], governed by \u201csecret algorithms protected by industrial secrecy, legal protections, obfuscation, so that intentional or unintentional discrimination becomes invisible and mitigation becomes impossible\u201d.\nawww.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing bhttp://www.businessinsider.com/how-algorithms-can-be-racist-2016-4\n, Vol. 1, No. 1, Article . Publication date: September 2018."}, {"heading": "2 THE BLACK-BOX EXPLANATION PROBLEM", "text": "Two different flavors of black-box explanation exist:\n\u2022 the eXplanation by Design (XbD) problem: given a dataset of training decision records, how to develop a machine learning decision model together with its explanation; \u2022 the Black Box eXplanation (BBX) problem: given the decision records produced by an inscrutable black box decision model, how to reconstruct an explanation for it.\nIn the XbD problem setting, the data scientist in charge of developing a decision model with machine learning is also supposed to provide an explanation of the model\u2019s logic, in order to prevent the model from making unfair, inaccurate or simply wrong decisions (such as the wolves on the snow) learned from artifacts and biases hidden in the training data and/or amplified or introduced by the learning algorithm. In this scenario, where the data scientist has full control over the model\u2019s creation process, the development of an explanation is essentially a further validation step in assessing the quality of the output model (in addition to testing for accuracy, absence of overfitting, etc.). At the same time, the explanation is an extra deliverable of the learning process, sustaining transparency and the trust of the stakeholders who will be adopting the model.\nIn the harder BBX problem setting, the data scientist aims to find an explanation for a black box designed by others. In this case, the original dataset on which the black box was trained is generally not known, and neither are the internals of the model. In fact, only the decision behaviour of the black box can, to some extent, be observed. This task has important variants, making the problem of revealing explanations increasingly difficult: can the black box be queried at will to obtain new decision examples, or only a given sample dataset of decision records is available? Is the complete set of features used by the decision model known, or only some of these features? Although attempts to tackle these problems by means of interpretable machine learning and discrimination-aware data mining exist for several years now, there has been an exceptional growth of research efforts in the last couple of years, with new emerging keywords such as black box explanation and explainable AI. We provide a comprehensive, up-to-date survey [21], and account here for the major recent trends. Many approaches to the XbD problem attempt at explaining the global logic of a black box by an associated interpretable classifier that mimics the black box. These methods are mostly designed for specific machine learning models, i.e., they are not agnostic, and often the interpretable classifier consists in a decision tree or in a set of decision rules. For example, decision trees have been adopted to explain neural networks [26] and tree ensembles [22, 42], while decision rules have been widely used to explain neural networks [1, 2] and support vector machines [16]. A few methods for global explanation are agnostic w.r.t. the learning model [23, 29]. A different stream of approaches, still in the XbD setting, focuses on the local behavior of a black box [21], searching for an explanation of the decision made for a specific instance. Some such approaches are model-dependent and aim, e.g., at explaining the decisions of neural networks by means of saliency masks, i.e., the portions of the input record (such as the regions of an input image) that are mainly responsible for the classification outcome [45, 47]. A few more recent methods are model-agnostic, such as LIME [37]. The main idea is to derive a local explanation for a decision outcome y on a specific instance x by learning an interpretable model from a randomly generated neighborhood of x , where each instance in the neighborhood is labelled by querying the black box. An extension of LIME using decision rules (called Anchors) is presented in [38], which uses a bandit algorithm that randomly constructs the rules with the highest coverage and precision.\nWhen the training set is available, decision rules are also widely used to proxy a black box model by directly designing a transparent classifier [21] which is locally or globally interpretable on its own [28, 32].\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nTo sum up, despite the soaring attention to the topic, the state of the art to date still exhibits ad-hoc, scattered results, mostly hard-wired with specific models. A widely applicable, systematic approach with a real impact has not emerged yet. In our view, a black box explanation framework should be: (1) model-agnostic, so it can be applied to any black box model; (2) logic-based, so that explanations can be made comprehensible to humans with diverse exper-\ntise, and support their reasoning; (3) both local and global, so it can explain both individual cases and the overall logic of the\nblack-box model; (4) high-fidelity, so it provides a reliable and accurate approximation of the black box behavior.\nThe four desiderata do not coexist in current proposals. Logic-based decision rules have proven useful in the sub-problem of explaining discrimination from a purely data-driven perspective, as demonstrated in the lively stream of research in discrimination-aware data mining, started in [34, 41], but it is unlikely that rules in their simplest form will solve the general explanation problem. Global rule-based models, trained on black box decision records, are often either inaccurate, oversimplistic proxies of the black box, or too complex, thus compromising interpretability. On the other hand, purely local models, such as LIME, do not yield an overall proxy of the black box, hence cannot solve the XbD and BBX problems in general terms. Here we propose to tackle the problem from a different perspective: more expressive rule languages equipped with novel rule learning methods, realizing a different, local-first mix of the local and global methods. This is the focus of the rest of the paper."}, {"heading": "3 HOW TO CONSTRUCT MEANINGFUL EXPLANATIONS?", "text": "Let us consider the XbD problem of discovering an explanation for a high-quality, non overfitting black box model b learned over a training dataset D of labelled examples (x ,y), where y is the class label and x is a vector of observed features; let us concentrate on binary classification, i.e., y \u2208 {0, 1}. Our framework works under three assumptions.\n\u2022 H \u00af 1: Logic explanations. The cognitive vehicle for offering explanations should be as close\nas possible to the language of reasoning, that is logic. From simple propositional rules up to more expressive, possibly causal, logic rules, many options of varying expressiveness exist to explore the trade-off between accuracy and interpretability of explanations. \u2022 H \u00af 2: Local explanations. The decision boundary for the black box b can be arbitrarily complex\nover the whole training datasetD, but in the neighborhood of each specific data point (x ,y) \u2208 D there is a high chance that the decision boundary is clear and simple, hence amenable to be captured by an interpretable explanation. \u2022 H \u00af 3: Explanation composition. There is a high chance that similar data points admit similar\nexplanations. Also, similar explanations are amenable to be composed together to yield global explanation of the black box.\nH2 is motivated by the observation that if all data points in the training set are surrounded by complex decision boundaries, then the black box b is likely to be in overfitting, unable to generalize from a training dataset of insufficient quality, thus contradicting the basic assumption. Analogous contradiction holds for H3, if any two data points admit very different explanations due to different decision boundaries. These assumptions suggest a two-step, local-first approach to the XbD explanation problem:\n(1) (local step) For any example in the training dataset D relabeled by the black box b, i.e. for any specific (x ,y \u2032), where y \u2032 = b(x) is the label assigned by b to x : query b to label a sufficient set of examples (local dataset) in the neighborhood of (x ,y \u2032) which are then used to derive an\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nexplanation e(x ,y \u2032) for b(x) = y \u2032. The explanation answers the question: why b has assigned class y \u2032 to x? and, possibly, also its counterfactual: what should be changed in x to obtain a different classification outcome? (2) (composition step) Consider as an initial global explanation the set of all local explanations e(x ,y \u2032) constructed at the local step for each individual example (x ,y \u2032) and synthesize a smaller set by iteratively composing and generalizing together similar explanations.\nWe discuss next the key issues of (i) a language for explanations, (ii) the inference of local explanations, and (iii) the bottom-up synthesis of global explanations."}, {"heading": "3.1 Rules languages for explanation", "text": "An explanation is a comprehensible representation of a decision model associated with a black box, acting as an interface between the model and the human. According to assumption H1, an explanation can be specified through decision rules expressed in various logic-based languages, characterized by (i) the form of predicates and constraints over features admissible in the rules, and (ii) statistical measures of confidence associated with the rules. As noted in Section 2, simplistic decision rules may not be expressive enough for proxying the decision behaviour of sophisticated machine learning models, such as deep neural networks, support vector machines or ensemble methods. Rule languages for explanation should range from simpler to more expressive alternatives:\n\u2022 plain association rules on nominal features, e.g., CheckingBalance=low, SavingBalance=low \u2192 Credit=no, \u2022 decision rules on features of generic type, e.g., CheckingBalance < 0, SavingBalance < 100 \u2192 Credit=no, \u2022 rules with inter-feature constraints, with reference to constraint languages of increasing complexity, e.g., CheckingBalance > 0, SavingBalance > 100, CreditBalance + SavingBalance < 200 \u2192 Credit=no, \u2022 rules with parameter features, which abstract a collection of inter-feature constraints, e.g., CreditBalance + SavingBalance \u2265 a, CreditBalance \u2212 SavingBalance \u2265 500\u2212a, 200 \u2265 a \u2265 300 \u2192 Credit=no.\nExplanations might also be equipped with counterfactuals [44], sets of rules with opposite decision and minimal change of the premise of a specific decision rule, in order to characterize under which slightly different conditions the conclusion of a rule is reverted. Examples of explanations with counterfactual are reported later. Finally, the above list could be extended by resorting to more expressive logics to deal with causation and/or time. The Probabilistic Computation Tree Logic [25], for instance, allows expressing Suppes probabilistic causation. Approaches that infer formula from temporal data have been successfully applied in the context of understanding cancer progression [6]. However, a critical point is to balance the expressiveness of the logic used with the computational complexity of reasoning over formulae in the logic. Entailment is decidable in polynomial time for linear constraints, but it becomes co-NP hard for parameterized linear constraints (see [12] for negative result and tractable fragments). More generally, a novel avenue of research opens here: causal inference and learning (see [35]) applied to the outcome of queries to a black box aimed at revealing the hidden causal structure implied by the black box when applied in the \u201creal world\". Causal explanations highlight which conditions on a sample x actually determine the black-box decision b(x). This is a central problem when correlation is not enough, e.g., in the context of discrimination litigation [15].\nStatistical measures associated to a rule A \u2192 B also range from simple to complex:\n, Vol. 1, No. 1, Article . Publication date: September 2018.\n\u2022 Rule support P(A,B), coverage P(A); and confidence, i.e., the conditional probability P(B | A) = P (A,B)P (A) ; \u2022 Rule lift: P (A,B)P (A)P (B) and other correlation scores, such as the maximum mutual information, the reduction in uncertainty of B when A is known: S(A,B) = H (A)+H (B)\u2212H (A,B)min[H (A),H (B)] where H denotes the entropy (or uncertainty) of a variable; \u2022 Statistical tests of the significance of the previous measures, w.r.t. various choices of null models/hypotheses [14]. \u2022 Causal extensions of the previous correlation-based measures, e.g., by propensity score reweighing [36] or by probabilistic causation confidence score derived from Suppes-Bayes causal networks [4].\nOperators which manipulate explanations together with their associated measures are also needed, such as composition operators that merge rules, in order to synthesize global explanations from local ones as we are explaining in the next section, as well as generalization operators that lift a collection of rules to a higher level of abstraction. An example of generalization consists in introducing parameters. E.g., the rules CreditBalance \u2264 200, CheckingBalance \u2264 300 \u2192 Credit=no and CreditBalance \u2264 300, CheckingBalance \u2264 200 \u2192 Credit=no have the minimal affine generalization CreditBalance \u2264 a, CheckingBalance \u2264 500\u2212a, 200 \u2264 a \u2264 300 \u2192 Credit=no. In such an example, the learned parameter a may reveal a latent feature used in decision-making. Details of the approach for learning parameterized linear systems are reported in our previous work [39]."}, {"heading": "3.2 Local explanations", "text": "The local-first approach that we propose requires the extraction of local explanations to be merged with some mechanism in order to get a global explanation. In the literature, as discussed above, there exist some approaches for finding local explanations like those presented in [20, 37, 38]. They aim at returning an individual explanation e for the decision assigned to each record x \u2208 X by the black box. Given a record x to be explained, those explanators return a local explanation e(x ,y \u2032) (where y \u2032 = b(x)) reasoning on a local dataset N (x ,y \u2032), generated in the neighborhood of (x ,y \u2032) using the black box b to assign class labels to the instances in the neighborhood. In particular, on top of N (x ,y \u2032), they build an interpretable classifier c(x) = y \u2032 from which it is possible to derive the explanation e(x ,y \u2032). These approaches mainly differ from each others on both: (i) the procedure used to create the local training dataset N (x ,y \u2032), and (ii) the derived interpretable classifier c .\nIn particular, LIME [38] uses a purely random neighborhood generation (see Figure 1 (left)) and as interpretable classifier c a linear model, while the weights of the coefficients (i.e., the features importance) form the explanation e (see [21] for more details). Anchor [38] uses a bandit algorithm for the neighborhood generation that randomly constructs the anchors. An anchor is a decision rule, i.e., the explanation e , that sufficiently ties a prediction locally such that changes to the values of features not in the rule do not affect the decision outcome. In [20] we propose LORE (LOcal Rule-based Explanations). LORE uses a genetic algorithm approach to generate the neighborhood N (x ,y \u2032) (see Figure 1 (right)) and a decision tree as interpretable classifier c , while the explanation consists in a rule derived from the decision tree classifier by following the path from the root to a leaf according to the values of x . Moreover, LORE also returns a set of counterfactual rules, suggesting the changes in the instance\u2019s features of x that may lead to a different outcome. To better understand these different ways of providing explanations for a record x we report in Figure 2 the three local explanations for an instance x of the well-known german dataset [10]\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nfrom UCI2. The top explanation is by LIME. Weights are associated to the categorical values in the instance x , and to continuous upper/lower bounds where the bounding values are taken from x . Each weight tells the user how much the decision would have changed for different (resp., smaller/greater) values of a specific categorical (resp., continuous) feature. LIME explanations are not straightforward to follow, compared to rule-based explanations of Anchor and LORE. The central explanation in Figure 2 is by Anchor, a single decision rule characterizing the contextual conditions for the decision of the black box. Anchor requires a discretization of continuous features, The bottom explanation in Figure 2 is by LORE. Rule r inherits the expressiveness of decision tree split conditions, e.g., on continuous features. Moreover, the counterfactual rules \u03a6 provides high-level and minimal-change contexts for reversing the outcome prediction of the black box."}, {"heading": "3.3 From local to global explanations", "text": "Instead of learning directly a global interpretable model that tries to imitate the black box, an alternative, more promising approach is to synthesize a global explanation from the bottom-up, starting from the collection of all local explanations, as discussed above. While many realizations of this idea are possible, here we discuss a natural one: the bottom-up construction of a dendrogram, a binary tree describing the compositions of pairs of (similar) explanations into a single, more general\n2https://archive.ics.uci.edu/ml/index.php\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nexplanation, to be used as a means to find, approximately, an optimal collection of explanations to proxy the overall behavior of a black box. Two basic functions over explanations are needed: a distance function d(e, e \u2032) \u2208 [0, 1] (with d(e, e \u2032) = 0 meaning that e and e \u2032 are identical, and d(e, e \u2032) = 1 meaning that e and e \u2032 are disjoint; and a merge functionm(e, e \u2032) = e \u2032\u2032 mapping e and e \u2032 into a (minimal) generalized explanation e \u2032\u2032 that subsumes both. A bottom-up algorithm to construct a dendrogram starting from the collection E = {e(x ,y) | (x ,y) \u2208 D}, i.e., all local explanations, is the following. (1) set E = {e(x ,y) | (x ,y) \u2208 D} (2) select e and e \u2032 such as d(e, e \u2032) is minimal in E, i.e., the two most similar explanations in E\naccording to d (3) set e \u2032\u2032 =m(e, e \u2032), i.e., merge e and e \u2032, (4) set E = E \\ {e, e \u2032} \u222a {e \u2032\u2032}, i.e., replace e and e \u2032 in E with e \u2032\u2032, (5) add a merge node in the dendrogram between the nodes corresponding to e and e \u2032 at height\nd(e, e \u2032), (6) repeat steps 2\u20135 until E contains a single explanation. The final task is now to exploit the dendrogram to find an optimal global explanation, that is a collection E\u0302 of explanations extracted from the dendrogram that covers all initial local explanations and maximizes an appropriate quality score q(E) that, for any collection E of explanations, measures both (i) the fidelity achieved adopting E for mimicking the black box b, and (ii) the number and size of the explanations in E, i.e., the complexity of the collection. Clearly, the goal is to maximize fidelity while minimizing complexity. Many alternatives are conceivable to define q, such as variants of the Minimum Description Length criterion (MDL), or the Bayesian Information Criterion (BIC). Also,\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nalternative cutting or pruning methods can be used to identify the best collection of explanations in the dendrogram. Figure 3 illustrates a possibility: compute the value of q(E) for all E resulting by cutting the dendrogram at all splitting points, and select the cut whose corresponding collection maximizes q. As a preliminary experimental validation of the local-first approach to global explanations, we compared it with two baselines: pure global classification [8, 11], i.e., learning a decision tree on the whole training dataset, and the collection of all local explanations obtained applying the local explanators LORE and Anchor [20, 38] separately to each example in the training dataset (removing duplicate rules). Each method works on the set of instances in the training, relabelled by the decisions assigned by the black box b (a random forest predictor in the experiment). To realize the dendrogram bottom-up method we start with the local rules generated by LORE and Anchor and apply a merge operator inspired to that of [40]. The distance function between explanations, needed to construct the dendrogram, is based on the Jaccard distance, computed on the two sets of data records covered by the explanations. We use BIC to identify the optimal global explanation,\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nmeasuring the explanation complexity by the number of rules. For rule classification we use the CPAR strategy of weighted voting proposed in [46]. We discuss, for instance, the results on the application of the three methods to the UCI dataset compas containing the features used by the COMPAS algorithm for scoring the crime recidivism risk of defendants for more than 10,000 individuals. We considered the binary problem of classifying \u201cLow-Medium\u201d and \u201cHigh\u201d risk. Although the adopted design choices for the local-to-global method are first-cut and a many unexplored alternatives exist, we observe that our local-to-global method produces a model with a comparable fidelity to the other two but, especially using LORE, with one order of magnitude less rules (from \u2248 600 to \u2248 50 rules). This is a remarkably promising outcome, that calls for far more extensive empirical validation along a wide number of design options."}, {"heading": "4 EXPLANATION DISCOVERY AND REASONING: RESEARCH DIRECTIONS", "text": "The proposed framework of local-first, bottom-up discovery of explanations based on highly expressive rules has a large potential for future research: a virtuous cycle may start, where more expressive, high-level rule languages call for novel ways to query and audit black box models and novel algorithms for learning rule in statistical and causal terms, which in turn may suggest novel ways to generalize rules and to devise reasoning mechanisms on top of. Higher-level rules will require new rule discovery algorithms, way beyond the state of art of association rule based classification algorithms [46], or traditional rule learning algorithms [17]. Moreover, the approach needs to be extended also beyond (binary) classification, i.e., to ordinal classification and for predicting numeric variables (e.g., regression rules), in order to deal also with ranking and scoring problems. Regarding reasoning mechanisms, simply providing the final user with the set of explanations computed by the discovery algorithms may not suffice, also depending on the expertise of the user, or on the need to interact with the explanations by asking high-level questions. Example questions include: For what reasons was my application rejected? What are the rules that apply to a specific population or profile?Which rules hide potential discrimination related to, e.g., protectedminorities? How do the confidence or other statistical and causal properties of a rule vary by changing a threshold value in the rule antecedent, or by dropping or adding a constraint? What combinations of features are most strongly correlated to (or are a cause of) a specific decision outcome? This final example question might be aimed, e.g., at discovering novel forms of discrimination towards vulnerable groups or profiles, or highlighting spurious rules due to artifacts in the collected training data. Based on the the algebraic properties of rule languages, the design of reasoning mechanisms for rule manipulation and filtering can provide meaningful answers for the final users. Appropriate interfaces must also be provided, including visual and textual presentation, as well as visual exploration for online analytics.\nText, images and non-relational data The discussion so far has focused on relational data, characterized by meaningful features. Current systems, however, deal with a heterogeneous variety of data sources, such as networks, spatiotemporal trajectories, text, images and multimedia. In fact, black box models such as deep learning and complex neural networks have shown a notable classification power in, e.g., image recognition problems. How can the explanation discovery process described in this paper be generalized to other, poorly structured, forms of data? An interesting aspect to study is how far the agnostic approach can be pushed by exploiting the results in semantic annotation that are rapidly emerging in the related fields, in order to map raw image data, text data, etc. into collections of meaningful objects. The idea is then to use this semantic transformation of decision records as an explanation set in the discovery process, in order to produce comprehensible rules. One example of a semantic\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nannotation tool in the text mining domain is TagMe [13], which maps selected keywords in a text to Wikipedia concepts. Similar tools capable of mapping image parts to meaningful concepts in a systematic way, when available, have the potential of generalizing the ad-hoc strategies adopted, e.g., in [37] and automatically discovering biased rules such as \u201cif there is a big white zone in the picture behind the animal, then it is a wolf\u201d. The adoption of semantic annotation tools can be used a pre-processing module for the explanation discovery process. One may object that, following our approach, the XbD or BBX problem is tackled relying on semantic annotation black boxes, thus explaining a black box through other black boxes. We observe that, in the end, explanation is always a translation of a complex object in terms of simpler ones, that the user can understand or, at least, fully trust. Therefore, explaining a black-box through simpler ones that are already understood or trusted is a viable approach in tasks such as object recognition, natural language understanding, etc., as higher-quality, fully validated machine learning basic components will become available.\nHidden features and background information In the BBX problem, a very challenging aspect is that the black box may use more information than explicitly asked the user, e.g., by inferring further features from the user\u2019s input and other available sources. Also, the decisions of a black box might be better understood if the indirect inferences adopted by the system, either consciously or not, are made explicit and brought to light. One example is indirect discrimination, as in the Amazon.com case, inadvertently using redlining rules that prevented minority neighbourhoods from participating in a program offering free sameday delivery. An interesting aspect to be investigated is to understand how the information on the explanation set can be extended with supplementary features from the wealth of open data available from official statistics and demographic institutes and other public organizations. The rule composition operators will provide means to merge background information and original features into learned statistical rules, e.g., expanding the ideas of the framework for inferring indirect discrimination rules described in [41]. Following previous approaches, the rule transformation operators of the algebra will enable, for instance, multiple rules such as ZIP=c \u2192 FreeDelivery=no combined with background information ZIP=c \u2192 MinorityNeighborhood=yes to be mapped into a new general rule MinorityNeighborhood=yes \u2192 FreeDelivery=no together with bounds on its statistical confidence and causal validity."}, {"heading": "5 CONCLUSIONS", "text": "The local-to-global framework for black box explanation, introduced in this paper, paves the road for a wide spectrum of further research works along three dimensions. First, the language for expressing explanations, which may draw inspiration from the rule-based, declarative languages developed since the 80\u2019s, such as constraint and inductive logic programming, as well as from the ideas of causal logic. Second, the inference of local explanations aimed at a specific decision instance, which calls for exploring alternative ways to query and audit the black box in the vicinity of the target instance to the purpose of revealing the statistical and causal logic adopted; this can possibly draw from the body of research in active learning and testing, such as fairness testing [18]. Third, the bottom-up generalization of the local explanations into global ones, which calls for algorithms that optimize the quality of explanations in terms of fidelity, simplicity, and coverage. A few recent proposals, including ours, are initial seeds along this road to design a systematic, agnostic method, i.e., one that does not take into account the internals of the decision model, even if they are known, as in the XbD problem. This widens applicability to real cases of the BBX problem, allowing dealing with generic decision models, which do not necessarily involve machine learning, but generally algorithms, humans or a mixture of them. In the XbD problem, the approach allows a data scientist to use, in principle, any kind of machine learning model.\n, Vol. 1, No. 1, Article . Publication date: September 2018.\nA critical aspect for this research endeavor in the general BBX problem is that it requires the availability of decision record data, i.e., examples to fuel the explanation discovery process. An interesting complementary research activity is how to favor the collection of data through participatory watchdog platforms, enabling users or consumers subject to automated decisionmaking to share their own decision records within a privacy-preserving, crowd-sourcing framework, in order to accumulate sufficient evidence for the explanation discovery process and expose the profiling logic of the black box. This would expand the applicability of the explanation technology beyond the XbD case, potentially helping to re-balance the information asymmetry between individual users and \u201cbig data\u201d companies. In fact, a technology for the explanation of black boxes would have a strong ethical impact. It may empower individuals against undesired, possibly illegal, effects of automated decisionmaking systems which may harm them, exploit their vulnerabilities, and violate their rights and freedom. It may provide practical tools for implementing the \u201cright of explanation\u201d provisions of the European GDPR, provided it delivers intuitive and usable explanations to users with different levels of expertise. It may improve industrial procedures and standards for the development of services and products powered by machine learning components, thus increasing the trust of companies and consumers in AI-powered products. It may empower citizens and policy makers with the ability of discovering new forms of discrimination towards vulnerable social groups and improving anti-discrimination norms and practice. We are evolving, faster than expected, from a time when humans are coding algorithms and carry responsibility of the resulting software quality and correctness, to a time when machines automatically learn algorithms from sufficiently many examples of the algorithms\u2019 expected input/output behavior. Requiring that machine learning and AI be explainable and comprehensible in human terms is not only instrumental for validating quality and correctness, but also for aligning the algorithms with human values and expectations, as well as preserving human autonomy and awareness in decision making."}, {"heading": "6 ACKNOWLEDGEMENT", "text": "This work is partially supported by the European Community\u2019s H2020 Program under the funding scheme \u201cINFRAIA-1-2014-2015: Research Infrastructures\u201d grant agreement 654024, http://www. sobigdata.eu, \u201cSoBigData: Social Mining & Big Data Ecosystem\u201d."}], "title": "Open the Black Box Data-Driven Explanation of Black Box Decision Systems", "year": 2018}