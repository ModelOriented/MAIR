{"abstractText": "In this paper, we introduce a novel interpreting framework that learns an interpretable model based on an ontology-based sampling technique to explain agnostic prediction models. Different from existing approaches, our algorithm considers contextual correlation among words, described in domain knowledge ontologies, to generate semantic explanations. To narrow down the search space for explanations, which is a major problem of long and complicated text data, we design a learnable anchor algorithm, to better extract explanations locally. A set of regulations is further introduced, regarding combining learned interpretable representations with anchors to generate comprehensible semantic explanations. An extensive experiment conducted on two real-world datasets shows that our approach generates more precise and insightful explanations compared with baseline approaches.", "authors": [{"affiliations": [], "name": "Phung Lai"}, {"affiliations": [], "name": "NhatHai Phan"}, {"affiliations": [], "name": "Han Hu"}, {"affiliations": [], "name": "Anuja Badeti"}, {"affiliations": [], "name": "David Newman"}, {"affiliations": [], "name": "Dejing Dou"}], "id": "SP:20ab570454b1e5dad43aaf1f36c9288ff4d32305", "references": [{"authors": ["S.M. Robnik", "I. Kononenko"], "title": "Explaining classifications for individual instances", "venue": "TKDE, vol. 20, no. 5, pp. 589\u2013600, 2008.", "year": 2008}, {"authors": ["Y. Goyal", "T. Khot", "D. Summers-Stay", "D. Batra", "D. Parikh"], "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "venue": "CVPR, 2017, pp. 6904\u20136913.", "year": 2017}, {"authors": ["D. Martens", "F. Provost"], "title": "Explaining data-driven document classifications", "venue": "2013.", "year": 2013}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "KDD, 2016, pp. 1135\u20131144.", "year": 2016}, {"authors": ["R.C. Fong", "A. Vedaldi"], "title": "Interpretable explanations of black boxes by meaningful perturbation", "venue": "ICCV, 2017, pp. 3429\u20133437.", "year": 2017}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra"], "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "venue": "ICCV, 2017, pp. 618\u2013626.", "year": 2017}, {"authors": ["H. Phan", "D. Dou", "H. Wang", "D. Kil", "B. Piniewski"], "title": "Ontology-based deep learning for human behavior prediction with explanations in health social networks", "venue": "Information Sciences, vol. 384, pp. 298\u2013313, 2017.", "year": 2017}, {"authors": ["R. Confalonieri", "F.M. delPrado", "S. Agramunt", "D. Malagarriga", "D. Faggion", "T. Weyde", "T.R. Besold"], "title": "An ontology-based approach to explaining artificial neural networks", "venue": "arXiv preprint arXiv:1906.08362, 2019.", "year": 1906}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "AAAI, 2018.", "year": 2018}, {"authors": ["H. Hu", "N. Phan", "J. Geller", "S. Iezzi", "H. Vo", "D. Dou", "S.A. Chun"], "title": "An ensemble deep learning model for drug abuse detection in sparse twitter-sphere", "venue": "MEDINFO\u201919), 2019.", "year": 2019}, {"authors": ["S. Nagrecha", "J.Z. Dillon", "N.V. Chawla"], "title": "Mooc dropout prediction: lessons learned from making pipelines interpretable", "venue": "WWW, 2017, pp. 351\u2013359.", "year": 2017}, {"authors": ["A. Adhikari", "D.M. Tax", "R. Satta", "M. Fath"], "title": "Example and feature importance-based explanations for black-box machine learning models", "venue": "arXiv preprint arXiv:1812.09044, 2018. \u2013 8 \u2013 Ontology-based Interpretable Machine Learning for Textual Data", "year": 1812}, {"authors": ["Y. Jia", "J. Bailey", "K. Ramamohanarao", "C. Leckie", "M.E. Houle"], "title": "Improving the quality of explanations with local embedding perturbations", "venue": "2019.", "year": 2019}, {"authors": ["L. Freddy", "W. Jiewen"], "title": "Semantic explanations of predictions", "venue": "vol. arXiv:1805.10587v1, 2018. [Online]. Available: https://arxiv.org/abs/1805.10587v1", "year": 1805}, {"authors": ["M. Banko", "M.J. Cafarella", "S. Soderland", "M. Broadhead", "O. Etzioni"], "title": "Open information extraction from the web.", "venue": "in IJCAI,", "year": 2007}, {"authors": ["F. Wu", "D.S. Weld"], "title": "Open information extraction using wikipedia", "venue": "ACL, 2010, pp. 118\u2013127.", "year": 2010}, {"authors": ["S. Soderland", "B. Roof", "B. Qin", "S. Xu", "O. Etzioni"], "title": "Adapting open information extraction to domainspecific relations", "venue": "AI magazine, vol. 31, no. 3, pp. 93\u2013 102, 2010.", "year": 2010}, {"authors": ["A. Fader", "S. Soderland", "O. Etzioni"], "title": "Identifying relations for open information extraction", "venue": "EMNLP, 2011, pp. 1535\u20131545.", "year": 2011}, {"authors": ["M. Schmitz", "R. Bart", "S. Soderland", "O. Etzioni"], "title": "Open language learning for information extraction", "venue": "EMNLP-IJCNLP, 2012, pp. 523\u2013534.", "year": 2012}, {"authors": ["S.M. Lundberg", "S.-I. Lee"], "title": "A unified approach to interpreting model predictions", "venue": "NeurIPS, 2017, pp. 4765\u20134774.", "year": 2017}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "ICML, 2017, pp. 3145\u20133153.", "year": 2017}, {"authors": ["M. Sundararajan", "A. Taly", "Q. Yan"], "title": "Gradients of counterfactuals", "venue": "arXiv preprint arXiv:1611.02639, 2016.", "year": 2016}, {"authors": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "title": "Striving for simplicity: The all convolutional net", "venue": "arXiv preprint arXiv:1412.6806, 2014.", "year": 2014}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.R. M\u00fcller", "W. Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PloS one, vol. 10, no. 7, p. e0130140, 2015.", "year": 2015}, {"authors": ["H. Knublauch", "R.W. Fergerson", "N.F. Noy", "M.A. Musen"], "title": "The prot\u00e9g\u00e9 owl plugin: An open development environment for semantic web applications", "venue": "ISWC. Springer, 2004, pp. 229\u2013243.", "year": 2004}, {"authors": ["E.W. Forgy"], "title": "Cluster analysis of multivariate data: efficiency versus interpretability of classifications", "venue": "Biometrics, vol. 21, pp. 768\u2013769, 1965.", "year": 1965}, {"authors": ["S. Barlas"], "title": "Prescription drug abuse hits hospitals hard: Tighter federal steps aim to deflate crisis", "venue": "Pharmacy and Therapeutics, vol. 38, no. 9, p. 531, 2013.", "year": 2013}, {"authors": ["L. Arras", "F. Horn", "G. Montavon", "K.R. M\u00fcller", "W. Samek"], "title": "What is relevant in a text document?\u201d: An interpretable machine learning approach", "venue": "PloS one, vol. 12, no. 8, p. e0181142, 2017.", "year": 1811}, {"authors": ["J. Ramos"], "title": "Using TF-IDF to determine word relevance in document queries", "venue": "iCML, vol. 242, 2003, pp. 133\u2013142.", "year": 2003}, {"authors": ["T. Mikolov", "K. Chen", "G.S. Corrado", "J.A. Dean"], "title": "Computing numeric representations of words in a highdimensional space", "venue": "2015, uS Patent 9,037,464.", "year": 2015}, {"authors": ["S. Hochreiter", "J. Schmidhuber"], "title": "Long short-term memory", "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.", "year": 1997}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "ADAM: A method for stochastic optimization", "venue": "arXiv preprint arXiv:1412.6980, 2014.", "year": 2014}, {"authors": ["J. Deng", "A.C. Berg", "K. Li", "L. Fei-Fei"], "title": "What does classifying more than 10,000 image categories tell us?", "venue": "in ECCV,", "year": 2010}], "sections": [{"text": "Index Terms\u2014ontology, interpretable machine learning, natural language processing, anchor, information extraction\nI. INTRODUCTION\nIn critical scenarios, such as clinical practices, having the ability to interpret machine learning (ML) model outcomes is significant to reduce the error rate and improve the trustworthiness of ML-based systems [1, 2]. To achieve this, typical approaches, called Interpretable ML (IML), are to train additional interpretable models to generate explanations, which usually are crucial features (i.e., important terms, in text analysis [3, 4] or super-pixels, in image processing [5, 6]), for each predicted outcome. However, most of existing IML algorithms usually treat input features independently, without considering their semantic correlations, especially in natural language processing. As a result, generated explanations commonly are fragmented, incomplete, and difficult to understand.\nAddressing this problem is a non-trivial task, since: (1) It is difficult to capture semantic correlations among features, which can be contextually rich and dynamic; (2) There is still a lack of scientific study on how to integrate semantic correlations among features into IML to generate semantic explanations, which are concise, complete, and easy to understand; and (3) The search space for semantic explanations can be large and complicated, given noisy and poor data. That results in a limited understanding of how to define semantic explanations, and effectively and efficiently identify them.\nIn literature, ontology, which encodes domain knowledge, can be used to capture semantic correlations among input features, such as entities, terms, phrases, concepts, etc. [7, 8]. However, there is an unexplored gap regarding how to guide\nthe learning process of an IML model based on ontology. Straightforwardly matching ontology and explaining data points, by randomly sampling co-occurring terms and concepts in conventional approaches, e.g., LIME [4], may not generate semantic explanations, since contextual information in the data is usually rich and complicated compared with the ontology. In addition, building an ontology that can sufficiently capture contextual information in the data is costly. Meanwhile, the traditional concept of anchor texts [9] can be used to narrow down the search space, by pinpointing generally important texts. However, the approach was not designed for each single and independent data point, i.e., at local level.\nOur contributions. To synergistically overcome these challenging issues, we propose a novel Ontology-based IML (OnML) to generate semantic explanations, by intergrating domain knowledge encoded in ontology and information extraction techniques into IML. In this paper, we consider a text classification model, in which text data is classified into different categories. Then, we learn a linear interpretable model by approximating the predictive model based on data sampled around the prediction outcome.\nIn order to achieve our goals, we first present a new concept of ontology-based tuples, each of which essentially is a set of correlated terms, words, and concepts semantically encoded and co-existed in the ontology and textual data. Departing from existing approaches, we identify and integrate ontology-based tuples into a new sampling approach, in which the semantic correlations among terms, words, and concepts are sampled and captured, instead of utilizing each of them independently.\nSecond, we propose a new concept of learnable anchor texts, to narrow down the search space for explanations. A learnable anchor text essentially is a contextual phrase, which can be expanded by adding nearby terms. For instance, anchors can be started with a predefined seed term having negative meanings, e.g., \u201cno,\u201d \u201cnot,\u201d \u201cillegal,\u201d and then be expanded to neighboring texts in order to effectively capture negative experiences and events, e.g., \u201cnot get any help.\u201d Anchors, which have the highest importance scores measuring their impacts upon the model outcome, will be chosen.\nThird, we introduce a set of regulations to combine ontology-based tuples, anchor texts, and triplexes extracted from the text, to generate semantic explanations. Each explanation is assigned an importance score. To our knowledge, OnML establishes the first connection among domain knowl-\nPersonal use is permitted, but republication/distribution requires IEEE permission. See http://www.ieee.org/publications standards/publications/rights/index.html for more information.\nar X\niv :2\n00 4.\n00 20\n4v 1\n[ cs\n.L G\n] 1\nA pr\n2 02\n0\nedge ontology, IML, and learnable anchor texts. Such a mechanism will greatly extend the applicability of ML, by fortifying the models in both interpretability and trustworthiness.\nFinally, extensive experiments conducted on two real-word datasets in critical applications, including drug abuse in the Twitter-sphere [10] and consumer complaint analysis1, to quantitatively and qualitatively evaluate our OnML, show that our algorithm generates concise, complete, and easy-tounderstand explanations, compared with existing mechanisms."}, {"heading": "II. BACKGROUND AND PROBLEM DEFINITION", "text": "In this section, we revisit IML, ontology-based approaches, and information extraction algorithms, which are often used to generate explanations. We further discuss the relation to previous frameworks and introduce our problem definition.\nLet D be a database that consists of N samples, each of which is a sample x \u2208 Rd associated with its label y. Each y is a one-hot vector of K categories y = {y1, y2, . . . , yK}. A classifier outputs class scores f : Rd \u2192 RK that maps an input x to a vector of scores f(x) = {f1(x), f2(x), . . . , fK(x)} s.t. \u2200k \u2208 [1,K] : fk(x) \u2208 [0, 1] and \u2211K k=1 fk(x) = 1. The highest-score class is selected as the predicted label for the sample. By minimizing a loss function L(f(x), y) that penalizes a mismatching between the prediction f(x) and the original value y, an optimal classifier is selected.\nInterpretable Machine Learning. Let us briefly revisit IML, starting with the definition of interpretable model. Given an interpretable model g, which provides insights and qualitative understanding about the prediction model f given an input x, there are two important criteria in learning g: 1) local fidelity, which implies the ability of interpretable models to approximate the prediction model in a vicinity of the input, and 2) interpretability, which is the sufficiently low complexity of interpretable models that make humans easy to understand the explanations. In textual data, the complexity, denoted as T (g), usually is the number of important words [3, 4], based upon that users can easily handle to evaluate the generated explanations.\nLet z be a sample of x, where z is generated by randomly selecting or removing features/words in x. \u03c6x(z) is a similarity function to measure the proximity between x and z. Given a d\u2032-dimensional binary vector z\u2032 \u2208 {0, 1}d\u2032 , z\u2032i = 1 indicates that the feature i-th (\u2208 x) is present in z, and vice-versa.\nTo achieve the interpretability and local fidelity, Ribeiro et al. [4] minimize a loss function L(f, g, \u03c6x), with a low complexity T (g), by solving the following problem:\ng\u2217 = arg min g L(f, g, \u03c6x) + T (g) (1)\nwhere L(f, g, \u03c6x) = \u2211\nz \u03c6x(z)(f(z) \u2212 g(z\u2032))2, \u03c6x(z) = exp(\u2212D(x, z)2/\u03c32) is an exponential kernel with D(x, z) is a distance function (e.g., cosine distance for textual data) with a width \u03c3, and g(z\u2032) = wgz\u2032.\nTo obtain the data z for learning g in Eq. 1, sampling approaches are employed. In LIME [4], the authors draw nonzero\n1https://www.consumerfinance.gov/data-research/consumer-complaints/\nelements of the original data x uniformly at random. Similar to this approach, a number of works follow [11, 12, 13]. Apart from the randomization, model decomposition is another line of learning g [1, 3], in which the prediction f(x) is decomposed on individual features to learn the effect of each feature on the outcome. These existing randomization and decomposition approaches treat features independently; therefore, they cannot capture correlations among features. This may not be practical in real-world scenarios, since features usually are highly and semantically correlated.\nOntology-based Approaches. To capture semantic correlations among input features and ontology can be applied. Ontology is used in [14] to filter and rank concepts from selected data points to conduct informative explanations. The explanations are derived in ontological forms. For example, the information, \u201ca 30 year-old individual, with an operation occurred in 1989,\u201d can be conveyed by the representation, \u201cTheSilentGeneration u OperationIn1980s.\u201d (TheSilentGeneration denotes people in the age range of 30-39.) However, building a rich contextual ontology is expensive, so typically ontology only captures a limited number of core concepts and their correlations. This is the reason why ontological forms cannot capture all common sense knowledge in the textual information. In reality, humans generally use natural languages in a variety of text presentations. Therefore, an appropriate combination of a single-form ontology with other approaches to generate semantic explanations is necessary.\nIn [8], Confalonieri et al. use ontology to learn an understandable decision tree, which is an approximation of a neural network classifier. Explanations are in a non-syntactic form, and they are not designed to explain a single and independent data point. Different from [8], we aim at generating semantic\n\u2013 2 \u2013\nexplanations for each input x. In this paper, generating semantic explanations is defined as a process of mapping a text to a representation of important information in a syntactic and understandable form.\nInformation Extraction. Apart from IML, information extraction (IE) is another direction to capture contextual information semantically. The first Open IE algorithm is TextRunner [15], which identifies arbitrary relation phrases in English sentences by automatically labeling data using heuristics for training the extractor. Following [15], a number of Open IE [16, 17, 18] were introduced. Unfortunately, these approaches ignore the context. OLLIE [19] includes contextual information; and extracts relations mediated by nouns, adjectives, and verbs; and outputs triplexes (subject, predicate, object). Compared to Open IE approaches, our algorithm mainly focuses on generating semantic explanations associated with the prediction label."}, {"heading": "III. ONTOLOGY-BASED INTERPRETABLE MACHINE LEARNING FOR TEXTUAL DATA", "text": "In this section, we formally present our proposed OnML framework (Fig. 1). Alg. 1 presents the main steps of our approach. Given an input x, an ontology O, and a set of all concepts C in O, we first present the notion of ontologybased tuples (Line 3), which will be used in an ontology-based sampling technique to learn the interpretable model g (Lines 4- 6). Next, we learn potential anchor texts using the input x and the model f(x) (Line 7). Meanwhile, OLIIE [19] is applied to extract triplexes, which have high confident scores, in x (Line 8). After learning g, learning anchor texts A, and extracting triplexes T , we introduce a set of regulations to combine them together to generate semantic explanations (Line 9). Let us first present the notion of ontology-based tuples as follows."}, {"heading": "A. Ontology-based Tuples", "text": "Given concepts A and B, A 7\u2192 B is used to indicate that A has a directed connection to B. In considerably correlated domains, such as text data, it is observed that 1) words appeared near to each other in a sentence have the same contextual information, and 2) different sentences usually have different contextual information. To encode the observations, we introduce a contextual constraint, as follows:\n\u03bbxk(xl) \u2264 \u03b3 (2)\nwhere xk and xl are two words in x, \u03b3 is a predefined threshold, and \u03bbxk(xl) measures the distance between the positions of xk and xl in x. In text data, if xk and xl belong to two sentences, they are considered to be violating the contextual constraint. Intuitively, the constraint is used to connect words 1) that appear near to each other in a sentence (contextual correlated) and 2) that belong to connected concepts in the ontology (conceptual correlated). If there is no contextual constraint, there can be mismatched information between the domain knowledge and the explanation extracted in the text.\nDefinition 1. Ontology-based tuple. Given xk and xl in x, (xk, xl) is called an ontology-based tuple, if and only if: (1)\nAlgorithm 1 OnML approach 1: Input: Input x; ontology O, and user-predefined anchor A0 2: Classify x by a prediction model f : Rd \u2192 RK 3: Find ontology-based tuples (xi, xj) in x based on concepts and\nrelations in O 4: Sample x, based on ontology-based tuples found by our sampling\ntechnique to obtain sampled data z \u2208 Z 5: Generate vectors of predictive scores f(z) with z \u2208 Z 6: Learn an interpretable model g based on f(z) and g(z\u2032) by Eq. 1 7: Learn anchor text by our anchor learning algorithm (Alg. 2) 8: Extract triplexes in x using an existing Open IE technique 9: Combine ontology-based tuples, learned anchors, and extracted\ntriplexes by our proposed regulations 10: Output: Semantic explanation E\n\u2203A,B \u2208 C s.t. xk \u2208 A and xl \u2208 B; (2) A 7\u2192 B; and (3) \u03bbxk(xl) \u2264 \u03b3.\nSince ontology has directed connections among its concepts, ontology-based tuples are asymmetric, i.e., (xk, xl) and (xl, xk) are different. For the sake of clarity without affecting the generality of the approach, we use a drug abuse ontology as an example (Fig. 2). Given the drug abuse ontology and x as \u201cShe uses orange juice and does not like weed. She knows that smoke causes addiction and headache.\u201d, list of ontology-based words can be found {use, weed, smoke, addiction, headache}. These words are in \u201cAbuse Behavior\u201d (use and smoke), \u201cDrug\u201d (weed), \u201cSide Effect\u201d (addiction), and \u201cSymptom\u201d (headache) concepts. Following the aforementioned conditions (Eq. 2 with \u03b3 = 3), two ontology-based tuples are found, which are (smoke, addiction) and (smoke, headache). In the meantime, (addiction, headache) and (weed, smoke) are not ontologybased tuples, since there is no directed connection between the \u201cSide Effect\u201d concept and the \u201cSymptom\u201d concept, and \u201cweed\u201d and \u201csmoke\u201d are in different sentences. By using the contextual constraint, we can eliminate \u201cuse weed,\u201d which is contextually incorrect, from the explanation."}, {"heading": "B. Ontology-based Sampling Technique", "text": "To integrate ontology-based tuples into learning g, we introduce a novel ontology-based sampling technique. To learn the local behavior of f in its vicinity (Eq. 1), we approximate L(f, g, \u03c6x) by drawing samples based on x, with the proximity indicated by \u03c6x. A sample z can be sampled as:\nz = ( \u222axi\u2208x,i6=k,i 6=l R(xi) ) \u222aR({xk, xl}) (3)\nwhere R(xi) and R({xk, xl}) are probabilities randomly drawn for each word xi \u2208 x(i 6= k, l) and words xk, xl \u2208 x together, respectively. If R is greater than a predefined threshold, then the word(s) will be included in z.\nIn our sampling process, xk and xl, i.e., an ontology-based tuple, are sampled together as a single element. This aims to integrate the semantic correlation between xk and xl, captured in an ontology-based tuple into the sampling process. In fact, we are sampling the semantic correlation, but not sampling each word/feature xk or xl independently. This enables us to measure the impact of this semantic correlation on f(x). In\n\u2013 3 \u2013\nAlgorithm 2 Anchors learning algorithm 1: Input: Input x; prediction model f ; number of sentences in x,\ndenoted as M ; user-predefined anchors A0 2: A \u2190 \u2205 (A : set of anchors for x) 3: for i \u2208M do 4: if any A0 appears in the sentence i then 5: Denote DA as a set of ordered words appearing after A0 in the sentence i in x 6: An \u2190 \u2205 (An is a set of candidate anchors) 7: Fn \u2190 \u2205 (Fn is a set of importance scores, associated with each candidate anchor) 8: for xj \u2208 DA do 9: An \u2190 A0 \u222a xj ; A0 \u2190 An; Fn \u2190 Fn \u222a IC(An) 10: Choose the best anchor for sentence i: Ai = argmaxAn Fn 11: else 12: Ai \u2190 \u2205 13: A \u2190 A\u222aAi 14: Output: A\naddition, words, which are not in any ontology-based tuple, are sampled independently. After sampling x (Eq. 3), we obtain the dataset Z that consists of sampled data points z associated with its label f(z). Z is used to learn g\u2217 by solving Eq. 1."}, {"heading": "C. Learnable Anchor Text", "text": "Before presenting our anchor mechanism, we introduce an importance score notion, which will be used to choose the best anchor and calculate the importance of generated explanations.\n1) Importance Score: To get insights into the importance of generated explanations and their impacts upon the model outcome, we calculate an importance score (IC) for each explanation. Intuitively, the higher importance score, the more important the explanation is. IC is calculated as:\nIC(r) = c\u0304r\n( f(x)\u2212 f(x/r) ) (4)\nwhere x/r is the original text x excluding words in the explanation r and c\u0304r is average coefficients of g\u2217 associated with all words in r.\n2) Anchor Text Learning Mechanism: It is challenging to work with long and poor data, e.g., large number of words, or misspelled text, since the contextual information is generally rich and complicated. Building an ontology to adequately represent such data is expensive, and insufficient in many cases. That results in a large undercovered search space for explanations. To address this problem, we introduce a learnable anchor mechanism to narrow down the search space.\nThe learning anchor technique is presented in Alg. 2. The anchor is initialized with an empty set (Line 2). A set of userpredefined anchors A0 is provided, which consists of startingwords that are further expanded by incrementally adding words to the end of the sentence. Then, the importance score of each candidate anchor is calculated, following Eq. 4. The top-1 anchor A, which has the highest important score, for each sentence are then chosen."}, {"heading": "D. Generating Semantic Explanations", "text": "We further apply OLLIE [19] to extract triplexes T (subject, predicate, and object) to identify the syntactic structure in a sentence, which can shape our explanations in a readable form. To generate semantic explanations E , we introduce a set of regulations to combine g\u2217, A, and T together: 1) E \u2286 Dx with Dx is a set of all words in x. 2) If there is no ontology-based tuple found, E will only consist of the learned anchor texts. 3) In a sentence, if there are two or more ontology-based tuples, we introduce four rules to merge them together:\n\u2022 Simplification: \u2013 Given (xk, xl) and (xk, xm), if xl and xm are in the\nsame concept, then the ontology-based explanation is {xk, xl and/or xm}. \u2013 Given (xk, xm) and (xl, xm), if xk and xl are in the same concept, then the ontology-based explanation is {xk and/or xl, xm}. \u2013 Given (xk, xl) and (xl, xm), then the ontology-based explanation is {xk, xl, xm}. \u2022 Union: Given (xk, xl), (xk, xm), (xl, xm), and {xk, xl, xm}, the ontology-based explanation is {xk, xl, xm}. \u2022 Adding Causal words: Semantic explanation can be in the form of a causal relation. Thus, if a causal word, e.g., \u201cbecause,\u201d \u201csince,\u201d \u201ctherefore,\u201d \u201cwhile,\u201d \u201cwhereas,\u201d \u201cthus,\u201d \u201cthereby,\u201d \u201cmeanwhile, \u201chowever,\u201d \u201chence,\u201d \u201cotherwise,\u201d \u201cconsequently,\u201d \u201cwhen,\u201d \u201cwhenever\u201d appears between any words in ontology-based tuples/explanations, we add the word to the explanation, following its position in x. \u2022 Combining with anchor texts A and triplexes T : After having ontology-based explanations, we combine them with A and T based on their positions in x. Then, the semantic explanation is generated from the beginning towards the end of all positions of words found in the ontology-based explanations, A, and T . For example, in the sentences, \u201cWe were filling out all the forms in the application. However, there is a letter in saying loss mitigation application denied for not sending information to us.\u201d, after the learning process, we obtain: 1) ontologybased explanation is (loss, application); 2) anchor text is \u201cnot sending information;\u201d and 3) triple is \u201ca letter; denied; mitigation application.\u201d The explanation E is \u201ca letter in saying loss mitigation application denied for not sending information.\u201d\n4) If different ontology-based tuples are in different sentences in x, due to the contextual constraint in Eq. 2, the explanation for each sentence follows the 3rd regulation.\nIt is worthy noting that we use aforementioned regulations to combine ontology-based tuples to be a longer ontological term. This makes the ontology used in a much better representation rather than independent and direct connections A 7\u2192 B.\n\u2013 4 \u2013"}, {"heading": "IV. EXPERIMENT", "text": "We have conducted extensive experiments on two realworld datasets, including drug abuse (Twitter-sphere [10]) and consumer complaint analysis from Consumer Financial Protection Bureau1."}, {"heading": "A. Baseline Approaches", "text": "Our OnML approach is evaluated in comparison with traditional approaches: (1) an interpretable model-agnostic explanation, i.e., LIME [4]; and (2) information extraction, i.e., OLLIE [19]. LIME is one of the state-of-the-art and well-applied approaches in IML, in which the predictions of any model are explained in a local region near the sample being explained. There are other algorithms sharing the same spirit as LIME, in terms of generating explanations [6, 20, 21, 22, 23, 24]. For the sake of clarity, we use LIME as a representative baseline regarding this line of research.\nThe key differences among OnML, OLLIE, and LIME are that OnML leverages domain knowledge to tie the expla-\nnations up to the predicted label and considers correlations among words in textual data to generate semantic explanations. Meanwhile, OLLIE focuses more on grammatical analysis to extract triples from the text. LIME generates fragmented interpretable components by learning a linear interpretable model locally around the prediction outcome and weight these components using coefficients of the interpretable model. In LIME and OLLIE, domain knowledge is not used."}, {"heading": "B. Datasets and Domain Ontologies", "text": "To validate the proposed method, we have developed two different domain ontologies, which are drug abuse ontology (Fig. 2) and consumer complaint ontology (in the Appendix) These ontologies were constructed for certain domains (e.g., drug abuse and consumer complaint) since it is necessary to capture specific semantic and causal relations among components. As default in Prote\u0301ge\u0301 [25], each arrow represented by its color demonstrates a certain type of causal relation in which its tail represents a domain and its head represents a\n\u2013 5 \u2013\nrange of the relation. For example, in the drug abuse ontology (Fig.2), purple arrow is for \u201cis involved with\u201d with domain is \u201cDrug\u201d and range is \u201cAbuse Behavior\u201d while green arrows are for \u201csuffer from\u201d. These ontologies were semi-manually generated, in which concepts were grouped and collected from the dataset by K-means clustering algorithm [26], and then judged by humans to reduce inappropriate concepts.\n1) Drug Abuse Dataset: We will use the term \u201cdrug abuse\u201d in the wider sense, including abuse and use of Schedule 1 drugs that are illegal and have no medical use (e.g. legal painkiller and weed) or illegally (e.g. getting drugs without prescription or even from blackmarket); and misuse of Schedule 2 drugs, which have medical uses, yet have a potential for severe addiction, and which can be life-threatening [27]. The drug abuse ontology captures different concepts collected from drug abuse tweets, grouped by K-means clustering algorithm, and then finalized by our team experts. Main concepts of the drug abuse ontology (DrugAO) (Fig. 2) capture correlation among key concepts, including abuse behaviors, drug types, drug sources, drug users, symptoms, side effects, and medical condition when using drug. Abuse behaviors concept is about behaviors of abusers, such as abuse, addict, blunt, etc. Drug types consists of different types of legal and illegal drugs, e.g., narcotics, cocaine, and weed. Drug sources is where drug users, who are the main objects of the ontology, gets drugs from. Symptoms and side effects are about different negative short-term and long-term effects of drugs on users. Medical condition contains terms about expression of disease and illness caused by using drugs. In total, DrugAO has 506 drug-abuse related terms (including slang terms and street names), and 18 relations.\nThe drug abuse dataset (Table I) consists of 9, 700 tweets labelled by [10] with a high agreement score. Among them, 3, 043 tweets are drug abuse tweets, labeled positive and the rest are non drug abuse tweets, labeled negative.\n2) Consumer Complaint Dataset: A consumer complaint is defined, here, as a complaint about a range of consumer financial products and services, sent to companies for response. In complaints, consumers typically talk about their mortgagerelated issues, such as: (1) Applying for a mortgage or refinancing an existing mortgage (application, credit decision, underwriting); (2) Closing on a mortgage (closing process, confusing or missing disclosures, cost); (3) Trouble during payment process (loan servicing, payment processing, escrow accounts); (4) Struggling to pay mortgage (loan modification, behind on payments, foreclosure); (5) Problem with credit report or credit score; (6) Problem with fraud alerts or security freezes, credit monitoring or identity theft protection services; and (7) Incorrect information on consumer\u2019s report or improper use of consumer\u2019s report. Main concepts of the consumer complaint ontology (ConsO) (Fig. 4) encode the relation among different entities related to consumer complaint: for instance, who is complaining; what happened to make consumers unhappy and then complaint; etc. There are six major concepts in ConsO, which are thing in role, complaint, event, event outcome, property, and product. Thing in role is\npeople and organizations related to complaint, such as buyers, investors, dealers, et,. Event and event outcome are about negative events happened that cause consumer complaints. Property is things belonging to consumers and product is substances of some parties (e.g., banks) offering to consumers. In total, we have 572 finance and product-related terms and 9 relations covered in our ontology. The consumer complaint dataset consists of 13, 965 mortgage-related complaints, labeled with 16 categories. These complaints were used for learning a model to predict the issue regarding each complaint."}, {"heading": "C. Experimental Settings", "text": "Our experiment focuses on validating whether: (1) Our OnML approach can be applied on different agnostic predictive models; and (2) Our approach can generate better explanations, compared with baseline approaches, in both quantitative and qualitative measures. Our ontologies, code, and data are available on Github2.\nTo achieve our goal, we carry out our evaluation through three approaches. First, by employing SVM and LSTM, we aim to illustrate that OnML works well with different agnostic predictive models. Second, we leverage the word deleting approach [28] as an quantitative evaluation. Third, we apply qualitative evaluation with Amazon Mechanical Turk (AMT).\n1) Model Configurations: In the drug abuse dataset, tweets were vectorized by TF-IDF [29] and then classified by a linear kernel SVM model. We achieved 83.6% accuracy. Tweets are short, i.e., the average and maximum numbers of words in a tweet are 12 and 37 (Table I). Therefore, it is not necessary to apply the anchor learning algorithm, which is designed to tighten down the search space for long text data.\nIn the consumer complaint dataset, Word2vec [30] is applied for feature vectorization. Then, a Long short-term memory (LSTM) [31] is trained as a prediction model. In LSTM, we used an embedding input layer with d = 300, one hidden layer of 64 hidden neurons, and a softmax output layer with 16 outputs. An efficient ADAM [32] optimization algorithm with learning rate 0.01 was employed to train LSTM. For the prediction model, we achieved 53% accuracy. We registered that this is a reliable performance, since the 16 categories are densely correlated resulting in a lower prediction accuracy\n2https://github.com/PhungLai728/OnML\n\u2013 6 \u2013\n[33]. Another reason for the low accuracy is the limited number of samples. We will collect more data in the future.\nFor sufficiently learning anchors in consumer complaints, we have chosen a set of negative terms as user-predefined anchors A0 = {not, no, illegal, against, without}. Importance scores in LIME are weights of the linear interpretable model. With OLLIE, importance scores of extracted triplexes are calculated in the same way as in our method (as shown in Eq. 4). LIME and OLLIE settings are used as default in [4, 19]. We only show OLLIE rules which have the confidence score greater than 0.7 and top-5 words from LIME. The contextual constraint \u03b3 in Eq. 2 is 3 for drug abuse and 10 for consumer complaint dataset. The pre-defined threshold in Eq. 3 is 0.5.\nTo be fair, we also combined the learned anchors to the results of OLLIE. In addition, another variation of our algorithm is to combine ontology-based terms and anchors, called Ontology algorithm. This is further used to comprehensively evaluate our proposed approach.\n2) Quantitative Evaluation: We use the word deleting approach [28], which deletes a sequence of words from a text and then re-classifies the text with missing words. By differences between the original text and the missing text, we examine the importance of the explanation to the prediction. Accuracy changes (AC) and prediction score changes (SC) are as:\nAC = Original accuracy\u2212 \u2211|test|\ni=1 Updating accuracy |test|\nSC = \u2211|test|\ni=1 IC(top-k explanations of i-th sample) |test|\nwhere the higher values of AC and SC indicate the more important explanations derived.\nIn our experiment, we deleted the top-k highest importance score explanations in OnML and OLLIE approaches and the top-m highest weighted words in LIME. To be fair, m is the number of words in the k-deleted explanations in OnML. In drug abuse, k = 1 since the tweet is typically short, and so there are not many explanations generated. In consumer complaint classifying, k \u2208 {1, 2, 3}.\n3) Qualitative Evaluation: We recruit human subjects on Amazon Mechanical Turk (AMT). This is a common means of evaluation for the needs of qualitative investigation by humans\n[6, 34]. Detailed guidance for each experiment is provided to users before they conduct the task.\nWe asked AMT workers to choose the best explanation by seeing side-by-side explanation algorithms. On top of that, we provided the original tweet/ complaint associated with their labels and prediction results. The visualization showing explanation results of the approaches is in Fig. 3. It is important to note that, in our real experiment, to avoid bias, name of each algorithm is hidden, and their positions in the visualization are randomized.\nWe were recruiting 4 users/tweets in the drug abuse and 5 users/complaints in the consumer complaint experiment. To quantify the voting results from AMT users, we use: (1) Count the total number of votes, called normal count, i.e., the best algorithm is chosen over all 1, 500 votes (5 users/complaint \u00d7 300 complaints); and (2) Count the majority number of votes, called majority count, i.e., the best algorithm for each complaint is the algorithm of the largest number over 5 votes."}, {"heading": "D. Experimental Results and Analysis", "text": "To evaluate the interpretability of each approach, 300 positive tweets and 300 complaints, randomly selected, were used.\n1) Drug Abuse Explanation: As in Table II, the accuracy is deducted significantly, and the predictive score changes the most in OnML. In fact, the values of AC and SC are 25.52% and 33.48% given OnML, compared with 15.47% and 23.52% given OLIIE, and 15.04% and 26.98% given LIME. This\n\u2013 7 \u2013\ndemonstrates that the explanations generated by our algorithm are more significant, compared with the ones generated by baseline approaches. In the evaluation by humans using AMT (Fig. 3), OnML clearly outperforms LIME and OLLIE. Text in the tweet is generally short and can be represented by several key words. Therefore, individual words learned by LIME can be sufficient to generate more insightful explanations. Meanwhile, OLLIE tends to extract all possible triplexes in the text, which can be redundant and wordy explanations.\n2) Consumer Complaint Explanation: The results on the consumer complain dataset further strengthen our results. Fig. 6 shows SC after deleting top-1, top-2, and top-3 explanations from OnML, Ontology, and OLLIE, as well as after deleting the most important words in LIME. In all three cases, score changes in OnML have the highest values, indicating that the explanations generated by OnML are the most significant to the prediction. In the evaluation by humans using AMT (Fig. 3), our OnML algorithm outperforms baseline approaches. Ontology approach achieves higher results than LIME and OLLIE. This shows the effectiveness of the ontology-based approach. LIME does not consider semantic correlations among words, resulting in a poor outcome.\n3) Completeness and Concision: In Fig. 3 (top), OnML generates \u201ci smoking weed,\u201d which provides concise and complete information about why it is predicted as a drug abuse tweet (smoking weed) and who was doing it (i) in a syntactic form. Meanwhile, 1) LIME derives relevant words to drug abuse (i.e., weed, smoking) without considering the correlation among these words; and 2) OLLIE generates lengthy and somewhat irrelevant explanations, e.g., \u201cchinese food; be eating on; a roof.\u201d In Fig. 3 (bottom), OnML derived semantic explanations for consumer complaints, which tell us that consumers were facing issues in loan refinance, e.g., \u201ccalled fha and they clain that fha does not review loans.\u201d Compared to OnML, Ontology generates laconic explanations, e.g., \u201cfha loan\u201d that give no sense of why consumer complaints. LIME provides a set of fragmented words and OLLIE generates wordy explanations, which are difficult to follow. More examples of explanation results are in the Appendix.\nOur key observations are: (1) Combining ontology-based tuples, learnable anchor texts, and information extraction can generate complete, concise, and insightful explanations to interpret the prediction model f ; and (2) Our OnML model outperforms other baseline approaches in both the quantitative and qualitative experiments, showing a promising result."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed a novel ontology-based IML to generate semantic explanations, by integrating interpretable models, ontologies, and information extraction techniques. A new ontology-based sampling technique was introduced, to encode semantic correlations among features/terms in learning interpretable representations. An anchor learning algorithm was designed to limit the search space of semantic explanations. Then, a set of regulations for connecting learned ontology-based tuples, anchor texts, and extracted triplexes is\nintroduced, to produce semantic explanations. Our approach achieves a better performance, in terms of semantic explanations, compared with baseline approaches, illustrating a better interpretability into ML models and data. Our approach paves an early brick on a new road towards gaining insights into machine learning using domain knowledge.\nAcknowledgment. The authors gratefully acknowledge the support from the National Science Foundation (NSF) grants CNS-1747798, CNS-1850094, and NSF Center for Big Learning (Oregon). We thank for the financial support from Wells Fargo. We also thank Nisansa de Silva (University of Oregon, USA) for valuable discussions."}], "title": "Ontology-based Interpretable Machine Learning for Textual Data", "year": 2020}