{
  "abstractText": "Explainable AI provides insight into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has focused on measures such as interpretability, trust, and usability of the explanation. Whether explainable AI can improve actual human decisionmaking and the ability to identify the problems with the underlying model are open questions. Using real datasets, we compare and evaluate objective human decision accuracy without AI (control), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct versus incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the why information provided in explainable AI may not enhance user decision-making, and further research may be needed to understand how to integrate explainable AI into real systems.",
  "authors": [
    {
      "affiliations": [],
      "name": "Yasmeen Alufaisan"
    },
    {
      "affiliations": [],
      "name": "Laura R. Marusich"
    }
  ],
  "id": "SP:cf7c00c36da471efceee32024c70beab2a13a633",
  "references": [
    {
      "authors": [
        "D. Gunning"
      ],
      "title": "Explainable artificial intelligence (xai)",
      "venue": "2017. [Online]. Available: https://www.darpa.mil/attachments/XAIProgramUpdate.pdf",
      "year": 2017
    },
    {
      "authors": [
        "K. Sokol"
      ],
      "title": "Fairness, accountability and transparency in artificial intelligence: A case study of logical predictive models",
      "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, 2019, pp. 541\u2013542.",
      "year": 2019
    },
    {
      "authors": [
        "T. Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "Artificial Intelligence, vol. 267, pp. 1\u201338, 2019.",
      "year": 2019
    },
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (xai)",
      "venue": "IEEE Access, vol. 6, pp. 52 138\u201352 160, 2018. 9",
      "year": 2018
    },
    {
      "authors": [
        "F. Doshi-Velez",
        "B. Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "R.R. Hoffman",
        "S.T. Mueller",
        "G. Klein",
        "J. Litman"
      ],
      "title": "Metrics for explainable ai: Challenges and prospects",
      "venue": "arXiv preprint arXiv:1812.04608, 2018.",
      "year": 1812
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "why should i trust you?\" explaining the predictions of any classifier",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 1135\u20131144.",
      "year": 2016
    },
    {
      "authors": [
        "\u2014\u2014"
      ],
      "title": "Anchors: High-precision model-agnostic explanations",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence, 2018.",
      "year": 2018
    },
    {
      "authors": [
        "I. Lage",
        "E. Chen",
        "J. He",
        "M. Narayanan",
        "B. Kim",
        "S. Gershman",
        "F. Doshi-Velez"
      ],
      "title": "An evaluation of the human-interpretability of explanation",
      "venue": "arXiv preprint arXiv:1902.00006, 2019.",
      "year": 1902
    },
    {
      "authors": [
        "M.M. Cummings"
      ],
      "title": "Man versus machine or man+ machine?",
      "venue": "IEEE Intelligent Systems,",
      "year": 2014
    },
    {
      "authors": [
        "G. Shmueli"
      ],
      "title": "To explain or to predict?",
      "venue": "Statistical science,",
      "year": 2010
    },
    {
      "authors": [
        "L. Edwards",
        "M. Veale"
      ],
      "title": "Enslaving the algorithm: From a \u201cright to an explanation\u201d to a \u201cright to better decisions\u201d?",
      "venue": "IEEE Security & Privacy, vol. 16,",
      "year": 2018
    },
    {
      "authors": [
        "Parliament",
        "Council of the European Union"
      ],
      "title": "General data protection regulation",
      "venue": "https: //eur-lex.europa.eu/eli/reg/2016/679/oj, 2016.",
      "year": 2016
    },
    {
      "authors": [
        "F. Doshi-Velez",
        "B. Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv, 2017. [Online]. Available: https://arxiv.org/abs/1702.08608",
      "year": 2017
    },
    {
      "authors": [
        "A. Datta",
        "S. Sen",
        "Y. Zick"
      ],
      "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
      "venue": "In Proceedings of the 2016 IEEE Symposium on Security and Privacy (SP), 2016, pp. 598\u2013617.",
      "year": 2016
    },
    {
      "authors": [
        "Y. Alufaisan",
        "Y. Zhou",
        "M. Kantarcioglu",
        "B. Thuraisingham"
      ],
      "title": "From myths to norms: Demystifying data mining models with instance-based transparency",
      "venue": "Proceedings of the 2017 IEEE 3rd International Conference on Collaboration and Internet Computing (CIC). IEEE, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "Y. Lou",
        "R. Caruana",
        "J. Gehrke"
      ],
      "title": "Intelligible models for classification and regression",
      "venue": "KDD\u201912, Beijing, China. ACM, August 2012.",
      "year": 2012
    },
    {
      "authors": [
        "T. Wang",
        "C. Rudin",
        "F. Doshi-Velez",
        "Y. Liu",
        "E. Klampfl",
        "P. MacNeille"
      ],
      "title": "A bayesian framework for learning rule sets for interpretable classification",
      "venue": "Journal of Machine Learning Research, vol. 18, no. 70, pp. 1\u201337, 2017. [Online]. Available: http://jmlr.org/papers/v18/16-003.html",
      "year": 2017
    },
    {
      "authors": [
        "G. Gigerenzer",
        "H. Brighton"
      ],
      "title": "Homo heuristicus: Why biased minds make better inferences",
      "venue": "Topics in cognitive science, vol. 1, no. 1, pp. 107\u2013143, 2009.",
      "year": 2009
    },
    {
      "authors": [
        "R.M. Dawes",
        "D. Faust",
        "P.E. Meehl"
      ],
      "title": "Clinical versus actuarial judgment",
      "venue": "Science, vol. 243, no. 4899, pp. 1668\u20131674, 1989.",
      "year": 1989
    },
    {
      "authors": [
        "D.N. Kleinmuntz",
        "D.A. Schkade"
      ],
      "title": "Information displays and decision processes",
      "venue": "Psychological Science, vol. 4, no. 4, pp. 221\u2013227, 1993.",
      "year": 1993
    },
    {
      "authors": [
        "L.J. Skitka",
        "K.L. Mosier",
        "M. Burdick"
      ],
      "title": "Does automation bias decision-making?",
      "venue": "International Journal of Human-Computer Studies,",
      "year": 1999
    },
    {
      "authors": [
        "F.C. Keil"
      ],
      "title": "Explanation and understanding",
      "venue": "Annu. Rev. Psychol., vol. 57, pp. 227\u2013254, 2006.",
      "year": 2006
    },
    {
      "authors": [
        "ProPublica"
      ],
      "title": "Macine bias",
      "venue": "https://www.propublica.org/article/ machine-bias-risk-assessments-in-criminal-sentencing, 2016.",
      "year": 2016
    },
    {
      "authors": [
        "M. Lichman"
      ],
      "title": "UCI machine learning repository",
      "venue": "http://archive.ics.uci.edu/ml\", 2013.",
      "year": 2013
    },
    {
      "authors": [
        "Y. Alufaisan",
        "M. Kantarcioglu",
        "Y. Zhou"
      ],
      "title": "Detecting discrimination in a black-box classifier",
      "venue": "In Proceedings of the 2016 IEEE 2nd International Conference on Collaboration and Internet Computing (CIC). IEEE, 2016. 10",
      "year": 2016
    },
    {
      "authors": [
        "J.R. De Leeuw"
      ],
      "title": "jspsych: A javascript library for creating behavioral experiments in a web browser",
      "venue": "Behavior research methods, vol. 47, no. 1, pp. 1\u201312, 2015.",
      "year": 2015
    },
    {
      "authors": [
        "J. Radford",
        "A. Pilny",
        "A. Reichelmann",
        "B. Keegan",
        "B.F. Welles",
        "J. Hoye",
        "K. Ognyanova",
        "W. Meleis",
        "D. Lazer"
      ],
      "title": "Volunteer science: An online laboratory for experiments in social psychology",
      "venue": "Social Psychology Quarterly, vol. 79, no. 4, pp. 376\u2013396, 2016.",
      "year": 2016
    },
    {
      "authors": [
        "J. Dressel",
        "H. Farid"
      ],
      "title": "The accuracy, fairness, and limits of predicting recidivism",
      "venue": "Science advances, vol. 4, no. 1, p. eaao5580, 2018.",
      "year": 2018
    },
    {
      "authors": [
        "J.Z. Bakdash",
        "L.R. Marusich"
      ],
      "title": "Repeated measures correlation",
      "venue": "Frontiers in psychology, vol. 8, pp. 1\u201313, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "A.D. Andre",
        "C.D. Wickens"
      ],
      "title": "When users want what\u2019s not best for them",
      "venue": "Ergonomics in design, vol. 3, no. 4, pp. 10\u201314, 1995.",
      "year": 1995
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Explainable AI is touted as the key for users to \u201cunderstand, appropriately trust, and effectively manage [AI systems]\u201d [1] with parallel goals of achieving fairness, accountability, and transparency [2].\nPreprint. Under review.\nar X\niv :2\n00 6.\n11 19\nThere are a multitude of reasons for explainable AI, but there is little empirical research for its impact on human decision-making [3, 4]. Prior behavioral research on explainable AI has primarily focused on human understanding/interpretability, trust, and usability for different types of explanations [5, 6, 7, 8, 9].\nTo fully achieve fairness and accountability, explainable AI should lead to better human decisions. Earlier research demonstrated that explainable AI can be understood by people [8]. Ideally, the combination of humans and machines will perform better than either alone [4], such as computerassisted chess [10], but this combination may not necessarily improve the overall accuracy of AI systems. While (causal) explanation and prediction share commonalities, they are not interchangeable concepts [4, 11, 12]. Consequently, a \"good\" explanation, interpretable model predictions, may not be sufficient for improving actual human decisions [4, 3] because of heuristics and biases in human decision-making rather than optimality [13]. Therefore, it is important to demonstrate whether, and what types of, explainable AI can improve the decision-making performance of humans using that AI, relative to performance using the predictions of \"black box\" AI predictions with no explanations and also compared to no AI prediction.\nIn this work, we empirically investigate whether explainable AI improves human decision-making using a two-choice classification experiment with real-world data. Using human subject experiments, we compared three different settings where a user needs to make a decision: 1) No AI prediction (Control), 2) AI predictions but no explanation, and 3) AI predictions with explanations. Our results indicate that, while providing the AI predictions tends to help users, the why information provided in explainable AI does not specifically enhance user decision-making."
    },
    {
      "heading": "2 Background and Related Work",
      "text": "Using Doshi-Velez and Kim\u2019s [5] framework for interpretable machine learning, our current work focuses on real humans, simplified tasks. Because our objective is on evaluating decision-making, we do not compare different types of explanations and instead used one of the best available explanations: anchor LIME [8]. We use real tasks here, although our tasks involve relatively simple decisions with two possible choices. Additionally, we use lay individuals rather than experts. Below, we discuss prior work that is related to our experimental approach."
    },
    {
      "heading": "2.1 Explainable AI/Machine Learning",
      "text": "While machine learning models largely remain opaque and their decisions are difficult to explain, there is an urgent need for machine learning systems that can \u201cexplain\u201d their reasoning. For example, European Union regulation requires a \u201cright to explanation\u201d for any algorithms that make decisions significantly impacting users with user-level predictors [14]. In response to the lack of consensus on the definition and evaluation of interpretability in machine learning, Doshi-Velez and Kim [5] propose a taxonomy for the evaluation of interpretability focusing on the synergy among human, application, and functionality. They contrast interpretability with reliability and fairness, and discuss scenarios in which interpretability is needed [15]. To unmask the incomprehensible reasoning made by these machine learning/AI models, researchers developed explainable models that are built on top of the machine learning model to explain their decisions. There are two categories of explainable models that provide explanations for the decisions made by machine learning models: feature-based and rule-based models. The feature-based models resemble feature selection where the model outputs the top features that explain the machine learning prediction and their associated weights [16, 7]. The rule-based models provide simple if-then-else rules to explain predictions [8, 17]. It has been proven that rule-based models provide higher precision when compared to feature-based models [8, 17].\nLou et al. [18] investigate the performance of generalized additive models (GAMs) that combine single-feature models through a linear function. Each single-feature model is referred to as a shape function that can be arbitrarily complex; however, there are no interactions between features. Therefore, GAMs are more accurate than simple linear models and can be easily interpreted by users. They propose a new model based on tree ensembles with an adaptive number of leaves. They also investigate how different shape functions influence the additive model. Their empirical study suggests that shallow bagged trees with gradient boosting appear to be the best method on low to medium dimensional datasets. Anchor LIME is an example of the current state-of-the-art explainable rule-based model [8]. It is a model-agnostic system that can explain predictions generated by any\nmachine learning model with high precision. The model provides rules, referred to as anchors, to explain the prediction for each instance. A rule is an anchor if it sufficiently explains the prediction locally such that any changes to the rest of the features, features not included in the anchor, do not affect the prediction. Anchors can be found in two different approaches: bottom-up approach and beam search. More details of anchor LIME can be found in [8]. Wang et al. [19] present a machine learning algorithm that produces Bayesian rule sets (BRS) comprising short rules in the disjunctive normal form. They develop two probabilistic models with prior parameters that allow the user to specify a desired size and shape and balance between accuracy and interpretability. They apply two priors\u2014beta-binomials and Poisson distribution\u2014to constrain the rule generation process and provide theoretical bounds for reducing computation by iteratively pruning the search space. In our experiments, we use anchor LIME to provide explanations for all our experimental evaluations due to the high human precision of anchor LIME as reported in [8]."
    },
    {
      "heading": "2.2 Human Experiments with Explainable AI and Human Decision-Making",
      "text": "Prior human experiments with explainable AI have concentrated on interpretability, trust, and subjective measures of usability (preferences and satisfaction), with work on decision-making performance remaining somewhat limited. For example, one study used a classification task with text information and varied the complexity of explanations. As the explanation complexity increased, users demonstrated more difficulty with interpretability (based on slower responses) and decreased usability, but no statistically significant impact on decision accuracy [9]. Comparisons were only among explanations of varying complexity; there was no AI prediction without an explanation. For tasks with text and images, the addition of explanations increased trust in the classifier and also helped users select the classifier with higher accuracy [7]. Another study was primarily designed to compare human performance between two different types of explanations and did include baseline conditions with no explanation [8]. In this work, users with anchors were more accurate and faster to predict out of sample model classification and strongly preferred the anchors [8]. Earlier results suggest explainable AI is interpretable, trustworthy, and usable to varying degrees, but this is not necessarily the same as a person making real-world decisions about the underlying data, such as whether to actually use the AI\u2019s prediction, whether the AI has made an error, and the role of explanations.\nPsychological research on decision-making suggests that \"good\" interpretability may not by itself be sufficient for enhancing decisions. People are not necessarily rational (i.e., maximizing an expected utility function). Instead, decisions are often driven by heuristics and biases [13]. Also, providing more information, even if relevant, does not necessarily lead people to making better decisions [20]. Bounded rationality in human decision-making using satisfying (near optimality) [20] is an alternative theory to heuristics and biases [13]. Regardless of the theoretical account for human decision-making, people, which can include experts [21], generally do not make fully optimal decisions.\nAt a minimum, explainable AI should not be detrimental to human decision-making. The literature on decision aids (a computational recommendation or prediction, typically without an explicit explanation) has mixed findings for human performance. Sometimes these aids are beneficial for human decision-making, whereas at other times they have negative effects on decisions [22, 23]. These mixed findings may be attributable to the absence of explanations. A common reason for explanation is to improve predictions or decisions [24]."
    },
    {
      "heading": "3 Methods",
      "text": "In this section, we first describe the two datasets used in our experiments. We then provide the details of our experimental design and hypotheses, participant recruitment, and general demographics of our sample."
    },
    {
      "heading": "3.1 Dataset",
      "text": "To conduct our experiments, we choose two different datasets that have been heavily used in prior research that tries to understand algorithmic fairness and accountability issues. For example, the COMPAS dataset has been used to detect potential biases in the criminal justice system [25]. The Census income dataset, which has been used to test many machine learning techniques, involves\npredictions of individuals\u2019 income status. This has been associated with potential biases in making decisions such as access to credit and job opportunities.\nWe choose these datasets primarily because they both involve real-world contexts that are understandable and engaging for human participants. Further, the two datasets differ widely in the number of features and in the overall accuracy classifiers can achieve in their predictions. This allows us to explore the effects of these differences on human performance; in addition, it ensures that our findings are not limited only to a specific dataset. We briefly discuss each dataset in more detail below.\nCOMPAS stands for Correctional Offender Management Profiling for Alternative Sanctions [25]. It is a scoring system used to assign risk scores to criminal defendants to determine their likelihood of becoming a recidivist. The data has 6,479 instances and 7 features. These features are gender, age, race, priors count, and charge degree risk score, and whether the defendants re-o?ended in two years or not. We let the binary re-o?ending feature be our class.\nCensus income (CI) data contains information used to predict individuals\u2019 income [26]. It has 32,561 instances and 12 features. These features are age, work class, education, marital status, occupation, relationship, race, sex, capital gain, capital loss, hours per week, and country. The class value is low income (less or equal to 50K) or high income (greater than 50K). We preprocessed the dataset to allow equal class distribution 1."
    },
    {
      "heading": "3.2 Experimental Design",
      "text": "Prior results demonstrate that people interpret, trust, and prefer explainable AI, suggesting it will improve the accuracy of human decisions. Hence, our primary hypotheses are that explainable AI would aid human decision-making. The hypotheses (H.) are as follows:\nH.1 Explainable AI enhances the decision-making process compared to only an AI prediction (without explanation) and a control condition with no AI.\n[H.1.a] A participant performs above chance in prediction tasks.\nH.2 A participant\u2019s decision accuracy is positively associated with AI accuracy.\nH.3 Average participant\u2019s decision accuracy does not outperform AI accuracy.\nH.4 Participants outperform AI accuracy more often with explainable AI over AI prediction alone.\nH.5 Participants follow explainable AI recommendation more often than AI-only recommendation.\nH.6 Explainable AI increases a participant\u2019s decision confidence.\nH.7 A participant\u2019s decision confidence is positively correlated with the accuracy of his/her decision.\nTo investigate these hypotheses, we used a 2 (Dataset: Census and COMPAS) x 3 (AI condition: Control, AI, and AI with Explanation) between-participants experimental design. The three AI conditions were:\n\u2022 Control: Participants were provided with no prediction or information from the AI. \u2022 AI: Participants were provided with only an AI prediction. \u2022 AI with Explanation: Participants received an AI prediction, as well as an explanation of the\nprediction using anchor LIME [8].\nTo achieve more than 80% statistical power to detect a medium effect size for this design, we planned for a sample size of N = 300 (50 per condition).\nIn all conditions, each trial consists of a description of an individual and a two-alternative forced choice for the classification of that individual. Each choice was correct on 50% of the trials, thus chance performance for human decision-making accuracy was 50%. Additionally, an AI prediction\n1The CI dataset is from 1994. We adjusted for inflation by using a present value of 88K. From 1994 to January 2020 (when the experiment was run) inflation in the United States was 76.45%: https://www.wolframalpha. com/input/?i=inflation+from+1994+to+jan+2020\nand/or explanation may appear, depending on the AI condition (see Figure 1). After a decision is made, participants are asked to enter their confidence in that choice, on a Likert scale of 1 (No Confidence) to 5 (Full Confidence). Feedback is then displayed, indicating whether or not the previous choice was correct.\nWe compared the prediction accuracy of Logistic Regression, Multi-layer Perceptron Neural Network with two layers of 50 units each, Random Forest, Support Vector Machine (SVM) with rbf kernel, and selected the best classifier for each dataset. We chose a Multi-layer Perceptron Neural Network for Census income data where it resulted in an overall accuracy of 82% and SVM with rbf kernel for COMPAS data with an overall accuracy of 68%. Census income accuracy closely matches the accuracy reported in the literature [26, 27, 28] and COMPAS accuracy matches the results published by ProPublica [25]. We split the data to 60% for training and 40% for testing to allow enough instances for the explanations generated using anchor LIME [8].\nIn our behavioral experiment, 50 instances were randomly sampled without replacement for each participant. Thus, AI accuracy was experimentally manipulated for participants (Census: mean AI accuracy = 83.85%, sd = 3.67%; COMPAS: mean AI accuracy = 69.18%, sd = 4.65%). Because of the sample size and large number of repeated trials per participant, there was no meaningful difference in mean AI accuracy for participants in the AI condition versus those in the AI explanation condition (p = 0.90)."
    },
    {
      "heading": "3.3 Participant Recruitment and Procedure",
      "text": "The experiment was created in jsPsych [29] and hosted on the Volunteer Science platform [30] 2. Participants were recruited using Amazon Mechanical Turk (AMT) and were compensated $4.00 each. We collected data from 50 participants in each of the six experimental conditions, for a total of 300 participants (57.67% male). Most participants were 18 to 44 years old (80.67%). The behavioral experiment research was approved as exempt by our institutional review board.\nParticipants read and agreed to a consent form, then received instructions on the task, specific to the experimental condition they were assigned to. They completed 10 practice trials, followed by 50 test trials and a brief questionnaire assessing general demographic information and comments on strategies used during the task. The median time to complete the practice and test trials was 18 minutes."
    },
    {
      "heading": "4 Results and Discussion",
      "text": "In this section we analyze and describe the effects of dataset, AI condition, and AI accuracy on the participants\u2019 decision-making accuracy, ability to outperform the AI, adherence to AI recommendations, confidence ratings, and reaction time.\n2https://volunteerscience.com/"
    },
    {
      "heading": "4.1 Participant Decision-Making Accuracy",
      "text": "We compared participants\u2019 mean accuracy in the experiment across conditions using a 2 (Dataset) x 3 (AI) factorial Analysis of Variance (ANOVA) (see Figure 2). We found significant main effects, with a small effect size for AI condition (F (2, 294) = 8.19, p < 0.001, \u03b72 = 0.04) and a nearly large effect for dataset condition (F (1, 294) = 46.51, p < 0.001, \u03b72 = 0.12). In addition, there was a significant interaction with a small effect size (F (2, 294) = 8.38, p < 0.001, \u03b72 = 0.05), indicating that the effect of AI condition depended on the dataset. Specifically, the large effect for increased accuracy with AI was driven by the Census dataset.\nContrary to H.1, explainable AI did not substantially improve decision-making accuracy over AI alone. We followed up on significant ANOVA effects by performing pairwise comparisons using Tukey\u2019s Honestly Significant Difference. These post-hoc tests indicated that participants who viewed the Census dataset showed improved accuracy over control when given an AI prediction (p < 0.01) and higher accuracy with AI explanation versus control (p < 0.001), but there was no statistically significant difference in participant accuracy for AI compared to AI explanation (p = 0.28). However, the COMPAS dataset had no significant differences in participant accuracy across pairwise comparisons for the three AI conditions (ps > 0.75). Also, the mean participant accuracy for the COMPAS control condition (mean = 63.7%, sd = 9.24%) was comparable to participant accuracy for prior decision-making research using the same dataset (mean = 62.8%, sd = 4.8%) [31].\nThere was strong evidence supporting H.1.a; the vast majority of participants had mean accuracy exceeding guessing (50% accuracy). The overall participant accuracy across all conditions was 65.65% (sd = 10.92%), with 90% (or 270 out of 300) of participants performing above chance on the classification task. This indicates that the task was challenging but feasible for almost all participants."
    },
    {
      "heading": "4.2 AI Accuracy and Participant Decision-Making Accuracy",
      "text": "We also evaluated the effect of the randomly varied AI accuracy for each participant on their decisionmaking accuracy. We used linear regression to analyze this relationship, specifying participant accuracy as the dependent variable and the following as independent variables: mean AI accuracy (per participant), AI condition, and dataset condition, see Figure 3. Regressions are represented by the solid lines with the shaded areas representing 95% confidence intervals. The control condition is not included in the analysis or figure, because the accuracy of the AI is not relevant if no AI prediction is presented to the participant. The overall regression model was significant with a large effect size, F (4, 195) = 21.23, p < 0.001, R2adjusted = 0.29. Consistent with H.2, there was a large main effect for AI accuracy (\u03b2 = 0.70, p < 0.001, R2 = 0.28). Also, there was a small AI accuracy and dataset interaction (\u03b2 = \u22120.07, p < 0.01, R2 = 0.03), reflecting the same interaction depicted in Figure 2. There were no significant regression differences for the dataset or AI versus AI Explanation, ps > 0.60; there was no significant effect of dataset because it largely drove AI accuracy. (It is not just that participants perform better with the higher mean AI accuracy of the Census dataset;\nboth datasets had large positive relationships with participant accuracy and corresponding mean AI accuracy shown in Figure 3.)"
    },
    {
      "heading": "4.2.1 Outperforming the AI Accuracy",
      "text": "An interesting question is whether the combination of AI and human decision-making can outperform either alone. The previous analyses showed that the addition of AI prediction information improved human performance over controls with humans alone. We also evaluated how often the human decision-making accuracy outperformed the accuracy of the corresponding mean AI prediction accuracy, which was experimentally manipulated. Although most participants performed well above chance, only a relatively small number of participants had decision accuracy exceeding their mean AI prediction (7% or 14 out of 200). This result largely supports H.3 and also shown above in Figure 3 where each dot represents an individual; also shown above, dots above the black dashed line show the participants that outperformed their mean AI prediction. The black dashed line shows equivalent performance for mean AI accuracy and mean participant accuracy."
    },
    {
      "heading": "4.2.2 Adherence to AI Model Predictions",
      "text": "Participants followed the AI predictions more often when the AI was correct versus when the AI was incorrect, indicating some recognition of when the AI makes bad predictions (see Figure 4, F (1, 196) = 36.15, p < 0.001, \u03b72p = 0.16). This was consistent with participant sensitivity to AI recommendations, evidence for H.2. Also, participants were better able to recognize correct versus incorrect AI predictions when they were in the Census condition, demonstrated in the significant interaction between AI correctness and dataset, F (1, 196) = 9.01, p < 0.01, \u03b72p = 0.04. None of the remaining ANOVA results were significant, ps > 0.16. Thus, there was no evidence for higher adherence to recommendations with explainable AI, which rejected H.5."
    },
    {
      "heading": "4.3 Confidence Ratings",
      "text": "We found that AI (without and with explanation) resulted in slightly increased mean confidence. There was a small effect of AI condition on mean confidence (see Figure 5, F (2, 294) = 3.58, p = 0.03, \u03b72 = 0.02). Post hoc tests indicated participants had significantly lower mean confidence in the control condition than AI, p < 0.03, but there were no statistical differences for other pairwise comparisons, ps > 0.25. This contradicted H.6, and there was no evidence of a confidence increase with explanations. In addition, there was no evidence for a main effect of dataset condition or interaction, ps > 0.84.\nConfirming H.7, we found a positive relationship for accuracy and confidence rating within individuals indicating that participants\u2019 confidence ratings were fairly well-calibrated with their actual decision accuracy. We calculated each participant\u2019s mean accuracy at each confidence rating they used, and then conducted a repeated measures correlation [32] (rrm = 0.48, p < 0.001)."
    },
    {
      "heading": "4.4 Additional Results",
      "text": "We also assessed reaction time and summarize self-reported decision-making strategies. These results are exploratory; there were no specific hypotheses. There was no significant main effect of AI condition on participants\u2019 reaction time (F (2, 294) = 2.13, p = 0.12, \u03b72 = 0.01). There was only a main effect of dataset condition (F (1, 294) = 28.52, p < 0.001, \u03b72 = 0.09), where participants took an average of 1600 ms longer in the Census condition than the COMPAS condition (see Figure 6). This effect was most likely due to the Census dataset having more variables for each instance than the COMPAS dataset, and thus requiring more reading time on each trial. The addition of an explanation did not meaningfully increase reaction time over an AI prediction only.\nSubjective measures, such as self-reported strategies and measures of usability, often diverge from objective measures of human performance [33, 34] such as actual decisions. Participants self-reported varying strategies to make their decisions, yet there was a clear benefit for AI prediction (without and with explanation). In the AI and AI explanation conditions: n = 80 indicated using the data without mentioning AI, n = 39 reported using a combination of the data and the AI, and only n = 16 said they primarily used, trusted, or followed the AI. Despite limited self-reported use of the AI in the two relevant conditions, decision accuracy was higher with AI (Figure 2), strongly associated with AI accuracy (Figure 3), and there was some sensitivity to whether the AI was followed when it was correct versus incorrect (Figure 4). Nearly 80% of user comments could be coded; blank and nonsense responses could not be coded."
    },
    {
      "heading": "4.5 Discussion",
      "text": "Our results show providing an AI prediction enhances human decision accuracy, but in opposition to the hypotheses, adding an explanation had no significant impact on decisions and the ability to outperform the AI. Past research has focused on user trust and understanding for explainable AI, rather than objective performance for decision-making. The lack of a significant, practically relevant effect for explainable AI was not due to lack of statistical power or ceiling performance\u2014nearly all participants consistently performed above chance, but well below perfect accuracy. These findings also illustrate the need to compare decision-making with explainable AI to other conditions including no AI and AI prediction without explanation. If we did not have an AI-only (decision aid) condition, a reasonable but flawed inference would have been that explainable AI enhances decisions."
    },
    {
      "heading": "5 Conclusions and Future Work",
      "text": "Existing research on explainable AI focuses on the usability, trust, and interpretability of the explanation. In this paper, we fill in the research blank by investigating whether explainable AI can improve human decision-making. We design a behavioral experiment in which each participant recruited using Amazon Mechanical Turk is asked to complete 50 test trials in one of six experimental conditions. Our experiment is conducted on two real datasets to compare human decision with an AI prediction and an AI with explanation. Our experimental results demonstrate that AI predictions alone can generally improve human decision accuracy, while the advantage of explainable AI is not conclusive. We also show that users tend to follow AI predictions more often when the AI predictions are accurate. In addition, AI with or without explanation can increase the confidence of human users which, on average, was well-calibrated to user decision accuracy.\nIn the future, we plan to investigate whether explainable AI can help improve fairness, safety, and ethics by increasing the transparency of AI models. Human decision-making is a key outcome measure, but it is certainly not the only goal for explainable AI. We also plan to explore the difference of distributions in the error space between human decision and AI predictions, especially at decision boundaries. Also, whether human-machine collaboration is feasible through interactions in closed feedback loops. We will also expand our datasets to include other data formats such as images.\nBroader Impact\nFor many important critical decisions, depending on the AI model prediction may not be enough. Furthermore, many recent regulations such as General Data Protection Regulation (GDPR) [14] allow the human to potentially audit of AI predictions. Therefore, it is critical to understand whether the explanations provided by explainable AI methods improve the overall prediction accuracy and help human decision makers to detect errors. To our knowledge, this is the first work that tries to understand the impact of explainable AI models in improving human decision-making. Our results indicate that although the existence of an AI model may improve human decision-making, the explanations provided may not automatically improve the accuracy. We believe that our results could help ignite the needed research to explore how to better integrate explanations, AI models, and human operators to have better outcomes compared with AI models or humans alone.\nAcknowledgments and Disclosure of Funding\nThe views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Combat Capabilities Development Command Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation. M.K. and Y.Z. were supported by the Army Research Laboratory/Army Research Office under grant W911NF-17-1-0356, and NSF IIS-1939728. We thank Jason Radford for help implementing the experiment on the Volunteer Science platform and Katelyn Morris for independently coding the comments. We also thank Dan Cassenti for input on the paper, and Jessica Schultheis and Alan Breacher for editing the paper."
    },
    {
      "heading": "1 Age Distribution",
      "text": "See Figure 1 for the age distribution of participants. Most participants were in the 25-34 age group.\n0\n40\n80\n120\n160\n18\u221224 25\u221234 35\u221244 45\u221254 55\u221264 65+ Age\nN um\nbe r\nof P\nar tic\nip an\nts\nFigure 1: Age distribution of participants."
    },
    {
      "heading": "2 Self-Reported Decision Strategies",
      "text": "We performed an exploratory analysis on the self-reported decision strategies, qualitatively described by users in their comments. We coded strategies using six categories\u2014see the first column in Table 2. First, comments were independently coded by two raters: the third author and a research assistant. Inter-rater reliability was good, \u03ba = 0.70, p < 0.001. Second, we reached consensus to resolve all discrepancies and finalize categorical coding of decision strategies.\nThe majority of participants commented that they primarily relied on the data to make decisions, especially in the control condition: n = 74. The dominant reported strategy in the two AI conditions was primarily using the data to make decisions without any mention of AI: n = 80. Primary use of AI (n = 16) was surprisingly low. Although partial use of AI, including the possibility of explanations in the explainable AI condition, was higher (n = 39), it was still limited compared to the more widely used strategy of making decisions using the data, whereas the number of participants using each strategy was generally comparable between the two datasets, see Table 2. There were minor\nPreprint. Under review.\nar X\niv :2\n00 6.\n11 19\n4v 1\n[ cs\n.L G\n] 1\n9 Ju\nn 20\n20\ndifferences in more frequent use of the AI and data together for COMPAS over the Census dataset. This was despite the higher mean AI accuracy for the Census income dataset over COMPAS.\nWe compared the accuracy of participants who self-reported different strategies, grouping them into those who mentioned using AI (in the AI and AI Explanation conditions; Primarily AI and AI and data), those who did not mention using AI (Data and gut, Primarily gut, and Primarily data), and those who provided blank or nonsense strategies (Could not be coded, labelled as NA). As shown in Figure 2, mean participant accuracy was very similar for those who mentioned using the AI versus those who did not. In the future, it may be useful to compare the types of decision errors for AI versus AI with explanation as well as errors based on self-reported strategies. Different types of errors could produce similar decisional accuracy, if the overall error rate is similar. However, participants whose self-reported strategies were blank or did not make sense did show a lower mean accuracy on the task, perhaps indicating a lower level of effort or engagement with the task overall with accuracy nearing or even at chance performance (50%). The number of users with no self-reported strategy was comparable across all six conditions."
    },
    {
      "heading": "3 Speed-Accuracy Tradeoff",
      "text": "We used linear regression to explore the tradeoff between speed and accuracy across participants\u2014that is, whether slower participants tended to be more accurate and vice versa. We found overall a small but significant speed-accuracy tradeoff (\u03b2 = 1.01, p < 0.001, \u03b72 = 0.08), where each additional second of reaction time predicted a 1.01% increase in accuracy. As shown in Figure 3, there was a small interaction between the dataset condition and reaction time (F (1, 288) = 6.65, p < 0.02, \u03b72 = 0.02), indicating that the speed-accuracy tradeoff was present for participants in the Census dataset condition (\u03b2 = 1.02), but it was near zero for those in the COMPAS dataset condition (\u03b2 = \u22120.02). We did not find a significant effect of AI condition on the speed-accuracy tradeoff."
    }
  ],
  "title": "Does Explainable Artificial Intelligence Improve Human Decision-Making?",
  "year": 2020
}
