{"abstractText": "As a contribution to interpretable machine learning research, we develop a novel optimization framework for learning accurate and sparse twolevel Boolean rules. We consider rules in both conjunctive normal form (AND-of-ORs) and disjunctive normal form (OR-of-ANDs). A principled objective function is proposed to trade classification accuracy and interpretability, where we use Hamming loss to characterize accuracy and sparsity to characterize interpretability. We propose efficient procedures to optimize these objectives based on linear programming (LP) relaxation, block coordinate descent, and alternating minimization. Experiments show that our new algorithms provide very good tradeoffs between accuracy and interpretability.", "authors": [{"affiliations": [], "name": "Guolong Su"}, {"affiliations": [], "name": "Kush R. Varshney"}, {"affiliations": [], "name": "Dmitry M. Malioutov"}], "id": "SP:d23b4e16ede70b3c7dba3eebff8f659e4c7e6220", "references": [{"authors": ["B. Baesens", "R. Setiono", "C. Mues", "J. Vanthienen"], "title": "Using neural network rule extraction and decision tables for credit-risk evaluation.Manag", "year": 2003}, {"authors": ["P. Clark", "T. Niblett"], "title": "The CN2 induction algorithm", "venue": "Mach. Learn.,", "year": 1989}, {"authors": ["P. Domingos"], "title": "Unifying instance-based and rule-based induction.Mach", "year": 1996}, {"authors": ["A. Knobbe", "B. Cr\u00e9milleux", "J. F\u00fcrnkranz", "M. Scholz"], "title": "From local patterns to global models: The LeGo approach to data mining", "venue": "InProc. ECML PKDD Workshop (LeGo-08),", "year": 2008}, {"authors": ["B. Letham", "C. Rudin", "T.H. McCormick", "D. Madigan"], "title": "Building interpretable classifiers with rules using Bayesian analysis.Department of Stat", "venue": "Tech. Report tr609, Univ. of Washington,", "year": 2012}, {"authors": ["M. Lichman"], "title": "UCI machine learning repository", "venue": "http://archive.ics.uci.edu/ml, Univ. of Calif., Irvine, School of Information and Computer Sciences,", "year": 2013}, {"authors": ["E. Lima", "C. Mues", "B. Baesens"], "title": "Domain knowledge integration in data mining using decision tables: case studies in churn prediction.J", "venue": "Oper. Res. Soc.,", "year": 2009}, {"authors": ["L.O. 1987. Ramig", "C. Fox", "S. Sapir"], "title": "Parkinson\u2019s disease: speech and voice disorders and their treatment with the lee silverman voice treatment", "venue": "Int. J. ManMach. Studies,", "year": 1987}, {"authors": ["S. Salzberg"], "title": "A nearest hyperrectangle learning method", "venue": "Mach. Learn.,", "year": 1991}, {"authors": ["K.R. Varshney"], "title": "Engineering safety in machine learning", "venue": "In Proc. Inf. Theory Appl. Workshop,", "year": 2016}, {"authors": ["T. Wang", "C. Rudin"], "title": "Learning optimized Or\u2019s of And\u2019s", "venue": "arXiv preprint arXiv:1511.02210,", "year": 2015}, {"authors": ["T. Wang", "C. Rudin", "F. Doshi-Velez", "Y. Liu", "E. Klampfl", "P. MacNeille"], "title": "Bayesian Or\u2019s of And\u2019s for interpretable classification with application to context aware recommender systems", "venue": "Technical report,", "year": 2015}], "sections": [{"text": "ar X\niv :1\n60 6.\n05 79\n8v 1\n[s ta\nt.M L]\n1 8\nAs a contribution to interpretable machine learning research, we develop a novel optimization framework for learning accurate and sparse twolevel Boolean rules. We consider rules in both conjunctive normal form (AND-of-ORs) and disjunctive normal form (OR-of-ANDs). A principled objective function is proposed to trade classification accuracy and interpretability, where we use Hamming loss to characterize accuracy and sparsity to characterize interpretability. We propose efficient procedures to optimize these objectives based on linear programming (LP) relaxation, block coordinate descent, and alternating minimization. Experiments show that our new algorithms provide very good tradeoffs between accuracy and interpretability."}, {"heading": "1. Introduction", "text": "In applications where machine learning is used to aid human decision-making, it is recognized that interpretability of models is an important objective for establishing trust, adoption and safety and for offering the possibility of auditing and debugging (Freitas, 2014; Varshney, 2016; Lima et al., 2009; Letham et al., 2012; Vellido et al., 2012). Interpretable models can be learned directly from data or can result from the approximation of black-box models (Craven & Shavlik, 1996; Baesens et al., 2003).\nBoolean rules are considered to be one of the most interpretable classification models (Freitas, 2014). A onelevel rule is a conjunctive clause (AND-rule) or disjunctive clause (OR-rule) whereas a two-level rule is a conjunctive"}, {"heading": "2016 ICML Workshop on Human Interpretability in Machine", "text": "Learning (WHI 2016), New York, NY, USA. Copyright by the author(s).\nnormal form (CNF; AND-of-ORs) or disjunctive normal form (DNF; OR-of-ANDs). A mathematical proxy for interpretability of Boolean rules isparsity, i.e. the total number of features used in the rule (Feldman, 2000).\nLearning accurate and sparse two-level Boolean rules is a considerable challenge since it is combinatorial (Kearns et al., 1987). Even the simpler problem of learning a one-level rule is NP-hard (Malioutov & Varshney, 2013). Unlike one-level rules, two-level rules can represent any Boolean function of the input features (Fu\u0308rnkranz et al., 2012); this expressiveness of two-level rules also suggests that they are more challenging to learn than one-level rules. Due to this complexity, most existing solutions focus on heuristic and greedy approaches.\nThe main contribution of this paper is to introduce a unified optimization framework for learning two-level Boolean rules that achieve good balance between accuracy and interpretability, as measured by the sparsity of the rule. The objective function is a weighted combination of (a) classification errors quantified by Hamming distance between the current rule and the closest rule that correctly classifies a sample and (b) sparsity. Based on this formulation, block coordinate descent and alternating minimization algorithms are developed, both of which use an LP relaxation approach. Experiments show that two-level rules can have considerably higher accuracy than one-level rules and may show improvements over cutting edge approaches.\nThe two-level Boolean rules in this work are examples of sparse decision rule lists (Rivest, 1987), which have been extensively studied in various fields. A number of strategies have been proposed (Fu\u0308rnkranz et al., 2012): covering (Rivest, 1987; Clark & Niblett, 1989; Cohen, 1995; Fu\u0308rnkranz, 1999), bottom-up (Salzberg, 1991; Domingos, 1996), multi-phase (Liu et al., 1998; Knobbe et al., 2008), and the distillation of trained decision trees into decision lists (Quinlan, 1987). Unlike our proposed approach, the above strategies lack a single, principled objective functio\nto drive rule learning. Moreover, they employ heuristics that leave room for improvements on both accuracy and rule simplicity. In addition to the symbolic approaches above, Bayesian approaches inLetham et al.(2012) and Wang et al.(2015) apply approximate inference algorithms to produce posterior distributions over decision lists; however, the assignment of prior and likelihood may not always be clear, and certain approximate inference algorithms may have high computational cost.\nThere has been some prior work on optimization-based formulations for rule learning, the most relevant being Malioutov & Varshney(2013), where an LP framework is proposed to learn one-level rules from which set covering and boosting are used to construct two-level rules. Although we apply this clause learning method as a component in our algorithms, our work has significant differences from Malioutov & Varshney(2013). As discussed earlier, two-level rules are significantly more expressive and much more challenging to learn than a one-level rule. In addition, the greedy style of the set covering method leaves room for improvement and the weighted combination of clauses in boosted classifiers reduces interpretability. Another work on DNF learning (Wang & Rudin, 2015) provides a mixed integer program (MIP) formulation named OOA and a different heuristic formulation OOAx. The MIP in OOA is similar to our formulation with a different cost (0-1 error) but lacks an LP relaxation. OOAx is similar to the heuristic multi-phase strategy above."}, {"heading": "2. Problem Formulation", "text": "We consider supervised binary classification given a training dataset ofn samples, where each sample has a label yi \u2208 {0, 1} andd binary features1 ai,j \u2208 {0, 1} (1\u2264j\u2264d). The goal is to learn a classifiery\u0302(\u00b7) in CNF (AND-of-ORs) that can generalize well from the training dataset.2 In the lower level of the rule, each ofR clauses is formed by the disjunction of a selected subset of input features; in the upper level, the predictor is obtained as the conjunction of all clauses. Letting the decision variableswj,r \u2208 {0, 1} represent whether to include thejth feature in therth clause, the clause and predictor outputs are given by\nv\u0302i,r =\nd \u2228\nj=1\n(ai,jwj,r) , for 1 \u2264 i \u2264 n, 1 \u2264 r \u2264 R. (1)\ny\u0302i = R \u2227\nr=1\nv\u0302i,r, for 1 \u2264 i \u2264 n. (2)\n1We assume the negation of each feature is included as another input feature.\n2The presentation focuses on CNF rules, but the proposed algorithms apply equally to learning DNF rules using De Morgan\u2019s laws.\nTo mitigate the need for careful specification of the number of clausesR, we allow clauses to be \u201cdisabled\u201d by padding the input feature matrix with a trivial \u201calways true\u201d feature ai,0 = 1 for all i, with corresponding decision variables w0,r for all clauses. Ifw0,r = 1, then therth clause has output1 and thus drops out of the upper-level conjunction in a CNF rule. The sparsity cost forw0,r, i.e. for disabling a clause, can be lower than other variables or even zero.\nIn learning Boolean rules, it is desirable to use a finergrained measure of accuracy than the usual0-1 loss to distinguish between degrees of incorrectness and indicate where corrections are needed. Herein we propose measuring the accuracy of a rule on a single sample in terms of the minimal Hamming distance from the rule to anideal rule, i.e. one that correctly classifies the sample. The Hamming distance between two CNF rules is the number ofwj,r that are different in the two rules. Thus the minimal Hamming distance represents the smallest number of modifications (i.e. negations) needed to correct a misclassification.\nFor mathematical formulation, we introduceideal clause outputs vi,r with 1 \u2264 i \u2264 n and1 \u2264 r \u2264 R to represent a CNF rule that correctly classifies theith sample. The values ofvi,r are always consistent with the ground truth labels, i.e.yi = \u2227R\nr=1 vi,r for all 1 \u2264 i \u2264 n. We let vi,r have a ternary alphabet{0, 1,DC}, wherevi,r = DC means that we \u201cdon\u2019t care\u201d about the value ofvi,r. With this setup, ifyi = 1, thenvi,r = 1 for all 1 \u2264 r \u2264 R; if yi = 0, thenvi,r0 = 0 for at least one value ofr0, and we can havevi,r = DC for all r 6= r0. In implementation, vi,r = DC implies the removal of theith sample in the training or updating for therth clause, which leads to a different training subset for each clause.\nFor a givenvi,r, the minimal Hamming distance between the rth clauses only of a CNF rule and an ideal rule can be derived as follows. Ifvi,r = 1, at most one positive feature needs to be included to produce the desired output, so the minimal Hamming distance is given bymax {\n0, 1\u2212 \u2211d\nj=1 ai,jwj,r }\n. If vi,r = 0, anywj,r with ai,jwj,r = 1 needs to be negated to be correct, resulting in a minimal Hamming distance of \u2211d\nj=1 ai,jwj,r . Summing overi, r and defining the sparsity cost as the sum of the numbers of features used in each clause, the problem is formulated as\nmin wj,r, vi,r\nn \u2211\ni=1\nR \u2211\nr=1\n[\n1vi,r=1 \u00b7max\n{\n0,\n(\n1\u2212\nd \u2211\nj=1\nai,jwj,r\n)}\n+ 1vi,r=0 \u00b7\nd \u2211\nj=1\nai,jwj,r\n]\n+ \u03b8 \u00b7\nR \u2211\nr=1\nd \u2211\nj=1\nwj,r (3)\ns.t.\nR \u2227\nr=1\nvi,r = yi, \u2200i, (4)\nvi,r \u2208 {0, 1,DC}, wj,r \u2208 {0, 1}, \u2200i, j, r.\nThe ideal clause output constraint (4) requires thatvi,r = 1 for all r if yi = 1, as noted above. Foryi = 0, vi,r = 0 needs to hold for at least one value ofr while all othervi,r can beDC. The Hamming distance is minimized by setting\nvi,r0 = 0, where r0 = argmin 1\u2264r\u2264R\n\n\nd \u2211\nj=1\nai,jwj,r\n\n . (5)\nThe binary variableswj,r can be further relaxed to0 \u2264 wj,r \u2264 1. However, the resulting continuous relaxation is generally non-convex forR > 1. Additional simplifications are proposed in Section3 to make the continuous relaxations more efficiently solvable.\nLastly, it can be seen that lettingR = 1 in formulation (3) recovers the formulation for one-level rule learning in Malioutov & Varshney(2013)."}, {"heading": "3. Optimization Approaches", "text": "This section proposes a block coordinate descent algorithm and an alternating minimization algorithm to solve the regularized Hamming loss minimization in (3)."}, {"heading": "3.1. Block Coordinate Descent Algorithm", "text": "This algorithm considers the decision variables in a single clause (wj,r with a fixed r) as a block of coordinates, and performs block coordinate descent to minimize the Hamming distance objective function in (3). Each iteration updates a single clause with all the other(R \u2212 1) clauses fixed, using the one-level rule learning algorithm in Malioutov & Varshney(2013). We denoter0 as the clause to be updated.\nThe optimization of (3) even with(R \u2212 1) clauses fixed still involves a joint minimization overwj,r0 and the ideal clause outputsvi,r for yi = 0 (vi,r = 1 for yi = 1 are fixed), so the exact solution could still be challenging. To simplify, we fix the values ofvi,r for yi = 0 andr 6= r0 to the actual clause outputsv\u0302i,r in (1) with the currentwj,r (r 6= r0). Now we assignvi,r0 for yi = 0: if there exists vi,r = v\u0302i,r = 0 with r 6= r0, then this sample is guaranteed to be correctly classified and we can assignvi,r0 = DC to minimize the objective in (3); in contrast, ifv\u0302i,r = 1 holds for all r 6= r0, then the constraint (4) requiresvi,r0 = 0.\nThis derivation leads to the updating process as follows. To update therth\n0 clause, we remove all samples that have\nlabel yi = 0 and are already predicted as0 by at least one of the other(R \u2212 1) clauses, and then update therth0 clause with the remaining samples using the one-level rule learning algorithm.\nThere are different choices of which clause to update in an iteration. For example, we can update clauses cyclically\nor randomly, or we can try the update for each clause and then greedily choose the one with the minimum cost. The greedy update is used in our experiments.\nThe initialization ofwj,r in this algorithm also has different choices. For example, one option is the set covering method, as is used in our experiments. Random or all-zero initialization can also be used."}, {"heading": "3.2. Alternating Minimization Algorithm", "text": "This algorithm alternately minimizes with respect to the decision variableswj,r and the ideal clause outputsvi,r in (3). Each iteration has two steps: updatevi,r with the currentwj,r, and updatewj,r with the newvi,r. The latter step is simpler and will be first discussed.\nWith fixed values ofvi,r, the minimization overwj,r is relatively straight-forward: the objective in (3) is separated intoR terms, each of which depends only on a single clause wj,r with a fixedr. Thus, all clauses are decoupled in the minimization overwj,r, and the problem becomes parallel learning ofR one-level clauses. Explicitly, the update of therth clause removes samples withvi,r = DC and then uses the one-level rule learning algorithm.\nThe update overvi,r with fixedwj,r follows the discussion in Section2: for positive samplesyi = 1, vi,r = 1, and for the negative samplesyi = 0, vi,r0 = 0 for r0 defined in (5) andvi,r = DC for r 6= r0. For negative samples with a \u201ctie\u201d, i.e. non-uniquer0 in (5), tie breaking is achieved by a \u201cclustering\u201d approach. First, for each clause1 \u2264 r0 \u2264 R, we compute its cluster center in the feature space by taking the average ofai,j (for eachj) over samplesi for whichr0 is minimal in (5) (including ties). Then, each sample with a tie is assigned to the clause with the closest cluster center in \u21131-norm among all minimalr0 in (5).\nSimilar to the block coordinate descent algorithm, various options exist for initializingwj,r in this algorithm. The set covering approach is used in our experiments."}, {"heading": "4. Numerical Evaluation", "text": "This section evaluates the algorithms with UCI repository datasets (Lichman, 2013). To facilitate comparison with the most relevant prior work (Malioutov & Varshney, 2013), we use all8 datasets in that work. Each continuous valued feature is converted to binary using10 quantile thresholds. In addition, we use2 large datasets: MAGIC gamma telescope (MAGIC) and Musk version 2 (Musk).\nThe goal is to learn a DNF rule (OR-of-ANDs) from each dataset. We use stratified10-fold cross validation and then average the error rates. All LPs are solved by CPLEX version 12 (IBM). The sparsity parameter\u03b8 is tuned between10\u22124 and50 using a second cross validation\nwithin the training partition.\nAlgorithms in comparison and their abbreviations are: block coordinate descent (BCD), alternating minimization (AM), one-level conjunctive rule learning (OCRL, equivalent to settingR = 1 for BCD or AM) and set covering (SC), the last two fromMalioutov & Varshney (2013), decision list in IBM SPSS (DList), decision trees (C5.0: C5.0 with rule set option in IBM SPSS, CART: classification and regression trees algorithm in Matlab\u2019s classregtree function), and RIPPER fromCohen(1995). We set the maximum number of clausesR = 5 for the BCD, AM, and SC algorithms, and set the maximum number of iterations in BCD and AM as100.\nWe first show the test error rates and the sparsity of the rules. The mean test error rates and the standard error of the mean are listed in Table1. Due to space constraints, we refer the reader toMalioutov & Varshney(2013) for results from other classifiers that are not interpretable; the accuracy of our algorithms is generally quite competitive with them. Table2 provides the10-fold average of the sparsity of the learned rules as a measure for interpretability. No features are counted if a clause is disabled. The last rows in these tables show the averaged ranking of each algorithm on each dataset.\nTable 1 shows that two-level rules obtained by our algorithms (BCD and AM) are more accurate than the one-level rules from OCRL for almost all datasets, which\ndemonstrates the expressiveness of two-level rules. Among optimization-based two-level rule learning approaches, BCD and AM generally have superior accuracy to SC. All these approaches substantially outperform DList in terms of accuracy on all datasets. Compared with C5.0, BCD and AM obtain rules with much higher interpretability (many fewer features) and quite competitive accuracy. Compared with CART, BCD has higher or equal accuracy on all datasets except for Musk, and AM is also superior overall. RIPPER appears to be slightly stronger than BCD and AM for the 8 datasets fromMalioutov & Varshney (2013). However, on the two larger datasets (MAGIC and Musk), RIPPER selects a rather large number of features, and further study on large datasets is needed to clarify the advantages and disadvantages of the algorithms. In addition, AM achieves the highest accuracy on Pima, and BCD obtains the highest accuracy on WDBC.\nBelow is an example of a learned rule that predicts Parkinson\u2019s disease from voice features. (It is consistent with known low frequency and volume change reduction in the voices of Parkinson\u2019s patients (Ramig et al., 2004).)\nIF 1. voice fractal scaling exponent> \u22126.7; OR 2. max vocal fundamental frequency< 236.4 Hz; AND\nmin vocal fundamental frequency< 181.0 Hz; AND shimmer:DDA< 0.0361; AND recurrence period density entropy< 0.480;\nTHEN this person has Parkinson\u2019s."}, {"heading": "5. Conclusion", "text": "Motivated by the need for interpretable classification models, this paper has provided an optimization-based formulation for two-level Boolean rule learning. These complement the more heuristic strategies in the literature on two-level Boolean rules. Numerical results show that twolevel Boolean rules typically have considerably lower error rate than one-level rules and provide very good tradeoffs between accuracy and interpretability with improvements over state-of-the-art approaches."}, {"heading": "Acknowledgment", "text": "The authors thank V. S. Iyengar, A. Mojsilovic\u0301, K. N. Ramamurthy, and E. van den Berg for conversations and support."}], "title": "Interpretable Two-Level Boolean Rule Learning for Classification", "year": 2020}