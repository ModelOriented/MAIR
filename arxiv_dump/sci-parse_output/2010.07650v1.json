{
  "abstractText": "Interpretable machine learning is an emerging field providing solutions on acquiring insights into machine learning models\u2019 rationale. It has been put in the map of machine learning by suggesting ways to tackle key ethical and societal issues. However, existing techniques of interpretable machine learning are far from being comprehensible and explainable to the end user. Another key issue in this field is the lack of evaluation and selection criteria, making it difficult for the end user to choose the most appropriate interpretation technique for its use. In this study, we introduce a meta-explanation methodology that will provide truthful interpretations, in terms of feature importance, to the end user through argumentation. At the same time, this methodology can be used as an evaluation or selection tool for multiple interpretation techniques based on feature importance.",
  "authors": [
    {
      "affiliations": [],
      "name": "Ioannis Mollas"
    },
    {
      "affiliations": [],
      "name": "Nick Bassiliades"
    },
    {
      "affiliations": [],
      "name": "Grigorios Tsoumakas"
    }
  ],
  "id": "SP:13f33581864b6be6d21ffc77167145ee8dc90591",
  "references": [
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking Inside the Black- Box: A Survey on Explainable Artificial Intelligence (XAI)",
      "venue": "IEEE Access, 6 ",
      "year": 2018
    },
    {
      "authors": [
        "P. Besnard",
        "A. Hunter"
      ],
      "title": "Argumentation Based on Classical Logic",
      "venue": "Springer US, Boston, MA",
      "year": 2009
    },
    {
      "authors": [
        "F. Bex",
        "D. Walton"
      ],
      "title": "Combining explanation and argumentation in dialogue",
      "venue": "Argument & Computation, 7 ",
      "year": 2016
    },
    {
      "authors": [
        "L. Breiman"
      ],
      "title": "Random forests",
      "venue": "Machine Learning, 45 ",
      "year": 2001
    },
    {
      "authors": [
        "L. Carstens",
        "F. Toni"
      ],
      "title": "Using argumentation to improve classification in natural language problems",
      "venue": "ACM Transactions on Internet Technology (TOIT), 17 ",
      "year": 2017
    },
    {
      "authors": [
        "C. Cayrol",
        "M.-C. Lagasquie-Schiex"
      ],
      "title": "On the acceptability of arguments in bipolar argumentation frameworks",
      "venue": "European Conference on Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Springer",
      "year": 2005
    },
    {
      "authors": [
        "O. Cocarascu",
        "F. Toni"
      ],
      "title": "Argumentation for machine learning: A survey",
      "venue": "COMMA,",
      "year": 2016
    },
    {
      "authors": [
        "J.S. Cramer"
      ],
      "title": "The origins of logistic regression",
      "venue": "SSRN Electronic Journal, ",
      "year": 2002
    },
    {
      "authors": [
        "K. Cyras",
        "O. Cocarascu",
        "F. Toni"
      ],
      "title": "Explanatory predictions with artificial neural networks and argumentation",
      "venue": "IJCAI/ECAI Workshop on Explainable Artificial Intelligence ",
      "year": 2018
    },
    {
      "authors": [
        "K. Cyras",
        "K. Satoh",
        "F. Toni"
      ],
      "title": "Explanation for case-based reasoning via abstract argumentation",
      "venue": "Computational Models of Argument - Proceedings of COMMA 2016, Potsdam, Germany, 12-16 September, 2016, IOS Press",
      "year": 2016
    },
    {
      "authors": [
        "J. Dastin"
      ],
      "title": "Amazon scraps secret ai recruiting tool that showed bias against women",
      "venue": "San Fransico, CA: Reuters. Retrieved on October, 9 ",
      "year": 2018
    },
    {
      "authors": [
        "P. Domingos"
      ],
      "title": "Knowledge discovery via multiple models",
      "venue": "Intelligent Data Analysis, 2 ",
      "year": 1998
    },
    {
      "authors": [
        "M. Du",
        "N. Liu",
        "F. Yang",
        "S. Ji",
        "X. Hu"
      ],
      "title": "On attribution of recurrent neural network predictions via additive decomposition",
      "venue": "The World Wide Web Conference",
      "year": 2019
    },
    {
      "authors": [
        "P.M. Dung"
      ],
      "title": "An argumentation-theoretic foundation for logic programming",
      "venue": "The Journal of logic programming, 22 ",
      "year": 1995
    },
    {
      "authors": [
        "T.L. Fine",
        "S.L. Lauritzen",
        "M. Jordan",
        "J. Lawless",
        "V. Nair"
      ],
      "title": "Feedforward Neural Network Methodology",
      "venue": "Springer-Verlag, Berlin, Heidelberg, 1st ed.",
      "year": 1999
    },
    {
      "authors": [
        "S. Geisser"
      ],
      "title": "Predictive inference",
      "venue": "vol. 55, CRC press",
      "year": 1993
    },
    {
      "authors": [
        "P. Gottlieb"
      ],
      "title": "Aristotle on non-contradiction",
      "venue": "The Stanford Encyclopedia of Philosophy, E. N. Zalta, ed., Metaphysics Research Lab, Stanford University, spring 2019 ed.",
      "year": 2019
    },
    {
      "authors": [
        "R. Kohavi"
      ],
      "title": "Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid",
      "venue": "Proceedings of the Second International Conference on Knowledge Discovery and Data Mining",
      "year": 1996
    },
    {
      "authors": [
        "J. Larson",
        "S. Mattu",
        "L. Kirchner"
      ],
      "title": "and J",
      "venue": "Angwin, How We Analyzed the COMPAS Recidivism Algorithm",
      "year": 2020
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.-I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds., Curran Associates, Inc.",
      "year": 2017
    },
    {
      "authors": [
        "D.A. Melis",
        "T. Jaakkola"
      ],
      "title": "Towards robust interpretability with self-explaining neural networks",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2018
    },
    {
      "authors": [
        "I. Mollas",
        "N. Bassiliades",
        "G. Tsoumakas"
      ],
      "title": "Lionets: Local interpretation of neural networks through penultimate layer decoding",
      "venue": "ECML PKDD 2019 AIMLAI XKDD Workshop. W\u00fcrzburg, Germany",
      "year": 2019
    },
    {
      "authors": [
        "A. Moore",
        "V. Murdock",
        "Y. Cai",
        "K. Jones"
      ],
      "title": "Transparent tree ensembles",
      "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, ACM",
      "year": 2018
    },
    {
      "authors": [
        "M. Mo\u017eina",
        "J. \u017dabkar",
        "I. Bratko"
      ],
      "title": "Argument based machine learning",
      "venue": "Artificial Intelligence, 171 ",
      "year": 2007
    },
    {
      "authors": [
        "I. Rahwan",
        "G.R. Simari"
      ],
      "title": "Argumentation in artificial intelligence",
      "venue": "vol. 47, Springer",
      "year": 2009
    },
    {
      "authors": [
        "I. Rahwan",
        "R. Simari"
      ],
      "title": "Guillermo",
      "venue": "Argumentation in artificial intelligence, vol. 47, Springer",
      "year": 2009
    },
    {
      "authors": [
        "G.D.P. Regulation"
      ],
      "title": "Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data",
      "venue": "and repealing directive 95/46, Official Journal of the European Union (OJ), 59 ",
      "year": 2016
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD \u201916, ",
      "year": 2016
    },
    {
      "authors": [
        "T. Ribeiro"
      ],
      "title": "Marco",
      "venue": "S. Singh, and C. Guestrin, Anchors: High-Precision Model-Agnostic Explanations, in Thirty-Second AAAI Conference on Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "R.R. Selvaraju",
        "M. Cogswell",
        "A. Das",
        "R. Vedantam",
        "D. Parikh",
        "D. Batra"
      ],
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "venue": "International Journal of Computer Vision, 128 ",
      "year": 2019
    },
    {
      "authors": [
        "V.N. Vapnik"
      ],
      "title": "The Nature of Statistical Learning Theory",
      "venue": "Second Edition, Statistics for Engineering and Information Science, Springer",
      "year": 2000
    },
    {
      "authors": [
        "A. Von Eye",
        "C.C. Clogg"
      ],
      "title": "Categorical variables in developmental research: Methods of analysis",
      "venue": "Elsevier",
      "year": 1996
    },
    {
      "authors": [
        "J. Westerhoff"
      ],
      "title": "N\u0101g\u0101rjuna\u2019s\u201d catuskoti",
      "venue": "Journal of Indian Philosophy, 34 ",
      "year": 2006
    }
  ],
  "sections": [
    {
      "text": "Interpretable machine learning is an emerging field providing solutions on acquiring insights into machine learning models\u2019 rationale. It has been put in the map of machine learning by suggesting ways to tackle key ethical and societal issues. However, existing techniques of interpretable machine learning are far from being comprehensible and explainable to the end user. Another key issue in this field is the lack of evaluation and selection criteria, making it difficult for the end user to choose the most appropriate interpretation technique for its use. In this study, we introduce a meta-explanation methodology that will provide truthful interpretations, in terms of feature importance, to the end user through argumentation. At the same time, this methodology can be used as an evaluation or selection tool for multiple interpretation techniques based on feature importance."
    },
    {
      "heading": "1 Introduction",
      "text": "While we witness a revolutionary adoption of artificial intelligence (AI) systems in our everyday activities, we notice that many of them advance through the field of machine learning (ML). As a result of this development of AI and ML, a number of ethical problems affecting our society have arisen, and thus the fields of explainable AI (XAI) and interpretable ML (IML) have emerged. Specifically, IML promises approaches for the identification of discrimination phenomena on ML models [11, 20], and compliance of industry on legal frameworks [28]. Eventually, ML practitioners and researchers, developing stronger and more accurate models through IML, could understand and explain their tasks and even identify issues, for example biases in a model, that would otherwise remain undetected.\nTechniques for IML models can be classified, among other aspects [1], as global techniques that expose the entire logic of a model and local techniques that aim to explain a single prediction made by a model. More-\n\u2217This research is supported by the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 825619, AI4EU Project \u2020Dept. of Informatics, Aristotle University of Thessaloniki, Greece, 54636, {iamollas,nbassili,greg}@csd.auth.gr\nover, when an interpretation technique can be applied indifferently to any ML model, we speak of a modelagnostic technique, and when it can be applied to a specific model, we have a model-specific technique. Feature Importance (FI) interpretation techniques calculate the influence of each feature to the prediction, in a global or local essence. Methods like LIME (modelagnostic) [29] or LioNets (model-specific) [23] are few of IML techniques which fall into this category.\nArgumentation is an AI field that concerns the concept of designing automated systems that address specific problems by providing syllogisms to support a thesis, a solution [26]. The solution is the conclusion, and the syllogism is a valid chain of reasoning, also referred to as a valid argument. Aristotle introduced the principle of contradiction, arguing that opposite statements can not be true simultaneously [18], thus opposing arguments can not be valid at the same time. On the other hand, the Tetralemma [34] supports that a statement can be true or not true at the same time, and therefore an argument can be valid and invalid in the same time. Based on these, different kinds of Argumentation Frameworks (AF) have been designed.\nIML and argumentation meet the same goal of persuading someone to accept the validity of a decision. There are works combining explanation and argumentation towards interactive dialogues [3], but in a theoretical and abstracted way. Whether the explanations are arguments or not is a matter of debate in the philosophy of science. An interesting view discriminates between the arguments and the explanations, provided that the arguments are used to justify something in doubt, while the explanations are used to express an interpretation of something that is incomprehensible [3].\nThat being said, several of the techniques used to acquire interpretations from ML models are approximations of the real interpretation, which is probably completely unknown. Therefore, the validity of them is questionable. This paper introduces the \u201cAltruist\u201d (Argumentative expLanaTions thRoUgh local InterpretationS of predicTive models) method for transforming FI interpretations of ML models into insightful and validated explanations using argumentation based on classi-\nSubmitted paper on SDM 2021 by SIAM\nar X\niv :2\n01 0.\n07 65\n0v 1\n[ cs\n.L G\n] 1\n5 O\nct 2\n02 0\ncal logic [2]. Altruist provides the local maximum truthful interpretation, as well as reasons for the truthfulness justification, and can be used as an easy-to-choose tool between X number of different interpretation techniques based on a few specific criteria. The horsepower of the Altruist is showcased in Section 5. Altruist has innate virtues such as truthfulness, transparency and user-friendliness that characterise it as an apt tool for the XAI community."
    },
    {
      "heading": "2 Background",
      "text": "In this section, we present the foundations of argumentation and IML.\n2.1 Argumentation. A lot of frameworks have been developed in Argumentation, with the same design horizon, which is a well-defined mathematical foundation [27]. Abstract [15], Bipolar [6] and Classic Logicbased [2] argumentation are few of the most well-known Argumentation Frameworks (AF ) in the AI field.\nIn Abstract Argumentation Framework (AAF ) [15], the argument is an abstract entity that is mainly defined by its relations to other arguments. There is no obligation to fully understand the semantic of such an argument. These relations between arguments are called attacks, and thus we can denote an AAF = \u3008AR, attacks\u3009, where AR is the set of arguments, and attacks are the binary relationships of the arguments in AR. The semantics of the AAF have several properties, such as Stable, Preferred, Complete and Grounded, among others.\nAnother well-established argumentation framework is the Bipolar Argumentation Framework (BAF ) [6], which presents the notions of support and attack between arguments. A BAF is represented as a triplet \u3008A,R+,R\u2212\u3009. where A is the set of arguments, while R+ and R\u2212 represent a binary relation of the arguments, the support and the attack, respectively.\nArgumentation based on Classical Logic [2] concerns a framework defined exclusively with logic rules and terms. A sequence of inference to a claim is an argument in this framework. Specifically, an argument is a pair \u3008\u03a6, \u03b1\u3009 such that \u03a6 is consistent (\u03a6 0\u22a5), \u03a6 ` \u03b1, and \u03a6 is a minimal subset of \u2206 (a knowledge base), which means that there is no \u03a6\u2032 \u2282 \u03a6 such that \u03a6\u2032 ` \u03b1. ` represents the classical consequence relation. In this framework counterarguments, the defeaters, are defined as well. \u3008\u03a8, \u03b1\u3009 is a counterargument for \u3008\u03a6, \u03b2\u3009 when \u03b2 ` \u00ac(\u03c61 \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u03c6n) for some {\u03c61, . . . , \u03c6n} \u2286 \u03a6. Furthermore, two more specific notions of a counterargument are defined as undercut and rebuttal arguments. Some arguments specifically contradict other argument\u2019s support which leads to the undercut notion.\nAn undercut for an argument \u3008\u03a6, \u03b1\u3009 is an argument \u3008\u03a8,\u00ac(\u03c61 \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u03c6n)\u3009 where {\u03c61, . . . , \u03c6n} \u2286 \u03a6. If there are two arguments in objection, we have the most direct form of dispute. This case is represented by the concept of a rebuttal. An argument \u3008\u03a8, \u03b2\u3009 is a rebuttal for an argument \u3008\u03a6, \u03b1\u3009 if \u03b2 \u2194 \u00ac\u03b1 is a tautology.\nArgumentation begins when an initial argument is put forward, and some claim is made. This will lead to an argumentation tree Tr with root node the initial argument. An objection can be posed in the form of a counterargument. This would be a lead to the argumentation tree. The latter is addressed in turn, ultimately giving rise to a counterargument. Finally, a judge function should decide if an argument tree Tr is rather Warranted or Unwarranted, based on marks assigned to each node as either U for undefeated or D for defeated. An argument tree Tr is judged as Warranted, Judge(Tr) = Warranted, if Mark(Ar) = U where Ar is the root node of Tr is undefeated. For all nodes Ai \u2208 Tr, if there is child Aj of Ai such that Mark(Aj) = U , then Mark(Ai) = D, otherwise Mark(Ai) = U.\n2.2 Interpretation of ML models. The ability of ML models to give users a valuable insight into their structure and decisions is known as interpretation feature [1]. Techniques which aim to shed light on the rationale of a ML model can be categorised as modelagnostic and model-specific approaches. Model-agnostic techniques are designed to interpret any ML model indifferently, while model-specific techniques concentrate on a particular family of ML algorithms. In addition, interpretation techniques can be differentiated by global and local-scopic approaches. Ultimately, the form of the interpretation, that is, the manner in which the technique presents its results, is also an essential aspect.\nDecision trees, rule-based and linear models are inherently (intrinsic) interpretable, and they can provide both local and global information. Model-agnostic interpretation techniques such as Anchors [30] choose to lay down rules for local explanation, as well as several model-specific approaches, providing global (e.g. [12]) or local (e.g. [24]) explanations.\nModel-specific techniques, such as GradCam [31] locally interpreting neural networks for image recognition, or object detection, present their findings using heatmaps, also known as saliency maps, or bounding boxes. In addition, LIME, a model-agnostic local-based interpretation technique, introduces a variation of its main algorithm focusing on such image-oriented models, providing as well saliency maps as explanations.\nThe aforementioned techniques are also able to provide their explanations in the form of feature impor-\nSubmitted paper on SDM 2021 by SIAM\ntance, when the input data are tabular or textual. In this family of model-agnostic interpretation techniques for black-box models there are the global-based variants of feature permutation importance (PI) methods [4], as well as SHAP [21], an alternative method for calculating the importance of a feature for both global and local aspects of any black-box model."
    },
    {
      "heading": "3 Related Work",
      "text": "A set of techniques use arguments as explanations for ML models [7]. AA-CBR [10] is an inherently interpretable ML model for classification tasks that combines case-based reasoning with AAF. In AA-CBR, there are cases, where each case is a set of features and an outcome, and the objective is to predict the outcome of new cases. ANNA [9] attempts to solve classification problems by using neural auto-encoders for feature selection, AAF for generation of arguments, and AA-CBR for prediction tasks, offering explanations in the form of arguments.\nABML [25] is a technique heavily inspired by the CN2 classification algorithm, which incorporates arguments into the learning process and aims to reduce the space of the hypotheses. By this and the interpretable essence of CN2, explanations can be given in the form of arguments. Lastly, CleAr [5] is a classification technique that incorporates knowledge in the form of arguments with supervised learning applied in computational linguistic tasks. The arguments are based on BAF extended with base scores, also known as QuAD or Quad framework and quantitative semantics.\nAnother critical domain within the IML research area is the evaluation metrics, which are available for benchmarking and selection processes. There are a few metrics, such as fidelity or the number of non-zero weights [1], that for researchers in this field are the most\ncommon options for assessing feature importance based interpretation techniques. Nonetheless, these metrics can not reflect the effectiveness of two or more approaches. As a result, measures such as robustness [22] and faithfulness [13] have been used to better represent the superiority of a technique over another.\nThe latter, faithfulness, is applicable in feature importance based techniques and takes the positive importance into account, to evaluate its truthfulness. We extend this concept by defining the importance (zj) assigned to a feature fj as truthful when the expected changes to the output of the ML model are correctly observed with respect to the changes that occur in the value of this feature."
    },
    {
      "heading": "4 Altruist",
      "text": "Having defined the concept of truthfulness in the previous section, we can present the Altruist, a methodology that aims to tackle a few problems of feature importance-based approaches, such as lack of userfriendliness and the probability of untruthfulness, using logic-based argumentation. The ultimate objective of Altruist is to present the maximum subset of the interpretation for an instance that will be truthful, as well as to provide a set of arguments to support this output. Altruist can provide reasons to justify why this maximum subset is truthful, as well as why the features excluded from the set were untruthful. Finally, it can be used as a selection or evaluation tool between multiple feature importance interpretation techniques.\nThe methodology of Altruist consists of 5 components and it is presented in Figure 1. The first component includes the ML model, the second component is the interpretation technique(s), the third component is the Altruist truthfulness investigator (ATI), the fourth component is the argumentation system, while the fifth\nSubmitted paper on SDM 2021 by SIAM\ncomponent offers the final interpretation, which will be truthful, and depending on the application can be even a dialogue or a tree.\n4.1 Machine Learning Model. The first component of the technique is the ML model to be interpreted. The ML model could be any model that it is able to provide continuous values as output (e.g. probabilities). This component is referred to as ML. This ML component is trained on the input dataset D = [xi, . . . , xN ], which contains N instances with |F | features, where F = [f1, . . . , f|F |]. Each xi \u2208 D instance has a set of values for the |F | features xi = [v1,i, . . . , v|F |,i]. The output of this component is the prediction probabilities for an instance xi.\n4.2 Feature Importance Technique(s). This component concerns the interpretation technique(s), and is highly correlated with the aforementioned component. The interpretation technique(s) must fall within the category of feature importance and must therefore provide explanations in the form of sets of features accompanied by an indicator of importance. Such techniques may be global or local, as well as model-agnostic or model-specific. The output of this component, given a specific xi, and the ML component, is denoted as Z = [z1, ..., z|F |], where zj \u2208 R. It is possible to have multiple interpretation techniques, in order to let Altruist choose the best (more truthful) interpretation. Then, for T different techniques we will have Z t, where t \u2208 [0, T ].\n4.3 Altruist Truthfulness Investigator. The third component of this methodology is the Altruist Truthfulness Investigator (ATI). In the case of a specific xi, this component is based on the interpretation techniques clarified in the previous component as input. ATI then measures the distribution of the training set, or the local neighbourhood, if the interpretation method is a technique which relies on neighbours generation, like LIME. Based on this distribution, a set of 2 \u2217 |F | tests are performed. The local monotonicity of each feature f is evaluated with regard to the effect of the interpretation technique (Positive, Negative or Neutral) and the distribution of the feature, in order to investigate the truthfulness of the assigned importance of each feature. This technique is compatible with datasets with continuous or categorical features (one-hot or ordinal encoded [33]).\nIt is worth demonstrating this with an example. For a random instance xi assigned to class Y with probability 0.7, the feature f1, with a value of v1,i = 1, has acquired an importance z1 = 0.5 (Positive). Altruist\nwill seek to increase and decrease the value of the feature by using Gaussian noise based on its distribution, vinc1,i = 1.21 and vdec1,i = 0.85. By querying the ML component it will observe the alteration of the model\u2019s output. In this example for the vinc1,i the model\u2019s output was raised to 0.85, and for the vdec1,i the output remained stable. For a positive importance, these alterations should be increased probability, for the increased value modification, and decreased probability, for the decreased value. Thus, the feature is characterised as untruthful by the investigator. For feature importances with negative notion we will expect the inverse behaviour, namely for an increased value vinc1,i to observe decreased probability, and for a decreased value vdec1,i increased probability. If the importance was neutral, z1 = 0, we would expect the probability not to be altered for both the increased and decreased values, vinc1,i and v dec 1,i , respectively, or to be altered between a very tight range \u03b4, e.g. 0.7499 to 0.7501 or 0.7497. Tolerance \u03b4 is defined either manually by the user or is set to a default value. 0.01 was selected after experimentation as default \u03b4 value because represents an insignificant alteration of a probability.\nThis component does not judge immediately the truthfulness of a feature, but it generates some predicates in the output, which will work as arguments for the following component. The reason why we need the following component is that it was simpler to turn this problem into an argumentation problem and solve it by using logic instead of manually coding it. In fact, using this approach, the results for the selection of features are also justifiable, so we have an all-inclusive transparent method. Finally, using logic and argumentation, the output of the system is a set of natural language arguments that can be easily used by the user, or can be even utilised in a user-chatbot dialog.\n4.4 Argumentation System. The fourth component, having as input the predicates generated in the ATI component, is responsible for testing the truthfulness of each feature, for determining the maximum set of features that are all truthful, and for providing explanations on this selection. An argumentation framework is employed to accomplish this. The AF of the system is based on Classical Logic. Thus, except from the kind of arguments that can exist in the framework, we should define the rebuttal and undercut attacks, as well as the argumentation tree and the judge function.\nFirstly, we have to define IMP-Importance (the effect of the feature), ALT-Alteration (the alteration on the value of the feature) and EXP-Expectation (the expected behaviour of the ML component) as described in the previous Section (4.3):\nSubmitted paper on SDM 2021 by SIAM\nIMP: Importance \u2208 [Positive, Negative, Neutral]\nALT: Alteration \u2208 [Increasing, Decreasing]\nEXP: Expectation \u2208 [Increasing, Decreasing, Remaining Stable]\nThen, the atoms that may exist in a dialogue between the user and the system, are presented in order to understand the interpretation, and even to question the truthfulness of the interpretation. There are six different kinds of atoms:\na: The explanation is untrusted\nb: The explanation is trusted\ncj : The zj is untruthful\ndj : The zj is truthful since it has an IMP influence, and when fj \u2019s value is ALT locally, we observe that the probability is EXP, and when fj \u2019s value is ALT locally, we observe that the probability is EXP\nej,ALT : fj has an IMP influence and is therefore expected the probability to be EXP by ALT its value.\nfj,ALT : fj \u2019s value got ALT and evaluated and the probability is EXP\nBased on the aforementioned atoms, we can present few arguments in the form of \u3008\u03a6, \u03b1\u3009, where \u03b1 is the claim of the argument and \u03a6 is the support of the argument:\n\u03b11: \u3008{a}, a\u3009\n\u03b12: \u3008{b, b\u2192 \u00aca},\u00aca\u3009\n\u03b13: \u3008{c1, . . . , cj , (c1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 cj)\u2192 \u00acb},\u00acb\u3009\n\u03b14: \u3008{dj , dj \u2192 \u00accj},\u00accj\u3009\n\u03b15: \u3008{ej,inc, ej,dec, (ej,inc \u2227 ej,dec)\u2192 \u00acdj},\u00acdj\u3009\n\u03b16: \u3008{fj,ALT , fj,ALT \u2192 \u00acej,ALT },\u00acej,ALT \u3009\nIt is important to note that the first argument, \u03b11, is trivial and is implemented in the argumentation context only to produce arguments that could easily be converted into discussions, e.g. in chatbots.\nA counterargument is defined as an argument that conflicts with another argument\u2019s claim or support. Now it is feasible to define the attack relations between such arguments. We will use the special cases of attacks, undercut and rebuttal as discussed in Section 2.1. Table 1 presents these attacks atts \u2208 [rebuttal, undercut].\nWe can proceed to the definition of the argumentation tree after determining the atoms, the arguments\nand the attacks between them. For each argumentation framework there are infinite argumentation trees. An argumentation tree begins when an initial argument is presented as a claim, and is called root argument. In the form of a counterargument, an objection, or objections, is raised. This is articulated in turn, eventually leading to a counterargument if it is feasible. In Altruist, the root argument is the \u03b11. Thus, the argumentation tree, which is going to be created, will be similar to the structure of the tree presented in Figure 2.\nThe ultimate goal is to decide whether the root argument of this tree is valid. We will use the following judge function Judge(Tr) = Unwarranted, if Mark(\u03b11) = D. This means that we are going to consider that the argumentation tree Tr is unwarranted, if the root argument \u03b11 is defeated. This will help us counter attack the claim that the interpretation given by the feature importance technique is not untrusted. The counter-arguments defeating the root argument and its supporters, are forming the explanation on why the interpretation is trusted, as well, towards more transparent actions. In order to judge the Tr we utilise a Prolog program, which will judge the tree, and it will provide the arguments in a natural language form.\nIn case the Prolog program judges the root argument \u03b11 as undefeated, and therefore the argumentation tree as Warranted, this would mean that one or more features were untruthful. Then, these features are discarded, and then by re-examining the argumentation tree, we expect to be Unwarranted. The output to the following component is the new reduced interpretation,\nSubmitted paper on SDM 2021 by SIAM\nor interpretations in case of many techniques, which will be Z \u2032 = [zt1, z t 2, z u 3 ..., z u |F |], where z t i the truthful feature importances, and zui the untruthful. An explanatory example is presented in Section 4.6.\n4.5 Maximum Truthful Calculator. The previous component will provide information about the features which are untruthful in the interpretation, for each FI technique if more than one is provided. Then, it will reform the interpretation excluding all the untruthful features zui \u2208Z \u2032. If there are multiple FI techniques, it will choose to present as a final interpretation the one with the minimal number of untruthful features, Eq. 4.1, since this interpretation will provide richer details, as more features appear, and more accurate results, that can be tested by the end user.\n(4.1) argmin t |[zdi |zdi \u2208 Z \u2032 t, i \u2208 [0, |Z \u2032 t|]]|\nMoreover, due to the transparent nature of argumentation, Altruist can explain why it selected an interpretation, why a feature is excluded or included. A detailed qualitative experiment will take place in the following Section 5. Later information can even be used by the system designer and modified to be displayed in a textual format, by converting the arguments into phrases in natural language.\n4.6 Illustrative Example. Before we proceed, we are giving a very simple example in order to better illustrate the aforementioned. Suppose we have a classification problem with only three features Age (\u2018A\u2019), Height (\u2018H\u2019) and Weight (\u2018W\u2019), which predicts the probability of \u2018Author\u2019s Paper Approval\u2019. A probability of [0,0.5) means that the paper will be rejected, while a probability of [0.5,1] means that this author\u2019s paper will be accepted. John is a PhD student who is 24 years old, he is short and has average weight, (H=170, W=61). The ML component predicted a probability of 0.25 of his paper to be accepted. John asked for an explanation, and the system administrator (using LIME) told him that, while his H is positive influencing (z2 = 0.5) the probability his paper to be accepted, his W has neutral influence (z3 = 0), and his A has negative influence (z1 = \u22120.7).\nNow, John can use the first argument \u03b11 claiming: a =\u201cThe explanation is not truthful\u201d. Thus, a user or a system can generally raise this argument in order to derogate the truthfulness of the interpretation. Subsequently, the system claims that b =\u201cThe explanation is truthful\u201d. This argument, \u03b12, is a rebuttal to \u03b11. Then, John can raise for each one of the |F | features a claim cj , j \u2208 [0, |F |], stating that he believes that \u201cThe zj is un-\ntruthful\u201d. Specifically, he raises three claims c1, c2, c3, composing argument \u03b13 which is an undercut attack to argument \u03b12. These claims are c1 =\u201cThe importance of A is untruthful\u201d, c2 =\u201cThe importance of H is untruthful\u201d and c3 =\u201cThe importance of W is untruthful\u201d. The system now is creating 3 claims, d1 =\u201cThe importance of A is truthful since it has a Negative influence, and when its value is Increasing locally, we observe that the probability is Decreasing, and when its value is Decreasing locally, we observe that the probability is Increasing\u201d, d2 =\u201cThe importance of H is truthful since it has a Positive influence, and when its value is Increasing locally, we observe that the probability is Increasing, and when its value is Decreasing locally, we observe that the probability is Decreasing\u201d, and finally, d3 =\u201cThe importance of W is truthful since it has a Neutral influence, and when its value is Increasing locally, we observe that the probability is Remaining Stable, and when its value is Decreasing locally, we observe that the probability is Remaining Stable\u201d. Three arguments \u03b14 are composed and work as undercut attacks to the \u03b13.\nJohn can push further the system, asking to prove these claims by raising six new claims e1,Inc, e1,Dec, e2,Inc, e2,Dec, e3,Inc, e3,Dec. For example, we present two of them: e2,Inc =\u201cH has a Positive influence and is therefore expected the probability to be Increased by Increasing its value\u201d and e2,Dec =\u201cH has a Positive influence and is therefore expected the probability to be Decreased by Decreasing its value\u201d. Each pair of these claims (e.g. e2,Inc, e2,Dec) form an argument \u03b15, which is an undercut attack to \u03b14. Finally, the system will provide its last six claims f1,Inc, f1,Dec, f2,Inc, f2,Dec, f3,Inc, f3,Dec, which each two of them is forming an argument \u03b16 which undercuts the argument \u03b15. These claims are for example: f2,Inc =\u201cH\u2019s value got Increased and evaluated and the probability is Increased as expected\u201d and f2,Dec =\u201cH\u2019s value got Decreased and evaluated and the probability is Decreased as expected\u201d.\nHowever, if any of these last arguments are missing, or cannot be provided by the system, this means that the root of the argumentation tree, which is the argument \u03b11 is judged as Warranted, and therefore the user can say that indeed the explanation was untrusted. Otherwise, if the system can provide all the final six arguments, the argumentation tree will be Unwarranted, and the argument \u03b11 will be invalid."
    },
    {
      "heading": "5 Experiments",
      "text": "In this section, we will present the ability of the Altruist to provide the local maximum truthful interpretation, followed by an explanation, in order to assess it qualitatively. In addition, we are going to quantitatively test\nSubmitted paper on SDM 2021 by SIAM\nAltruist on a range of FI techniques. In the experiments, Altruist will be evaluated in 3 separate datasets, 3 uninterpretable ML models and 1 interpretable (Table 2), as well as 4 FI techniques. Nevertheless, the following experiments are not intended to identify the best model or the best FI technique.\nSpecifically, we are utilising the datasets: Banknote [14] (identification of real or fake banknotes), Heart Statlog [14] (prediction of absence or presence of a heart\u2019s disease) and Adult Census [19] (prediction if income exceeds 50K/yr or not), and the ML models: Logistic Regression (LR) [8], Support Vector Machines (SVM) [32], Random Forests (RF) [4] and Neural Networks (NN) [16]. In order to provide the unbiased performance of each algorithm, 10-fold cross validation grid searches1 were performed [17]. The results are presented in Table 2.\nThe interpretation techniques selected for this set of experiments are PI, LIME, SHAP, and when available the models\u2019 intrinsic interpretation. Specifically, only LR and RF can provide intrinsic and pseudo-intrinsic interpretations, respectively.\n1The grid search parameters can be found in GitHub: https: //github.com/iamollas/Altruist. In addition, the optimal set of parameters for each model per dataset, the selection and engineering of features and the undersampling strategies (used in the Adult Census) can also be found in the repo\n5.1 Qualitative. For the qualitative experiments, we select the Banknote dataset, due to the small number of features, which will make the example clearer and easier to follow. The ML models we are selecting are the SVM, which achieves the perfect f1 score (100%), and the LR, which always provides truthful interpretation, in order to assess that Altruist will judge the interpretations correctly. The Banknote dataset contains instances with 4 features F = [f1, f2, f3, f4], where f1 is the variance, f2 the skew, f3 the curtosis and finally f4 the entropy. We will take the following two instances: x248 = [0.380, 0.780, 0.757,\u22120.445] and x942 = [\u22123.38, 13.77, 17.93,\u22122.03]. The SVM model classified the 248th banknote as fake with an 81.45% probability, while the LR classified the 942th as real with a probability of 47.01%.\nIn Figure 3 and 4, there are the original interpretations provided by LIME and PI for the SVM\u2019s prediction and the LIME\u2019s and the intrinsic interpretations for the LR\u2019s prediction, respectively. Altruist discovered on the interpretations of the SVM model, untruthful importance given to variance and skew in LIME\u2019s interpretation, as well as entropy\u2019s importance provided by PI. Similarly, on the LR\u2019s interpretations, Altruist found two untruthful importances, those of curtosis and entropy, provided by LIME, while it found the intrinsic interpretation truthful. Thus, for the SVM\u2019s interpretation it proposed the PI interpretation to be provided to the user, but with the entropy\u2019s importance skewed. At the same time, it provided the user with the intrinsic interpretation of LR, once it was completely truthful. To provide a specific example, Altruist judged the SVM PI\u2019s positive importance given to entropy feature as untruthful due to the fact that it observed the probability to decrease from 81.45% to 9.10%, when the value \u22120.445 changed to 1.642, and the probability to increase from 81.45% to 98.17%, when the value \u22120.445 changed to \u22122.531, instead of decreasing.\nDue to the fact that the backend mechanism of Altruist relies on argumentation, an argumentation tree\nSubmitted paper on SDM 2021 by SIAM\nlike the one presented in Figure 2 can be generated and can be used to justify this decision. However, the argumentation tree of this example was not presented here due to the space limitation.\n5.2 Quantitative. In order to quantitatively evaluate Altruist\u2019s ability to detect untruthful features, as well as to select the best interpretation technique among many for an instance\u2019s prediction, we will test it in 3 different datasets, 3 uninterpretable ML model and 1 interpretable (Table 2). However, the following experiment is not meant to qualify the best model or the best interpretation technique. We use Altruist to compare different interpretation techniques on three different datasets, and as a selection tool. The results are visible in the Table 3, describing the mean percentage of untruthful features appearing on the interpretations.\nBanknote: For this dataset, the SVM model achieves the higher F1 (100%). Among the 4 models, LR provides the most truthful interpretations, second comes the NN, and third the SVM. At the same time, every interpretation techniques struggles to provide truthful explanations for the RF.\nHeart (Statlog): For the Heart (Statlog) dataset, the SVM model achieves the higher F1 (81.95%). Among the 4 models, LR provides the most truthful interpretations, second comes the SVM model with PI technique, and third the NN with SHAP. At the same time, every interpretation technique struggles to provide truthful explanations for the RF model.\nAdult Census: For the Adult Census dataset, the SVM model achieves the higher F1 score (95.93%). Among the 4 models, LR provides the most truthful interpretations, second comes the SVM with the PI technique, and third the RF model with SHAP. In contrast to the other 2 test cases, interpretations for the RF model provided by SHAP seems to have few untruthful features.\nThe effect of Altruist was not commented in the aforementioned. On the basis of the experiments referred to above, we can infer that Altruist is a crit-\nical tool for evaluating interpretations given by nonintrinsic techniques such as LIME, SHAP and PI, detecting 43.08% of untruthful features on average, in 35 out of 36 tests. Moreover, when Altruist is used as a selection tool (an ensemble) achieves the lowest percent in every case. Provided that no interpretation approach has prevailed over the others, it appears to be an ideal tool for selecting the best technique automatically, in a setup where several techniques are used in parallel to interpret the prediction of an instance."
    },
    {
      "heading": "6 Conclusion",
      "text": "IML has emerged as an important research area to interpret ML algorithms. A lot of IML approaches are presenting their interpretations in a features-importance manner. Argumentation is the concept of designing automated systems that address specific issues by providing syllogisms that support the thesis, solution and conclusion. In this paper, Altruist is presented, deploying an innovative technique combining feature importance interpretation techniques and argumentation, transforming untruthful interpretations on the decisionmaking of ML models into profound explanations for the end users, which are justifiable as well. Altruist provides the local maximum truthful interpretation, as well as the justification for the truthfulness. Moreover, it can be used as a tool for automatic selection of the most truthful interpretation among a variety of X different interpretation techniques. In future work, Altruist will be evaluated in other ML tasks (e.g. regression, multi-label classification). Another aspect that could be investigated is the alteration of categorical feature values, and the effect of the localness of features. Finally, a human-oriented evaluation will be assessed to validate its usefulness."
    }
  ],
  "title": "Altruist: Argumentative Explanations through Local Interpretations of Predictive Models\u2217",
  "year": 2020
}
