{"abstractText": "Bayesian likelihood-free methods implement Bayesian inference using simulation of data from the model to substitute for intractable likelihood evaluations. Most likelihoodfree inference methods replace the full data set with a summary statistic before performing Bayesian inference, and the choice of this statistic is often difficult. The summary statistic should be low-dimensional for computational reasons, while retaining as much information as possible about the parameter. Using a recent idea from the interpretable machine learning literature, we develop some regression-based diagnostic methods which are useful for detecting when different parts of a summary statistic vector contain conflicting information about the model parameters. Conflicts of this kind complicate summary statistic choice, and detecting them can be insightful about model deficiencies and guide model improvement. The diagnostic methods developed are based on regression approaches to likelihood-free inference, in which the regression model estimates the posterior density using summary statistics as features. Deletion and imputation of part of \u2217Corresponding author: a0095911@u.nus.edu 1 ar X iv :2 01 0. 07 46 5v 1 [ st at .M E ] 1 5 O ct 2 02 0 the summary statistic vector within the regression model can remove conflicts and approximate posterior distributions for summary statistic subsets. A larger than expected change in the estimated posterior density following deletion and imputation can indicate a conflict in which inferences of interest are affected. The usefulness of the new methods is demonstrated in a number of real examples.", "authors": [{"affiliations": [], "name": "Yinan Mao"}, {"affiliations": [], "name": "Xueou Wang"}, {"affiliations": [], "name": "David J. Nott"}, {"affiliations": [], "name": "Michael Evans"}], "id": "SP:9ca64485ac9de512ee068ff927e71f192e63ec2e", "references": [{"authors": ["C.W. Anderson", "S.G. Coles"], "title": "The largest inclusions in a piece of steel", "venue": "Extremes 5 (3), 237\u2013252.", "year": 2002}, {"authors": ["M.J. Bayarri", "M.E. Castellanos"], "title": "Bayesian checking of the second levels of hierarchical models", "venue": "Statistical Science 22, 322\u2013343.", "year": 2007}, {"authors": ["M.A. Beaumont", "W. Zhang", "D.J. Balding"], "title": "Approximate Bayesian computation in population genetics", "venue": "Genetics 162, 2025\u20132035.", "year": 2002}, {"authors": ["P.G. Bissiri", "C.C. Holmes", "S.G. Walker"], "title": "A general framework for updating belief distributions", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 78 (5), 1103\u20131130.", "year": 2016}, {"authors": ["M.G.B. Blum", "O. Fran\u00e7ois"], "title": "Non-linear regression models for approximate Bayesian computation", "venue": "Statistics and Computing 20, 63\u201375.", "year": 2010}, {"authors": ["M.G.B. Blum", "M.A. Nunes", "D. Prangle", "S.A. Sisson"], "title": "A comparative review of dimension reduction methods in approximate Bayesian computation", "venue": "Statistical Science 28 (2), 189\u2013208.", "year": 2013}, {"authors": ["P. Bortot", "S. Coles", "S. Sisson"], "title": "Inference for stereological extremes", "venue": "Journal of the American Statistical Association", "year": 2007}, {"authors": ["G.E.P. Box"], "title": "Sampling and Bayes\u2019 inference in scientific modelling and robustness (with discussion)", "venue": "Journal of the Royal Statistical Society, Series A 143, 383\u2013430.", "year": 1980}, {"authors": ["K. Csill\u00e9ry", "O. Franc\u00e7ois", "M.G.B. Blum"], "title": "abc: an R package for approximate Bayesian computation (ABC)", "venue": "Methods in Ecology and Evolution 3 (3), 475\u2013479.", "year": 2012}, {"authors": ["T. Dinev", "M. Gutmann"], "title": "Dynamic likelihood-free inference via ratio estimation (DIRE)", "venue": "arXiv:1810.09899 .", "year": 2018}, {"authors": ["C.C. Drovandi", "A.N. Pettitt", "A. Lee"], "title": "Bayesian indirect inference using a parametric auxiliary model", "venue": "Statistical Science 30 (1), 72\u201395.", "year": 2015}, {"authors": ["R. Erhardt", "S.A. Sisson"], "title": "Modelling extremes using approximate Bayesian computation", "venue": "D. Dey and J. Yan (Eds.), Extreme Value Modelling and Risk Analysis, pp. 281\u2013306.", "year": 2016}, {"authors": ["M. Evans"], "title": "Measuring Statistical Evidence Using Relative Belief", "venue": "Taylor & Francis.", "year": 2015}, {"authors": ["M. Evans", "H. Moshonov"], "title": "Checking for prior-data conflict", "venue": "Bayesian Analysis 1, 893\u2013914.", "year": 2006}, {"authors": ["J. Fan", "C. Ma", "Y. Zhong"], "title": "A selective overview of deep learning", "venue": "arXiv preprint arXiv:1904.05526 .", "year": 2019}, {"authors": ["Y. Fan", "D.J. Nott", "S.A. Sisson"], "title": "Approximate Bayesian computation via regression density estimation", "venue": "Stat 2 (1), 34\u201348.", "year": 2013}, {"authors": ["M. Fasiolo", "N. Pya", "S.N. Wood"], "title": "A comparison of inferential methods for highly nonlinear state space models in ecology and epidemiology", "venue": "Statistical Science 31, 96\u2013118.", "year": 2016}, {"authors": ["D.T. Frazier", "C. Drovandi"], "title": "Robust approximate Bayesian inference with synthetic likelihood", "venue": "arXiv preprint arXiv:1904.04551 .", "year": 2019}, {"authors": ["D.T. Frazier", "C. Drovandi", "R. Loaiza-Maya"], "title": "Robust approximate Bayesian computation: An adjustment approach", "venue": "arXiv plreprint arXiv:2008.04099 .", "year": 2020}, {"authors": ["D.T. Frazier", "C.P. Robert", "J. Rousseau"], "title": "Model misspecification in approximate Bayesian computation: consequences and diagnostics", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82 (2), 421\u2013444.", "year": 2020}, {"authors": ["A. Gelman", "X.-L. Meng", "H. Stern"], "title": "Posterior predictive assessment of model fitness via realized discrepancies", "venue": "Statistica Sinica", "year": 1996}, {"authors": ["W. Gurney", "S. Blythe", "R. Nisbet"], "title": "Nicholson\u2019s blowflies revisited", "venue": "Nature 287, 17\u201321.", "year": 1980}, {"authors": ["R. Izbicki", "A.B. Lee"], "title": "Converting high-dimensional regression to high-dimensional conditional density estimation", "venue": "Electronic Journal of Statistics 11, 2800\u20132831.", "year": 2017}, {"authors": ["R. Izbicki", "A.B. Lee", "T. Pospisil"], "title": "ABC\u2013CDE: Toward approximate Bayesian computation with complex high-dimensional data and limited simulations", "venue": "Journal of Computational and Graphical Statistics (To appear).", "year": 2019}, {"authors": ["N. Klein", "D.J. Nott", "M.S. Smith"], "title": "Marginally-calibrated deep distributional regression", "venue": "arXiv preprint arXiv:1908.09482 .", "year": 2019}, {"authors": ["J. Li", "D.J. Nott", "Y. Fan", "S.A. Sisson"], "title": "Extending approximate Bayesian computation methods to high dimensions via Gaussian copula", "venue": "Computational Statistics and Data Analysis 106, 77\u201389.", "year": 2017}, {"authors": ["W. Li", "P. Fearnhead"], "title": "Convergence of regression-adjusted approximate Bayesian computation", "venue": "Biometrika 105 (2), 301\u2013318.", "year": 2018}, {"authors": ["Marin", "J.-M.", "P. Pudlo", "C.P. Robert", "R.J. Ryder"], "title": "Approximate Bayesian computational methods", "venue": "Statistics and Computing 22 (6), 1167\u20131180.", "year": 2012}, {"authors": ["M. Mayer"], "title": "missRanger: Fast Imputation of Missing Values", "venue": "R package version 2.1.0.", "year": 2019}, {"authors": ["N. Meinshausen"], "title": "Quantile regression forests", "venue": "J. Mach. Learn. Res. 7, 983\u2013999.", "year": 2006}, {"authors": ["S. Moritz", "T. Bartz-Beielstein"], "title": "imputeTS: Time Series Missing Value Imputation in R", "venue": "The R Journal 9 (1), 207\u2013218.", "year": 2017}, {"authors": ["A. Nicholson"], "title": "An outline of the dynamics of animal populations", "venue": "Australian Journal of Zoology 2 (1), 9\u201365.", "year": 1954}, {"authors": ["D.J. Nott", "X. Wang", "M. Evans", "B.-G. Englert"], "title": "Checking for prior-data conflict using prior-to-posterior divergences", "venue": "Statistical Science 35 (2), 234\u2013253.", "year": 2020}, {"authors": ["G. Papamakarios", "I. Murray"], "title": "Fast -free inference of simulation models with Bayesian conditional density estimation", "venue": "Advances in Neural Information Processing Systems", "year": 2016}, {"authors": ["G. Papamakarios", "D. Sterratt", "I. Murray"], "title": "Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows", "venue": "K. Chaudhuri and M. Sugiyama (Eds.), Proceedings of Machine Learning Research, Volume 89, pp. 837\u2013848.", "year": 2019}, {"authors": ["N.G. Polson", "V. Sokolov"], "title": "Deep learning: A Bayesian perspective", "venue": "Bayesian Analysis 12 (4), 1275\u20131304.", "year": 2017}, {"authors": ["A.M. Presanis", "D. Ohlssen", "D.J. Spiegelhalter", "D.D. Angelis"], "title": "Conflict diagnostics in directed acyclic graphs, with applications in Bayesian evidence synthesis", "venue": "Statistical Science 28, 376\u2013397.", "year": 2013}, {"authors": ["L.F. Price", "C.C. Drovandi", "A.C. Lee", "D.J. Nott"], "title": "Bayesian synthetic likelihood", "venue": "Journal of Computational and Graphical Statistics 27 (1), 1\u201311.", "year": 2018}, {"authors": ["P. Probst", "M.N. Wright", "A.-L. Boulesteix"], "title": "Hyperparameters and tuning strategies for random forest", "venue": "WIREs Data Mining and Knowledge Discovery 9 (3), e1301.", "year": 2019}, {"authors": ["O. Ratmann", "C. Andrieu", "C. Wiuf", "S. Richardson"], "title": "Model criticism based on likelihood-free inference, with an application to protein network evolution", "venue": "Proceedings of the National Academy of Sciences 106 (26), 10576\u201310581.", "year": 2009}, {"authors": ["O. Ratmann", "P. Pudlo", "S. Richardson", "C. Robert"], "title": "Monte Carlo algorithms for model assessment via conflicting summaries", "venue": "arXiv preprint arXiv:1106.5919 .", "year": 2011}, {"authors": ["L. Raynal", "J.-M. Marin", "P. Pudlo", "M. Ribatet", "C.P. Robert", "A. Estoup"], "title": "ABC random forests for Bayesian parameter inference", "venue": "Bioinformatics 35 (10), 1720\u20131728.", "year": 2018}, {"authors": ["A. R\u00e9nyi"], "title": "On measures of entropy and information", "venue": "Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, Berkeley, Calif., pp. 547\u2013561. University of California Press.", "year": 1961}, {"authors": ["W. Ricker"], "title": "Stock and Recruitment", "venue": "Journal of the Fisheries Research Board of Canada 11 (5), 559\u2013623.", "year": 1954}, {"authors": ["J. Ridgway"], "title": "Probably approximate Bayesian computation: nonasymptotic convergence of ABC under misspecification", "venue": "arXiv preprint arXiv:1707.05987 .", "year": 2017}, {"authors": ["M. Robnik-\u0160ikonja", "I. Kononenko"], "title": "Explaining classifications for individual instances", "venue": "IEEE Trans. Knowl. Data Eng", "year": 2008}, {"authors": ["S. Sisson", "Y. Fan", "M. Beaumont"], "title": "Handbook of Approximate Bayesian Computation", "year": 2018}, {"authors": ["S. Sisson", "Y. Fan", "M. Beaumont"], "title": "Overview of Approximate Bayesian Computation", "venue": "S. Sisson, Y. Fan, and M. Beaumont (Eds.), Handbook of Approximate Bayesian Computation, Chapman & Hall/CRC Handbooks of Modern Statistical Methods. Boca Raton, Florida: CRC Press, Taylor & Francis Group.", "year": 2018}, {"authors": ["O. Thomas", "H. Pesonen", "R.S.-L. ao", "H. de Lencastre", "S. Kaski", "J. Corander"], "title": "SplitBOLFI for for misspecification-robust likelihood free inference in high dimensions", "venue": "arXiv preprint arXiv:2002.09377 .", "year": 2020}, {"authors": ["H. Tong"], "title": "Threshold models in time series analysis\u201430 years on", "venue": "Statistics and its Interface 4 (2), 107\u2013118.", "year": 2011}, {"authors": ["S. van Buuren"], "title": "mice: Multivariate imputation by chained equations in R", "venue": "Groothuis-Oudshoorn", "year": 2011}, {"authors": ["R.D. Wilkinson"], "title": "Approximate Bayesian computation (ABC) gives exact results under the assumption of model error", "venue": "Statistical Applications in Genetics and Molecular Biology 12 (2), 129 \u2013 141.", "year": 2013}, {"authors": ["S.N. Wood"], "title": "Statistical inference for noisy nonlinear ecological dynamic systems", "venue": "Nature 466, 1102\u20131107.", "year": 2010}, {"authors": ["H. Zhang", "F.H. Nieto"], "title": "TAR: Bayesian Modeling of Autoregressive Threshold Time Series Models", "venue": "R package version 1.0.", "year": 2017}, {"authors": ["L.M. Zintgraf", "T.S. Cohen", "T. Adel", "M. Welling"], "title": "Visualizing deep neural network decisions: Prediction difference analysis", "venue": "arXiv preprint arXiv:1702.04595 .", "year": 2017}], "sections": [{"text": "Bayesian likelihood-free methods implement Bayesian inference using simulation of data from the model to substitute for intractable likelihood evaluations. Most likelihoodfree inference methods replace the full data set with a summary statistic before performing Bayesian inference, and the choice of this statistic is often difficult. The summary statistic should be low-dimensional for computational reasons, while retaining as much information as possible about the parameter. Using a recent idea from the interpretable machine learning literature, we develop some regression-based diagnostic methods which are useful for detecting when different parts of a summary statistic vector contain conflicting information about the model parameters. Conflicts of this kind complicate summary statistic choice, and detecting them can be insightful about model deficiencies and guide model improvement. The diagnostic methods developed are based on regression approaches to likelihood-free inference, in which the regression model estimates the posterior density using summary statistics as features. Deletion and imputation of part of\n\u2217Corresponding author: a0095911@u.nus.edu\nar X\niv :2\n01 0.\n07 46\nthe summary statistic vector within the regression model can remove conflicts and approximate posterior distributions for summary statistic subsets. A larger than expected change in the estimated posterior density following deletion and imputation can indicate a conflict in which inferences of interest are affected. The usefulness of the new methods is demonstrated in a number of real examples.\nKeywords. Approximate Bayesian computation; Bayesian model criticism; influence measures; likelihood-free inference; model misspecification."}, {"heading": "1 Introduction", "text": "Often scientific knowledge relevant to statistical model development comes in the form of a possible generative process for the data. Computation of the likelihood is sometimes intractable for models specified in this way, and then likelihood-free inference methods can be used if simulation of data from the model is easily done. A first step in most likelihood-free inference algorithms is to reduce the full data set to a low-dimensional summary statistic, which is used to define a distance for comparing observed and simulated data sets. The choice of this summary statistic must be done carefully, balancing computational and statistical considerations (Blum et al., 2013). Once summary statistics are chosen, Bayesian likelihood-free inference can proceed using a variety of techniques, such as approximate Bayesian computation (ABC) (Marin et al., 2012; Sisson et al., 2018a) synthetic likelihood (Wood, 2010; Price et al., 2018) or regression based approaches (Beaumont et al., 2002; Blum and Franc\u0327ois, 2010; Fan et al., 2013; Raynal et al., 2018).\nSometimes different components of the summary statistic vector bring conflicting information about the model parameter. This can arise due to model deficiencies that we wish to be aware of, and can complicate summary statistic choice. Furthermore, it can lead to computational difficulties when observed summary statistics are hard to match under the assumed model. We develop some diagnostics for detecting when this problem occurs, based on a recent idea in the interpretable machine learning literature. The suggested diagnostics are used in conjunction with regression-based approaches to likelihood-free inference.\nThe following example of inference on a Poisson mean, which is discussed in Sisson et al. (2018b), illustrates the motivation for our work. Here the likelihood is tractable, but the example is useful for pedagogical purposes. Suppose data y = (y1, . . . , y5) > = (0, 0, 0, 0, 5)> are observed, and that we model y as a Poisson random sample with mean \u03b7. For Bayesian inference we use a Gamma(1, 1) prior density for \u03b7. The gamma prior is conjugate, and the posterior density is Gamma(1 + 5y\u0304, 6), where y\u0304 is the sample mean of y. Letting s2 denote\nthe sample variance of y, both y\u0304 and s2 are point estimates of \u03b7, since for the Poisson model the mean and variance are equal to \u03b7; furthermore, y\u0304 is a minimal sufficient statistic.\nConsider a standard rejection ABC method for this problem, which involves generating \u03b7 values from the prior, generating summary statistics under the model conditional on the generated \u03b7 values, and then keeping the \u03b7 values which lead to simulated summary statistics closest to those observed. See Sisson et al. (2018b) for further discussion of basic ABC methods. Consider first the summary statistic y\u0304, and apply rejection ABC with 100, 000 prior samples and retaining 500 of these in the ABC rejection step. This gives a very close approximation to the true posterior (Figure 1). The computations were implemented using\nthe default implementation of rejection ABC in the R package abc (Csille\u0301ry et al., 2012). Reduction of the full data to y\u0304 involves no loss of information, as y\u0304 is sufficient.\nSince we observe values of y\u0304 = 1 and s2 = 5, these summaries bring conflicting information about \u03b7, since they are both point estimates of \u03b7 and take quite different values. If we consider the summary statistic (y\u0304, s2), and apply the same rejection ABC algorithm as before with 100, 000 prior samples and 500 accepted samples, the estimated posterior density changes appreciably \u2013 see Figure 1. Although (y\u0304, s2) is still sufficient, and the corresponding posterior is the same as the posterior given y\u0304, the ABC computational algorithm performs poorly because the observed summary statistic is hard to match with a reasonable fixed computational budget. Simultaneously observing y\u0304 = 1 and s2 = 5 is unusual under the assumed model for any\nvalue of the model parameter. This results in accepting parameter samples having simulated summary statistic values far away from the observed value of (y\u0304, s2). Also shown in Figure 1 is the posterior conditional on s2, which is even further from the true posterior than the posterior conditional on (y\u0304, s2).\nThe conflicting information in the summary statistics y\u0304 and s2 may indicate a model failure, motivating a more flexible model than the Poisson, such as negative binomial, in which the mean and variance are not required to be equal. Sisson et al. (2018b) considered this example partly as a demonstration of the difficulties which arise in greedy algorithms for summary statistic choice. An analyst following a policy of adding one summary statistic at a time, and stopping when the posterior density no longer changes appreciably, would incorrectly judge (y\u0304, s2) to be a better summary statistic than y\u0304. Our example illustrates that if different parts of a summary statistic vector are conflicting, then detecting this can be insightful about model failures and can suggest model improvements. This is the motivation for the conflict detection diagnostics developed here.\nThe structure of the paper is as follows. In the next Section we describe regression approaches to likelihood-free inference. Section 3 then discusses a recent suggestion in the literature on interpretable machine learning, which has the goal of explaining how a flexible classifier makes the class prediction it does for a certain observation. This method is based on deletion of part of a feature vector, followed by imputation of what was deleted and assessment of how evidence for the observed class changes following the deletion and imputation process. In regression approaches to computing likelihood-free inferences, where regression models are considered with simulated parameters as the response values and simulated summary statistics as features, we show that a similar approach can be used to estimate the changes in a posterior distribution under sequential updating for summary statistic subsets. If the change in the posterior density is larger than expected when a subset of summary statistics is deleted, this may indicate that different parts of the summary statistic vector carry conflicting information about the parameter.\nIn Section 4, our diagnostics are formalized and placed in the broader framework of Bayesian model checking. We make connections with the literature on prior-data conflict, suggest a method of calibration, and also review related literature on model checking for likelihood-free methods. Section 5 considers three examples. First we revisit the pedagogical example above and show how our diagnostics apply in that simple case, before considering two further real examples. The first real example considers data on stereological extremes, where the model is not flexible enough to fit well in both the lower and upper tails. The second example concerns an ecological time series model. An analysis of these data using summary\nstatistics derived from an auxiliary threshold autoregressive model is first considered, where the summary statistics can be thought of as capturing the different dynamic behaviour of the time series at high and low levels. Conflicts between summary statistic subsets for different regimes are insightful for understanding deficiencies in the original model. An analysis of these data without summary statistics is also considered, where the regression model is based on a convolutional neural network, and the feature vector consists of the whole time series. Here we explore temporally localized parts of the series not fitted well by the model. Section 6 gives some concluding discussion."}, {"heading": "2 Regression approaches to likelihood-free inference", "text": "Consider a Bayesian inference problem with data d and a parametric model for d with density p(d|\u03b7), with parameter \u03b7. The prior density for \u03b7 is denoted p(\u03b7), with corresponding posterior density p(\u03b7|dobs) \u221d p(\u03b7, dobs) = p(\u03b7)p(dobs|\u03b7), where dobs denotes the observed value of d. Suppose that computation of the likelihood p(dobs|\u03b7) is intractable, so that likelihood-free inference approaches are needed to approximate the posterior distribution. In this case, we can consider summary statistics S = S(d), with Sobs = S(dobs), and writing p(S|\u03b7) for the sampling density of S, we use likelihood-free inference methods to approximate p(\u03b7|Sobs) \u221d p(\u03b7)p(Sobs|\u03b7).\nThe posterior density p(\u03b7|Sobs) is the conditional density of \u03b7 given S = Sobs in the joint Bayesian model for (\u03b7, S) specified by p(\u03b7, S) = p(\u03b7)p(S|\u03b7). Since regression is conditional density estimation, we can fit regression models to samples (\u03b7(i), S(i)), i = 1, . . . , N from p(\u03b7, S) in order to estimate p(\u03b7|S) for any S. The responses are \u03b7(i), and the features are S(i). The regression predictive density given S = Sobs, denoted p\u0303(\u03b7|Sobs), is an estimate of p(\u03b7|Sobs).\nThere are many flexible regression approaches that have been applied to computation of likelihood-free inferences, such as quantile regression forests (Meinshausen, 2006; Raynal et al., 2018), copula methods (Li et al., 2017; Fan et al., 2013; Klein et al., 2019), neural approaches of various kinds (Papamakarios and Murray, 2016; Papamakarios et al., 2019) and orthogonal series methods (Izbicki and Lee, 2017; Izbicki et al., 2019) among others. In our later applications to the detection of conflicting information in summary statistics we will be concerned with the effects of conflict on inference for scalar functions of the parameter, and so only univariate regression models are needed for estimating one-dimensional marginal posterior distributions. To save notation we will use the notation \u03b7 both for the model parameter as well as a one-dimensional function of interest of it, where the meaning intended will be clear\nfrom the context.\nIn checking for summary statistic conflicts, we will partition S as S = (S>A , S > B ) > and we ask\nif the information about \u03b7 in SA conflicts with that in SB. We write Sobs = (S > A,obs, S > B,obs) >. Our conflict diagnostic will compare an estimate of p(\u03b7|Sobs) to an estimate of p(\u03b7|SA,obs). If the change in the estimated posterior distribution is large, this may indicate that the summary statistics carrying conflicting information about the parameter \u03b7. We must define what large means, and this is an issue of calibration of the diagnostics.\nTo overcome the computational difficulties of estimating the posterior distributions, we will first consider a likelihood-free regression method with features S to compute p(\u03b7|Sobs). Then based on this same regression, the method of Zintgraf et al. (2017) described in Section 3 will be used to approximate p(\u03b7|SA,obs) by deletion and multiple imputation of SB. This can be done without refitting the regression. This last point is important, because sometimes fitting very flexible regression models can be computationally burdensome. For example, in the application of Section 5.2, we consider a training sample of 500, 000 observations, where we fit a quantile regression forest in which algorithm fitting parameters are tuned by crossvalidation. Applying the method of Zintgraf et al. (2017) for approximating the posterior density given SA,obs, using the fitted regression with features S, is much less computationally demanding than refitting the regression with features SA to approximate the subset posterior density directly. Since we may wish to consider many different choices of SA and SB for diagnostic purposes, this is another reason to avoid the approach of refitting the regression model with different choices of the features.\nAlthough the main idea of our method is to compare estimates of p(\u03b7|Sobs) and p(\u03b7|SA,obs), making this precise will involve the development of some further background. We describe the method of Zintgraf et al. (2017) next, and then in Section 4 we formalize our diagnostics and consider connections to the existing literature on Bayesian model checking."}, {"heading": "3 Interpretable machine learning using imputation", "text": "We describe a method developed in Zintgraf et al. (2017) for visualizing classifications made by a deep neural network. Their approach builds on earlier work of Robnik-S\u030cikonja and Kononenko (2008). The method deletes part of a feature vector, and then replaces the deleted part with an imputation based on the other features. The change in the observed class probability is then examined to measure of how important the deleted features were to the classification made. To avoid repetition, the discussion below will be concerned with a general regression model where the response variable can be continuous or discrete, although Zintgraf\net al. (2017) deals only with the case where the response is a discrete class label. It is the case of regression models with a continuous response that is relevant to our discussion of likelihood-free inference.\nSuppose that (yi, xi), i = 1, . . . , n are response and feature vector pairs in some training set of n observations. The feature vectors are p-dimensional, written as xi = (xi1, . . . , xip) >. We assume the responses are scalar valued, although extension to multivariate responses is possible. Write Y = (y1, . . . , yn) > and X = [x>1 , . . . , x > n ] > so that Y is an n-vector and X is an n\u00d7 p matrix. We consider Bayesian inference in this regression model with a prior density p(\u03b8) for \u03b8, and likelihood p(Y |\u03b8,X). Note the different notation here to Section 2, where we considered a likelihood-free inference problem with data d and parameter \u03b7. Here Y and X are the responses and features in the regression, and in likelihood-free regression the features X will be simulated summary statistics, whereas the responses y are simulated values of the parameters \u03b7, hence the different notation. More precisely, in the likelihood-free problems of interest to us, and in the notation of Section 2, yi = \u03b7 (i) and xi = S (i), i = 1, . . . , n.\nThe posterior density of \u03b8 in the regression model is\np(\u03b8|Y,X) \u221d p(\u03b8)p(Y |\u03b8,X).\nFor some feature vector x\u2217, and corresponding value of interest for the response y\u2217, we can compute the predictive density of y\u2217 given x\u2217, Y,X as\np(y\u2217|x\u2217, Y,X) = \u222b p(y\u2217|\u03b8, x\u2217)p(\u03b8|Y,X)d\u03b8,\nwhere p(y\u2217|\u03b8, x\u2217) is the likelihood term for y\u2217 and it is assumed that (y\u2217, x\u2217) is conditionally independent of (Y,X) given \u03b8 and x\u2217.\nSuppose that some subector of x\u2217 is not observed, say the ith component x\u2217i . Assume that the features x\u2217, x1, . . . , xn are modelled as independent and identically distributed from a density p(x|\u03bb), where the parameters \u03bb are disjoint from \u03b8, and that \u03bb and \u03b8 are independent in the prior. We write x\u2217\u2212i for x \u2217 with x\u2217i deleted. Then under the above assumptions,\np(y\u2217|x\u2217\u2212i, X, Y ) = \u222b p(y\u2217|x\u2217, Y,X)p(x\u2217i |x\u2217\u2212i, X)dx\u2217i , (1)\nwhere\np(x\u2217i |x\u2217\u2212i, X) = \u222b p(x\u2217i |x\u2217\u2212i, \u03bb)p(\u03bb|X)d\u03bb.\nEquation (1) says that to get p(y\u2217|x\u2217\u2212i, Y,X) we can average p(y\u2217|x\u2217, y,X) over the distribution of x\u2217i where x \u2217 i is imputed from x \u2217 \u2212i. In the discussion above only deletion of the ith component\nx\u2217i of x \u2217 has been considered, but the generalization to removing arbitrary subsets of x\u2217 is immediate. In the likelihood-free problems discussed later, the expression (1) will give us a way of estimating the posterior distribution of \u03b7 based on summary statistic subsets using a regression fitted using the full set of summary statistics and a convenient imputation model. We will partition the summary statistic S as S = (S>A , S > B ) >, and then with y\u2217 = \u03b7 and x\u2217 = Sobs where Sobs = (S > A,obs, S > B,obs) > is the observed value of S, the formula (1) will give us a way of estimating the posterior distributon p(\u03b7|SA,obs) based on the regression predictive distribution using features S.\nIn practice we might approximate p(x\u2217i |x\u2217\u2212i, X) as p(x\u2217i |x\u2217\u2212i, \u03bb\u0302) for some point estimate \u03bb\u0302 = \u03bb\u0302(X) of \u03bb. For example, we might assume a multivariate normal model for x1, . . . , xn (in which case \u03bb is the mean and covariance matrix of the normal model) and plug in the sample mean and covariance matrix as the point estimate. This is the approach considered in Zintgraf et al. (2017). Robnik-S\u030cikonja and Kononenko (2008) assumed the features to be independent in their model. Zintgraf et al. (2017) note that more sophisticated imputation methods can be used. We can also approximate p(y\u2217|x\u2217, Y,X) by p(y\u2217|x\u2217, \u03b8\u0302) for some point estimate \u03b8\u0302 of \u03b8.\nIn the case of a classification problem where the response is a class label, we can follow Zintgraf et al. (2017) and Robnik-S\u030cikonja and Kononenko (2008) and measure how much class probabilities have changed upon deletion and imputation of x\u2217i using an evidence measure, called weight of evidence:\nWE(y\u2217|x\u2217) = log2 {\np(y\u2217|x\u2217, Y,X) 1\u2212 p(y\u2217|x\u2217, Y,X)\n} \u2212 log2 { p(y\u2217|x\u2217\u2212i, Y,X)\n1\u2212 p(y\u2217|x\u2217\u2212i, Y,X)\n} . (2)\nWE(y\u2217|x\u2217) is positive (negative) if observing x\u2217i rather than imputing it from x\u2217\u2212i makes the class y\u2217 more (less) probable. Zintgraf et al. (2017) generally consider the observed class for y\u2217. Measures of evidence suitable for the regression context are considered further in the next section.\nZintgraf et al. (2017) consider problems in image classification where the feature vector x\u2217 is a vector of intensities of pixels in an image. Because of the smoothness of images, if only one pixel is deleted, then it can be imputed very accurately from its neighbours. If this is the case, WE(y\u2217|x\u2217) is not useful for measuring the importance of a pixel to the classification. Zintgraf et al. (2017) suggest to measure pixel importance by deleting all image patches of a certain size, and then average weight of evidence measures for patches that contain a certain pixel. They plot images of the averaged weight of evidence measure as a way of visualizing the importance of different parts of an image for a classification made by the network. They\nalso consider deletion and imputation for latent quantities in deep neural network models.\nAn appropriate imputation method will vary according to the problem at hand, and there is no single method that will be appropriate for all problems. In our later examples for computing likelihood-free inferences we consider three different imputation methods. The first uses linear regression in a situation where the features are approximately linearly related, and we use the implementation in the mice package in R (van Buuren and Groothuis-Oudshoorn, 2011). The second is based on random forests, as implemented in the missRanger package (Mayer, 2019). Our final example considers time series data, and in the application where we consider the raw series as the feature vector, we use the methods in the imputeTS package (Moritz and Bartz-Beielstein, 2017). Further details are given in Section 5."}, {"heading": "4 The diagnostic method", "text": "In the notation of Section 2, the method of Zintgraf et al. (2017) will allow us to estimate the posterior density p(\u03b7|SA,obs), from a regression using the full set of summary statistics S as the features. This is described in more detail in Section 4.2 below. We can then compare estimates of p(\u03b7|SA,obs) and p(\u03b7|Sobs). If the difference between these two posterior distributions is large, then it indicates that different values of the parameter are providing a good fit to the data in the likelihood terms p(SA,obs|\u03b7) and p(SB,obs|SA,obs, \u03b7), indicating a possible model failure. The weight of evidence measure of Zintgraf et al. (2017) doesn\u2019t apply directly as a way of numerically describing the difference in the posterior distributions p(\u03b7|SA,obs) and p(\u03b7|Sobs) in the likelihood-free application. Here we use measures of discrepancy considered in the priordata conflict checking literature (Nott et al., 2020) based on the concept of relative belief (Evans, 2015) instead of weight of evidence. We place our diagnostics within the framework of Bayesian model checking, and this suggests a method for calibrating the diagnostics. The discussion of prior-data conflict checking is relevant here, because the comparison of p(\u03b7|Sobs) to p(\u03b7|SA,obs) can be thought of as a prior-to-posterior comparison in the situation where SA has already been observed, and then p(\u03b7|SA,obs) is the prior for further updating by the likelilhood term p(SB,obs|SA,obs, \u03b7). A more detailed explanation of this is given below."}, {"heading": "4.1 Prior-data conflict checking", "text": "Prior-data conflicts occur if there are values of the model parameter that receive likelihood support, but the prior does not put any weight on them (Evans and Moshonov, 2006; Presanis et al., 2013). In other words, prior-data conflict occurs when the prior puts all its mass out in the tails of the likelihood. Nott et al. (2020) consider some methods for checking for prior-data\nconflict based on prior-to-posterior divergences. In the present setting of a posterior density p(\u03b7|S) for summary statistics S and with prior p(\u03b7), the checks of Nott et al. (2020) would check for conflict between p(\u03b7) and p(S|\u03b7) using a prior-to-posterior \u03b1-divergence (Re\u0301nyi, 1961) taking the form\nR\u03b1(S) = 1\n\u03b1\u2212 1 log \u222b { p(\u03b7|S) p(\u03b7) }\u03b1\u22121 p(\u03b7|S) d\u03b7, (3)\nwhere \u03b1 > 0. These divergences are related to relative belief functions (Evans, 2015) measuring evidence in terms of the prior-to-posterior change. Consider a function \u03c8 = T (\u03b7) of \u03b7. We define the relative belief function for \u03c8 as\nRB(\u03c8|d) = p(\u03c8|d) p(\u03c8) = p(d|\u03c8) p(d) , (4)\nwhere p(\u03c8|d) is the posterior density for \u03c8 given d, p(\u03c8) is the prior density, p(d|\u03c8) is the marginal likelihood given \u03c8, and p(d) is the prior predictive density at d. For a given value of \u03c8, if RB(\u03c8|d) > 1, this means there is evidence in favour of \u03c8, whereas if RB(\u03c8|d) < 1 there is evidence against. See Evans (2015) for a deeper discussion of reasons why relative belief is an attractive measure of evidence, and for a systematic approach to inference based on it.\nAs \u03b1 \u2192 1, (3) gives the prior-to-posterior Kullback-Leibler divergence (which is the posterior expectation of log RB(\u03b7|S)), and letting \u03b1 \u2192 \u221e gives the maximum value of the log relative belief, log RB(\u03b7|S) = log p(\u03b7|S)/p(\u03b7). So the statistic (3) is a measure of the overall size of a relative belief function, and a measure of how much our beliefs have changed from prior to posterior. Nott et al. (2020) suggest to compare the value of R\u03b1(Sobs) to the distribution of R\u03b1(S), S \u223c p(S) where p(S) is the prior predictive density for S,\np(S) = \u222b p(S|\u03b7)p(\u03b7) d\u03b7,\nby using the tail probability\npS = P (R\u03b1(S) \u2265 R\u03b1(Sobs)). (5)\nThe tail probability (5) is the probability that the overall size of the relative belief function RB(\u03b7|Sobs) is large compared to what is expected for data following the prior predictive density p(S). So a small value for the tail probability (5) suggest the prior and likelihood contain conflicting information, as the change from prior to posterior is larger than expected. See Evans and Moshonov (2006) and Nott et al. (2020) for further discussion of prior-data conflict checking, and why the check in Nott et al. (2020) satisfies certain logical requirements for such a check."}, {"heading": "4.2 Definition of the diagnostics", "text": "In the present work, we have a summary statistic S partitioned as S = (S>A , S > B ) >, and it is conflict between SA and SB that interests us. This is related to prior-data conflict checking in the following way. Suppose that we have the posterior density p(\u03b7|SA). We can regard p(\u03b7|SA) as a prior density to be updated by the likelihood term p(SB|SA, \u03b7) to get the posterior density p(\u03b7|S). That is, p(\u03b7|S) \u221d p(\u03b7|SA)p(SB|SA, \u03b7). The density p(\u03b7|SA) usefully reflects information in SA about \u03b7, if we have checked both the model for p(SA|\u03b7) and for prior-data conflict between p(\u03b7) and p(SA|\u03b7) in conventional ways. If updating p(\u03b7|SA) by the information in SB gives a posterior density p(\u03b7|S) that is surprisingly far from p(\u03b7|SA), this indicates that SA and SB bring conflicting information about \u03b7. The appropriate reference distribution for the check is\np(SB|SA) = \u222b p(SB|SA, \u03b7)p(\u03b7|SA)d\u03b7,\nsince we are conditioning on SA in the prior that reflects knowledge of SA before updating for SB.\nSuppose that the regression model used for the likelihood-free inference gives an approximate posterior density p\u0303(\u03b7|S) for summary statistic value S. The change in the approximate posterior when deleting SB,obs from Sobs can be examined using a mulitple imputation procedure, using the method of Zintgraf et al. (2017). We produce M imputations SB(i), i = 1, . . . ,M of SB and writing S(i) = (S > A,obs, SB(i) >)> and following (1) we approximate p(\u03b7|SA,obs) by\np\u0303(\u03b7|SA,obs) = 1\nM M\u2211 i=1 p\u0303(\u03b7|S(i)).\nThe change in the approximate posterior density from p\u0303(\u03b7|SA,obs) to p\u0303(\u03b7|Sobs) can be summarized by the maximum log relative belief\nR\u221e(SB,obs|SA,obs) = sup \u03b7 log p\u0303(\u03b7|Sobs) p\u0303(\u03b7|SA,obs) . (6)\nOur discussion of the prior-data conflict checks of Nott et al. (2020) suggests that an appropriate reference distribution for calibrating a Bayesian model check using (6) would compute the tail probability\np = P (R\u221e(SB|SA,obs) \u2265 R\u221e(SB,obs|SA,obs)), (7)\nwhere SB \u223c p(SB|SA,obs). The imputations in the multiple imputation procedure approximate draws from p(SB|SA,obs), and we suggest drawing a fresh set of imputations S\u2217B(i),\ni = 1, . . . ,M\u2217, separate from those used in computing p\u0303(\u03b7|SA,obs), and to approximate (7) by\np\u0303 = 1\nM\u2217 M\u2217\u2211 i=1 I(R\u221e(S \u2217 B(i)|SA,obs) \u2265 R\u221e(SB,obs|SA,obs)), (8)\nwhere I(A) is the indicator function which is 1 when event A occurs and zero otherwise. Our suggested checks use the statistic (6), calibrated using the estimated tail probability (8)."}, {"heading": "4.3 The logic of Bayesian model checking", "text": "Evans and Moshonov (2006) discuss a strategy for model checking based on a decomposition of the joint Bayesian model p(\u03b7, d) = p(\u03b7)p(d|\u03b7), generalizing an earlier approach due to Box (1980). Different terms in the decomposition should be used for different purposes such as inference and checking of different model components. We do not discuss their approach in detail, but an important principle is that for checking model components we should check the sampling model first, and only afterwards check for prior-data conflict. The reason is that if the model for the data is inadequate, sound inferences cannot result no matter what the prior is.\nIn our analysis, the posterior distribution for \u03b7 is\np(\u03b7|Sobs) \u221d p(\u03b7)p(Sobs|\u03b7). (9)\nOur diagnostics consider sequential Bayesian updating in two stages,\np(\u03b7|SA,obs) \u221d p(\u03b7)p(SA,obs|\u03b7), (10)\nand\np(\u03b7|Sobs) \u221d p(\u03b7|SA,obs)p(SB,obs|SA,obs, \u03b7). (11)\nIn (10), we can check the sampling model p(SA|\u03b7) and then check for conflict between p(\u03b7) and p(SA|\u03b7), and if these checks are passed, then p(\u03b7|SA) usefully reflects information about \u03b7 in p(SA|\u03b7). We could then proceed to check the components in (11). Checking p(SA|\u03b7) in (10) and then p(SB|SA, \u03b7) in (11) is not the same as checking p(S|\u03b7) in (9). The reason is that there can be values of \u03b7 providing a good fit to the data for p(SA|\u03b7) and values of \u03b7 providing a good fit to the data in p(SB|SA, \u03b7), but these need not be the same parameter values.\nFormally, we could imagine an expansion of the original model from\np(S|\u03b7) = p(SA|\u03b7)p(SB|SA, \u03b7), (12)\nto\np(S|\u03b71, \u03b72) = p(SA|\u03b71)p(SB|SA, \u03b72), (13)\nso that the original parameter \u03b7 is now allowed to vary between the two likelihood terms, with the original model corresponding to \u03b71 = \u03b72. If a good fit to the data can only be obtained with \u03b71 6= \u03b72, one would expect that our diagnostics would detect this, as the likelihood terms are peaked in different regions of the parameter space, and this will become evident in the sequential updating of the posterior for the different summary statistic subsets.\nThe inherent asymmetry in SA and SB in the decomposition (12) has consequences for interpretation of the diagnostics. To explain this, consider once more the toy Poisson model in the introduction. Consider first SA = y\u0304 and SB = s 2. Since y\u0304 is sufficient, we have p(\u03b7|SA) = p(\u03b7|S), and our diagnostics would not detect any conflict. However, conventional model checking of the likelihood term p(SB|SA, \u03b7) would detect that the model fits poorly in this case. On the other hand, if SA = s\n2 and SB = y\u0304, then the term p(SB|SA, \u03b7) depends on \u03b7 since s2 is non-sufficient, and p(\u03b7|S) is very different to p(\u03b7|SA). In this case, our diagnostics do indeed reveal the conflict as we show later in Section 5.1. The idea of a conflict between summary statistics is being formalized here in terms of conflict between the likelihood terms p(SA|\u03b7) and p(SB|SA, \u03b7) and this is not symmetric in SA and SB. There is nothing wrong with this, but the interpretation of the diagnostics needs to be properly understood."}, {"heading": "4.4 Other work on model criticism for likelihood-free inference", "text": "We discuss now related work on Bayesian model criticism for likelihood-free inference, which is an active area of current research. Theoretical examinations of ABC methods under model misspecification have been given recently by Ridgway (2017) and Frazier et al. (2020). The latter authors contrast the behaviour of rejection ABC and regression adjusted ABC methods (Beaumont et al., 2002; Blum and Franc\u0327ois, 2010; Li and Fearnhead, 2018) in the case of incompatible summary statistics, where the observed summary statistics can\u2019t be matched by the model for any parameter. Regression adjustment methods can behave in undesirable ways under incompatibility, and the authors suggest a modified regression adjustment for this case. They develop misspecification diagnostics based on algorithm acceptance probabilities, and comparing rejection and regression-adjusted ABC inferences.\nRatmann et al. (2009) considered model criticism based on a reinterpretation of the algorithmic tolerance parameter in ABC. They consider a projection of data onto the ABC error, and use a certain predictive density for this error for model criticism. Ratmann et al. (2011) gives a succinct discussion of the fundamentals of the approach and computational\naspects of implementation. In synthetic likelihood approaches to likelihood-free inference (Wood, 2010; Price et al., 2018) recent work of Frazier and Drovandi (2019) has considered some model expansions that are analogous to the method of Ratmann et al. (2011) for the synthetic likelihood. Their method makes Bayesian synthetic likelihood methods less sensitive to assumptions, and the posterior distribution of the expansion parameters can also be insightful about the nature of any misspecification. The work of Wilkinson (2013), although not focusing on model criticism, is also related to Ratmann et al. (2011). Wilkinson (2013) shows how the ABC tolerance can be thought of as relating to an additive \u201cmodel error\u201d, and that ABC algorithms give exact results under this interpretation. Thomas et al. (2020) discuss Bayesian optimization approaches to learning high-dimensional posterior distributions in the ABC setting in the context of the coherent loss-based Bayesian inference framework of Bissiri et al. (2016). Their work is focused on robustness to assumptions and computation in high-dimensions, rather than on methods for diagnosing model misspecification. Recent work of Frazier et al. (2020) considers a robust ABC approach related to the model expansion synthetic likelihood method of Frazier and Drovandi (2019). This method can also be applied in conjunction with regression adjustment, and the regression adjustment approach can have better behaviour in the case of misspecification.\nAlso relevant to our work is some of the literature on summary statistic choice. In particuclar, Joyce and Marjoram (2008) have considered an approach to this problem which examines changes in an estimated posterior density upon addition of a new summary statistic. This is a useful method for a difficult problem, but their method is not used as a diagnostic for model misspecification, and in fact can sometimes perform poorly in exactly this situation.\nIn a sense, model criticism for Bayesian likelihood-free inference is conceptually no different to Bayesian model criticism with a tractable likelihood. However, issues of computation and summary statistic choice sometimes justify the use of different methods. For general discussions of principles of Bayesian model checking see Gelman et al. (1996), Bayarri and Castellanos (2007) and Evans (2015)."}, {"heading": "5 Examples", "text": ""}, {"heading": "5.1 Poisson model", "text": "We return to the toy motivating example given in the introduction on inference for a Poisson mean, where data y = (y1, . . . , y5) > = (0, 0, 0, 0, 5)> are observed. For these data the sample mean is y\u0304 = 1 and the sample variance is s2 = 5. In the Poisson model, y\u0304 and s2 are both\npoint estimates of the Poisson mean \u03b7. The estimates bring conflicting information about the parameter, because simultaneously observing y\u0304 = 1 and s2 = 5 is unlikely for any value of \u03b7. Figure 1 in the introduction showed ABC estimates of the posterior density for different summary statistic choices, for the prior described in Section 1. Recall that the rejection ABC algorithm performs poorly for the summary statistic (y\u0304, s2)>, because of the difficulty of matching the observed values, and this may indicate a poorly fitting model.\nWe apply the diagnostic method of Section 4 for this example, to show how it can alert the user to the conflicting information in the summaries. The summary statisitcs y\u0304 and s2 are approximately linearly related to each other, since they both estimate \u03b7, and so we use linear regression for imputation using the R package mice (van Buuren and GroothuisOudshoorn, 2011). The mice function from this package is used to generate M = 100 multiple imputations with the Bayesian linear regression method and default settings for other tuning parameters. For the regression likelihood-free inference, we are using the quantile regression forests approach (Meinshausen, 2006) adapted to the ABC setting by Raynal et al. (2018) and available in the R package abcrf. For the random forests regression we use 10, 000 simulations from the prior, and we tune three parameters of the random forests algorithm (\u2018m.try\u2019, \u2018min.node.size\u2019 and \u2018sample.fraction\u2019) using the tuneRanger package (Probst et al., 2019). Figure 2 shows the true posterior density, and some random forests estimates of it. There are three estimates obtained from the fitted random forests regression with the full set of features (y\u0304, s2). The first is based on the observed summary statistic for (y\u0304, s2). The second uses the same fitted regression but replaces the observed summary statistics with (y\u0304, s\u03022), where s\u03022 denotes an imputed value for s2 from the observed y\u0304. With imputation the estimate shown\nis an average over M imputed values. The third uses (\u0302\u0304y, s2) where \u0302\u0304y is an imputed value for y\u0304 based on the observed s2. Also shown are density estimates where the regression is fitted using only features y\u0304, and where the regression is fitted only using features s2.\nWe make four observations. First, using the full summary statistic (y\u0304, s2) with quantile regression forests results in an accurate estimate of the true posterior density, in contrast to the ABC algorithm described in the introduction. The reason for this is the variable selection capability of the quantile regression forest approach, which results in the summary statistic s2 being ignored as it is not informative given the minimal sufficient statistic y\u0304. This is what is expected for our diagnostic, following the discussion of Section 4.3. Our second observation is that imputing s2 from y\u0304 results in very little change to the estimated posterior density. Again this is because the random forest regression fit ignores s2, and so replacing its observed value with a very different imputation is unimportant. Our third observation is that imputing y\u0304 from s2 does result in a posterior density with very different inferential implications. Finally,\nfitting the quantile regression forest with features (y\u0304, s2) and then deleting and imputing one of the features for the observed data gives a similar posterior density estimate to that obtained by fitting the quantile regression with only the feature that was not deleted. This is desirable, because if we examine deletion and imputation for many subsets of features it may be computationally intensive to refit regression models repeatedly, but the imputation process is less computationally burdensome. Finally, Figure 3 illustrates our suggested calibration of\nR\u221e(SB,obs|SA,obs) for the choices SA = y\u0304, SB = s2 and SA = s2, SB = y\u0304 and M\u2217 = 100. The vertical lines in the graphs are the observed statistic values, and the histograms shows reference distribution values for the statistics for imputations of SB from SA,obs. The estimated tail probability (8) corresponds to the proportion of reference distribution values above the observed value. The change in the posterior density when y\u0304 is imputed from s2 is surprisingly large (bottom panel).\nWhat would we do in this and similar examples once a conflict is found? In this example it is natural to change the model so that the mean and variance need not be the same (using a negative binomial model rather than Poisson, for instance). Alternatively, if the problem is such that the current model is \u201cgood enough\u201d in the sense that the fitted model reproducing aspects of the data we care about in the application at hand, we might change the summary statistics so that the conflict is removed while retaining the important information."}, {"heading": "5.2 Stereological extremes", "text": "The next example was discussed in Bortot et al. (2007) and examines an ellpitical model for diameters of inclusions in a block of steel. The size of the largest inclusion is thought to be an important quantity related to the steel strength. The ellpitical model, which has an intractable likelihood, extends an earlier spherical model considered in Anderson and Coles (2002). We do not give full details of the model but refer to Bortot et al. (2007) for a more detailed description.\nThere are 3 parameters in the model. The first is the intensity \u03bb of the homogeneous Poisson point process of the inclusion locations. There are also two parameters in a model for the inclusion diameters. This model is a generalized Pareto distribution with scale parameter \u03c3 and shape \u03be, for diameters larger than a threshold which is taken as 5\u00b5m. Writing \u03b8 = (\u03bb, \u03c3, \u03be)>, the prior density for \u03b8 is uniform on [2, 200] \u00d7 [0, 10] \u00d7 [\u22125, 5]. The observed data consists of N = 112 inclusion diameters observed from a single planar slice. The summary statistic used consists of N and the quantiles of diameters at levels q = 0, 0.05, 0.1, 0.2, 0.8, 0.9, 0.95, 1. Here there may be some interest in whether the simple Pareto model can fit well for the observations in the lower and upper tail simultaneously. Quantile-based summary statistics for ABC were also considered in Erhardt and Sisson (2016). It is expected that the Pareto model will fit the upper quantiles well, but using only the most\nextreme quantiles might result in a loss of information. Capturing the behaviour of the diameters in the upper tail is most important for the application here, given the relationship between the largest inclusions and steel strength.\nWe apply our diagnostic method in the following way. First, we used quantile regression forests to estimate the marginal posterior densities for each parameter \u03bb, \u03c3, \u03be separately, and using the full set of summary statistics, which we denote by S. We can write S = (N,S>L , S > U ) >, where SL denotes the vector of lower quantiles at levels q = 0, 0.05, 0.1, 0.2 and SU denotes the vector of upper quantiles at levels q = 0.8, 0.9, 0.95, 1. In fitting the random forests regressions we used 500, 000 simulations of parameters and summary statistics from the prior, and the random forest tuning parameters were chosen using the tuneRanger package in R. We apply our diagnostic by deleting and imputing SL and SU respectively to see if there is conflicting information in the lower and upper quantiles about the parameters. The imputation is done using the R package missRanger, which uses a random forests approach (Mayer, 2019). We use the missRanger function in this package with M = 100 and 20 trees with other tuning parameters set at the default values. Figure 4 shows how the estimated marginal posterior densities change for the various parameters upon deletion and imputation of SL and SU .\nWe make two observations. First, when the upper quantiles are imputed from the lower ones, the estimated marginal posterior densities change substantially for \u03c3 and \u03be, the parameters in the inclusion model. This does suggest that the Pareto model is not able to simultaneously fit the lower and upper quantiles well. Given the importance in this application of capturing the behaviour of the largest diameters, we might remove the lower quantiles from the vector of summary statistics to obtain a model that is fit for purpose. Secondly, and unlike our first example, the results of deletion and imputation do not match very well with the estimated posterior densities obtained by refitting the regression for parameter subsets directly. However, the estimates based on deletion and imputation are sufficiently good for the diagnostic purpose of revealing the conflict, and the computational burden of imputation is much less than refitting, given that our regression training set contains 500, 000 observations, and that algorithm parameters need to be tuned by cross-validation.\nFigure 5 shows our suggested calibration of R\u221e(SB,obs|SA,obs) for the choices SA = (N,SL), SB = SU and SA = (N,SU), SB = SL and M \u2217 = 100. The vertical lines in the graphs are the observed statistic values, and the histograms shows reference distribution values for the statistics for imputations of SB from SA,obs. When SU is imputed from SL (bottom row), the change in the estimated posterior density is surprising for \u03bb and \u03c3."}, {"heading": "5.3 Ricker model with threshold-autoregressive auxiliary model", "text": "Here we consider a Ricker model (Ricker, 1954), a simple model for the dynamics of animal population sizes in ecology. Likelihood-free inference will be considered using summary statistics which are point estimates from an auxiliary model with tractable likelihood.\nThe auxiliary model is a two-component self-exiciting threshold autoregression (SETAR) model (Tong, 2011), in which the two auto-regressive components describe dynamic behaviour of the process at high and low levels respectively. Maximum likelihood estimates of the SETAR model parameters provide the summary statistics. The use of an auxiliary models with tractable likelihood is a common way to obtain summary statistics for likelihood-free inference - see Drovandi et al. (2015) for a unified discussion of such approaches. We focus on whether there is conflict between the summary statistics for the two autoregressive components in our auxiliary model. Our checks can be insightful about the ways in which the Ricker model fails to fit well at high and low levels.\nLet Nt, t \u2265 0 be a series of unobserved population sizes, and suppose we observe values dt \u223c Poisson(\u03c6Nt), where \u03c6 is a sampling parameter. The series Nt has some initial value N0 and one-step conditional distributions are defined by\nNt+1 = rNt exp(\u2212Nt + et+1),\nwhere r is a growth parameter and et \u223c N(0, \u03c32) is an independent environmental noise series. We write \u03b8 = (log \u03c6, log r, log \u03c3)> for the parameters, and the prior density for \u03b8 is uniform on the range [11, 13] \u00d7 [\u22120.02, 0.04] \u00d7 [\u22122,\u22120.5]. The prior was chosen to achieve a reasonable scale and variation based on prior predictive simulations. The Ricker model can exhibit chaotic behaviour when the environmental noise variance is small. In this case, it can be challenging\nto fit the model using methods which perform state estimation to approximate the likelihood. Wood (2010) and Fasiolo et al. (2016) discuss further the motivations for using likelihoodfree methods for time series models with chaotic behaviour. Model misspecification could be another reason for conditioning only on data summaries that we care about \u2013 the model for the full data may be considered as a device for implicitly defining a model for summary statistics, and it may be that we only care about good model specification for certain summary statistics important in the application.\nFor a time series Xt, t = 0, 1, . . . T , the SETAR model used to obtain summary statistics\ntakes the form\nXt = { a0 + a1Xt\u22121 + a2Xt\u22122 + t, t \u223c N(0, \u03c12) if Xt\u22121 < c b0 + b1Xt\u22121 + b2Xt\u22122 + t, t \u223c N(0, \u03b62) if Xt\u22121 \u2265 c .\nIndependence is assumed at different times for the noise sequence t. The parameter c is a threshold parameter, and the dynamics of the process switches between two autoregressive components of order 2 depending on whether the threshold is exceeded. To obtain our summary statistics, we fit this model to the observed data, and fix c based on the observed data fit. With this fixed c the SETAR model is then fitted to any simulated data series d to obtain maximum likelihood estimates of the SETAR model parameters, which are the summary statistics denoted by S = S(d). We write S = (S>L , S > U ) >, where SL = (a\u03020, a\u03021, a\u03022, \u03c1\u0302) >\nand SU = (\u0302b0, b\u03021, b\u03022, \u03b6\u0302) > are maximum likelihood estimates for the autoregressive component parameters for low and high levels respectively. The SETAR models are fitted using the TAR package (Zhang and Nieto, 2017) in R. In simulating data from the model there were some cases where there were no values of the series above the threshold c. Since the number of such cases was small, we simply discarded these simulations.\nThe observed data are a series of blowfly counts described in Nicholson (1954). Nicholson\u2019s experiments examined the effect of different feeding restrictions on blowfly population dynamics. We use the series for the adult food limitation condition shown in Figure 3 of Nicholson (1954). The Ricker model fits the blowfly data poorly, but the example is instructive for illustrating methodology for model checking. A much better model is described in Wood (2010), based on a related model considered in Gurney et al. (1980). In the model of Wood (2010), the population size is a sum of a recruitment and survival process, with the recruitment process related to previous population size at some time lag. This model is able to reproduce the strong periodicity and other complex features of the data, which the simple Ricker model does not do.\nWe compute ABC posterior estimates for each parameter using quantile regression forests, once again tuning algorithm parameters by cross-validation using the tuneRanger package. The quantile regression forests are fitted based on 50, 000 values from the prior with corresponding TAR summary statistics. We consider fitting the random forests regression using summaries S, SL only and SU only. For the fit with summary statistics S we consider the posterior density estimated using the observed S, as well as values of S where SL is imputed from SU , and where SU is imputed from SL. The imputation is done using the R package missRanger (Mayer, 2019) using the missRanger function with M = 100 and 20 trees with other tuning parameters set at the default values. The results are shown in Figure 6.\nWe make two observations. First, the posterior density for logR is very different when only SU is used compared to either SL only or S. This suggests that the dynamics at high and low levels of the series are inconsistent with a single fixed value of the growth parameter R. Second, the posterior densities estimated using quantile regression forests with features SU only, are estimated quite well by the corresponding estimates using features S and deletion and imputation. For the posterior densities estimated using SL only, they are also estimated quite well for R, but not the other parameters, using features S and deletion and imputation of SU .\nFigure 7 shows our suggested calibration of R\u221e(SB,obs|SA,obs) for the choices SA = SL, SB = SU and SA = SU , SB = SL. The vertical lines in the graphs are the observed statistic values, and the histograms shows reference distribution values for the statistics for imputations\nof SB from SA,obs. When SU is imputed from SL (bottom row), the change in the estimated posterior density for logR is surprising."}, {"heading": "5.4 Ricker model without summary statistics: convolutional neural", "text": "networks\nWe now consider using the whole time series as the feature vector in the Ricker model, and use a convolutional neural network as the regression method. Convolutional networks are not described in detail here; the reader is referred to Polson and Sokolov (2017) and Fan et al. (2019) for introductory discussions suitable for statisticians. Dinev and Gutmann (2018) considered the use of convolutional networks for automated summary statistic choice for likelihood-free inference for time series and showed that a single architecture can provide good performance in a range of problems. In fitting the convolutional network we use a squared error loss, and after obtaining point estimates for the network parameters and response variance we use a plug-in normal predictive distribution.\nWriting, as before, \u03b7 for both the full parameter \u03b7 = (log \u03c6, log r, log \u03c3)>, as well as some scalar function of interest (where the meaning intended is clear from the context), we estimate the marginal posterior density for \u03b7 from samples (\u03b7(i), d(i)) \u223c p(\u03b7)p(d|\u03b7), i = 1, . . . , N . We used N = 10, 000 below. The regression model is\n\u03b7(i) = fw(d (i)) + (i),\nfor errors (i), where fw(\u00b7) is a convolutional neural network with weights w. Figure 8 shows the architecture used, which is based on the architecture shown in Figure 6 (a) of Dinev and\nGutmann (2018). Training of the network was done using the Keras API for Python, using the Adam optimizer with default settings. The model was trained for 50 epochs with a batch size of 64. After estimating (w, \u03c32 ) as (w\u0302, \u03c3\u0302 2 ), the estimated posterior density for data d is N(fw\u0302(dobs), \u03c3\u0302 2 ).\nWe wish to develop some diagnostics useful for measuring the influence of the observation at time t on inference. A method is developed here similar to that of Zintgraf et al. (2017) for measuring the influence of a pixel in image classification. Deletion of \u201cwindows\u201d of the raw series containing t can be considered (i.e. SB are the values of the series in a window of width k, say, containing t, and SA is the remainder of the series) and then we consider averages of the statistics R\u221e(SB|SA) over all windows B containing t. In this example we will consider windows of size k = 4, and window values are imputed from one neighbouring value to the left and right of the window. At the boundary points where only one neighbouring value is available we use this. The method is described more precisely in the Appendix, as well as our approach to the multiple imputation. When computing the maximum log relative belief statistic (6), we do not truncate the normal predictive densities for the regression model to the support of the parameter space, but we do compute the supremum over this support.\nFor the parameter log \u03c3, Figure 9 shows a plot of the raw series values with points t with\ncalibration probability p\u0303t for our diagnostic less than 0.05 indicated by vertical lines (top panel), and a plot of the p\u0303t values themsleves indicated by coloured bands (bottom panel). Some of the smallest values in the series around four of the local minima are influential for the estimation of log \u03c3. Similar plots for the other two parameters did not show any points where the calibration probabilities are very small, except for one point near the boundary for log \u03c6; however, in that case the failure of the imputation method to handle boundary cases well may be an issue. Compared to our previous analysis, the approach using convolutional networks examines misfit for temporally localized parts of the series. Poor fitting regions seem to particularly affect the environmental noise variance parameter that describes variation unexplained by the model dynamics."}, {"heading": "6 Discussion", "text": "We have discussed a method for detecting the existence of conficting summary statistics using regression approaches to likelihood-free inference. The approach uses a recent idea from the interpretable machine learning literature, based on deletion and imputation of part of a feature vector to judge the importance of subsets of features for the prediction. In likelihoodfree regression approximations, where the regression predictive distribution is the estimated posterior distribution, the method estimates the posterior distribution conditioning only on a subset of features, and compares this with the full posterior distribution. The deletion and imputation process is less computationally burdensome than fitting a flexible regression model for different feature subsets directly. The approach can be related to prior-data conflict checking methods examining the consistency of different sources of information, and in this interpretation the posterior distribution based on a subset of the features is considered as the prior, which is updated by the likelihood term for the remaining features. Based on this connection, a way to calibrate the diagnostics is suggested.\nA concern expressed in the work of Zintgraf et al. (2017) was possible sensitivity of their procedure to the imputation method used. That is a concern in our applications also, although we used a number of different imputation methods in the examples and did not find this sensitivity to be a problem for reasonable choices. The diagnostics described are computed using regression approaches to likelihood-free inference, but they may be useful even if another likelihood-free inference approach is employed for inferential purposes, or in the case where the likelihood is tractable. We believe that insightful methods for model criticism are very important, since they can result in meaningful model expansions and refinements, and ultimately more appropriate inferences and decisions."}, {"heading": "Acknowledgements", "text": "Michael Evans was supported by a grant from the Natural Sciences and Engineering Research Council of Canada.\nAppendix\nDetails of diagnostic for the example of Section 5.4\nWe describe how we implement our diagnostic for the example of Section 5.4. Roughly speaking, all windows B of width k are considered including a time t, and then we average R\u221e(SB|SA) over B where SB consists of the series values in B and SA is the remaining values.\nTo make the method precise we need some further notation. Let d = {di : 1 \u2264 i \u2264 T} denote a time series of length T . For some subset C of the times, C \u2286 {1, . . . , T}, we write d(C) = {di : i \u2208 C} and d(\u2212C) = {di : i /\u2208 C}. Let t \u2208 {1, . . . , T} be a fixed time. Let WKt = {C t,k 1 , . . . , C t,k nt,k } denote the set of all windows of width k containing t of the form {l, . . . , l + k \u2212 1} for some l. For each j = 1, . . . , nt,k suppose that dt,kj is some time series of length T , and write dt,k. = (d t,k 1 , . . . , d t,k nt,k ). Write dt,kobs for the value of d t,k . where d t,k j = dobs for all j = 1, . . . , nt,k where dobs is the observed series. Let d t,k,\u2217 . denote the value of d t,k . where dt,k,\u2217j (\u2212C t,k j ) = dobs(\u2212C t,k j ) and d t,k,\u2217 j (C t,k j ) \u223c p(d(C t,k j )|dobs(\u2212C t,k j )) i.e. the observation for dt,k,\u2217j in C t,k j are generated from the conditional prior predictive given the observed value for the remainder of the series. The draws dt,k,\u2217j (C t,k j ) are independent for different j. Let\nRt,k(dt,k. ) = 1\nnt,k nt,k\u2211 j=1 R\u221e(d t,k j (C t,k j )|d t,k j (\u2212C t,k j )).\nWe base our diagnostic on Rt,k(dt,kobs), calibrated by\npt = P (R t,k(dt,k,\u2217. ) \u2265 Rt,k(d t,k obs)), (14)\nand estimate (14) by\np\u0303t = 1\nM\u2217 M\u2217\u2211 i=1 I(Rt,k(dt,k,i. ) \u2265 Rt,k(d t,k obs)), (15)\nwhere dt,k,i. , i = 1, . . . ,M \u2217 are approximations of draws of dt,k,\u2217. based on imputation, i.e. we have imputed dt,k,\u2217j (C t,k j ) from dobs(\u2212C t,k j ) independently for each j and i.\nDetails of the imputation method for the example of Section 5.4\nFigure 10 illustrates the idea behind the window-based imputation we use in the example of Section 5.4. We need to impute values of the series for a window of size k which has been deleted (indicated by the blue region in the figure). A larger window around the one of width k is considered (red patch in the figure). To impute, we first obtain a mean imputation using the functions in the imputeTS package (Moritz and Bartz-Beielstein, 2017). In particular, for imputing a conditional mean we use the na_interpolation function in imputeTS with spline interpolation and default settings for other tuning parameters. For multiple imputation, we add noise to the conditional mean by fitting a stationary Gaussian autoregressive model of order one to the observed series and then consider zero mean Gaussian noise, where the covariance matrix of the noise is the conditional covariance matrix of the autoregressive process in the blue region given the remaining observations in the red patch. Note that although the series values are counts, these counts are generally large and we treat them as continuous quantities in the imputation procedure."}], "title": "Detecting conflicting summary statistics in likelihood-free inference", "year": 2020}