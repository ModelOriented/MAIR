{"abstractText": "Increasing number of sectors which affect human lives, are using Machine Learning (ML) tools. Hence the need for understanding their working mechanism and evaluating their fairness in decision-making, are becoming paramount, ushering in the era of Explainable AI (XAI). In this contribution we introduced a few intrinsically interpretable models which are also capable of dealing with missing values, in addition to extracting knowledge from the dataset and about the problem. These models are also capable of visualisation of the classifier and decision boundaries: they are the angle based variants of Learning Vector Quantization. We have demonstrated the algorithms on a synthetic dataset and a real-world one (heart disease dataset from the UCI repository). The newly developed classifiers helped in investigating the complexities of the UCI dataset as a multiclass problem. The performance of the developed classifiers were comparable to those reported in literature for this dataset, with additional value of interpretability, when the dataset was treated as a binary class problem. Keywords\u2014 adaptive distances, learning vector quantization, non-linear visualization, interpretability", "authors": [{"affiliations": [], "name": "Kerstin Bunte"}], "id": "SP:021750465783c68dd85b4857e624a7cddeb4c6f0", "references": [{"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["Adrien Bibal", "Ben\u00f4\u0131t Fr\u00e9nay"], "title": "Interpretability of machine learning models and representations: an introduction", "venue": "In ESANN,", "year": 2016}, {"authors": ["Diogo V Carvalho", "Eduardo M Pereira", "Jaime S Cardoso"], "title": "Machine learning interpretability", "venue": "A survey on methods and metrics. Electronics,", "year": 2019}, {"authors": ["Andreas Backhaus", "Udo Seiffert"], "title": "Classification in highdimensional spectral data: Accuracy vs. interpretability vs. model size", "year": 2014}, {"authors": ["K. Bunte", "P. Schneider", "B. Hammer", "F.-M. Schleif", "T. Villmann", "M. Biehl"], "title": "Limited Rank Matrix Learning, Discriminative Dimension Reduction and Visualization", "venue": "Neural Networks,", "year": 2012}, {"authors": ["Alexander Schulz", "Andrej Gisbrecht", "Barbara Hammer"], "title": "Using discriminative dimensionality reduction to visualize classifiers", "venue": "Neural Processing Letters,", "year": 2015}, {"authors": ["Cl\u00e9ment B\u00e9nard", "G\u00e9rard Biau", "S\u00e9bastien Da Veiga", "Erwan Scornet"], "title": "Sirus: making random forests interpretable", "venue": "arXiv preprint arXiv:1908.06852,", "year": 2019}, {"authors": ["A.S. Sato", "K. Yamada"], "title": "Generalized learning vector quantization", "venue": "Advances in Neural Information Processing Systems, volume 8, pages 423\u2013429", "year": 1996}, {"authors": ["B. Hammer", "T. Villmann"], "title": "Generalized relevance learning vector quantization", "venue": "Neural Networks, 15(8\u20139):1059 \u2013 1068", "year": 2002}, {"authors": ["P. Schneider", "M. Biehl", "B. Hammer"], "title": "Relevance matrices in learning vector quantization", "venue": "M. Verleysen, editor, Proc. of the 15th European Symposium on Artificial Neural Networks (ESANN), pages 37\u201343, Bruges, Belgium", "year": 2007}, {"authors": ["Petra Schneider", "Michael Biehl", "Barbara Hammer"], "title": "Adaptive relevance matrices in learning vector quantization", "venue": "Neural computation,", "year": 2009}, {"authors": ["S. Ghosh", "E.S. Baranowski", "R. van Veen", "Gert-Jan de Vries", "M. Biehl", "W. Arlt", "P. Tino", "K. Bunte"], "title": "Comparison of strategies to learn from imbalanced classes for computer aided diagnosis of inborn steroidogenic disorders", "venue": "European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning,", "year": 2017}, {"authors": ["K. Bunte", "E.S. Baranowski", "W. Arlt", "P. Tino"], "title": "Relevance learning vector quantization in variable dimensional spaces", "venue": "GCPR", "year": 2016}, {"authors": ["Petra Schneider", "Michael Biehl", "Barbara Hammer"], "title": "Distance learning in discriminative vector quantization", "venue": "Neural computation,", "year": 2009}, {"authors": ["Kerstin Bunte", "Barbara Hammer", "Axel Wism\u00fcller", "Michael Biehl"], "title": "Adaptive local dissimilarity measures for discriminative dimension reduction of labeled data", "year": 2010}, {"authors": ["T.A. Driscoll", "N. Hale"], "title": "and L", "venue": "N. Trefethen. Chebfun guide", "year": 2014}, {"authors": ["Andras Janosi", "William Steinbrunn", "Matthias Pfisterer", "Robert Detrano"], "title": "Heart disease data set, UCI machine learning", "year": 1988}, {"authors": ["Mai Shouman", "Tim Turner", "Rob Stocker"], "title": "Applying k-nearest neighbour in diagnosing heart disease patients", "venue": "International Journal of Information and Education Technology,", "year": 2012}, {"authors": ["Humar Kahramanli", "Novruz Allahverdi"], "title": "Design of a hybrid system for the diabetes and heart diseases", "venue": "Expert systems with applications,", "year": 2008}, {"authors": ["Jesmin Nahar", "Tasadduq Imam", "Kevin S Tickle", "Yi- Ping Phoebe Chen"], "title": "Computational intelligence for heart disease diagnosis: A medical knowledge driven approach", "venue": "Expert Systems with Applications,", "year": 2013}, {"authors": ["Sellappan Palaniappan", "Rafiah Awang"], "title": "Intelligent heart disease prediction system using data mining techniques", "venue": "In 2008 IEEE/ACS international conference on computer systems and applications,", "year": 2008}, {"authors": ["Nitesh V. Chawla", "Kevin W. Bowyer", "Lawrence O. Hall", "W. Philip Kegelmeyer"], "title": "Smote: synthetic minority over-sampling technique", "venue": "Journal of artificial intelligence research,", "year": 2002}], "sections": [{"text": "ar X\niv :2\n00 5.\n03 63\n2v 2\n[ cs\n.L G\n] 8\nM ay\nIncreasing number of sectors which affect human lives, are using Machine Learning (ML) tools. Hence the need for understanding their working mechanism and evaluating their fairness in decision-making, are becoming paramount, ushering in the era of Explainable AI (XAI). In this contribution we introduced a few intrinsically interpretable models which are also capable of dealing with missing values, in addition to extracting knowledge from the dataset and about the problem. These models are also capable of visualisation of the classifier and decision boundaries: they are the angle based variants of Learning Vector Quantization. We have demonstrated the algorithms on a synthetic dataset and a real-world one (heart disease dataset from the UCI repository). The newly developed classifiers helped in investigating the complexities of the UCI dataset as a multiclass problem. The performance of the developed classifiers were comparable to those reported in literature for this dataset, with additional value of interpretability, when the dataset was treated as a binary class problem.\nKeywords\u2014 adaptive distances, learning vector quantization, non-linear visualization, interpretability"}, {"heading": "1 Introduction", "text": "In this era of increasing number of machine learning (ML) algorithms being deployed in various sectors, including finance, healthcare, criminology, justice, politics, manufacturing, and logistics, more and more human lives are impacted by them. Consequently there is a rising need of transparency and interpretability of the models [1\u20133] to achieve comprehensible decisions. ML algorithms with greater predictive powers are often more complex and behave like a black box, i.e. the working logic of these models is concealed from the human experts, thus obviating any way of verifying the reasoning and thus, the fairness of system [3]. However role of ML in high-stake prediction applications concerning human lives demand that its decisions be explainable by\n\u2217 c\u00a920XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works\nhumans [3].\nHowever, there have been debates about the meaning of the term interpretability, and how to compare interpretability of different classifiers, especially when comparing models of distinct types. To tackle this problem Backhaus and Seiffert proposed 3 criteria [2,4]: (1) the model\u2019s ability to perform feature selection from the input pattern, (2) the model\u2019s ability to provide typical data points representing a class, and (3) model parameters having information about the decision boundary directly encoded. Different strategies have been proposed: including model-agnostic pre- or post-processing methods such as univariate feature selection [3] and post hoc visualisation of decision boundaries [5, 6]. This contribution focuses on intrinsically interpretable techniques and hence model-specific examples. Using these criteria Support Vector Machines (SVM) models [2] are graded 1 out of 3 because they satisfy only criteria (3), to contain information about the decision boundary. In Decision trees (DTs) [7] rules are interpretable. A typically higher performance classifier, Random Forest (RF), is built by bagging several DTs on random subsets of the data. However ensembling compromises on interpretability. Naive Bayes (NB) assumes independence of features which leads to interpretability of individual features and their contribution for decision making. However it lacks the ability to account for feature interactions for the target outcome [3]. In this paper we aim to develop a competitive classifier in terms of performance, which is also easily interpretable, and can be visualised, satisfying criteria 1-3 [4].\nNearest Prototype Classification (NPC) is an intuitive learning scheme where a novel sample gets assigned the class label of its closest prototype. Thus techniques implementing it, such as Generalized LVQ (GLVQ) [8] for example. often allow interpretation of the prototypes as representative of class information allowing transparency with respect to (2). The Generalized Relevance LVQ (GRLVQ) [9] extension to it additionally provide feature relevance determination by introduction of an adaptive parameterized dissimilarity. This weighs the importance of features for the classification and makes this extension fulfill criteria (1) as well. Further adaptations allow for multi-variate and class-wise feature analysis [10,11] and visualisation of decision boundaries [5]. However certain datasets, such as medical data, often contain missing values, heterogeneous measurements, and frequently exhibit imbalanced classes which often hinder the straightforward application of ML algorithms.\nWe addressed the aforementioned challenges by introducing an angular adaptive dissimilarity measure and an oversampling\nstrategy in [12]. In this contribution we present and demonstrate extensions to [12] which allow for knowledge discovery from nonlinearly separable datasets exhibiting the mentioned hindrances. The proposed interpretable classifiers are demonstrated on a synthetic and a publicly available dataset. These classifiers are capable of class-wise and multi-variate feature analysis and visualisation of non-linear decision boundaries (see section 2), thus satisfying at least 2 of the 3 criteria of [4]. Detailed explanation of GLVQ and its extensions relevant to this paper can be found in section 2."}, {"heading": "2 Methods", "text": "In this section we present the interpretable LVQ algorithm capable of dealing with missingness and proposed extensions for non-linear decision boundaries and visualisation. We assume training is based on S data samples {xi \u2208 RD}Si=1 accompanied by a label c(xi) belonging to one of C classes and a set of adaptive prototypes w \u2208 RD with labels c(w). A new data sample receives a label following a prototype-based nearest neighbor classification scheme: by assigning the label of the closest prototype with c(wJ ) = argminJ d J i using a dissimilarity measure dJi = d(xi,wJ ). The paper by [8] introduced Generalized LVQ (GLVQ), in which the prototype positions were optimsed using the following cost function:\nE = S \u2211\ni=1\n\u03a6\n(\ndJi \u2212 dKi dJi + d K i\n)\n, (1)\nwith dJi being the Euclidean distance of each training sample to the closest prototype of the same class c(xi) = c(wJ ) and d K i the closest prototype with another class label. \u03a6 is a monotonic function and we set it to the identity \u03a6(a) = a throughout this contribution. Learning takes place by adapting the prototypes w, e.g. by stochastic gradient descent updating the closest correct and wrong prototypes wL, L \u2208 {J,K} using the derivatives \u2207wL = \u2202E\n\u2202wL :\n\u2202E\n\u2202wJ =\nS \u2211\ni=1\n\u03b3 J i \u2202dJi \u2202wJ and \u2202E \u2202wK =\nS \u2211\ni=1\n\u03b3 K i \u2202dKi \u2202wK with\n\u03b3 J i = 2dKi (dJi + d K i ) 2 and \u03b3Ki = \u22122dJi (dJi + d K i ) 2 (2)\nAfter training the prototypes can often be considered typical representatives of their class and their characteristics can be investigated for interpretation.\nSince the Euclidean distance is sensitive to missing values the authors introduced an angle-based variant ALVQ allowing learning in variable dimensional spaces [12,13]:\nd L i = g\u03b2(b) = e(\u2212\u03b2(b\u22121)) \u2212 1 e(2\u03b2) \u2212 1 with b = xi \u00b7wL \u2016xi\u2016\u2016wL\u2016 . (3)\nThe exponential function g\u03b2(b) transforms the angle b = cos \u03b8 \u2208 [\u22121, 1] into dissimilarities in [0,1] with the hyper-parameter \u03b2 influencing the slope, e.g. \u03b2 \u2192 0 leading to a near linear relationship. In presence of missing data the angle b and derivatives are computed with the available dimensions only. Optimization takes place deriving the cost function E Eq. (1-2) with changed\ndissimilarity dLi adding:\n\u2202dLi \u2202wL = \u2202g\u03b2(b) \u2202b \u00b7 \u2202b \u2202wL and (4)\n\u2202g\u03b2(b)\n\u2202b = \u2212\u03b2 exp(\u2212\u03b2b+ \u03b2) exp(2\u03b2) \u2212 1 . (5)\nThe update rules of GLVQ contains forces attracting the closest correct prototype for each data sample and repulsion of the closest one with a different class label. For example in an imbalanced 2 class problem the Euclidean variant might push the minority class prototype far away from the data all together, since it is being repelled more often by the majority class than attracted by the minority class. ALVQ classifies on the hypersphere, so a prototype cannot be infinitely repelled without returning on the other side, leading to more stable behaviour facing imbalance. Finally, the dissimilarity measure dLi can be parameterized leading to several powerful extensions with varying potential for further interpretation. We group the novel angle extensions into three categories, namely global, local and 2 matrix, as explained in the following subsections."}, {"heading": "2.1 Global relevance matrix", "text": "First extensions to GLVQ introduced parameterized dissimilarity measures based on the quadratic form:\nd L i = (xi \u2212wL)\u22a4\u039b(xi \u2212wL) , (6)\nwith the semi-definite matrix \u039b \u2208 RD\u00d7D containing additional parameters for optimization. A variant called Relevance GLVQ (GRLVQ) [9] assumes \u039b to be a diagonal matrix with \u2211D\ni=1 \u039b 2 ii =\n1. The diagonal elements ri = \u039b 2 ii allow learning of discriminant feature directions, which automatically reduces the influence of less relevant measurement dimensions. However GRLVQ is univariate and does not take into account features which are relevant only in combination with another. Generalized Matrix LVQ (GMLVQ) [10,11,14] tackles this issue by allowing a full matrix \u039b, ensuring semi-definiteness by the decomposition \u039b = \u2126\u22a4\u2126 and optimizing E with respect to \u2126 \u2208 RD\u00d7D. Since dLi can be rewritten as squared Euclidean distance in the space linearly transformed by \u2126: dLi = ( \u2126xi \u2212 \u2126wL )2 , [5] used the concept for discriminant visualisation. This is achieved by limiting the rank of \u039b using a rectangular matrix \u2126 \u2208 RM\u00d7D with M \u2264 D, which in turn can be used to visualise the piecewise linear decision boundaries if M \u2208 {2, 3}.\nSimilarly, to extend ALVQ to global relevances we proposed a parameterized computation of the angle [12,13]:\nb = b\u2126 = x \u22a4 i \u2126 \u22a4\u2126wL\n\u2016xi\u2016\u2126\u2016wL\u2016\u2126 with \u2016v\u2016\u2126 =\n\u221a v \u22a4\u2126\u22a4\u2126v , (7)\nwith corresponding derivatives:\n\u2202b\u2126 \u2202wL =\nxi\u2126 \u22a4\u2126\u2016wL\u20162\u2126 \u2212 xi\u2126\u22a4\u2126wL \u00b7wL\u2126\u22a4\u2126\n\u2016xi\u2016\u2126\u2016wL\u20163\u2126 (8)\n\u2202b\u2126 \u2202\u2126md = xi,m\n\u2211\nj \u2126jdw L j + w L m\n\u2211\nj \u2126jdxi,j\n\u2016xi\u2016\u2126\u2016wL\u2016\u2126 \u2212 xi\u2126\u22a4\u2126wL\n\u00b7 [ xi,m \u2211 j \u2126jdxi,j\n\u2016xi\u20163\u2126\u2016wL\u2016\u2126 +\nwLm \u2211 j \u2126jdw L j\n\u2016xi\u2016\u2126\u2016wL\u20163\u2126\n]\n, (9)\nwhere xi,m denotes dimension m of vector xi. As before the diagonal of \u039b = \u2126T\u2126 denotes the individual feature relevances\nfor the classification and \u2126 can be rectangular \u2126 \u2208 RM\u00d7D with M \u2264 D to be used for visualisation. Resulting visualisations are one M dimensional hyper-spheres where the angle-based classification takes place. The global Euclidean and angle implementation will be abbreviated by LVQg and ALVQg respectively."}, {"heading": "2.2 Local relevance matrix", "text": "The localized extension LGMLVQ [11] allows more complex modeling and prototype or class-wise feature relevance determination by attaching metric tensors \u03a8c to each prototype or each class (based on the user\u2019s choice):\nd c i = (xi \u2212wc)\u22a4\u03a8c\u22a4\u03a8c(xi \u2212wc) . (10)\nThis Euclidean variant is powerful for finding solutions to nonlinearly separable multi-class problems. The diagonal of the local metric tensors \u039bc = \u03a8\nc\u22a4\u03a8c contain local or class-wise feature relevances, which can be investigated by the user for classspecific discriminative information. However, visualising the decision boundaries is not trivial and non-linear mappings based on charting can be found in [5,15].\nIn this contribution we extend ALVQ learning with missing data to local relevances following similar principles:\nb = b\u03a8L = x \u22a4 i \u03a8 L\u22a4\u03a8LwL\n\u2016xi\u2016\u03a8L\u2016wL\u2016\u03a8L . (11)\nThe corresponding derivatives of b\u03a8L are as follows:\n\u2202b\u03a8L \u2202wL =\nxi\u03a8 L\u22a4\u03a8L\u2016wL\u20162\u03a8L \u2212 xi\u03a8L\u22a4\u03a8LwL \u00b7wL\u03a8L\u22a4\u03a8L\n\u2016xi\u2016\u03a8L\u2016wL\u20163\u03a8L (12)\n\u2202b\u03a8L \u2202\u03a8Lmd =\nxi,m \u2211 j \u03a8Ljdw L j + w L m \u2211 j \u03a8Ljdxi,j\n\u2016xi\u2016\u03a8L\u2016wL\u2016\u03a8L \u2212\nxi\u03a8 L\u22a4\u03a8LwL\n[\nxi,m \u2211 j \u03a8Ljdxi,j \u2016xi\u20163\u03a8L\u2016wL\u2016\u03a8L + wLm \u2211 j \u03a8Ljdw L j \u2016xi\u2016\u03a8L\u2016wL\u20163\u03a8L\n]\n(13)\nSimilarly to the Euclidean version the local matrices can lead to valuable insight about local or class-wise relevant features and visualisation of the non-linear decision boundaries needs additional effort. The local Euclidean and angle implementation will be abbreviated by LVQl and ALVQl respectively."}, {"heading": "2.3 2 matrix decomposition for visualisation", "text": "As a compromise between linear dimensionality reduction and visualisation of non-linear decision boundaries [5] introduced a composition of the matrix in the quadratic form Eq. (6) with two matrices:\nd c i = (xi \u2212wc)\u22a4\u2126\u22a4\u03a8c\u22a4\u03a8c\u2126(xi \u2212wc) , (14)\nwith \u2126 \u2208 RM\u00d7D and \u03a8c \u2208 RM\u00d7M . The data and prototypes are therefore transformed linearly to the M -dimensional space and the local metric tensors define the non-linear decision boundaries in that space. If the intrinsic dimensionality is more than M \u2208 {2, 3} a loss of information in classification and visualisation is inevitable, however the cost function ensures that this loss is minimized.\nIn this contribution we similarly extend ALVQ for visualisation with non-linear decision boundaries:\nb = b2M = x \u22a4 i \u2126 \u22a4\u03a8L\u22a4\u03a8L\u2126wL\n\u2016xi\u20162M\u2016wL\u20162M (15)\nwith \u2016v\u20162M = \u221a v \u22a4\u2126\u22a4\u03a8L\u22a4\u03a8L\u2126v and derivatives:\n\u2202b2M \u2202wL = xi\u2126 \u22a4\u03a8L\u22a4\u03a8L\u2126\u2016wL\u201622M \u2016xi\u20162M\u2016wL\u201632M \u2212\nxi\u2126 \u22a4\u03a8L\u22a4\u03a8L\u2126wL \u00b7wL\u2126\u22a4\u03a8L\u22a4\u03a8L\u2126\n\u2016xi\u20162M\u2016wL\u201632M (16)\n\u2202b2M \u2202\u2126 = 2x\u22a4i \u03a8 L\u22a4\u03a8L\u2126wL \u2016xi\u20162M\u2016wL\u20162M \u2212 xi\u2126\u22a4\u03a8L\u22a4\u03a8L\u2126wL\u00b7\n[\nxi\u03a8 L\u22a4\u03a8L\u2126xi\n\u2016xi\u201632M\u2016wL\u20162M +\nw L\u03a8L\u22a4\u03a8L\u2126wL\u2016\n\u2016xi\u20162M\u2016wL\u201632M\n]\n(17)\n\u2202b2M \u2202\u03a8L =\n2x\u22a4i \u2126 \u22a4\u03a8L\u2126wL \u2016xi\u20162M\u2016wL\u20162M \u2212 xi\u2126\u22a4\u03a8L\u22a4\u03a8L\u2126wL\u00b7 [\nxi\u2126 \u22a4\u03a8L\u2126xi\n\u2016xi\u201632M\u2016wL\u20162M +\nw L\u2126\u22a4\u03a8L\u2126wL\n\u2016xi\u20162M\u2016wL\u201632M\n]\n(18)\nThe 2 matrix Euclidean and angle implementation will be abbreviated by LVQ2M and ALVQ2M respectively."}, {"heading": "3 Datasets", "text": "We demonstrate our newly developed classifiers on two datasets: a synthetic 2-class dataset and a publicly available multi-class heart disease dataset as explained in the following subsections."}, {"heading": "3.1 Synthetic non-linear dataset (Football)", "text": "We used the open-source software system Chebfun [16] to create a synthetic 2 class dataset resembling the pattern of a football (see Fig.1). The function producing the pattern is f(x) = 2 sinh(5x1 \u00b7 x2 \u00b7 x3) with f(x) \u2264 0.5 belonging to class 0 and f(x) > 0.5 to class 1. We created 5000 samples for train-\ning and validation splits in cross-validation and additional 25000 samples serve as hold-out test set to investigate the generalization ability of the classifier. The data is available online\u2217. Performance on this dataset is reported in terms of training and test errors, as well as sensitivity and specificity."}, {"heading": "3.2 Heart disease dataset from UCI", "text": "This dataset, also known as the Cleveland heart disease (HD) dataset [17], contains 303 subjects in total (164 healthy, and 139 with varying degrees of heart problems). The predictor variable is originally 5 unique values, 0 indicating healthy (164), while 1 (55 subjects), 2 (36 subjects), 3 (35 subjects), and 4 (13 subjects) indicating patients with different heart conditions. Furthermore, six subjects contain missing values. The dataset originally consists of 76 features but most research has been done on a subset of 13 of these. For easy comparison we investigate the same 13 features and details about them can be found at the UCI repository [17].\n\u2217github.com/sreejita\u2212 rug/Synthetic Chebfun football.git\nExploratory analysis showed that while there is a very good separation between healthy and HD subjects considered in binary classification, the multi-class problem differentiating between the 4 classes of HD patients turns out to be remarkably difficult. Therefore, besides the more interesting multi-class problem, we added an investigation of the binary sub-problem to compare the performance to the majority of earlier results reported on this dataset. However, unlike most contributions we did not discard entries with missing values, since our method can be trained in variable dimensional spaces. According to [17] the missing values in the data were replaced by a value of -9. For the binary problem we report the performance keeping this, to compare to earlier results. In the multi-class analysis however we revert the -9s to NaNs.\nLiterature on the heart disease dataset investigating the binary problem, showed good performance by SVMs with nonlinear kernels, neural networks, k-nearest neighbour (kNN) using k = 16, 19, 28, Fischer Discriminant Analysis (FDA), Linear Discriminant Analysis (LDA), NB and ensemble classifiers such as RF [18\u201320]. Although these classifiers perform well in binary classification of this HD dataset, direct interpretation and visualization of the trained models remains difficult, with exception of the RF. Models with enhanced interpretability as proposed in this contribution can alternatively deliver additional insight. This is also demonstrated on the more interesting multi-class problem."}, {"heading": "4 Experiments", "text": "In this section we explain the experimental setup for the synthetic and heart disease dataset and the performance metrics used for comparison. Results are summarized in tables with abbreviations as introduced before: global feature relevances Euclidean and angle based (LVQg and ALVQg), local relevances (LVQl and ALVQl), Random Forest (RF), and the 2 matrix versions providing visualisations of the nonlinear decision boundaries (LVQ2M and ALVQ2M ) The superscripts denote the value of hyperparameters \u03b2 for ALVQ and the number of trees in the RF classifier."}, {"heading": "4.1 Synthethic data", "text": "We demonstrate the difference of the localized and 2 matrix Euclidean LVQ versions and our angle based extensions on the synthetic football pattern data set. Therefore, we performed a 10- fold cross validation for comparison and model selection with 5000 samples. The generalization ability of the selected model is evaluated on 25000 hold-out test samples and performance is reported in terms of training and test errors, as well as sensitivity and specificity. We use 3 prototypes per class (\u2118 = 6) and class-wise matrices on this dataset."}, {"heading": "4.2 Heart disease data", "text": "We compare the proposed angle LVQ variants with results from the literature [20, 21]. Contrary to past results our method can perform on the existing dimensions only. This avoids imputation and offers additional insights in the form of feature relevance determination and visualisation of the decision boundaries. This dataset was z-score transformed in each fold using the mean and standard deviation of the corresponding training set. Earlier results were typically acquired by 10-fold cross validation, since most of them simplify the problem to two classes, combining all diseases into one. However, we use 5-fold cross validation, since\nthe smallest minority class contained only 14 subjects justifying only a lower number of folds for the analysis of the multi-class problem. Albeit the multi-class problem being severely more difficult we show that the enhanced interpretability offers additional insight into the problem.\nTable 1 shows an overview of the experiments performed and intrinsic method hyperparameters. The imbalance of the classes is handled by the Synthetic Minority Oversampling TEchnique (SMOTE) as described in [22]. SMOTEg denotes a geodesic variant for oversampling on a hypersphere as introduced and explained in [12]. They were used to oversample all minority classes in the training set to contain the same number of samples as the majority class (Healthy). Based on exploratory analysis we chose k = 3 nearest neighbours for both SMOTE and SMOTEg. We investigated the intrinsic dimensionality by training full rank matrices \u2126 for which subsequent Eigen-value decomposition of the resulting metric tensor \u039b delivered insight into the required dimensions for classification. Afterwards we limit the rank for visualisation purpose. We experimented with varying number of prototypes per class (1, 2 and 3), such that \u2118 \u2208 {2, 4, 6} for the binary class problem, and \u2118 \u2208 {5, 10, 15} for 5-class problem, and investigated the influence of the hyperparameter \u03b2 with \u03b2 \u2208 {1, 5, 10, 50, 80, 100}. As proposed in [23] we set minimum observation(s) per tree leaf in RF to 1, and number of random variables at each decision split to \u221a D = \u221a 13 \u2248 4."}, {"heading": "5 Results and Discussion", "text": "This section contains the detailed comparison of results from experiments performed on both datasets, followed by discussion and visualizations as enabled by the proposed ALVQ2M . For the real-world heart disease we also showed a detailed investigation of interpretable parameters leading to further insight into the classification performed. RFs, which are also interpretable to some extent, makes it possible to extract feature importance. Therefore we were able to compare findings from the ALVQs and RFs."}, {"heading": "5.1 Synthetic football dataset results", "text": "Table 2 summarizes the performance of the classifiers in terms of error on training and test set during cross validation and report the sensitivity and specificity with respect to the hold-out test set. We included the results using different hyperparameters \u03b2 to provide information about the robustness and selected the model exhibiting best training performance as highlighted in boldface. As expected, earlier LVQ extensions perform worse on this non-Euclidean data set as depicted in the first 2 rows. The local relevance angle LVQ (ALVQl) clearly outperforms the other two being the most complex and flexible model with the largest number of parameters handling the nonlinearities of this data best. However, as mentioned before, visualization of the deci-\nsion boundaries with local metric tensors is not straightforward. Therefore we demonstrate the 2 matrix extension ALVQ2M with complexity and performance in between the global and local variants. Figure 2 shows a corresponding example visualization of the nonlinear decision boundaries and prototypes in the spherical classification space seen from 3 different perspectives. Individual data samples have been omitted in the illustration to reduce visual clutter, but can be added of course for investigation."}, {"heading": "5.2 Heart disease (binary class problem) results", "text": "First, we investigated the binary subproblem combining all diseases to one class and estimate the intrinsic dimensionality by investigating the eigenvalue profile of the trained \u039b = \u2126\u22a4\u2126 with full rank \u2126 \u2208 RD\u00d7D and one prototype per class. Since there is not enough data to create a hold-out generalization set we report the sensitivity and specificity of the classifiers as observed on the test set of the cross-validation splits. Figure 3 shows box plots of the estimates of the intrinsic dimensionality according to different settings of the hyperparameter \u03b2 and the average performance of corresponding models is summarized in Table 3. Even though there are 13 features in the dataset much lower dimensionality seems necessary for classification as indicated by most Eigenvalues being close to 0. The best performing \u03b2 depicts only three Eigenvalues significantly bigger than 0 indicating the problem can be visualized in three dimensions with limited loss of information. Thus we re-train the models by limiting the rank to three (M = 3).\nAs before we perform model selection and highlight in boldface based on the best training set performance and report sensitiv-\nity and specificity on the respective test splits. Reducing the rank of the matrix regularizes the model leading to improved generalization performance as depicted in Table 4. We also notice that the Euclidean versions of LVQ, i.e. GMLVQ, exhibits poor performance on this data set. This might be due to the presence of missing data, which the angle version is able to deal with. RF with 100 and more trees have had perfect training, but the sensitivity on the validation set is similar to that of angle LVQ. We also observe similar performance in comparison with results reported in [19] for the NB and Multi Layer Perceptron (MLP) marked as NBKol and MLPKol. They used 10-fold crossvalidation, but standard deviation across the different splits or training and test error were not reported.\nClassifiers of the LVQ family can also identify relevant features for a particular task, along with finding typical representatives of each class (prototypes). Figure 4 shows the feature relevances and prototypes of the healthy and disease class learned during training corresponding to the best setting (ALVQ1g) in Table 4. The features 3 (Chest pain type), 12 (number of major vessels as coloured by fluoroscopy) and 13 (status of heart, w.r.t the\norgan being normal having had the anomaly fixed, and having a reversible defect) are among the most highly relevant ones, followed by features 2 (sex), 8 (maximum heart rate achieved) and 9 (exercise induced angina). Important features extracted from RF models are shown in Fig. 5. RF and ALVQ feature sets agree with regard to features 3, 8, 9, 10, 12, and 13 being the more distinguishing ones, whereas features 4, 5 and 7 do not contribute as much. In contrast to RF we can also investigate the prototypes of the healthy and patient class. Notably, the features found also visibly differ in the adapted prototypes of the healthy and patient. We see in Figure 4, that feature 3 value lie below the 0.5 mark for class-Healthy whereas it is higher that than 0.5 mark for the HD prototype. Similarly for value of features 12, 13, 2, and 9. For features 8 and 10 the opposite trend is seen: the maximum heart rate achieved for the prototype describing the healthy subjects was much higher than the 0.5 mark whereas for the patients it was significantly lower than that mark. Conversely, features which were not deemed highly relevant by our classifier, such as features 1 (age), 4 (resting blood pressure), and 7 (resting ECG), are seen to have values in the mid-part of the prototype plots for both the classes, thus indicating that they are not as integral to distinguishing between healthy subjects and HD patients. These findings also agree with those mentioned in [20,21]."}, {"heading": "5.3 Heart disease (5-class problem) results", "text": "More challenging and potentially more interesting is the investigation of the 5 class problem keeping the original HD sub-classes. Since there are 5 classes we show the performance in terms of training and test errors, and class-wise accuracies. The classwise accuracy of the Healthy class (C0) is the same as specificity and therefore omitted in the following. Table 5 shows that the class-wise accuracies during validation are better from the more complex local model of angle LVQ (ALVQl), whose prototypes and local relevances are depicted in Figure 7. Additional interpretation can be gained by using the proposed 2 matrix variant ALVQ2M . For this problem we compared using 1, 2 and 3 prototypes per class but report only the results using 2 prototypes per class, since it depicted the best averaged class-wise accuracy on training. The study in [20] attempts to investigate the disease classes considering one class versus all classification. Their highest sensitivity per condition in this simplified setting were reported to be: 0.891 (Healthy, Sequential minimal optimization (SMO)), 0.321 (HD class 1, IBK from Weka), 0.405 (HD class 2, NB), 0.472 (HD class 3, NB) and 0.214 (HD class 4, IBK) furthermore confirming the complexity of the multi-class problem we investigate. Table 5 shows that the performance of RF and the ALVQ classifiers were comparable in sensitivity and specificity in the more complex 5-class setting. However the ALVQ models can provide additional insight by prototypes and visualizations.\nFigure 6 shows the decision boundaries of an example ALVQ2M using \u03b2 = 5 showing best performance according to Table 5. The picture confirms the non-linearity of this dataset when investigated as 5-class problem. Individual data samples are again omitted to avoid visual clutter but can be added and investigated with respect to their distance to the decision boundaries. The corresponding cross-validation relevances of \u2126\u22a4\u2126 and prototypes are depicted in Figure 8. Next we investigate the models trained for the multi-class problem in more detail to hypothesize why this problem is so difficult. Figure 7 illustrates\nTable 5: 5-class HD: mean performance (std) comparison of ALVQ variants and RF\nMethod Etrain Etest Sens Spec C1 C2 C3 C4 ALVQ100g 0.34 (0.032) 0.54 (0.081) 0.07 (0.043) 0.68 (0.130) 0.19 (0.111) 0.22 (0.209) 0.20 (0.180) 0.25 (0.260) ALVQ80g 0.33 (0.034) 0.52 (0.075) 0.08 (0.048) 0.69 (0.090) 0.21 (0.125) 0.23 (0.167) 0.26 (0.188) 0.29 (0.298) ALVQ150g 0.32 (0.043) 0.53 (0.061) 0.08 (0.053) 0.71 (0.090) 0.20 (0.134) 0.18 (0.148) 0.22 (0.143) 0.13 (0.204) ALVQ10g 0.35 (0.071) 0.48 (0.056) 0.08 (0.060) 0.76 (0.063) 0.21 (0.154) 0.21 (0.168) 0.26 (0.196) 0.23 (0.281) ALVQ5g 0.35 (0.071) 0.50 (0.074) 0.04 (0.045) 0.77 (0.097) 0.11 (0.112) 0.19 (0.153) 0.26 (0.210) 0.29 (0.313) ALVQ1g 0.37 (0.063) 0.49 (0.053) 0.05 (0.049) 0.79 (0.088) 0.12 (0.125) 0.19 (0.167) 0.25 (0.161) 0.22 (0.288) ALVQ100 l 0.24 (0.048) 0.51 (0.067) 0.08 (0.060) 0.69 (0.090) 0.20 (0.155) 0.31 (0.131) 0.34 (0.193) 0.13 (0.226) ALVQ80 l 0.21 (0.034) 0.49 (0.049) 0.09 (0.071) 0.73 (0.066) 0.22 (0.186) 0.31 (0.134) 0.28 (0.208) 0.15 (0.240) ALVQ50 l 0.18 (0.038) 0.49 (0.062) 0.09 (0.066) 0.72 (0.061) 0.22 (0.168) 0.26 (0.152) 0.33 (0.164) 0.17 (0.276) ALVQ10 l 0.16 (0.025) 0.48 (0.049) 0.08 (0.065) 0.76 (0.065) 0.20 (0.168) 0.28 (0.173) 0.31 (0.149) 0.05 (0.150) ALVQ5 l 0.16 (0.025) 0.49 (0.052) 0.07 (0.061) 0.74 (0.084) 0.17 (0.159) 0.30 (0.206) 0.34 (0.179) 0.05 (0.132) ALVQ1 l 0.17 (0.022) 0.50 (0.056) 0.07 (0.053) 0.74 (0.089) 0.18 (0.138) 0.23 (0.155) 0.31 (0.189) 0.08 (0.167) ALVQ1002M 0.38 (0.064) 0.53 (0.071) 0.09 (0.066) 0.68 (0.109) 0.23 (0.172) 0.22 (0.190) 0.22 (0.160) 0.27 (0.315) ALVQ802M 0.36 (0.058) 0.55 (0.074) 0.06 (0.053) 0.67 (0.121) 0.15 (0.136) 0.21 (0.149) 0.25 (0.212) 0.23 (0.308) ALVQ502M 0.35 (0.059) 0.51 (0.075) 0.07 (0.063) 0.70 (0.122) 0.18 (0.161) 0.21 (0.154) 0.35 (0.207) 0.21 (0.232) ALVQ12M 0.34 (0.050) 0.51 (0.063) 0.07 (0.046) 0.72 (0.098) 0.17 (0.117) 0.24 (0.177) 0.26 (0.160) 0.24 (0.268) ALVQ52M 0.31 (0.033) 0.49 (0.073) 0.06 (0.052) 0.76 (0.108) 0.16 (0.133) 0.26 (0.168) 0.31 (0.198) 0.17 (0.252) ALVQ12M 0.32 (0.043) 0.49 (0.072) 0.06 (0.050) 0.75 (0.089) 0.15 (0.129) 0.26 (0.180) 0.30 (0.216) 0.30 (0.337) RF 50 0.0 (0.002) 0.46 (0.039) 0.06 (0.044) 0.85 (0.054) 0.15 (0.112) 0.31 (0.110) 0.15 (0.095) 0.06 (0.134) RF 100 0.0 (0.0) 0.46 (0.030) 0.03 (0.028) 0.88 (0.060) 0.07 (0.069) 0.30 (0.164) 0.09 (0.071) 0.0 (0.0) RF 150 0.0 (0.0) 0.44 (0.022) 0.05 (0.036) 0.88 (0.049) 0.13 (0.095) 0.28 (0.120) 0.23 (0.117) 0.0 (0.0) RF 200 0.0 (0.0) 0.45 (0.020) 0.04 (0.039) 0.87 (0.049) 0.09 (0.102) 0.33 (0.144) 0.20 (0.117) 0.10 (0.204)\nALV Ql classifier with \u03b2 = 5 and \u03a8c of dimension 3 \u00d7 13, the hyperparameter setting which showed best performance among angle local LVQ according to Table 5. We compare the feature relevance from \u03a8c (Fig. 7) with those from figures 4 and 5. Features 3, 12, and 13 were among the most relevant features for the binary class problem. However, for the multi-class problem, on checking the prototype of each class, we see that these features do not have a distinct value boundary which could help in identification of the different classes. If we consider feature 12 (Ca) for all the prototypes we can see how easily healthy subjects and patients from class sick-1 would be confused, similarly patients of sick class 2 would be easily confused with those from sick class 3. According to Fig. 8 features 12 (Ca) and 13 (Thal) are still the most relevant ones. However the prototypes show that these features are good for distinguishing between healthy\nand the rest of the classes, but not that efficient for differentiating between the heart disease classes themselves. These plots further explain why the specificity (or the class-wise accuracy of Healthy class) remained high even for the multi-class problem, whereas the class-wise accuracies were comparably poor for the remaining."}, {"heading": "6 Conclusion and future Work", "text": "In this contribution we proposed three interpretable extensions of the angular nearest prototype based classifier, namely global angle LVQ, local angle LVQ and a 2 matrix version allowing visualisation of the non-linear decision boundaries. These set of classifiers are able to handle missingness as well as make knowledge extraction straightforward. As increasing number of human-centric sectors are becoming dependent on machine learning, understanding the exact working and underlying mechanisms behind a decision made by a model, are becoming paramount. Some classifiers depict comparable (and some even slightly higher) performance than these newly introduced classifiers. However, the proposed classifiers captivate due to their interpretability and the possibility to shed light on what exactly makes a classification problem difficult. This is highlighted in the given analysis of the 5 class heart disease identification problem where we achieve comparable performance to the RF. Even though the 13 out of 76 features were capable of distinguishing between healthy and heart disease patients, but features which can differentiate between all these 5 classes satisfactory seem not to be among these 14 features. Future contributions should investigate the larger feature set and the insight we can gain from it using interpretable classifiers."}, {"heading": "Acknowledgment", "text": "We thank the Center for Information Technology of the University of Groningen for their support and for providing access to the Peregrine high performance computing cluster. We also thank the Rosalind Franklin fellowship, co-funded by the European Union\u2019s Seventh Framework Program for research, technological development and demonstration under grant agreement no 600211, and H2020-MSCAIF-2014, project ID 659104."}], "year": 2020}