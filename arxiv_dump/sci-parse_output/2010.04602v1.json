{"abstractText": "Explainable artificial intelligence (XAI) can help foster trust in and acceptance of intelligent and autonomous systems. Moreover, understanding the motivation for an agent\u2019s behavior results in better and more successful collaborations between robots and humans. However, not only can humans benefit from a robot\u2019s explanation but the robot itself can also benefit from explanations given to him. Currently, most attention is paid to explaining deep neural networks and black-box models. However, a lot of these approaches are not applicable to humanoid robots. Therefore, in this position paper, current problems with adapting XAI methods to explainable neurorobotics are described. Furthermore, NICO, an open-source humanoid robot platform, is introduced and how the interaction of intrinsic explanations by the robot itself and extrinsic explanations provided by the environment enable efficient", "authors": [{"affiliations": [], "name": "Tom Weber"}, {"affiliations": [], "name": "Stefan Wermter"}], "id": "SP:d609a18b9945445a2073f1c0cdfe7234b62ed53d", "references": [{"authors": ["A. Abdul", "J. Vermeulen", "D. Wang", "Lim"], "title": "B", "venue": "Y.; and Kankanhalli, M.", "year": 2018}, {"authors": ["Anjomshoae"], "title": "Explainable Agents and Robots: Results from a Systematic Literature Review", "venue": "In AAMAS \u201919: Proceedings of the 18th International Conference on Autonomous Agents and Multi-Agent Systems,", "year": 2019}, {"authors": ["Barredo Arrieta"], "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities", "year": 2020}, {"authors": ["Besold"], "title": "T", "venue": "R.; d\u2019Avila Garcez, A.; Bader, S.; Bowman, H.; Domingos, P.; Hitzler, P.; Kuehnberger, K.-U.; Lamb, L. C.; Lowd, D.; Lima, P. M. V.; de Penning, L.; Pinkas, G.; Poon, H.; and Zaverucha, G.", "year": 2017}, {"authors": ["Ciatto, G.", "Schumacher"], "title": "M", "venue": "I.; Omicini, A.; and Calvaresi, D.", "year": 2020}, {"authors": ["A. Das"], "title": "and Rad", "venue": "P.", "year": 2020}, {"authors": ["R.C. Fong"], "title": "and Vedaldi", "venue": "A.", "year": 2017}, {"authors": ["R. Geirhos", "J.-H. Jacobsen", "C. Michaelis", "R. Zemel", "W. Brendel", "M. Bethge", "Wichmann"], "title": "F", "venue": "A.", "year": 2020}, {"authors": ["A. Ghorbani", "J. Wexler", "Zou"], "title": "J", "venue": "Y.; and Kim, B.", "year": 2019}, {"authors": ["Kerzel"], "title": "NICO \u2013 Neuro-inspired companion: A developmental humanoid robot platform for multimodal interaction", "year": 2017}, {"authors": ["Kerzel"], "title": "Neuro-Robotic Haptic Object Classification by Active Exploration on a Novel Dataset", "venue": "International Joint Conference on Neural Networks (IJCNN),", "year": 2019}, {"authors": ["B. Kim", "M. Wattenberg", "J. Gilmer", "C.J. Cai", "J. Wexler", "Vi\u00e9gas"], "title": "F", "venue": "B.; and Sayres, R.", "year": 2018}, {"authors": ["G. Klien", "D.D. Woods", "J.M. Bradshaw", "R.R. Hoffman", "Feltovich"], "title": "P", "venue": "J.", "year": 2004}, {"authors": ["Lapuschkin"], "title": "Unmasking Clever Hans Predictors and Assessing What Machines", "venue": "Really Learn. Nature Communications", "year": 2019}, {"authors": ["Montavon"], "title": "Layer-Wise Relevance Propagation: An Overview", "venue": "Explainable AI: Interpreting,", "year": 2019}, {"authors": ["Samek Montavon", "G. M\u00fcller 2018] Montavon", "W. Samek", "K.-R. M\u00fcller"], "title": "Methods for Interpreting and Understanding Deep Neural Networks", "venue": "Digital Signal Processing", "year": 2018}, {"authors": ["Neerincx"], "title": "M", "venue": "A.; van der Waa, J.; Kaptein, F.; and van Diggelen, J.", "year": 2018}, {"authors": ["A.K. Pandey"], "title": "and Gelin", "venue": "R.", "year": 2018}, {"authors": ["G.I. Parisi", "R. Kemker", "Part"], "title": "J", "venue": "L.; Kanan, C.; and Wermter, S.", "year": 2019}, {"authors": ["Ribeiro"], "title": "M", "venue": "T.; Singh, S.; and Guestrin, C.", "year": 2016}, {"authors": ["Ribeiro"], "title": "M", "venue": "T.; Singh, S.; and Guestrin, C.", "year": 2018}, {"authors": ["W. Samek"], "title": "and M\u00fcller", "venue": "K.-R.", "year": 2019}, {"authors": ["Selvaraju"], "title": "R", "venue": "R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; and Batra, D.", "year": 2017}, {"authors": ["M. Shvo", "T.Q. Klassen", "McIlraith"], "title": "S", "venue": "A.", "year": 2020}, {"authors": ["Szegedy"], "title": "Going Deeper with Convolutions", "year": 2015}, {"authors": ["G.G. Towell", "Shavlik"], "title": "J", "venue": "W.", "year": 1994}, {"authors": ["Vinanzi"], "title": "Would a Robot Trust You? Developmental Robotics Model of Trust and Theory of Mind", "venue": "Philosophical Transactions of The Royal Society B Biological Sciences", "year": 2019}, {"authors": ["Griffiths Wermter", "S. Heinrich 2017] Wermter", "S. Griffiths", "S. Heinrich"], "title": "Crossmodal Lifelong Learning in Hybrid Neural Embodied Architectures. Behavioral and Brain Sciences 40:e280", "year": 2017}, {"authors": ["Q. Zhang", "R. Cao", "F. Shi", "Wu"], "title": "Y", "venue": "N.; and Zhu, S.-C.", "year": 2018}], "sections": [{"heading": "Introduction", "text": "Neuro-inspired robots make use of connectionist models to tackle problems that are not easily solvable with symbolic methods and the success of complex deep neural networks (DNN) over the past decade made it possible for agents to acquire robust behavior through learning. Such flexible adaptation to problems, for example learning to distinguish objects just by touching them (Kerzel et al. 2019), is still impossible to achieve with symbolic representations. However, this advance in performance thanks to DNNs has disadvantages regarding explainable artificial intelligence since deep learning models can essentially be considered as black-box models to the human interpreter. The sub-symbolic nature of their distributed representations is not inherently explainable, i.e. incomprehensible to a human observer and needs post hoc explanations. In contrast, symbolic approaches like first-order logic or planners are inherently explainable but largely unable to deal with certain problems that are crucial for effective robotic intelligence like the efficient perception and robust learning of complex environments.\nExplanations that can be produced from the inner states and algorithms of an agent, i.e. that can be produced from\nCopyright \u00a9 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nwithin the agent, we describe as intrinsic explanations. For instance, explaining a classification decision which is done by a perceptive module, with a description of its relevant features. Almost all existing literature on XAI falls within this category. An autonomous robot, however, is situated in an environment. Through interaction with it, the robot can obtain information about prior uncertainties or knowledge gaps. Therefore, explanations that the robot receives from an external source in its environment are extrinsic explanations.\nCurrently, explainable human-robot interaction (HRI) faces two major obstacles. Neural networks constitute an irreplaceable part of modern robotics. However, the development of successful explanation methods of deep architectures is still in its infancy, and the intrinsic explanations that an agent can deliver are respectively limited, especially regarding the constraints of HRI contexts. The generation of an explanation is not sufficient if the communication to the receiver fails due to unintelligible information representation or a lack of information. The low-level explanations though feature visualization provided by a lot of XAI methods rarely enable the end-user to interpret the agent\u2019s behavior unambiguously. Therefore, we will review a few promising techniques and their limitations for explainable robots. Additionally, the communication between robots and humans narrows the choice of modalities that can be used to explain which renders numerous explainability methods unusable without modification.\nNeural networks offer robots the possibility to adapt and learn from extrinsic explanations that they receive from the environment, in an interactive way. The robot can learn how to shape its intrinsic explanations to make them more suitable for its audience. XAI methods for HRI should be extended to an interactive framework between intrinsic and extrinsic explanations to fully account for the nature of such situations."}, {"heading": "XAI for Neurorobotics", "text": "Methods for explaining data-driven models are largely treated separately from methods for explaining goal-driven agents (for reviews of the former, cf. (Barredo Arrieta et al. 2020), for the latter, cf. (Anjomshoae et al. 2019)).\nState-of-the-art neuro-cognitive humanoid robots, like the\nar X\niv :2\n01 0.\n04 60\n2v 1\n[ cs\n.R O\n] 9\nO ct\n2 02\nNICO (see figure 1), are best realized in a multimodal neurosymbolic hybrid fashion (Wermter, Griffiths, and Heinrich 2017). Symbolic processes represent high-level cognitive concepts like beliefs, goals, and intentions, which are grounded in and make use of neural architectures for complex cognitive processes like perception. Still, despite this interconnection, the explanation methods are conceptionally kept apart from each other, instead of being integrated. The PeCoX framework separates the explanation of hybrid agents into two submodules, the cognitive XAI for symbolic and the perceptive XAI for sub-symbolic processes (Neerincx et al. 2018). Both are detached from one another, as they need different techniques. The explanation of the cognitive parts is straightforward. Due to their symbolic nature, they are inherently interpretable so that goals, beliefs, and intentions can often simply be externalized by e.g. verbal communication. On the other hand, the post hoc explanation of conventional DNNs is much less straightforward, with a diverse research landscape.\nThe majority of explanation techniques for deep neural networks try to find and visualize relevant features in the input space that are responsible for classification (Montavon, Samek, and Mu\u0308ller 2018; Samek and Mu\u0308ller 2019; Lapuschkin et al. 2019). The approaches can be divided into black-box methods that do not assume specific underlying structures and are only concerned with input-output pairings, e.g. local interpretable model-agnostic explanations (LIME) or meaningful perturbations (Ribeiro, Singh, and Guestrin 2016; Fong and Vedaldi 2017), and methods that take into account the distributed hierarchical architecture and propagation mechanism of neural networks, like GradCAM or layerwise relevance propagation (LRP) (Selvaraju et al. 2017; Montavon et al. 2019). Grad-CAM, for example, analyses the gradients in the classification layer of convolutional neural networks and uses the gradient flow back into preceding layers to construct a heatmap of important regions in the input space. By overlaying the heatmap with an input image for a visual classification, relevant features of the image are highlighted. These input features can be considered domain-specific, low-level knowledge, and not necessarily interpretable because they can require substantial top-down knowledge and processing in order to be interpretable for the user. The amount of knowledge needed varies between modalities. Whereas words and images might be intuitively interpretable even for a lay user, other feature spaces are less intelligible to human thinking. A heatmap of specific frequencies in a spectrogram is beyond anyone\u2019s understanding, except perhaps, for experts in the respective field.\nMost works applying XAI techniques pertain to the visual field and image classifiers. Communicating visual explanations that highlight important features in the input image with the means of a humanoid robot is problematic for various reasons. The feature space of an input image is usually quite large and highly correlated. A single highlighted pixel value does not mean very much for a human interpreter whereas a neighborhood of pixels or several superpixels contains ambiguities. However, even if a subset of correlated pixels might be representing a concept, it must be identified by the explanation method as such.\nExplanations in an HRI context should, therefore, take into account that human minds do not operate in feature space. However, few works try to abstract from pure feature visualization to higher-level concepts which are more akin to human knowledge processing like concept-based reasoning (Kim et al. 2018). However, even methods that can generate meaningful visual concepts for explanations stay on the sub-symbolic level and do not describe the concepts semantically (Ghorbani et al. 2019). There is still a significant gap in the research literature of explaining neural networks with higher-level knowledge and abstracting from lowerlevel feature visualizations, especially outside the visual domain (Das and Rad 2020). Concept-level explanations are easier to transfer into semantic representations which would help robots communicate them.\nHowever, it is not always straightforward to understand the explanation of a classification, even if it shows somewhat interpretable input features or even concepts. Anchors, a promising explanation method, tries to find conjunctive rules that locally explain a classification (Ribeiro, Singh, and Guestrin 2018). In cases where the rules are chaining together inherently interpretable chunks like words, simple rules can explain decisions with a high degree of precision. Their limitation lies in not being able to capture the (nonlinear) interaction of the features which might lead to false interpretations on the user\u2019s side. Imagine a situation where the user is presented with a seemingly linear explanation of a decision that is actually non-linear. A further problem is that local explanations do not account for the global behavior of a model. An agent explaining a decision locally cannot give a guarantee of making a similar decision in a similar situation. This is problematic for HRI contexts, where the future prediction of robot behavior through explanations plays a big role in building a successful relationship between humans and robots (Klien et al. 2004). Generalization from this rule without further information might prove catastrophic in predicting future agent behavior.\nHybrid agents can be regarded as being goal-driven and data-driven at the same time. While trying to achieve a certain objective, the perceptions they form about their environment shape the way they obtain objectives and ultimately, the realized behavior. By explaining both of these aspects separately, the task to make the connection and infer their interaction is left to the user which leaves a lot of room to ambiguities.\nThe transfer of concepts and explanations from the subsymbolic to the symbolic level is crucial when relaying information from robots to humans. After all, the explanations must be communicated within the means of the agent. Considering humanoid robots, except for special models like the Pepper (Pandey and Gelin 2018), they are confined to the channels of communication that humans are familiar with, i.e. speech and gestures. But relaying the importance of pixel values or regions in an image through speech alone is not very feasible. Explanations need therefore be explicitly catered to or transformed into a modality that is communicable by the robot."}, {"heading": "Getting NICO to explain", "text": "NICO, the Neuro-Inspired COmpanion, is a humanoid robot designed for human-robot interaction (Kerzel et al. 2017). The NICO, as seen in figure 1, can capture data about its environment from two cameras as eyes for stereoscopic visual input, from two microphones as ears for auditive input, and from haptic sensors on his hands and fingers for tactile input. In order to communicate with humans, there are several LEDs in its head to simulate mouth and eyebrows to show facial expressions. Further, NICO possesses speakers to transmit sounds and speech. Lastly, two degrees of freedom (DoF) for yaw and pitch head movements and at least eight DoF per arm, depending on the exact hand model, allow to convey information via complex gestures. The platform that NICO is built upon mainly consists of dynamixel motors and open-sourced 3d printed parts. The robot can be flexibly adapted to specific needs depending on the experimental circumstances (for a more detailed description see (Kerzel et al. 2017)).\nIntrinsic explanations made by NICO would benefit the positive relations and successful teamwork with other collaborators. Understanding its own mechanisms can also help to gain useful insight and information about the environment, that was not explicitly learned. Suppose that NICO is given a command by a human collaborator and has the task to grasp one out of a few identifiable objects in front of it, but is not equipped with an explicit object localizer (for a visualization of the scene, see figure 1). An image classifier, like the Inception Network (Szegedy et al. 2015), only lets NICO identify the objects but not necessarily their location. Figure 1 shows such a scenario. Once a specific object is identified, NICO can utilize a feature visualization technique like Grad-CAM to highlight the relevant parts of its\nvisual input that are responsible for the classification. Figure 2 shows the input image that NICO receives from one of its visual sensors on the left side and the application of the Grad-CAM algorithm for the classification of \u201dapple\u201d on the right side. By exposing those regions of the input image that contain the object in question, the explanation method GradCAM offers the possibility of weakly-supervised location. The heatmap can be converted into a binary mask by setting a threshold and, subsequently, a bounding box can be drawn around the mask (Selvaraju et al. 2017). This way, NICO can learn to grasp objects without ever being exposed to localization training. Thus, intrinsic explanations can make an agent more versatile by exploiting the mechanisms of neural networks that were not explicitly trained.\nExplanations should be explored in an interactive context with the goal of mutual understanding. The roles of explanator and explanee need not be fixed with robots being the former and humans being the latter (Ciatto et al. 2020). Successful collaboration requires the robot to know about the human and vice versa. Representing each others\u2019 beliefs and cognitions in a Theory of Mind is necessary to predict the other\u2019s behavior, provide meaningful explanations, and, ultimately, trust each other (Shvo, Klassen, and McIlraith 2020; Vinanzi et al. 2019). Here, neural networks play a crucial role in enabling an agent with the ability for continual learning (Parisi et al. 2019). However, the use of deep learning models can result in learning unintended shortcuts that do not generalize well (Geirhos et al. 2020). Intrinsic explanations made by NICO can help expose those shortcuts. Without the help of extrinsic explanations that describe bettersuited features or a better classification process, those shortcomings cannot be overcome.\nAs laid out in the PeCoX framework, the mere generation of an explanation does not constitute the complete process of explaining. In order to effectively convey explanations, it is also necessary to communicate them properly, so that they can be successfully interpreted by the recipient (Neerincx et al. 2018). This holds especially true in HRI contexts where communication and interaction with a robot is the central idea. Due to being designed as a humanoid companion, NICO underlies certain constraints in perceiving the environment. In fact, all the modalities that are accessible for sensing the environment, i.e. visual, auditive, and tac-\ntile, as well as all the modalities for communicating, i.e. gestures, facial expressions, and speech, are shared with humans. Hence, this constraint ensures that the modalities of the explanations made by NICO are inherently familiar and more interpretable to human information processing.\nThe most suitable medium for exchanging explanations in HRI contexts are rules and relations. Not only can they be expressed verbally and carry clearly defined semantics, but they are also interpretable by humans and robots alike. Hybrid neural models like knowledge-based neural networks that combine symbolic knowledge with the learning capacity of neural networks (Towell and Shavlik 1994; Besold et al. 2017) are a promising method for interactive learning through mutual explanations. Robots can utilize the rules given by an explanator and insert their symbolic knowledge into their neural models to learn better behaviors by extrinsic explanations.\nIn order to illustrate the necessity of neuro-symbolic hybrid approaches for explainable human-robot interaction, once again picture the grasping example with NICO. One can illustrate the effectiveness of explanation methods by analyzing a failed interaction between NICO and a collaborator, for instance after NICO has grasped the wrong object specified by the human partner in the given command. Two main origins of failure can be indicated. Either, NICO did not understand the verbal command correctly or it did not classify the correct object successfully. Whereas feature visualisation techniques like LRP allow NICO to identify the most salient words in the given command and to communicate them to the human partner, thus explaining what it understood to be important. However, the same cannot be said about the visual object classification component. After all, feature visualization as an explanation is only as intelligible as the features themselves. Semantic features like words are much easier to understand than the values of individual pixels. Also, it is unclear how a robot can effectively communicate such heatmaps to the interaction partner. Therefore, such feature importance methods are not suitable for modalities with features that are by themselves not meaningful for a human. This illustrates a common problem that given explanations from popular XAI algorithms do no translate well into human-machine interaction contexts and need adaptations and extensions based on symbolic approaches (Abdul et al. 2018). For example, a knowledge-extraction approach that constructs a knowledge graph out of a deep neural network classification and extracts meaningful relations between features could be applied to NICO\u2019s visual input to explain the (mis)classification (Zhang et al. 2018). Such relations are symbolically represented and can thus be communicated to the collaborator verbally."}, {"heading": "Conclusion", "text": "Proficiently using neural networks and explaining their behavior necessitates that they are explainable in an intelligible manner by verbal communication. Only a few methods, so far, enable the representation of an explanation in concepts (Ribeiro, Singh, and Guestrin 2018; Kim et al. 2018; Ghorbani et al. 2019). Further research about how to extract rules and semantic knowledge from neural networks\nwill prove worthwhile for explainable neurorobotics. By communicating the intrinsic explanations about their behavior, robots will be enabled to interact with collaborators to reach mutual understanding and expose flaws in reasoning and decision processes. Interaction partners can then provide extrinsic explanations to correct their mistakes. Through inserting the corrected knowledge back into the neural modules, robots can achieve the possibility to learn from interactions in a meaningful manner."}, {"heading": "Acknowledgements", "text": "The authors gratefully acknowledge partial support from the German Research Foundation DFG under project CML (TRR 169)."}], "title": "Integrating Intrinsic and Extrinsic Explainability: The Relevance of Understanding Neural Networks for Human-Robot Interaction", "year": 2020}