{
  "abstractText": "Understanding black-box machine learning models is important towards their widespread adoption. However, developing globally interpretable models that explain the behavior of the entire model is challenging. An alternative approach is to explain black-box models through explaining individual prediction using a locally interpretable model. In this paper, we propose a novel method for locally interpretable modeling \u2013 Reinforcement Learning-based Locally Interpretable Modeling (RL-LIM). RL-LIM employs reinforcement learning to select a small number of samples and distill the black-box model prediction into a low-capacity locally interpretable model. Training is guided with a reward that is obtained directly by measuring agreement of the predictions from the locally interpretable model with the black-box model. RL-LIM near-matches the overall prediction performance of black-box models while yielding human-like interpretability, and significantly outperforms state of the art locally interpretable models in terms of overall prediction performance and fidelity.",
  "authors": [
    {
      "affiliations": [],
      "name": "Jinsung Yoon"
    },
    {
      "affiliations": [],
      "name": "Sercan \u00d6. Ar\u0131k"
    }
  ],
  "id": "SP:d847206f4159d93b8e52f77cc12f119bbb3395cf",
  "references": [
    {
      "authors": [
        "David Alvarez-Melis",
        "Tommi S Jaakkola"
      ],
      "title": "On the robustness of interpretability methods",
      "venue": "arXiv preprint arXiv:1806.08049,",
      "year": 2018
    },
    {
      "authors": [
        "Yoshua Bengio",
        "J\u00e9r\u00f4me Louradour",
        "Ronan Collobert",
        "Jason Weston"
      ],
      "title": "Curriculum learning",
      "venue": "In International Conference on Machine Learning,",
      "year": 2009
    },
    {
      "authors": [
        "Adam Bloniarz",
        "Ameet Talwalkar",
        "Bin Yu",
        "Christopher Wu"
      ],
      "title": "Supervised neighborhoods for distributed nonparametric regression",
      "venue": "In Artificial Intelligence and Statistics,",
      "year": 2016
    },
    {
      "authors": [
        "Tianqi Chen",
        "Carlos Guestrin"
      ],
      "title": "Xgboost: A scalable tree boosting system",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2016
    },
    {
      "authors": [
        "L.H. Gilpin",
        "D. Bau",
        "B.Z. Yuan",
        "A. Bajwa",
        "M. Specter",
        "L. Kagal"
      ],
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "venue": "IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA),",
      "year": 2018
    },
    {
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "title": "Deep residual learning for image recognition",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "year": 2016
    },
    {
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "title": "Distilling the knowledge in a neural network",
      "venue": "arXiv preprint arXiv:1503.02531,",
      "year": 2015
    },
    {
      "authors": [
        "Eric Jang",
        "Shixiang Gu",
        "Ben Poole"
      ],
      "title": "Categorical reparameterization with gumbel-softmax",
      "venue": "In International Conference on Learning Representations,",
      "year": 2016
    },
    {
      "authors": [
        "Lu Jiang",
        "Zhengyuan Zhou",
        "Thomas Leung",
        "Li-Jia Li",
        "Li Fei-Fei"
      ],
      "title": "Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels",
      "venue": "In International Conference on Machine Learning,",
      "year": 2018
    },
    {
      "authors": [
        "Ulf Johansson",
        "Cecilia S\u00f6nstr\u00f6d",
        "Ulf Norinder",
        "Henrik Bostr\u00f6m"
      ],
      "title": "Trade-off between accuracy and interpretability for predictive in silico modeling",
      "venue": "Future medicinal chemistry,",
      "year": 2011
    },
    {
      "authors": [
        "Guolin Ke",
        "Qi Meng",
        "Thomas Finley",
        "Taifeng Wang",
        "Wei Chen",
        "Weidong Ma",
        "Qiwei Ye",
        "TieYan Liu"
      ],
      "title": "Lightgbm: A highly efficient gradient boosting decision tree",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "Pang Wei Koh",
        "Percy Liang"
      ],
      "title": "Understanding black-box predictions via influence functions",
      "venue": "In International Conference on Machine Learning,",
      "year": 2017
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Ece Kamar",
        "Rich Caruana",
        "Jure Leskovec"
      ],
      "title": "Interpretable & explorable approximations of black box models",
      "venue": "arXiv preprint arXiv:1707.01154,",
      "year": 2017
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Ece Kamar",
        "Rich Caruana",
        "Jure Leskovec"
      ],
      "title": "Faithful and customizable explanations of black box models",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society,",
      "year": 2019
    },
    {
      "authors": [
        "David R Legates",
        "Gregory J McCabe"
      ],
      "title": "Evaluating the use of goodness-of-fit measures in hydrologic and hydroclimatic model validation",
      "venue": "Water Resources Research,",
      "year": 1999
    },
    {
      "authors": [
        "Zachary C Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "arXiv preprint arXiv:1606.03490,",
      "year": 2016
    },
    {
      "authors": [
        "Scott M Lundberg",
        "Su-In Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "Gregory Plumb",
        "Denali Molitor",
        "Ameet S Talwalkar"
      ],
      "title": "Model agnostic supervised local explanations",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Gregory Plumb",
        "Maruan Al-Shedivat",
        "Eric Xing",
        "Ameet Talwalkar"
      ],
      "title": "Regularizing black-box models for improved interpretability",
      "venue": "arXiv preprint arXiv:1902.06787,",
      "year": 2019
    },
    {
      "authors": [
        "Colin Raffel",
        "Minh-Thang Luong",
        "Peter J Liu",
        "Ron J Weiss",
        "Douglas Eck"
      ],
      "title": "Online and linear-time attention by enforcing monotonic alignments",
      "venue": "In International Conference on Machine Learning,",
      "year": 2017
    },
    {
      "authors": [
        "Marc\u2019Aurelio Ranzato",
        "Sumit Chopra",
        "Michael Auli",
        "Wojciech Zaremba"
      ],
      "title": "Sequence level training with recurrent neural networks",
      "venue": "arXiv preprint arXiv:1511.06732,",
      "year": 2015
    },
    {
      "authors": [
        "Mengye Ren",
        "Wenyuan Zeng",
        "Bin Yang",
        "Raquel Urtasun"
      ],
      "title": "Learning to reweight examples for robust deep learning",
      "venue": "In International Conference on Machine Learning,",
      "year": 2018
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2016
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Anchors: High-precision model-agnostic explanations",
      "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,",
      "year": 2018
    },
    {
      "authors": [
        "Cynthia Rudin"
      ],
      "title": "Please Stop Explaining Black Box Models for High Stakes Decisions",
      "year": 2018
    },
    {
      "authors": [
        "Ramprasaath R Selvaraju",
        "Michael Cogswell",
        "Abhishek Das",
        "Ramakrishna Vedantam",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
      "year": 2017
    },
    {
      "authors": [
        "Avanti Shrikumar",
        "Peyton Greenside",
        "Anshul Kundaje"
      ],
      "title": "Learning important features through propagating activation differences",
      "venue": "In International Conference on Machine Learning-Volume,",
      "year": 2017
    },
    {
      "authors": [
        "Erik \u0160trumbelj",
        "Igor Kononenko"
      ],
      "title": "Explaining prediction models and individual predictions with feature contributions",
      "venue": "Knowledge and Information Systems,",
      "year": 2014
    },
    {
      "authors": [
        "Pang-Ning Tan"
      ],
      "title": "Introduction to Data Mining",
      "venue": "Pearson Education India,",
      "year": 2018
    },
    {
      "authors": [
        "Mikl\u00f3s Vir\u00e1g",
        "Tam\u00e1s Nyitrai"
      ],
      "title": "Is there a trade-off between the predictive power and the interpretability of bankruptcy models? the case of the first hungarian bankruptcy prediction model",
      "venue": "Acta Oeconomica,",
      "year": 2014
    },
    {
      "authors": [
        "Tongzhou Wang",
        "Jun-Yan Zhu",
        "Antonio Torralba",
        "Alexei A. Efros"
      ],
      "title": "Dataset distillation, 2019",
      "venue": "URL https://openreview.net/forum?id=Sy4lojC9tm",
      "year": 2019
    },
    {
      "authors": [
        "Ronald J Williams"
      ],
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "venue": "Machine Learning,",
      "year": 1992
    },
    {
      "authors": [
        "Wojciech Zaremba",
        "Ilya Sutskever"
      ],
      "title": "Reinforcement learning neural turing machines-revised",
      "venue": "arXiv preprint arXiv:1505.00521,",
      "year": 2015
    },
    {
      "authors": [
        "Xingxing Zhang",
        "Mirella Lapata"
      ],
      "title": "Sentence simplification with deep reinforcement learning",
      "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "year": 2017
    },
    {
      "authors": [
        "Yujia Zhang",
        "Kuangyan Song",
        "Yiming Sun",
        "Sarah Tan",
        "Madeleine"
      ],
      "title": "Udell. why should you trust my explanation? understanding uncertainty in lime explanations",
      "year": 1904
    },
    {
      "authors": [
        "Bolei Zhou",
        "Aditya Khosla",
        "Agata Lapedriza",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "title": "Learning deep features for discriminative localization",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "year": 2016
    }
  ],
  "sections": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Artificial Intelligence (AI) is advancing at a rapid pace, particularly with recent advances in deep neural networks and ensemble methods (Goodfellow et al., 2016; He et al., 2016; Chen & Guestrin, 2016; Ke et al., 2017). This progress has been fueled by \u2018black-box\u2019 machine learning models where the decision making is controlled by complex non-linear interactions between many parameters that are difficult for humans to understand and interpret. However, in many real-world applications AI systems are not only expected to perform well but are also required to be interpretable: doctors need to understand why a particular treatment is recommended, and financial institutions need to understand why a loan was declined. Use cases of model interpretability vary across applications: it can provide trust to users by showing rationales behind decisions, enable detection of systematic failure cases, and provide actionable feedback for improving models (Rudin, 2018).\nMany studies have suggested a trade-off between performance and interpretability (Vira\u0301g & Nyitrai, 2014; Johansson et al., 2011). This is correct in that globally interpretable models, which attempt to explain the entire model behavior, typically yield considerably worse performance than \u2018blackbox\u2019 models (Lipton, 2016). To go beyond the performance limitations of globally interpretable models, another promising direction is locally interpretable models, which instead of explaining the entire model explain a single prediction (Ribeiro et al., 2016). Methodologically, while a globally interpretable model fits a single inherently interpretable model (such as a linear model or a shallow decision tree) to the entire training set, locally interpretable models aim to fit an inherently interpretable model locally, i.e. for each instance individually, by distilling knowledge from a high performance black-box model. Such locally interpretable models are very useful for real-world AI deployments to provide succinct and human-like explanations to users. They can be used to identify systematic failure cases (e.g. by seeking common trends in input dependence for failure cases), detect biases (e.g. by quantifying feature importance for a particular variable), and provide actionable feedback to improve a model (e.g. understand failure cases and what training data to collect).\n*Work done as an intern at Google Cloud.\nar X\niv :1\n90 9.\n12 36\n7v 1\n[ cs\n.L G\n] 2\n6 Se\np 20\n19\nTo be useful in practice, locally interpretable models need to maximize two objectives: (i) the overall prediction performance (how well it predicts compared to the ground truth labels) \u2013 for the model to be accurate, and (ii) fidelity (how well it approximates the \u2018black-box\u2019 model predictions) \u2013 to ensure the model is reliably approximating the black-box model\u2019s predictions in the neighborhood of interest (Plumb et al., 2019; Lakkaraju et al., 2019). To this end, a few methods have recently been proposed for locally interpretable modeling: Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al., 2016), Supervised Local modeling methods (SILO) (Bloniarz et al., 2016), and Model Agnostic Supervised Local Explanations (MAPLE) (Plumb et al., 2018). LIME in particular has gained notable popularity and has been deployed in many applications due to its simplicity. However, the overall prediction performance and fidelity metrics are not reaching desired levels in many cases (Alvarez-Melis & Jaakkola, 2018; Zhang et al., 2019; Ribeiro et al., 2018; Lakkaraju et al., 2017). Indeed, as we show in our experiments, there are frequent cases where existing locally interpretable models even underperform commonly low-performing globally interpretable models.\nOne of the fundamental challenges to fit a locally interpretable model is the representational capacity difference while applying distillation. Black-box machine learning models, such as deep neural networks or ensemble models, have much larger representational capacity than locally interpretable models. This can result in underfitting with conventional distillation techniques, leading to suboptimal performance (Hinton et al., 2015; Wang et al., 2019). We address this fundamental challenge by proposing a novel Reinforcement Learning-based method to fit Locally Interpretable Models which we call RL-LIM. RL-LIM efficiently utilizes the small representational capacity of locally interpretable models by training with a small number of samples that are determined to have the highest value contribution to the fitting of a locally interpretable model. In order to select these highest-value instances, we train instance-wise weight estimators (modeled with deep neural networks) using a reinforcement signal that quantifies the fidelity metric (i.e. how well does the model approximate the black-box model predictions). The contributions of this paper can be summarized as:\n1. We introduce the first method that tackles interpretability through data-weighted training, and show that reinforcement learning is highly effective for end-to-end training of such a model. 2. We show that distillation of a black-box model into a low-capacity interpretable model can be significantly improved by fitting with a small subset of relevant samples that is controlled efficiently by our method. 3. On various classification and regression datasets, we demonstrate that RL-LIM significantly outperforms alternative models (LIME, SILO and MAPLE) in overall prediction performance and fidelity metrics \u2013 in most cases, the overall performance of locally interpretable models obtained by RL-LIM is very similar to complex black-box models."
    },
    {
      "heading": "2 RELATED WORK",
      "text": "Locally interpretable models: There are various approaches to interpret black-box models \u2013 (Gilpin et al., 2018) provides a good overview. One approach is to directly decompose the prediction into feature attributions by considering what-if cases. Shapley values (S\u030ctrumbelj & Kononenko, 2014) and their computationally-efficient variants (Lundberg & Lee, 2017) are commonly-used methods in this category. Other notable methods are based on activation differences, e.g. DeepLIFT (Shrikumar et al., 2017), or saliency maps using the gradient flows, e.g. CAM (Zhou et al., 2016) and Grad-CAM (Selvaraju et al., 2017). In this paper, we focus on the direction of locally interpretable modeling \u2013 distilling a black-box model into an interpretable model for each input instance.\nLocally Interpretable Model-agnostic Explanation (LIME) (Ribeiro et al., 2016) is the most popular method for locally interpretable modeling. LIME is based on modifying a data instance by tweaking the feature values and then learning from the impact of the modifications on the output. A fundamental challenge for LIME is the need for a meaningful distance metric to determine neighborhoods, as simple metrics like Euclidean distance may yield poor fidelity in some cases and the estimation can be highly-sensitive to normalization (Alvarez-Melis & Jaakkola, 2018) especially with categorical variables. Supervised Local modeling methods (SILO) (Bloniarz et al., 2016)) aims to improve LIME by determining the neighborhoods for each instance using ad-hoc tree-based ensemble methods. Model Agnostic Supervised Local Explanations (MAPLE) (Plumb et al., 2018) furthers adds a method for feature selection on top of SILO \u2013 it utilizes ad-hoc tree-based ensemble methods to determine the weights of training instances for each target instance and uses the weights to opti-\nmize a locally interpretable model. However, SILO and MAPLE still have shortcomings because the tree-based ensemble methods are optimized independently from the locally interpretable model \u2013 lack of joint optimization results in suboptimal fidelity for the locally interpretable model. Overall, to construct a locally interpretable model, a key problem is how to select the optimal training instances for each testing instance, because the selected training instances mostly determine the constructed locally interpretable model. The number of possibilities for training instance selection is extremely large (exponential in the number of training instances). LIME heuristically utilizes Euclidean distances, whereas SILO and MAPLE use ad-hoc tree-based ensemble methods. Our proposed method, RL-LIM, takes a very different perspective: to properly and efficiently explore the large possible solution space, RL-LIM utilizes reinforcement learning to find the optimal policy that selects the training instances that maximize the fidelity of the locally interpretable model.\nData-weighted training: Optimal weighing of training data is a paramount problem in machine learning. By upweighting valuable instances and downweighting the low quality or problematic instances, better performance can be obtained in certain learning scenarios, such as imbalanced or noisy labels (Jiang et al., 2018). One approach for data weighting is utilizing Influence Functions (Koh & Liang, 2017), that are based on oracle access to gradients and Hessian-vector products. Jointly-trained student-teacher methods constitute another approach (Jiang et al., 2018; Bengio et al., 2009) to learn a data-driven curriculum. Using the feedback from the teacher network, training instance-wise weights are learned for the student model. Aligned with our motivations, meta learning is considered for data weighting in Ren et al. (2018). Their proposed method utilizes gradient descent-based meta learning, guided by a small validation set, to maximize the target performance.\nIn this work we consider data-weighted training for a novel purpose: interpretability. Unlike gradient descent-based meta learning, our approach uses reinforcement learning to integrate the reward directly with the fidelity metric. Aforementioned works estimate the same ranking of training instances for the entire dataset. Instead, our method yields an instance-wise ranking of training data points, different for each testing instance. This enables efficient distillation of a black-box model prediction into a locally interpretable model."
    },
    {
      "heading": "3 REINFORCEMENT LEARNING-BASED MODELING",
      "text": "We consider a training dataset D = {(xi, yi), i = 1, ..., N} \u223c P for training of a black-box model f , where xi \u2208 X is the feature vector in a d-dimensional feature space X and yi \u2208 Y is the corresponding label in a label space Y . We also assume that there exists a probe dataset Dp = {(xpj , y p j ), j = 1, ...,M} \u223c P where M is the number of probe instances. The probe dataset is used to evaluate the model performance to guide meta-learning as in Ren et al. (2018). If there is no explicit probe dataset, we can randomly partition a subset of the training dataset as the probe dataset and the remainder as the training dataset. RL-LIM is composed of three models:\n1. Black-box model f : X \u2192 Y \u2013 any machine learning model that needs to be explained (e.g. a deep neural network or a decision tree-based ensemble model),\n2. Locally interpretable model g\u03b8 : X \u2192 Y \u2013 an inherently interpretable model by design (e.g. a linear model or a shallow decision tree),\n3. Instance-wise weight estimation model h\u03c6 : X \u00d7 X \u00d7 Y \u2192 [0, 1] \u2013 a function that outputs the instance-wise weights to fit the locally interpretable model. It uses concatenation of a probe feature, a training feature, and a corresponding black-box model prediction on the training feature as its inputs. It can be a complex machine learning model \u2013 e.g. here a deep neural network.\nOur objective is to construct an accurate locally interpretable model g\u03b8 such that the predictions made by it are similar to the predictions of the given black-box model f\u2217 \u2013 i.e. the locally interpretable model has high fidelity. We use a loss function, L : Y \u00d7 Y \u2192 R to quantify the fidelity of the locally interpretable model (e.g. mean absolute error, lower the better).\nThe representational capacity difference between the black-box model and the locally interpretable model is the bottleneck we aim to address. Ideally, to avoid underfitting, locally interpretable models should be learned with a minimal number of training instances that are most effective in capturing the model behavior. We propose an instance-wise weight estimation model h\u03c6 to estimate the probabilities of training instances that should be used for fitting the locally interpretable model. Integrating\nwith the accurate locally interpretable modeling goal, we propose the following objective:\nmin h\u03c6\nExp\u223cPX [ L(f\u2217(xp), g\u2217\u03b8(xp)(x p)) ] + \u03bbExp,x\u223cPX [ h\u03c6(xp, x, f\u2217(x)) ] s.t. g\u2217\u03b8(xp) = argming\u03b8 Ex\u223cPX [ h\u03c6(xp, x, f\u2217(x))\u00d7 Lg(f\u2217(x), g\u03b8(x))\n] (1) where \u03bb \u2265 0 is a hyper-parameter that controls the number of training instances used to fit the locally interpretable model (we study the impact of performance on \u03bb in Section 4.2), and h\u03c6(xp, x, f\u2217(x)) represents the instance-wise weight for each training pair (x, f\u2217(x)) for the probe data xp. Lg is the loss function to fit the locally interpretable model, for which we use the mean squared error between predicted values for regression and logits for classification. \u03c6 and \u03b8 are the trainable parameters, whereas f\u2217 (the pre-trained black-box model) is fixed.\nThe first term in the objective function Exp\u223cPX [ L(f\u2217(xp), g\u2217\u03b8(xp)(x p)) ]\nrepresents the local prediction differences between black-box model and locally interpretable model (referred to as fidelity metric). The second term in the objective function Exp,x\u223cPX [ h\u03c6(xp, x, f\u2217(x)) ] represents the expected number of selected training points to fit the locally interpretable model. Lastly, the constraint ensures that the locally interpretable model is derived from weighted loss function, where weights are the output of the instance-wise weight estimator h\u03c6. Our formulation does not assume any constraint on g\u03b8 \u2013 it could be any inherently interpretable model suitable for the data type of interest. Next, we describe how Eq. (1) can be efficiently addressed with reinforcement learning."
    },
    {
      "heading": "3.1 TRAINING AND INFERENCE",
      "text": "The RL-LIM method, shown in Fig. 1, can be thought of as encompassing 5 stages:\n\u2022 Stage 0 \u2013 Black-box model training: This stage is the preliminary stage for RL-LIM. Given the training set D, the black-box model f is trained to minimize a loss function (Lf ) (e.g. mean squared error for regression or cross-entropy for classification), i.e., f\u2217 = argminf 1 N \u2211N i=1 Lf (f(xi), yi). If the pre-trained black-box model is already saved, we can\nskip this stage and retrieve the given pre-trained black-box model to f\u2217. \u2022 Stage 1 \u2013 Auxiliary dataset construction: Using the pre-trained black-box model f\u2217, we create\nauxiliary training and probe datasets, as D\u0302 = {(xi, y\u0302i), i = 1, ..., N} (where y\u0302i = f\u2217(xi)) and D\u0302p = {(xpj , y\u0302 p j ), j = 1, ...,M} (where y\u0302 p j = f\n\u2217(xpj )), respectively. These auxiliary datasets (D\u0302, D\u0302p) are used for instance-wise weight estimation models and locally interpretable model training.\n\u2022 Stage 2 \u2013 Interpretable baseline training: To improve the stability of the instance-wise weight estimator training, a baseline model is observed to be beneficial. As the baseline model gb : X \u2192 Y , we use a globally interpretable model (such as a linear model or shallow decision tree) optimized to replicate the predictions of the black-box model: g\u2217b = argming 1 N \u2211N i=1 L(g(xi), y\u0302i). \u2022 Stage 3 \u2013 Instance-wise weight estimator training: We train an instance-wise weight estimator using the auxiliary datasets (D\u0302, D\u0302p). To encourage exploration, we consider probabilistic selection, with a sampler block that is based on the output of the instance-wise weight estimator \u2013 h\u03c6(xpj , xi, y\u0302i) represents the probability that (xi, y\u0302i) is selected to train locally interpretable model for the probe instance xpj . Let the binary vector c(x p j ) \u2208 {0, 1}N represent the selection opera-\ntion, such that (xi, y\u0302i) is selected for training locally interpretable model for xpj when ci(x p j ) = 1. Correspondingly, \u03c1\u03c6(xp) is the probability mass function for c(xpj ) given h\u03c6(\u00b7):\n\u03c1\u03c6(xpj , c(x p j )) = N\u220f i=1 [ h\u03c6(xpj , xi, f \u2217(xi))ci(x p j ) \u00b7 (1\u2212 h\u03c6(xpj , xi, f \u2217(xi)))1\u2212ci(x p j ) ]\nAs the original form of the optimization problem in Eq. (1) is intractable due to the expectation operations, we employ approximations:\n\u2013 The sample mean is used as an approximation of the first term of the objective function as 1 M \u2211M j=1 L(f\u2217(x p j ), g \u2217 \u03b8(xpj )\n(xpj ))). \u2013 The second term of the objective, which represents the average selection probability, is\napproximated as the number of selected instances (divided by N ) to have ||c(xpj )||1 = 1 N \u2211N i=1 |ci(x p j )|.\n\u2013 The constraint term is approximated using the sample mean of the training loss as g\u2217 \u03b8(xpj ) =\nargming\u03b8 1 N \u2211N i=1 [ ci(xpj ) \u00b7 Lg(f\u2217(xi), g\u03b8(xi)) ] .\nThe sampler block yields a non-differential objective, and we cannot train the instance-wise weight estimator using conventional gradient descent-based optimization. There are approximations such as training in expectation (Raffel et al., 2017) or Gumbel-softmax (Jang et al., 2016). Instead, motivated by its many successful applications (Ranzato et al., 2015; Zaremba & Sutskever, 2015; Zhang & Lapata, 2017), we use REINFORCE algorithm (Williams, 1992) such that the selection action is rewarded by the performance of its impact. The loss function for the instance-wise weight estimator l(\u03c6) is expressed as:\nl(\u03c6) = Expj\u223cPX [ Ec(xpj )\u223c\u03c1\u03c6(xpj ,\u00b7) [ L(f\u2217(xpj ), g \u2217 \u03b8(xpj ) (xpj ))) + \u03bb||c(x p j )||1 ]] To apply the REINFORCE algorithm, we directly compute the gradient\u2207\u03c6 l\u0302(\u03c6) as:\n\u2207\u03c6 l\u0302(\u03c6) = Expj\u223cPX [ Ec(xpj )\u223c\u03c1\u03c6(xpj ,\u00b7) [ L(f\u2217(xpj ), g \u2217 \u03b8(xpj ) (xpj ))) + \u03bb||c(x p j )||1 ] \u2207\u03c6 log \u03c1\u03c6(xpj , c(x p j )) ]\nUsing the gradient\u2207\u03c6 l\u0302(\u03c6), we employ the following steps iteratively to update the parameters of the instance-wise weight estimator \u03c6: 1. Estimate instance-wise weights wi(xpj ) = h\u03c6(x p j , xi, y\u0302i) and instance-wise selection vector\nci(xpj ) \u223c Ber(wi(x p j )) for each training and probe instance in a mini-batch.\n2. Optimize the locally interpretable model with the selection vector for each probe instance:\ng\u2217\u03b8(xpj ) = argmin\ng\u03b8 N\u2211 i=1 [ ci(xpj ) \u00b7 Lg(f \u2217(xi), g\u03b8(xi)) ]\n3. Update the instance-wise weight estimation model parameter \u03c6:\n\u03c6\u2190 \u03c6\u2212 \u03b1 M M\u2211 j=1 [ L(f\u2217(xpj ), g \u2217 \u03b8(xpj ) (xpj ))\u2212 Lb(x p j ) + \u03bb||c(x p j )||1 ] \u00b7 \u2207\u03c6 log \u03c1\u03c6(xpj , c(x p j ))\nwhere \u03b1 > 0 is a learning rate and Lb(xpj ) = L(f\u2217(x p j ), g \u2217 b (x p j )) is the baseline loss against which we benchmark the performance improvement. We repeat the steps above until convergence.\n\u2022 Stage 4 \u2013 Interpretable inference: Unlike when training, we use a fixed instance-wise weight estimator (without the sampler and interpretable baseline) and merely fit the locally interpretable model at inference. Given the test instance xt, we obtain the selection probabilities from the instance-wise weight estimator, and using these as the weights, we fit the locally interpretable model via weighted optimization. The outputs of the trained interpretable model are the instancewise predictions and the corresponding explanations (e.g., local dynamics of the black-box model predictions at xt given by the coefficients of the fitted linear model)."
    },
    {
      "heading": "3.2 COMPUTATIONAL COST",
      "text": "In this subsection, we analyze the computational cost of RL-LIM for training and inference. As a representative and commonly used example, we assume linear regression as the locally interpretable model, which has a computational complexity of O(d2N) + O(d3) to fit, where d is the number of features and N is the number of training instances. When N >> d (which is often the case in practice), the training computational complexity is approximated as O(d2N) (Tan, 2018). Training: Given a pre-trained black-box model, Stage 1 involves running inferenceN times and the total complexity depends on the complexity of the black-box model. Unless the black-box model is very complex, the computational complexity of Stage 1 becomes much smaller than Stage 3. Stage 2 has negligible computational overhead. At Stage 3, we iteratively train the instance-wise weight estimator and fit the locally interpretable model from scratch using weighted optimization. Therefore, the computational complexity is O(d2NNI) where NI is the number of iterations in Stage 3 (typically NI < 10, 000 until convergence). Thus, the training complexity scales roughly linearly with the number of training instances.\nInterpretable inference: To infer with the locally interpretable model, we need to fit the locally interpretable model after obtaining the instance-wise weights from the trained instance-wise weight estimator. Thus, for each testing instance, the computational complexity is O(d2N).1\nFor instance, on a single NVIDIA V100 GPU, on Facebook Comment dataset (consisting\u223c 600,000 samples), RL-LIM yields a training time of less than 5 hours (including Stage 1, 2 and 3) and an interpretable inference time of less than 10 seconds per a testing instance. On the other hand, LIME results in much longer interpretable inference time (around 30 seconds per a testing instance) due to acquiring a large number of black-box model predictions for the inputs perturbations, whereas SILO and MAPLE are similar to RL-LIM."
    },
    {
      "heading": "4 EXPERIMENTS",
      "text": "We compare RL-LIM to multiple benchmarks on 3 synthetic datasets and 5 UCI public datasets. The source-code can be found at https://github.com/google-research/ google-research/tree/master/rllim.\nDatasets: The 3 public datasets for regression problems are: (1) Blog Feedback, (2) Facebook Comment, (3) News Popularity; the other 2 public datasets for classification problems are: (4) Adult Income, (5) Weather. Details of the data descriptions can be found in the hyper-links of each dataset\n1A subset of the training dataset can be used to reduce complexity (with decreased fidelity).\n(colored in blue). Data statistics can be found in Table 3 in Appendix A. In this section, we mainly focus on the tabular datasets because the local dynamics are more important and useful to explain for them; however, RL-LIM method can be generalized to other data types in a straightforward way.\nBlack-box models: We focus on approximating black-box models that are shown to yield competitive performance on the target tasks: 3 tree-based ensemble methods (1) XGBoost (Chen & Guestrin, 2016), (2) LightGBM (Ke et al., 2017), (3) Random Forests (RF) (Breiman, 2001); and deep neural networks (4) Multi-layer Perceptron (MLP). Also, we use (5) Ridge Regression (RR) and (6) Regression Tree (RT) (for regression) and (7) Logistic Regression (LR) and (8) Decision Tree (DT) (for classification) as globally interpretable models to benchmark.2 We focus on two types of locally interpretable models: (1) Ridge regression, (2) Shallow regression tree (with a max depth of 3). We report the performance with ridge regression for regression and with shallow regression tree for classification in this section. The results of the other two combinations (with ridge regression for classification and with shallow regression tree for regression) are described in Appendix E.\nComparisons to previous work: We compare the performance of RL-LIM with three competing methods: (1) Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al., 2016), (2) Supervised Local modeling methods (SILO) (Bloniarz et al., 2016), (3) Model Agnostic Supervised Local Explanations (MAPLE) (Plumb et al., 2018).\nPerformance metrics: To evaluate the performance of locally interpretable models using real-world datasets, we quantify the overall prediction performance and its fidelity. We assume a disjoint testing dataset Dt = {(xtk, ytk)}Lk=1 for evaluation. For the overall prediction performance, we compare the predictions of the locally interpretable models with the ground-truth labels. We use Mean Absolute Error (MAE) for regression and Average Precision Recall (APR) for classification. For fidelity, we compare the outputs (predicted values for regression and logits for classification) of the locally interpretable models and of the black-box model. We consider two metrics: R2 score (Legates & McCabe, 1999) and Local MAE (LMAE). The details of the metrics are described in Appendix C.\nImplementation details: We implement instance-wise weight estimator using a multi-layer perceptron with tanh activation. The number of hidden units and layers are optimized by the crossvalidation. In most cases, 5-layer perceptron with 100 hidden units performs reasonably-well across all datasets. All features are normalized to be between zero and one, using standard minmax scaler. Categorical variables are transformed using one-hot encoding."
    },
    {
      "heading": "4.1 EXPERIMENTS ON SYNTHETIC DATASETS \u2013 RECOVERING LOCAL DYNAMICS",
      "text": "On real-world datasets it is challenging to directly evaluate the explanation quality of the locally interpretable models due to the absence of ground-truth explanations. Thus we initially focus on synthetic datasets (with known ground-truth explanations) to directly evaluate how well the locally interpretable models can recover the underlying local dynamics. We construct three synthetic datasets such that the 11-dimensional input features X are sampled from N (0, I) and Y are:\n1. Syn1: Y = X1 + 2X2 if X10 < 0 and Y = X3 + 2X4 if X10 \u2265 0 2. Syn2: Y = X1 + 2X2 if X10 + eX11 < 1 and Y = X3 + 2X4 if X10 + eX11 \u2265 1 3. Syn3: Y = X1 + 2X2 if X10 +X311 < 0 and Y = X3 + 2X4 if X10 +X 3 11 \u2265 0\nAll three datasets have different local dynamics in different input regimes. We directly use the ground truth function as the black-box model and focus on how well locally interpretable modeling can capture the local dynamics. We evaluate the performance of capturing local dynamics using Absolute Weight Difference (AWD): AWD = ||w \u2212 w\u0302||, where w is the ground truth coefficients to generate Y and w\u0302 is the derived coefficient from the locally interpretable models. We use the estimated coefficients of the ridge regression as the derived local dynamics (w\u0302).\nAs shown in Fig. 2, RL-LIM significantly outperforms other benchmarks in discovering the local dynamics on all three datasets and in different regimes. RL-LIM can actively learn the linear and non-linear decision boundaries for the local dynamics. Note that LIME completely fails to recover the local dynamics as it uses the Euclidean distance uniformly for all features and cannot distinguish\n2We use python packages (including Sklearn and Tensorflow) to implement those predictive models and the details can be found in the hyper-links (colored in blue) of each model and Appendix B.\nthe special properties of the features that alter the local dynamics. SILO and MAPLE only use the predictions to discover the local dynamics; thus, it is hard to discover the decision boundary that depends on the other variables which are independent to the predictions. Fig. 5 in Appendix D shows the learning curves of RL-LIM demonstrating the efficiency of reinforcement learning."
    },
    {
      "heading": "4.2 THE EFFECT OF THE NUMBER OF SELECTED SAMPLES ON FIDELITY",
      "text": "In RL-LIM, optimal distillation is enabled by using a small subset of training instances to fit the low-capacity locally interpretable model. The number of selected instances is controlled by \u03bb in our method \u2013 if \u03bb is high/low, RL-LIM penalizes more/less on the number of selected instances; thus, less/more instances are selected to construct the locally interpretable model.\nWe analyze the efficacy of \u03bb in controlling the likelihood of selection and the dependency of fidelity on \u03bb. We expect that if we select a too small/large number of training instances, the locally interpretable model will overfit/underfit which negatively affects the fidelity in both cases. Fig. 3 shows that there is a clear relationship between \u03bb and the local fidelity. If \u03bb is too large, RL-LIM selects too small number of instances; thus, the fitted locally interpretable model is less accurate (due to overfitting). On the other hand, if \u03bb is too small, RL-LIM selects too large number of instances and deteriorates fidelity (due to underfitting). To achieve the optimal \u03bb, we conduct cross-validation experiments and select \u03bb which achieves the best validation fidelity (e.g. \u03bb = 0.5 in Syn2). Fig. 3 shows the average selection probability of the training instances for each \u03bb. As \u03bb increases, the average selection probabilities monotonically decrease due to the higher penalty on the number of selected training instances. Note that even using a small portion of training instances, RL-LIM can accurately distill the predictions of black-box models into locally interpretable models which is crucial to understand and interpret the predictions using the most relevant training instances."
    },
    {
      "heading": "4.3 EXPERIMENTS ON REAL DATASETS \u2013 OVERALL PERFORMANCE AND FIDELITY",
      "text": "On multiple real datasets, we evaluate the overall prediction performance and fidelity. For the regression and classification problems, we use ridge regression and shallow regression trees as the locally interpretable model. More results can be found in Appendix E.\nAs can be seen in Table 1, the performance of globally interpretable ridge regression (trained on the entire dataset from the scratch) is much worse than other complex non-linear models, implying that modeling non-linear relationships between the features and the labels is important towards high prediction performance. For other locally interpretable modeling methods (LIME, SILO, MAPLE), the performance is far worse than the original black-box model, showing that they fail at efficiently distilling the non-linear black-box models. In some cases (especially on the Facebook dataset), the performance of the benchmarks is even worse than the performance of global ridge regression (highlighted in red), questioning the value of using these locally interpretable models instead of globally interpretable ridge regression.\nIn contrast, RL-LIM achieves similar overall prediction performance to the black-box models and significantly outperforms global ridge regression. Table 1 also compares the fidelity in terms of R2 score for regression using ridge regression as the locally interpretable model (LMAE results can be found in Appendix E.3). We observe that R2 scores for some cases (especially on Facebook dataset and LIME) are negative which represent that the outputs of the locally interpretable models are even worse than the constant mean value estimator. On the other hand, RL-LIM achieves higher and positive R2 values consistently for all datasets and black-box models than other benchmarks.\nTable 2 shows a similar analysis for classification using shallow regression trees (with max depth of 3) as the locally interpretable model3. The overall prediction performance of four black-box models are significantly better than the globally interpretable decision tree which demonstrates the superior fitting by complex black-box models. Among the locally interpretable models, RL-LIM achieves the best APR and R2 score for most cases, underlining its strength in distilling the predictions of the black-box model accurately. In some cases, the benchmarks (especially for LIME) achieve lower overall prediction performance than the globally interpretable decision tree (highlighted in red). The overall prediction performance and fidelity metrics of all locally interpretable models seem better for classification problems than regression problems. We expect that the predictions of black-box models are mostly highly confident, i.e. located near 0 or 1; thus, locally interpretable models can easily distill the predictions of the black-box models for classification than regression.\n3Regression trees are used to model logit outputs for classification."
    },
    {
      "heading": "4.4 QUALITATIVE ANALYSES \u2013 INTERPRETATIONS OF RL-LIM ON ADULT INCOME DATASET",
      "text": "We qualitatively analyze the local explanations provided by RL-LIM on the Adult Income dataset (qualitative analyses on Weather dataset can be found in Appendix E.4). Although RL-LIM is able to provide local explanations for each individual separately, we analyze its explanations in subgroup granularity for better visualization and understanding (instance granularity analyses are described in Appendix E.4). Fig. 4 represents the feature importance (derived by RL-LIM as the local explanations) for five subgroups in predicting the annual income using XGBoost as the black-box model. We use ridge regression as the locally interpretable model and the absolute value of fitted coefficients as the estimated feature importance. As can be observed in Fig. 4, for age subgroups, capital gain seems much more important for mature people (older than 25) than young people (younger than 25). For education subgroups, capital gain/loss, occupation, and native countries are more critical for highly-educated people (Doctorate, Prof-school, and Masters graduates) than the others. We do not discover notable biases of black-box models for gender, marital status, and race subgroups."
    },
    {
      "heading": "5 CONCLUSIONS",
      "text": "We propose a novel method for locally interpretable modeling of pre-trained black-box models. Our proposed method employs reinforcement learning to select a small number of valuable instances and use them to train a low-capacity locally interpretable model. The selection mechanism is guided with a reward obtained from the similarity of predictions of the locally interpretable model and the blackbox model. Our approach near-matches the performance of black-box models and significantly outperforms alternative techniques in terms of overall prediction performance and fidelity metrics consistently across various datasets and black-box models."
    },
    {
      "heading": "6 ACKNOWLEDGEMENTS",
      "text": "Discussions with Besim Avci, Henry Tappen and Zizhao Zhang are gratefully acknowledged."
    },
    {
      "heading": "A DATA STATISTICS",
      "text": ""
    },
    {
      "heading": "B HYPER-PARAMETERS OF THE PREDICTIVE MODELS",
      "text": "In this paper, we use 8 different predictive models. For each predictive model, the corresponding hyper-parameters used in the experiments are as follows:\n\u2022 XGBoost: booster - gbtree, max depth - 6, learning rate - 0.3, number of estimators - 1000, max depth - 6, reg alpha - 0\n\u2022 LightGBM: booster - gbdt, max depth - None, learning rate - 0.1, number of estimators - 1000, min data in leaf - 20\n\u2022 Random Forests: number of estimators - 1000, criterion - gini, max depth - None, warm start - False\n\u2022 Multi-layer Perceptron: Number of layers - 4, hidden units - [feature dimensions, feature dimensions/2, feature dimensions/4, feature dimensions/8], activation function - relu, early stoping - True with patient 10, batch size - 256, maximum number of epochs - 200, optimizer - Adam\n\u2022 Ridge Regression: alpha - 1 \u2022 Regression Tree: max depth - 3, criterion - gini \u2022 Logistic Regression: solver - lbfgs, no regularization \u2022 Decision Tree: max depth - 3, criterion - gini\nWe follow the default settings for the other hyper-parameters that are not mentioned here."
    },
    {
      "heading": "C PERFORMANCE METRICS",
      "text": "\u2022 Mean Absolute Error (MAE):\nMAE = E(xt,yt)\u223cP ||g\u2217\u03b8(xt)(x t)\u2212 yt)||1 '\n1\nL L\u2211 k=1 ||g\u2217\u03b8(xtk)(x t k)\u2212 ytk||1,\n\u2022 Local MAE (LMAE):\nLMAE = Ext\u223cPX ||g\u2217\u03b8(xt)(x t)\u2212 f\u2217(xt)||1 '\n1\nL L\u2211 k=1 ||g\u2217\u03b8(xtk)(x t k)\u2212 f\u2217(xtk))||1,\n\u2022 R2 score (Legates & McCabe, 1999):\nR2 = 1\u2212 Ext\u223cPX ||f\u2217(xt)\u2212 g\u2217\u03b8(xt)(x t)||22 Ext\u223cPX ||f\u2217(xt)\u2212 Ex\u0302t\u223cPX [f\u2217(x\u0302 t)]||22 ' 1\u2212\n1 L \u2211L k=1 ||f\u2217(xtk)\u2212 g\u2217\u03b8(xtk)(x t k)||22\n1 L \u2211L k=1 ||f\u2217(xtk)\u2212 1 L \u2211L k=1[f \u2217(xtk)]||22 .\nIf R2 = 1, the predictions of the locally interpretable model perfectly match the predictions of the black-box model. On the other hand, if R2 = 0, the locally interpretable model performs as similar as the constant mean value estimator. If R2 < 0, the locally interpretable model performs worse than the constant mean value estimator."
    },
    {
      "heading": "D LEARNING CURVES OF RL-LIM",
      "text": ""
    },
    {
      "heading": "E ADDITIONAL RESULTS",
      "text": "E.1 REGRESSION WITH SHALLOW REGRESSION TREE AS THE LOCALLY INTERPRETABLE MODEL\nE.2 CLASSIFICATION WITH RIDGE REGRESSION AS THE LOCALLY INTERPRETABLE MODEL\nE.3 REGRESSION WITH RIDGE REGRESSION AS THE LOCALLY INTERPRETABLE MODEL - FIDELITY ANALYSIS IN TERMS OF LOCAL MAE (LMAE)\nE.4 QUALITATIVE ANALYSES \u2013 INTERPRETATIONS OF RL-LIM ON WEATHER DATASET\nWe qualitatively analyze the local explanations provided by RL-LIM on Weather dataset at subgroup granularity. Fig. 6 shows the feature importance for six subgroups in predicting whether it will rain tomorrow, using XGBoost as the black-box model. We use ridge regression as the locally interpretable model and the absolute value of fitted coefficients as the estimated feature importance. For rain fall subgroups, humidity and wind gust speed seem more important for heavy rain (rain fall \u2265 5) than light rain (rain fall < 5). For temperature subgroups, rainfall, wind gust speed and humidity are more important for cold days (temperature (at 3pm)\u2264 10) than warm day (temperature (at 3pm) \u2265 20). In general, for heavy rain, fast wind speed, low pressure, and low temperature subgroups, humidity, wind gust speed and rain fall variables are more critical to predict whether it will rain tomorrow than light rain, slow wind speed, high pressure, and high temperature subgroups . We do not discover notable biases of the black-box model for humidity subgroups.\nWe further analyze the local explanations provided by RL-LIM on the Weather dataset at instance granularity. Fig. 6 represents the feature importance (derived by RL-LIM as the local explanations) for 10 instances belong to a subgroup with \u2018rain fall \u2264 1, wind speed (at 3pm) \u2265 5, and temperature (at 3pm) > 30\u2019 and the other 10 instances belong to the other subgroup with \u2018rain fall > 15, wind speed (at 3pm) > 25, and temperature (3pm) < 10\u2019. Other experiment settings are the same with the previous analyses in the subgroup granularity. There are clear differences between feature importance of two subgroups (left and right in Fig. 6). Even within the same subgroup, we can observe differences in feature importance across different instances, that are efficiently provided by RL-LIM."
    }
  ],
  "title": "RL-LIM: REINFORCEMENT LEARNING-BASED LOCALLY INTERPRETABLE MODELING",
  "year": 2019
}
