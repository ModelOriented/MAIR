{
  "abstractText": "Artificial Intelligence (AI) systems act as blackboxes, concealing their inner workings from the human operator. However, the level of opacity varies from one system category to another. We provide an overview of the level of opacity of these AI systems.",
  "authors": [
    {
      "affiliations": [],
      "name": "Steven Wark"
    },
    {
      "affiliations": [],
      "name": "Marcin Nowina-Krowicki"
    },
    {
      "affiliations": [],
      "name": "Crisrael Lucero"
    },
    {
      "affiliations": [],
      "name": "Douglas Lange"
    },
    {
      "affiliations": [],
      "name": "Mor Vered"
    },
    {
      "affiliations": [],
      "name": "Piers Howe"
    },
    {
      "affiliations": [],
      "name": "Tim Miller"
    },
    {
      "affiliations": [],
      "name": "Liz Sonenberg"
    },
    {
      "affiliations": [],
      "name": "Eduardo Velloso"
    },
    {
      "affiliations": [],
      "name": "Abdulrahman Baqais"
    },
    {
      "affiliations": [],
      "name": "Zubair Baig"
    },
    {
      "affiliations": [],
      "name": "Marthie Grobler"
    },
    {
      "affiliations": [],
      "name": "Joshua Newn"
    },
    {
      "affiliations": [],
      "name": "Ronal Singh"
    },
    {
      "affiliations": [],
      "name": "Prashan Madumal"
    },
    {
      "affiliations": [],
      "name": "Frank Vetere"
    }
  ],
  "id": "SP:f5729c93c46c4d64c5c38014462221da9f3484be",
  "references": [
    {
      "authors": [
        "W. Samek"
      ],
      "title": "Layer-wise relevance propagation for deep neural network architectures",
      "venue": "In Information Science and Applications (ICISA)",
      "year": 2016
    },
    {
      "authors": [
        "T. Brennan",
        "W. Dieterich"
      ],
      "title": "Correctional Offender Management Profiles for Alternative Sanctions (COMPAS)",
      "venue": "Handbook of Recidivism Risk/Needs Assessment Tools",
      "year": 2017
    },
    {
      "authors": [
        "P. Domingos"
      ],
      "title": "The Master Algorithm: How the Quest for the Ultimate Learning Machine will Remake",
      "venue": "Our World. Basic Books,",
      "year": 2018
    },
    {
      "authors": [
        "D. Mascharka",
        "P. Tran",
        "R. Soklaski",
        "A. Majumdar",
        "March"
      ],
      "title": "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4942-4950)",
      "year": 2018
    },
    {
      "authors": [
        "K.B. Watters",
        "H.M. Coyle"
      ],
      "title": "Forensic Statistical Tool (FST): A Probabilistic Genotyping Software Program For Human Identification.\" Jurimetrics",
      "year": 2016
    }
  ],
  "sections": [
    {
      "text": "Steven Wark, Marcin Nowina-Krowicki, Crisrael Lucero, Douglas Lange\nMor Vered, Piers Howe, Tim Miller, Liz Sonenberg, Eduardo Velloso\nAbdulrahman Baqais, Zubair Baig, Marthie Grobler\nJoshua Newn, Ronal Singh, Prashan Madumal, Eduardo\nVelloso, Frank Vetere\nPrashan Madumal, Tim Miller, Frank Vetere and Liz Sonenberg\nTransparency and Opacity in AI Systems: An Overview"
    },
    {
      "heading": "Abstract",
      "text": "Artificial Intelligence (AI) systems act as blackboxes, concealing their inner workings from the human operator. However, the level of opacity varies from one system category to another. We provide an overview of the level of opacity of these AI systems."
    },
    {
      "heading": "Author Keywords",
      "text": "Opacity; transparency; human comprehension.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honoured. For all other uses, contact the owner/author(s).\n2018, Melbourne, Australia.\nt held by the owner/author(s).\nAbdulrahman Baqais\nFreelance Researcher\nDhahran, Saudi Arabia\nphd_abdulrahman@yahoo.com\nZubair Baig\nCSIRO | Data61\nMelbourne, Australia\nzubair.baig@data61.csiro.au\nMarthie Grobler\nCSIRO | Data61\nMelbourne, Australia\nmarthie.grobler@data61.csiro.au"
    },
    {
      "heading": "ACM Classification Keywords",
      "text": "H.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous; See http://acm.org/about/class/1998 for the full list of ACM classifiers. This section is required."
    },
    {
      "heading": "Introduction",
      "text": "Artificial Intelligence (AI) systems span a wide spectrum of applications. They can be perceived as a standalone system as found in robots, or as a major component of an autonomous vehicle or smart watch. AI systems typically adopt a blackbox approach, wherein the inner workings are concealed from the human interface, limiting the hu to fully comprehend the reasoning process in decision-making. We highlight the various categories of AI systems based on their levels of transparencies and opacity in terms of human interaction."
    },
    {
      "heading": "Transparency vs Opacity",
      "text": "Transparent systems typically provide software developers with visualizations of the system execution process. This facilitates a better understanding of the inner workings of the system, but often degrades system speed and performance [5]. AI systems differ from conventional software systems as they are more opaque in their execution, allowing software developers to focus on the software development process without concerning themselves with transparency enhancements. In addition, opacity ascertains that the system is hard to replicate; safeguarding their Intellectual Property and retaining their competitiveness in market.\nWhilst AI systems are developed to improve process efficiencies in the industry, the current lack of\ntransparency restricts clear understanding of the\ndecision. This impacts liability and hinders preventative maintenance by human actors. Consider the application of AI by the judicial system. Such systems bear publicly-available data. The process of reaching a decision by an AI system is not always deterministic and as such it is almost impossible to challenge an outcome. As an example, an investigation conducted by ProPublica prompted the drafting and enactment of New Yor confirm that the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) [4] risk assessments were more likely to erroneously identify black defendants as presenting a high risk for repeating offences, at almost twice the rate as white defendants (43% vs 23%). Due to the opaque nature of the AI system , neither defendants nor the court systems utilizing COMPAS had visibility into why such assessments yielded significant rates of mislabeling based on race.\nWhilst opacity is popularly adopted as a property in AI system design, incorporating an element of transparency in these systems would facilitate efficient testing and debugging, albeit with degraded AI system performance [5]. The movement to transparent AI is enforced through the introduction of the General Data Protection Regulation (GDPR) [2]. This regulation restricts AI systems that require large volumes of input data to train, from accessing data elements that hold privacy concerns. The GDPR further mandates that data-driven algorithms explain how the data has been processed to reach decisions [2]. Consequently, it is expected to see an increasing number of transparent AI systems in the market in the near future. In this\nTransparency: The quality of being open in meaning, in such a way that it is easy for others to see what actions are being performed.\nOpacity: The quality of being obscure in meaning; also, lacking transparency.\ninstance, transparency is regarded as a tool to achieve accountability. For example, the COMPAS AI system was challenged in the Craig Loomi criminal case [1]. In his defense, the defendant pointed out that the COMPAS system was gender-biased because it considered his gender as a factor to compute an assessment score. The decision would not have been contested if there was transparency."
    },
    {
      "heading": "AI Opacity Levels Explained",
      "text": "Figure 1 illustrates the opacity levels of various types of AI systems: neural networks, metaheuristics, machine learning and fuzzy logic. There are differences between algorithms within each category in terms of the level of opacity at different levels.\nNeural networks typically receive the input as-is from a data set. The input stage is therefore transparent. Metaheuristics such as the Genetic Algorithm start out with a random initialization of inputs, such as the initial population. Thus, the process is colored opaque. Machine learning systems are typically operating on preprocessed data; already subject to feature engineering or normalization. As the inputs are not entirely raw, the opacity levels are colored grey. Fuzzy logic on the other hand involves opaque modification of the fuzzy inputs based on a certain membership function, and a transparent output. For the operations stage, machine learning includes regression or classification techniques, which are transparent; decision trees and support vector machine operations are transparent, as they are based on explicit mathematical equations. Fuzzy logic, neural networks and metaheuristics are also opaque in the way they update their results through modification or evaluation, before producing an output whereas, machine learning\noutputs are classes, clusters or coefficients; which fall between fully opaque and fully transparent."
    },
    {
      "heading": "Explainable AI",
      "text": "Since the concept of AI is often difficult to explain through easily understood principles, the human thoughts related to the domain of human made synthetic knowledge and intelligent decision making capabilities in itself depict a large blackbox with few relatable real-world explanations. As such, various schools of thought exist to categorize AI systems into opaque or transparent [6]. Each of these schools have a different thought pattern to make it easier for humans to mentally grasp the scope and capabilities of AI. By enabling humans to understand and visualize the inner workings of AI systems, they become better equipped to perceive the range of AI capabilities and enable facilitation of magnitude-scale and complex decision-making that the human mind cannot otherwise easily fathom.\nThe Logical School assumes intelligence is synonymous to logic and an AI system is composed of a list of\npredicates and logical operations. The system is not opaque and though it encompasses complex relationships, it is still interpretable. Similarly, the Bayesian School assumes that solutions to any problem can be reached probabilistically. Though randomness exists in the process, the inner functions are expressed through mathematical functions and as such they are not opaque. The Machine Learning School is based on statistical concepts and essentially is not opaque for small data sets. However, for improved precision and accuracy, a machine learning system requires large volume data sets, with many dimensions. These two factors make it difficult to interrupt the inner workings of such a system, making them opaque. Deep Neural Networks are extremely opaque, as the underlying mathematics associated with back propagation and weight interactions is very complex and the large number of nodes, layers and interconnecting structures make the system uninterpretable.\nIn order to classify AI systems according to their transparency, we define a taxonomy in Figure 2. The most transparent system (Level 1) is easy to understand, has a simple architecture and runs for a few iterations. In contrast, systems that incorporate randomization and comprise a complex processing architecture based on hard mathematical concepts, regardless of the number of iterations, are considered opaque and they serve as the fundamental definition of a blackbox (Level 8)."
    },
    {
      "heading": "Conclusion",
      "text": "We have identified several categories of AI systems based on their levels of opacity (or transparency). The recent shift towards more transparent AI systems will ensure a better comprehension in terms of the logic\nand attributions that enable the system to make automated intelligent decisions, guided by human intelligence.\nReferences 1. https://h2o.law.harvard.edu/collages/44824,\nAccessed on 18/October/2018\n2. Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) Official Journal of the European Union, Vol. L119 (4 May 2016), pp. 1-88\n3. Samek, W., 2016. Layer-wise relevance propagation for deep neural network architectures. In Information Science and Applications (ICISA) 2016 (pp. 913-922). Springer, Singapore.\n4. Brennan, T. & Dieterich, W. (2017). Correctional Offender Management Profiles for Alternative Sanctions (COMPAS). Handbook of Recidivism Risk/Needs Assessment Tools. 49-75. 10.1002/9781119184256.ch3.\n5. Domingos, P. 2018. The Master Algorithm: How the Quest for the Ultimate Learning Machine will Remake Our World. Basic Books, Inc., New York, NY, USA.\n6. Mascharka, D., Tran, P., Soklaski, R. and Majumdar, A., 2018, March. Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4942-4950).\n7. Watters, K. B., & Coyle, H. M. (2016). \"Forensic Statistical Tool (FST): A Probabilistic Genotyping Software Program For Human Identification.\" Jurimetrics 56.2 (2016): 183-195.\nFigure 2: Transparency levels of AI systems (most to least\ntransparent top to bottom)\nCodes\n0\n0.5\n1\n1.5\n2 2.5 Human-Human static explainee Human-Human static explainer Human-Explainer agent Human-Explainee agent Human-Human QnA Human-Human multiple explainee"
    }
  ],
  "year": 2018
}
