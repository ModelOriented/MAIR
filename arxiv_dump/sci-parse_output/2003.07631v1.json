{
  "abstractText": "With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning such as Deep Learning (DL), LSTMs, and kernel methods are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.",
  "authors": [
    {
      "affiliations": [],
      "name": "Wojciech Samek"
    },
    {
      "affiliations": [],
      "name": "Gr\u00e9goire Montavon"
    },
    {
      "affiliations": [],
      "name": "Sebastian Lapuschkin"
    },
    {
      "affiliations": [],
      "name": "Christopher J. Anders"
    }
  ],
  "id": "SP:0212dbc9d6a510a4bf796998350f56cbfb98d93e",
  "references": [
    {
      "authors": [
        "J. Adebayo",
        "J. Gilmer",
        "I.J. Goodfellow",
        "B. Kim"
      ],
      "title": "Local explanation methods for deep neural networks lack sensitivity to parameter values",
      "venue": "International Conference on Learning Representations (ICLR)",
      "year": 2018
    },
    {
      "authors": [
        "C. Agarwal",
        "D. Schonfeld",
        "A. Nguyen"
      ],
      "title": "Removing input features via a generative model to explain their attributions to classifier\u2019s decisions",
      "venue": "CoRR, abs/1910.04256",
      "year": 2019
    },
    {
      "authors": [
        "M. Alber",
        "S. Lapuschkin",
        "P. Seegerer",
        "M. H\u00e4gele",
        "K.T. Sch\u00fctt",
        "G. Montavon",
        "W. Samek",
        "K.-R. M\u00fcller",
        "S. D\u00e4hne",
        "P.-J. Kindermans"
      ],
      "title": "iNNvestigate neural networks! Journal of Machine Learning Research",
      "venue": "20:93:1\u201393:8",
      "year": 2019
    },
    {
      "authors": [
        "M. Ancona",
        "E. Ceolini",
        "C. \u00d6ztireli",
        "M. Gross"
      ],
      "title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
      "venue": "CoRR, abs/1711.06104",
      "year": 2017
    },
    {
      "authors": [
        "M. Ancona",
        "E. Ceolini",
        "C. \u00d6ztireli",
        "M. Gross"
      ],
      "title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
      "venue": "International Conference of Learning Representations (ICLR)",
      "year": 2018
    },
    {
      "authors": [
        "C.J. Anders",
        "T. Marin\u010d",
        "D. Neumann",
        "W. Samek",
        "K.-R. M\u00fcller",
        "S. Lapuschkin"
      ],
      "title": "Analyzing imagenet with spectral relevance analysis: Towards imagenet un-hans\u2019ed",
      "venue": "CoRR, abs/1912.11425",
      "year": 2019
    },
    {
      "authors": [
        "C.J. Anders",
        "G. Montavon",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Understanding patch-based learning of video data by explaining predictions",
      "venue": "In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning,",
      "year": 2019
    },
    {
      "authors": [
        "J.A. Arjona-Medina",
        "M. Gillhofer",
        "M. Widrich",
        "T. Unterthiner",
        "J. Brandstetter",
        "S. Hochreiter"
      ],
      "title": "Rudder: Return decomposition for delayed rewards",
      "venue": "Advances in Neural Information Processing Systems, pages 13544\u201313555",
      "year": 2019
    },
    {
      "authors": [
        "L. Arras",
        "J.A. Arjona-Medina",
        "M. Widrich",
        "G. Montavon",
        "M. Gillhofer",
        "K.-R. M\u00fcller",
        "S. Hochreiter",
        "W. Samek"
      ],
      "title": "Explaining and interpreting lstms",
      "venue": "Explainable AI, volume 11700 of Lecture Notes in Computer Science, pages 211\u2013238. Springer",
      "year": 2019
    },
    {
      "authors": [
        "L. Arras",
        "F. Horn",
        "G. Montavon",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "What is relevant in a text document?\u201d: An interpretable machine learning approach",
      "venue": "PLoS ONE, 12",
      "year": 2019
    },
    {
      "authors": [
        "L. Arras",
        "G. Montavon",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "Explaining recurrent neural network predictions in sentiment analysis",
      "venue": "Proceedings of the EMNLP\u201917 Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA), pages 159\u2013168",
      "year": 2017
    },
    {
      "authors": [
        "A.B. Arrieta",
        "N. D\u0131\u0301az-Rodr\u0131\u0301guez",
        "J.D. Ser",
        "A. Bennetot",
        "S. Tabik",
        "A. Barbado",
        "S. Garcia",
        "S. Gil-Lopez",
        "D. Molina",
        "R. Benjamins",
        "R. Chatila",
        "F. Herrera"
      ],
      "title": "Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
      "venue": "Information Fusion,",
      "year": 2020
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PLoS ONE, 10(7):e0130140",
      "year": 2015
    },
    {
      "authors": [
        "D. Baehrens",
        "T. Schroeter",
        "S. Harmeling",
        "M. Kawanabe",
        "K. Hansen",
        "K.-R. M\u00fcller"
      ],
      "title": "How to explain individual classification decisions",
      "venue": "Journal of Machine Learning Research, 11:1803\u20131831",
      "year": 2010
    },
    {
      "authors": [
        "D. Balduzzi",
        "M. Frean",
        "L. Leary",
        "J.P. Lewis",
        "K.W. Ma",
        "B. McWilliams"
      ],
      "title": "The shattered gradients problem: If resnets are the answer",
      "venue": "then what is the question? In ICML, volume 70 of Proceedings of Machine Learning Research, pages 342\u2013350. PMLR",
      "year": 2017
    },
    {
      "authors": [
        "D. Bau",
        "J.-Y. Zhu",
        "H. Strobelt",
        "B. Zhou",
        "J.B. Tenenbaum",
        "W.T. Freeman",
        "A. Torralba"
      ],
      "title": "Visualizing and understanding generative adversarial networks",
      "venue": "arXiv preprint arXiv:1901.09887",
      "year": 2019
    },
    {
      "authors": [
        "S. Bazen",
        "X. Joutard"
      ],
      "title": "The Taylor decomposition: A unified generalization of the Oaxaca method to nonlinear models",
      "venue": "Working papers, HAL",
      "year": 2013
    },
    {
      "authors": [
        "S. Becker",
        "M. Ackermann",
        "S. Lapuschkin",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "Interpreting and explaining deep neural networks for classification of audio signals",
      "venue": "CoRR, abs/1807.03418",
      "year": 2018
    },
    {
      "authors": [
        "F. Berkenkamp",
        "M. Turchetta",
        "A. Schoellig",
        "A. Krause"
      ],
      "title": "Safe modelbased reinforcement learning with stability guarantees",
      "venue": "Advances in neural information processing systems, pages 908\u2013918",
      "year": 2017
    },
    {
      "authors": [
        "A. Binder",
        "M. Bockmayr",
        "M. H\u00e4gele",
        "S. Wienert",
        "D. Heim",
        "K. Hellweg",
        "A. Stenzinger",
        "L. Parlow",
        "J. Budczies",
        "B. Goeppert",
        "D. Treue",
        "M. Kotani",
        "M. Ishii",
        "M. Dietel",
        "A. Hocke",
        "C. Denkert",
        "K.-R. M\u00fcller",
        "F. Klauschen"
      ],
      "title": "Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles",
      "venue": "CoRR, abs/1805.11178",
      "year": 2018
    },
    {
      "authors": [
        "C.M. Bishop"
      ],
      "title": "Neural Networks for Pattern Recognition",
      "venue": "Oxford University Press, Inc., USA",
      "year": 1996
    },
    {
      "authors": [
        "B. Blankertz",
        "R. Tomioka",
        "S. Lemm",
        "M. Kawanabe",
        "K.-R. M\u00fcller"
      ],
      "title": "Optimizing spatial filters for robust eeg single-trial analysis",
      "venue": "IEEE Signal processing magazine, 25(1):41\u201356",
      "year": 2008
    },
    {
      "authors": [
        "N. Carlini",
        "D. Wagner"
      ],
      "title": "Towards evaluating the robustness of neural networks",
      "venue": "2017 IEEE Symposium on Security and Privacy (SP), pages 39\u201357. IEEE",
      "year": 2017
    },
    {
      "authors": [
        "R. Caruana",
        "Y. Lou",
        "J. Gehrke",
        "P. Koch",
        "M. Sturm",
        "N. Elhadad"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1721\u20131730",
      "year": 2015
    },
    {
      "authors": [
        "J. Castro",
        "D. G\u00f3mez",
        "J. Tejada"
      ],
      "title": "Polynomial calculation of the shapley value based on sampling",
      "venue": "Computers & Operations Research, 36(5):1726\u20131730",
      "year": 2009
    },
    {
      "authors": [
        "P. Chong",
        "Y.X.M. Tan",
        "J. Guarnizo",
        "Y. Elovici",
        "A. Binder"
      ],
      "title": "Mouse authentication without the temporal aspect \u2013 what does a 2d-cnn learn? In 2018 IEEE Security and Privacy Workshops (SPW)",
      "venue": "pages 15\u201321. IEEE",
      "year": 2018
    },
    {
      "authors": [
        "T. Cui",
        "P. Marttinen",
        "S. Kaski"
      ],
      "title": "Recovering pairwise interactions using neural networks",
      "venue": "CoRR, abs/1901.08361",
      "year": 2019
    },
    {
      "authors": [
        "J. Deng",
        "W. Dong",
        "R. Socher",
        "L.-J. Li",
        "K. Li",
        "L. Fei-Fei"
      ],
      "title": "Imagenet: A large-scale hierarchical image database",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 248\u2013255",
      "year": 2009
    },
    {
      "authors": [
        "K. Dhamdhere",
        "M. Sundararajan",
        "Q. Yan"
      ],
      "title": "How important is a neuron? CoRR",
      "venue": "abs/1805.12233",
      "year": 2018
    },
    {
      "authors": [
        "I.S. Dhillon",
        "Y. Guan",
        "B. Kulis"
      ],
      "title": "Kernel k-means: spectral clustering and normalized cuts",
      "venue": "KDD, pages 551\u2013556. ACM",
      "year": 2004
    },
    {
      "authors": [
        "A. Dombrowski",
        "M. Alber",
        "C.J. Anders",
        "M. Ackermann",
        "K.-R. M\u00fcller",
        "P. Kessel"
      ],
      "title": "Explanations can be manipulated and geometry is to blame",
      "venue": "NeurIPS, pages 13567\u201313578",
      "year": 2019
    },
    {
      "authors": [
        "G. Dornhege"
      ],
      "title": "J",
      "venue": "d. R. Mill\u00e1n, T. Hinterberger, D. McFarland, K.-R. M\u00fcller, et al. Toward brain-computer interfacing, volume 63. MIT press Cambridge, MA",
      "year": 2007
    },
    {
      "authors": [
        "F. Doshi-Velez",
        "B. Kim"
      ],
      "title": "A roadmap for a rigorous science of interpretability",
      "venue": "CoRR, abs/1702.08608",
      "year": 2017
    },
    {
      "authors": [
        "E. Eidinger",
        "R. Enbar",
        "T. Hassner"
      ],
      "title": "Age and gender estimation of unfiltered faces",
      "venue": "IEEE Transactions on Information Forensics and Security, 9(12):2170\u20132179",
      "year": 2014
    },
    {
      "authors": [
        "H.J. Escalante",
        "S. Escalera",
        "I. Guyon",
        "X. Bar\u00f3",
        "Y. G\u00fc\u00e7l\u00fct\u00fcrk",
        "U. G\u00fc\u00e7l\u00fc"
      ],
      "title": "and M",
      "venue": "van Gerven. Explainable and interpretable models in computer vision and machine learning. Springer",
      "year": 2018
    },
    {
      "authors": [
        "R. Fong",
        "M. Patrick",
        "A. Vedaldi"
      ],
      "title": "Understanding deep networks via extremal perturbations and smooth masks",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2950\u20132958",
      "year": 2019
    },
    {
      "authors": [
        "R.C. Fong",
        "A. Vedaldi"
      ],
      "title": "Interpretable explanations of black boxes by meaningful perturbation",
      "venue": "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 3449\u20133457",
      "year": 2017
    },
    {
      "authors": [
        "W. Gale",
        "L. Oakden-Rayner",
        "G. Carneiro",
        "L.J. Palmer",
        "A.P. Bradley"
      ],
      "title": "Producing radiologist-quality reports for interpretable deep learning",
      "venue": "16th IEEE International Symposium on Biomedical Imaging, ISBI 2019, Venice, Italy, April 8-11, 2019, pages 1275\u20131279",
      "year": 2019
    },
    {
      "authors": [
        "J. Gama",
        "I. Zliobaite",
        "A. Bifet",
        "M. Pechenizkiy",
        "A. Bouchachia"
      ],
      "title": "A survey on concept drift adaptation",
      "venue": "ACM Comput. Surv., 46(4):44:1\u2013 44:37",
      "year": 2014
    },
    {
      "authors": [
        "S. Ghosal",
        "D. Blystone",
        "A.K. Singh",
        "B. Ganapathysubramanian",
        "A. Singh",
        "S. Sarkar"
      ],
      "title": "An explainable deep machine vision framework for plant stress phenotyping",
      "venue": "Proceedings of the National Academy of Sciences, 115(18):4613\u20134618",
      "year": 2018
    },
    {
      "authors": [
        "X. Glorot",
        "A. Bordes",
        "Y. Bengio"
      ],
      "title": "Deep sparse rectifier neural networks",
      "venue": "AISTATS, volume 15 of JMLR Proceedings, pages 315\u2013 323. JMLR.org",
      "year": 2011
    },
    {
      "authors": [
        "I. Goodfellow",
        "Y. Bengio",
        "A. Courville"
      ],
      "title": "Deep learning",
      "venue": "MIT press",
      "year": 2016
    },
    {
      "authors": [
        "I.J. Goodfellow",
        "J. Shlens",
        "C. Szegedy"
      ],
      "title": "Explaining and harnessing adversarial examples",
      "venue": "ICLR (Poster)",
      "year": 2015
    },
    {
      "authors": [
        "B. Goodman",
        "S.R. Flaxman"
      ],
      "title": "European union regulations on algorithmic decision-making and a \u201cright to explanation",
      "venue": "AI Magazine, 38(3):50\u201357",
      "year": 2017
    },
    {
      "authors": [
        "J. Gu",
        "V. Tresp"
      ],
      "title": "Contextual prediction difference analysis",
      "venue": "CoRR, abs/1910.09086",
      "year": 2019
    },
    {
      "authors": [
        "J. Gu",
        "Y. Yang",
        "V. Tresp"
      ],
      "title": "Understanding individual decisions of cnns via contrastive backpropagation",
      "venue": "ACCV (3), volume 11363 of Lecture Notes in Computer Science, pages 119\u2013134. Springer",
      "year": 2018
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "F. Turini",
        "F. Giannotti",
        "D. Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM Comput. Surv., 51(5):93:1\u201393:42",
      "year": 2019
    },
    {
      "authors": [
        "A. Gupta",
        "J. Johnson",
        "L. Fei-Fei",
        "S. Savarese",
        "A. Alahi"
      ],
      "title": "Social GAN: socially acceptable trajectories with generative adversarial networks",
      "venue": "CVPR, pages 2255\u20132264. IEEE Computer Society",
      "year": 2018
    },
    {
      "authors": [
        "M. H\u00e4gele",
        "P. Seegerer",
        "S. Lapuschkin",
        "M. Bockmayr",
        "W. Samek",
        "F. Klauschen",
        "K.-R. M\u00fcller",
        "A. Binder"
      ],
      "title": "Resolving challenges in deep learning-based analyses of histopathological images using explanation methods",
      "venue": "CoRR, abs/1908.06943",
      "year": 2019
    },
    {
      "authors": [
        "S. Haufe",
        "F.C. Meinecke",
        "K. G\u00f6rgen",
        "S. D\u00e4hne",
        "J. Haynes",
        "B. Blankertz",
        "F. Bie\u00dfmann"
      ],
      "title": "On the interpretation of weight vectors of linear models in multivariate neuroimaging",
      "venue": "NeuroImage, 87:96\u2013 110",
      "year": 2014
    },
    {
      "authors": [
        "K. He",
        "X. Zhang",
        "S. Ren",
        "J. Sun"
      ],
      "title": "Deep residual learning for image recognition",
      "venue": "CVPR, pages 770\u2013778. IEEE Computer Society",
      "year": 2016
    },
    {
      "authors": [
        "J. Heo",
        "S. Joo",
        "T. Moon"
      ],
      "title": "Fooling neural network interpretations via adversarial model manipulation",
      "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 2921\u20132932",
      "year": 2019
    },
    {
      "authors": [
        "S. Hochreiter",
        "J. Schmidhuber"
      ],
      "title": "Long short-term memory",
      "venue": "Neural computation, 9(8):1735\u20131780",
      "year": 1997
    },
    {
      "authors": [
        "J. Hochuli",
        "A. Helbling",
        "T. Skaist",
        "M. Ragoza",
        "D.R. Koes"
      ],
      "title": "Visualizing convolutional neural network protein-ligand scoring",
      "venue": "Journal of Molecular Graphics and Modelling",
      "year": 2018
    },
    {
      "authors": [
        "A. Holzinger"
      ],
      "title": "From machine learning to explainable ai",
      "venue": "2018 World Symposium on Digital Intelligence for Systems and Machines (DISA), pages 55\u201366",
      "year": 2018
    },
    {
      "authors": [
        "A. Holzinger",
        "G. Langs",
        "H. Denk",
        "K. Zatloukal",
        "H. M\u00fcller"
      ],
      "title": "Causability and explainability of artificial intelligence in medicine",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(4):e1312",
      "year": 2019
    },
    {
      "authors": [
        "S. Hong",
        "D. Yang",
        "J. Choi",
        "H. Lee"
      ],
      "title": "Interpretable Text-to-Image Synthesis with Hierarchical Semantic Layout Generation",
      "venue": "pages 77\u201395. Springer International Publishing, Cham",
      "year": 2019
    },
    {
      "authors": [
        "S. Hooker",
        "D. Erhan",
        "P.-J. Kindermans",
        "B. Kim"
      ],
      "title": "A benchmark for interpretability methods in deep neural networks",
      "venue": "Advances in Neural Information Processing Systems, pages 9734\u20139745",
      "year": 2019
    },
    {
      "authors": [
        "F. Horst",
        "S. Lapuschkin",
        "W. Samek",
        "K.-R. M\u00fcller",
        "W.I. Sch\u00f6llhorn"
      ],
      "title": "Explaining the unique nature of individual gait patterns with deep learning",
      "venue": "Scientific Reports, (9):2391",
      "year": 2019
    },
    {
      "authors": [
        "G.B. Huang",
        "M. Ramesh",
        "T. Berg",
        "E. Learned-Miller"
      ],
      "title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
      "venue": "Technical Report 07-49,",
      "year": 2007
    },
    {
      "authors": [
        "F.N. Iandola",
        "M.W. Moskewicz",
        "K. Ashraf",
        "S. Han",
        "W.J. Dally",
        "K. Keutzer"
      ],
      "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size",
      "venue": "CoRR, abs/1602.07360",
      "year": 2016
    },
    {
      "authors": [
        "B.K. Iwana",
        "R. Kuroki",
        "S. Uchida"
      ],
      "title": "Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation",
      "venue": "CoRR, abs/1908.04351",
      "year": 2019
    },
    {
      "authors": [
        "M.H. Jarrahi"
      ],
      "title": "Artificial intelligence and the future of work: Human- AI symbiosis in organizational decision making",
      "venue": "Business Horizons,",
      "year": 2018
    },
    {
      "authors": [
        "Y. Jia",
        "E. Shelhamer",
        "J. Donahue",
        "S. Karayev",
        "J. Long",
        "R. Girshick",
        "S. Guadarrama",
        "T. Darrell"
      ],
      "title": "Caffe: Convolutional architecture for fast feature embedding",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia, pages 675\u2013678",
      "year": 2014
    },
    {
      "authors": [
        "G. Katz",
        "C.W. Barrett",
        "D.L. Dill",
        "K. Julian",
        "M.J. Kochenderfer"
      ],
      "title": "Reluplex: An efficient SMT solver for verifying deep neural networks",
      "venue": "CAV (1), volume 10426 of Lecture Notes in Computer Science, pages 97\u2013117. Springer",
      "year": 2017
    },
    {
      "authors": [
        "J. Kauffmann",
        "M. Esders",
        "G. Montavon",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "From clustering to cluster explanations via neural networks",
      "venue": "CoRR, abs/1906.07633",
      "year": 2019
    },
    {
      "authors": [
        "J. Kauffmann",
        "K.-R. M\u00fcller",
        "G. Montavon"
      ],
      "title": "Towards explaining anomalies: A deep Taylor decomposition of one-class models",
      "venue": "Pattern Recognition,",
      "year": 2020
    },
    {
      "authors": [
        "D.R. Kelley",
        "Y. Reshef",
        "M. Bileschi",
        "D. Belanger",
        "C.Y. McLean",
        "J. Snoek"
      ],
      "title": "Sequential regulatory activity prediction across chromosomes with convolutional neural networks",
      "venue": "Genome research, pages gr\u2013 227819",
      "year": 2018
    },
    {
      "authors": [
        "J. Khan",
        "J.S. Wei",
        "M. Ringn\u00e9r",
        "L.H. Saal",
        "M. Ladanyi",
        "F. Westermann",
        "F. Berthold",
        "M. Schwab",
        "C.R. Antonescu",
        "C. Peterson",
        "P.S. Meltzer"
      ],
      "title": "Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks",
      "venue": "Nature Medicine,",
      "year": 2001
    },
    {
      "authors": [
        "B. Kim",
        "M. Wattenberg",
        "J. Gilmer",
        "C.J. Cai",
        "J. Wexler",
        "F.B. Vi\u00e9gas",
        "R. Sayres"
      ],
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV)",
      "venue": "ICML, volume 80 of Proceedings of Machine Learning Research, pages 2673\u20132682. PMLR",
      "year": 2018
    },
    {
      "authors": [
        "P.-J. Kindermans",
        "K.T. Sch\u00fctt",
        "M. Alber",
        "K.-R. M\u00fcller",
        "D. Erhan",
        "B. Kim",
        "S. D\u00e4hne"
      ],
      "title": "Learning how to explain neural networks: Patternnet and patternattribution",
      "venue": "6th International Conference on 23 Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings",
      "year": 2018
    },
    {
      "authors": [
        "C. Kittel"
      ],
      "title": "Introduction to solid state physics",
      "venue": "volume 8. Wiley New York",
      "year": 2004
    },
    {
      "authors": [
        "F. Klauschen",
        "K.-R. M\u00fcller",
        "A. Binder",
        "M. Bockmayr",
        "M. H\u00e4gele",
        "P. Seegerer",
        "S. Wienert",
        "G. Pruneri"
      ],
      "title": "S",
      "venue": "de Maria, S. Badve, et al. Scoring of tumor-infiltrating lymphocytes: From visual estimation to machine learning. Seminars in cancer biology, 52:151\u2013157",
      "year": 2018
    },
    {
      "authors": [
        "M. Kohlbrenner",
        "A. Bauer",
        "S. Nakajima",
        "A. Binder",
        "W. Samek",
        "S. Lapuschkin"
      ],
      "title": "Towards best practice in explaining neural network decisions with LRP",
      "venue": "CoRR, abs/1910.09840",
      "year": 2019
    },
    {
      "authors": [
        "N. Kokhlikyan",
        "V. Miglani",
        "M. Martin",
        "E. Wang",
        "J. Reynolds",
        "A. Melnikov",
        "N. Lunova",
        "O. Reblitz-Richardson"
      ],
      "title": "Pytorch captum",
      "venue": "https://github.com/pytorch/captum",
      "year": 2019
    },
    {
      "authors": [
        "K. Kourou",
        "T.P. Exarchos",
        "K.P. Exarchos",
        "M.V. Karamouzis",
        "D.I. Fotiadis"
      ],
      "title": "Machine learning applications in cancer prognosis and prediction",
      "venue": "Computational and Structural Biotechnology Journal, 13:8\u2013 17",
      "year": 2015
    },
    {
      "authors": [
        "F. Kratzert",
        "M. Herrnegger",
        "D. Klotz",
        "S. Hochreiter",
        "G. Klambauer"
      ],
      "title": "NeuralHydrology \u2013 Interpreting LSTMs in Hydrology",
      "venue": "pages 347\u2013362. Springer International Publishing, Cham",
      "year": 2019
    },
    {
      "authors": [
        "O.Z. Kraus",
        "L.J. Ba",
        "B.J. Frey"
      ],
      "title": "Classifying and segmenting microscopy images with deep multiple instance learning",
      "venue": "Bioinformatics, 32(12):52\u201359",
      "year": 2016
    },
    {
      "authors": [
        "A. Krizhevsky",
        "I. Sutskever",
        "G.E. Hinton"
      ],
      "title": "Imagenet classification with deep convolutional neural networks",
      "venue": "NIPS, pages 1106\u20131114",
      "year": 2012
    },
    {
      "authors": [
        "W. Landecker",
        "M.D. Thomure",
        "L.M.A. Bettencourt",
        "M. Mitchell",
        "G.T. Kenyon",
        "S.P. Brumby"
      ],
      "title": "Interpreting individual classifications of hierarchical networks",
      "venue": "IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013, Singapore, 16-19 April, 2013, pages 32\u201338",
      "year": 2013
    },
    {
      "authors": [
        "S. Lapuschkin",
        "A. Binder",
        "G. Montavon",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "Analyzing classifiers: Fisher vectors and deep neural networks",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2912\u20132920",
      "year": 2016
    },
    {
      "authors": [
        "S. Lapuschkin",
        "A. Binder",
        "G. Montavon",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "The layer-wise relevance propagation toolbox for artificial neural networks",
      "venue": "Journal of Machine Learning Research, 17(114):1\u20135",
      "year": 2016
    },
    {
      "authors": [
        "S. Lapuschkin",
        "A. Binder",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "Understanding and comparing deep neural networks for age and gender classification",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCVW), pages 1629\u201338",
      "year": 2017
    },
    {
      "authors": [
        "S. Lapuschkin",
        "S. W\u00e4ldchen",
        "A. Binder",
        "G. Montavon",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Unmasking Clever Hans predictors and assessing what machines really learn",
      "venue": "Nature Communications, 10(1):1096",
      "year": 2019
    },
    {
      "authors": [
        "Y. LeCun",
        "Y. Bengio",
        "G. Hinton"
      ],
      "title": "Deep learning",
      "venue": "Nature, 521(7553):436",
      "year": 2015
    },
    {
      "authors": [
        "Y.A. LeCun",
        "L. Bottou",
        "G.B. Orr",
        "K.-R. M\u00fcller"
      ],
      "title": "Efficient backprop",
      "venue": "Neural networks: Tricks of the trade, pages 9\u201348. Springer",
      "year": 2012
    },
    {
      "authors": [
        "K. Leino",
        "S. Sen",
        "A. Datta",
        "M. Fredrikson",
        "L. Li"
      ],
      "title": "Influencedirected explanations for deep convolutional networks",
      "venue": "2018 IEEE International Test Conference (ITC), pages 1\u20138. IEEE",
      "year": 2018
    },
    {
      "authors": [
        "M. Lin",
        "Q. Chen",
        "S. Yan"
      ],
      "title": "Network in network",
      "venue": "International Conference of Learning Representations (ICLR)",
      "year": 2014
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "ACM Queue, 16(3):30",
      "year": 2018
    },
    {
      "authors": [
        "Y. Liu",
        "K. Barr",
        "J. Reinitz"
      ],
      "title": "Fully interpretable deep learning model of transcriptional control",
      "venue": "bioRxiv",
      "year": 2019
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "NIPS, pages 4768\u20134777",
      "year": 2017
    },
    {
      "authors": [
        "S. Ma",
        "X. Song",
        "J. Huang"
      ],
      "title": "Supervised group lasso with applications to microarray data analysis",
      "venue": "BMC Bioinformatics, 8",
      "year": 2007
    },
    {
      "authors": [
        "J. MacDonald",
        "S. W\u00e4ldchen",
        "S. Hauch",
        "G. Kutyniok"
      ],
      "title": "A ratedistortion framework for explaining neural network decisions",
      "venue": "CoRR, abs/1905.11092",
      "year": 2019
    },
    {
      "authors": [
        "T. Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "Artificial Intelligence, 267:1\u201338",
      "year": 2019
    },
    {
      "authors": [
        "V. Mnih",
        "K. Kavukcuoglu",
        "D. Silver",
        "A.A. Rusu",
        "J. Veness",
        "M.G. Bellemare",
        "A. Graves",
        "M.A. Riedmiller",
        "A. Fidjeland",
        "G. Ostrovski",
        "S. Petersen",
        "C. Beattie",
        "A. Sadik",
        "I. Antonoglou",
        "H. King",
        "D. Kumaran",
        "D. Wierstra",
        "S. Legg",
        "D. Hassabis"
      ],
      "title": "Human-level control through deep reinforcement learning",
      "venue": "Nature, 518(7540):529\u2013533",
      "year": 2015
    },
    {
      "authors": [
        "G. Montavon"
      ],
      "title": "Gradient-based vs",
      "venue": "propagation-based explanations: An axiomatic comparison. In Explainable AI, volume 11700 of Lecture Notes in Computer Science, pages 253\u2013265. Springer",
      "year": 2019
    },
    {
      "authors": [
        "G. Montavon",
        "A. Binder",
        "S. Lapuschkin",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Layer-wise relevance propagation: An overview",
      "venue": "Explainable AI, volume 11700 of Lecture Notes in Computer Science, pages 193\u2013209. Springer",
      "year": 2019
    },
    {
      "authors": [
        "G. Montavon",
        "S. Lapuschkin",
        "A. Binder",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Explaining nonlinear classification decisions with deep Taylor decomposition",
      "venue": "Pattern Recognition, 65:211\u2013222",
      "year": 2017
    },
    {
      "authors": [
        "G. Montavon",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Methods for interpreting and understanding deep neural networks",
      "venue": "Digital Signal Processing, 73:1\u201315",
      "year": 2018
    },
    {
      "authors": [
        "G.F. Mont\u00fafar",
        "R. Pascanu",
        "K. Cho",
        "Y. Bengio"
      ],
      "title": "On the number of linear regions of deep neural networks",
      "venue": "NIPS, pages 2924\u20132932",
      "year": 2014
    },
    {
      "authors": [
        "N. Morch",
        "U. Kjems",
        "L.K. Hansen",
        "C. Svarer",
        "I. Law",
        "B. Lautrup",
        "S. Strother",
        "K. Rehm"
      ],
      "title": "Visualization of neural networks using saliency maps",
      "venue": "Proceedings of ICNN\u201995-International Conference on Neural Networks, volume 4, pages 2085\u20132090",
      "year": 1995
    },
    {
      "authors": [
        "A. Mordvintsev",
        "C. Olah"
      ],
      "title": "and M",
      "venue": "Tyka. Inceptionism: Going deeper into neural networks",
      "year": 2015
    },
    {
      "authors": [
        "K.-R. M\u00fcller",
        "S. Mika",
        "G. R\u00e4tsch",
        "K. Tsuda",
        "B. Sch\u00f6lkopf"
      ],
      "title": "An introduction to kernel-based learning algorithms",
      "venue": "IEEE transactions on neural networks, 12(2):181\u2013201",
      "year": 2001
    },
    {
      "authors": [
        "M. Narayanan",
        "E. Chen",
        "J. He",
        "B. Kim",
        "S. Gershman",
        "F. Doshi- Velez"
      ],
      "title": "How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation",
      "venue": "CoRR, abs/1802.00682",
      "year": 2018
    },
    {
      "authors": [
        "A.M. Nguyen",
        "A. Dosovitskiy",
        "J. Yosinski",
        "T. Brox",
        "J. Clune"
      ],
      "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
      "venue": "NIPS, pages 3387\u20133395",
      "year": 2016
    },
    {
      "authors": [
        "A.M. Nguyen",
        "J. Yosinski",
        "J. Clune"
      ],
      "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "venue": "CVPR, pages 427\u2013436. IEEE Computer Society",
      "year": 2015
    },
    {
      "authors": [
        "A.M. Nguyen",
        "J. Yosinski",
        "J. Clune"
      ],
      "title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks",
      "venue": "CoRR, abs/1602.03616",
      "year": 2016
    },
    {
      "authors": [
        "R. Okuta",
        "Y. Unno",
        "D. Nishino",
        "S. Hido",
        "C. Loomis"
      ],
      "title": "Cupy: A numpy-compatible library for nvidia gpu calculations",
      "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)",
      "year": 2017
    },
    {
      "authors": [
        "T.E. Oliphant"
      ],
      "title": "A guide to NumPy",
      "venue": "volume 1. Trelgol Publishing USA",
      "year": 2006
    },
    {
      "authors": [
        "G. Papadopoulos",
        "P.J. Edwards",
        "A.F. Murray"
      ],
      "title": "Confidence estimation methods for neural networks: a practical comparison",
      "venue": "IEEE Trans. Neural Networks, 12(6):1278\u20131287",
      "year": 2001
    },
    {
      "authors": [
        "Y. Park",
        "B. Kwon",
        "J. Heo",
        "X. Hu",
        "Y. Liu",
        "T. Moon"
      ],
      "title": "Estimating pm2",
      "venue": "5 concentration of the conterminous united states via interpretable convolutional neural networks. Environmental Pollution, 256:113395",
      "year": 2020
    },
    {
      "authors": [
        "V. Petsiuk",
        "A. Das",
        "K. Saenko"
      ],
      "title": "RISE: randomized input sampling for explanation of black-box models",
      "venue": "British Machine Vision Conference 2018, BMVC 2018, Northumbria University, Newcastle, UK, September 3-6, 2018, page 151",
      "year": 2018
    },
    {
      "authors": [
        "K. Preuer",
        "G. Klambauer",
        "F. Rippmann",
        "S. Hochreiter",
        "T. Unterthiner"
      ],
      "title": "Interpretable Deep Learning in Drug Discovery",
      "venue": "pages 331\u2013 345. Springer International Publishing, Cham",
      "year": 2019
    },
    {
      "authors": [
        "G. Quellec",
        "K. Charri\u00e8re",
        "Y. Boudi",
        "B. Cochener",
        "M. Lamard"
      ],
      "title": "Deep image mining for diabetic retinopathy screening",
      "venue": "Medical Image Analysis, 39:178\u2013193",
      "year": 2017
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "why should I trust you?\u201d: Explaining the predictions of any classifier",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages 1135\u20131144",
      "year": 2016
    },
    {
      "authors": [
        "L. Rieger",
        "P. Chormai",
        "G. Montavon",
        "L. Hansen",
        "K.-R. M\u00fcller"
      ],
      "title": "Structuring Neural Networks for More Explainable Predictions",
      "venue": "pages 115\u2013131. Springer",
      "year": 2019
    },
    {
      "authors": [
        "F. Rosenblatt"
      ],
      "title": "The perceptron: A probabilistic model for information storage and organization in the brain",
      "venue": "Psychological Review, 65(6):386\u2013 408",
      "year": 1958
    },
    {
      "authors": [
        "A.S. Ross",
        "M.C. Hughes",
        "F. Doshi-Velez"
      ],
      "title": "Right for the right reasons: Training differentiable models by constraining their explanations",
      "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19- 25, 2017, pages 2662\u20132670",
      "year": 2017
    },
    {
      "authors": [
        "R. Rothe",
        "R. Timofte",
        "L. Van Gool"
      ],
      "title": "Dex: Deep expectation of apparent age from a single image",
      "venue": "Proceedings of the IEEE 24 International Conference on Computer Vision Workshops, pages 10\u2013 15",
      "year": 2015
    },
    {
      "authors": [
        "O. Russakovsky",
        "J. Deng",
        "H. Su",
        "J. Krause",
        "S. Satheesh",
        "S. Ma",
        "Z. Huang",
        "A. Karpathy",
        "A. Khosla",
        "M.S. Bernstein",
        "A.C. Berg",
        "F. Li"
      ],
      "title": "Imagenet large scale visual recognition challenge",
      "venue": "International Journal of Computer Vision, 115(3):211\u2013252",
      "year": 2015
    },
    {
      "authors": [
        "W. Samek",
        "A. Binder",
        "G. Montavon",
        "S. Lapuschkin",
        "K.-R. M\u00fcller"
      ],
      "title": "Evaluating the visualization of what a deep neural network has learned",
      "venue": "IEEE transactions on neural networks and learning systems, 28(11):2660\u20132673",
      "year": 2016
    },
    {
      "authors": [
        "W. Samek",
        "G. Montavon",
        "A. Vedaldi",
        "L.K. Hansen",
        "K.-R. M\u00fcller"
      ],
      "title": "editors",
      "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, volume 11700 of Lecture Notes in Computer Science. Springer",
      "year": 2019
    },
    {
      "authors": [
        "J. Schmidhuber"
      ],
      "title": "Deep learning in neural networks: An overview",
      "venue": "Neural networks, 61:85\u2013117",
      "year": 2015
    },
    {
      "authors": [
        "B. Sch\u00f6lkopf",
        "A.J. Smola"
      ],
      "title": "Learning with Kernels: support vector machines",
      "venue": "regularization, optimization, and beyond. Adaptive computation and machine learning series. MIT Press",
      "year": 2002
    },
    {
      "authors": [
        "B. Sch\u00f6lkopf",
        "A.J. Smola",
        "K.-R. M\u00fcller"
      ],
      "title": "Nonlinear component analysis as a kernel eigenvalue problem",
      "venue": "Neural Computation, 10(5):1299\u20131319",
      "year": 1998
    },
    {
      "authors": [
        "K.T. Sch\u00fctt",
        "F. Arbabzadah",
        "S. Chmiela",
        "K.R. M\u00fcller",
        "A. Tkatchenko"
      ],
      "title": "Quantum-chemical insights from deep tensor neural networks",
      "venue": "Nature Communications, 8:13890",
      "year": 2017
    },
    {
      "authors": [
        "K.T. Sch\u00fctt",
        "M. Gastegger",
        "A. Tkatchenko",
        "K.-R. M\u00fcller"
      ],
      "title": "Quantum-chemical insights from interpretable atomistic neural networks",
      "venue": "Explainable AI, volume 11700 of Lecture Notes in Computer Science, pages 311\u2013330. Springer",
      "year": 2019
    },
    {
      "authors": [
        "R.R. Selvaraju",
        "M. Cogswell",
        "A. Das",
        "R. Vedantam",
        "D. Parikh",
        "D. Batra"
      ],
      "title": "Grad-CAM: Visual explanations from deep networks via gradient-based localization",
      "venue": "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 618\u2013626",
      "year": 2017
    },
    {
      "authors": [
        "C. Shan"
      ],
      "title": "Learning local features for age estimation on real-life faces",
      "venue": "Proceedings of the 1st ACM international workshop on Multimodal pervasive video analysis, pages 23\u201328. ACM",
      "year": 2010
    },
    {
      "authors": [
        "L.S. Shapley"
      ],
      "title": "17",
      "venue": "a value for n-person games. In Contributions to the Theory of Games (AM-28), Volume II. Princeton University Press",
      "year": 1953
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Kundaje"
      ],
      "title": "Learning important features through propagating activation differences",
      "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 3145\u20133153",
      "year": 2017
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Shcherbina",
        "A. Kundaje"
      ],
      "title": "Not just a black box: Learning important features through propagating activation differences",
      "venue": "CoRR, abs/1605.01713",
      "year": 2016
    },
    {
      "authors": [
        "A. Shrikumar",
        "J. Su",
        "A. Kundaje"
      ],
      "title": "Computationally efficient measures of internal neuron importance",
      "venue": "CoRR, abs/1807.09946",
      "year": 2018
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "ICLR (Workshop Poster)",
      "year": 2014
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Zisserman"
      ],
      "title": "Very deep convolutional networks for large-scale image recognition",
      "venue": "ICLR",
      "year": 2015
    },
    {
      "authors": [
        "D. Smilkov",
        "N. Thorat",
        "B. Kim",
        "F.B. Vi\u00e9gas",
        "M. Wattenberg"
      ],
      "title": "Smoothgrad: removing noise by adding noise",
      "venue": "CoRR, abs/1706.03825",
      "year": 2017
    },
    {
      "authors": [
        "C. Soneson",
        "S. Gerster",
        "M. Delorenzi"
      ],
      "title": "Batch effect confounding leads to strong bias in performance estimates obtained by crossvalidation",
      "venue": "PLoS ONE,",
      "year": 2014
    },
    {
      "authors": [
        "J.T. Springenberg",
        "A. Dosovitskiy",
        "T. Brox",
        "M.A. Riedmiller"
      ],
      "title": "Striving for simplicity: The all convolutional net",
      "venue": "International Conference of Learning Representations (ICLR)",
      "year": 2015
    },
    {
      "authors": [
        "I. Sturm",
        "S. Lapuschkin",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Interpretable deep neural networks for single-trial eeg classification",
      "venue": "Journal of Neuroscience Methods, 274:141\u2013145",
      "year": 2016
    },
    {
      "authors": [
        "D. Su",
        "H. Zhang",
        "H. Chen",
        "J. Yi",
        "P. Chen",
        "Y. Gao"
      ],
      "title": "Is robustness the cost of accuracy? - A comprehensive study on the robustness of 18 deep image classification models",
      "venue": "ECCV (12), volume 11216 of Lecture Notes in Computer Science, pages 644\u2013661. Springer",
      "year": 2018
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Taly",
        "Q. Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 3319\u20133328",
      "year": 2017
    },
    {
      "authors": [
        "W.R. Swartout",
        "J.D. Moore"
      ],
      "title": "Explanation in second generation expert systems",
      "venue": "Second Generation Expert Systems, pages 543\u2013585. Springer Berlin Heidelberg",
      "year": 1993
    },
    {
      "authors": [
        "C. Szegedy",
        "W. Zaremba",
        "I. Sutskever",
        "J. Bruna",
        "D. Erhan",
        "I.J. Goodfellow",
        "R. Fergus"
      ],
      "title": "Intriguing properties of neural networks",
      "venue": "ICLR (Poster)",
      "year": 2014
    },
    {
      "authors": [
        "A.W. Thomas",
        "H.R. Heekeren",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "Interpretable LSTMs for whole-brain neuroimaging analyses",
      "venue": "CoRR, abs/1810.09945",
      "year": 2018
    },
    {
      "authors": [
        "A.W. Thomas",
        "H.R. Heekeren",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "Analyzing neuroimaging data through recurrent deep learning models",
      "venue": "Frontiers in Neuroscience, 13:1321",
      "year": 2019
    },
    {
      "authors": [
        "H. Traunm\u00fcller",
        "A. Eriksson"
      ],
      "title": "The frequency range of the voice fundamental in the speech of male and female adults",
      "venue": "Unpublished manuscript",
      "year": 1995
    },
    {
      "authors": [
        "M. Tsang",
        "D. Cheng",
        "Y. Liu"
      ],
      "title": "Detecting statistical interactions from neural network weights",
      "venue": "ICLR (Poster). OpenReview.net",
      "year": 2018
    },
    {
      "authors": [
        "B. Ustun",
        "A. Spangher",
        "Y. Liu"
      ],
      "title": "Actionable recourse in linear classification",
      "venue": "FAT, pages 10\u201319. ACM",
      "year": 2019
    },
    {
      "authors": [
        "V. Vapnik"
      ],
      "title": "The nature of statistical learning theory",
      "venue": "Springer",
      "year": 1995
    },
    {
      "authors": [
        "M.M.-C. Vidovic",
        "N. G\u00f6rnitz",
        "K.-R. M\u00fcller",
        "G. R\u00e4tsch",
        "M. Kloft"
      ],
      "title": "Opening the black box: Revealing interpretable sequence motifs in kernel-based learning algorithms",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 137\u2013 153. Springer",
      "year": 2015
    },
    {
      "authors": [
        "C. Von der Malsburg"
      ],
      "title": "Binding in models of perception and brain function",
      "venue": "Current opinion in neurobiology,",
      "year": 1995
    },
    {
      "authors": [
        "A. Warnecke",
        "D. Arp",
        "C. Wressnegger",
        "K. Rieck"
      ],
      "title": "Don\u2019t paint it black: White-box explanations for deep learning in computer security",
      "venue": "CoRR, abs/1906.02108",
      "year": 2019
    },
    {
      "authors": [
        "C.K. Williams",
        "C.E. Rasmussen"
      ],
      "title": "Gaussian processes for machine learning",
      "venue": "MIT press Cambridge, MA",
      "year": 2006
    },
    {
      "authors": [
        "Y. Yang",
        "V. Tresp",
        "M. Wunderle",
        "P.A. Fasching"
      ],
      "title": "Explaining therapy predictions with layer-wise relevance propagation in neural networks",
      "venue": "IEEE International Conference on Healthcare Informatics, ICHI 2018, New York City, NY, USA, June 4-7, 2018, pages 152\u2013162",
      "year": 2018
    },
    {
      "authors": [
        "I.-C. Yeh"
      ],
      "title": "Modeling of strength of high-performance concrete using artificial neural networks",
      "venue": "Cement and Concrete Research,",
      "year": 1998
    },
    {
      "authors": [
        "K. Young",
        "G. Booth",
        "B. Simpson",
        "R. Dutton",
        "S. Shrapnel"
      ],
      "title": "Deep neural network or dermatologist? In Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support",
      "venue": "pages 48\u201355. Springer",
      "year": 2019
    },
    {
      "authors": [
        "T. Zahavy",
        "N. Ben-Zrihem",
        "S. Mannor"
      ],
      "title": "Graying the black box: Understanding dqns",
      "venue": "ICML, volume 48 of JMLR Workshop and Conference Proceedings, pages 1899\u20131908. JMLR.org",
      "year": 2016
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "European Conference Computer Vision - ECCV 2014, pages 818\u2013833",
      "year": 2014
    },
    {
      "authors": [
        "J. Zhang",
        "Z. Bargal"
      ],
      "title": "Sarah Adeland Lin",
      "venue": "J. Brandt, X. Shen, and S. Sclaroff. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126(10):1084\u20131102",
      "year": 2018
    },
    {
      "authors": [
        "Z. Zhang",
        "P. Chen",
        "M. McGough",
        "F. Xing",
        "C. Wang",
        "M. Bui",
        "Y. Xie",
        "M. Sapkota",
        "L. Cui",
        "J. Dhillon"
      ],
      "title": "et al",
      "venue": "Pathologist-level interpretable whole-slide cancer diagnosis with deep learning. Nature Machine Intelligence, 1(5):236\u2013245",
      "year": 2019
    },
    {
      "authors": [
        "B. Zhou",
        "D. Bau",
        "A. Oliva",
        "A. Torralba"
      ],
      "title": "Interpreting deep visual representations via network dissection",
      "venue": "IEEE transactions on pattern analysis and machine intelligence, 41(9):2131\u20132145",
      "year": 2018
    },
    {
      "authors": [
        "B. Zhou",
        "A. Khosla",
        "\u00c0. Lapedriza",
        "A. Oliva",
        "A. Torralba"
      ],
      "title": "Learning deep features for discriminative localization",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2921\u20132929",
      "year": 2016
    },
    {
      "authors": [
        "X.-P. Zhu",
        "J.-M. Dai",
        "C.-J. Bian",
        "Y. Chen",
        "S. Chen",
        "C. Hu"
      ],
      "title": "Galaxy morphology classification with deep convolutional neural networks",
      "venue": "Astrophysics and Space Science, 364(4):55",
      "year": 2019
    },
    {
      "authors": [
        "L.M. Zintgraf",
        "T.S. Cohen",
        "T. Adel",
        "M. Welling"
      ],
      "title": "Visualizing deep neural network decisions: Prediction difference analysis",
      "venue": "International Conference on Learning Representations (ICLR), 2017",
      "year": 2017
    }
  ],
  "sections": [
    {
      "text": "Index Terms\u2014Interpretability, deep learning, neural networks, black box models, explainable artificial intelligence, model transparency, kernel methods, LSTMs\nI. INTRODUCTION\nA main goal of machine learning is to learn accurate decision systems respectively predictors that can help automatizing tasks, that would otherwise have to be done by humans. Machine Learning (ML) has supplied a wealth of algorithms that have demonstrated important successes in the sciences and industry; most popular ML work horses are considered to be kernel methods (e.g. [150], [126], [104], [125], [154]) and particularly during the last decade deep learning methods (e.g. [21], [43], [87], [86], [124], [54]) have gained highest popularity.\nAs ML is increasingly used in real-world applications, a general consensus has emerged that high prediction accuracy alone may not be sufficient in practice [85], [24], [123]. Instead, in practical engineering of systems, critical features that are typically considered beyond excellent prediction itself\n\u2020 W. Samek and G. Montavon contributed equally to this work. \u2217 Coresponding authors: W. Samek, G. Montavon and K.-R. Mu\u0308ller. W. Samek and S. Lapuschkin are with Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany (e-mail: wojciech.samek@hhi.fraunhofer.de; sebastian.lapuschkin@hhi.fraunhofer.de)\nG. Montavon and C. Anders are with the Dept. of Computer Science and Electrical Engineering, Technische Universita\u0308t Berlin, 10587 Berlin, Germany (e-mail: gregoire.montavon@tu-berlin.de; christopher.anders@campus.tuberlin.de).\nK.-R. Mu\u0308ller is with the Dept. of Computer Science and Electrical Engineering, Technische Universita\u0308t Berlin, 10587 Berlin, Germany, and also with the Dept. of Brain and Cognitive Engineering, Korea University, Seoul 136-713, South Korea and Max Planck Institute for Informatics, 66123 Saarbru\u0308cken, Germany (e-mail: klaus-robert.mueller@tu-berlin.de).\nare (a) robustness of the system to measurement artefacts or adversarial perturbations [141], (b) resilience to drifting data distributions [40], (c) ability to accurately assess the confidence of its own predictions [111], [107], (d) safety and security aspects [19], [66], [23], [153], (e) legal requirements or adherence to social norms [45], [49], (f) ability to complement human expertise in decision making [64], or (g) ability to reveal to the user the interesting correlations it has found in the data [70], [127].\nOrthogonal to the quest for better and more holistic machine learning models, interpretable ML [123], [56], [90], [100], [162], [48], [16], [12] has developed as a subfield of machine learning that seeks to augment the training process, the learned representations and the decisions with human-interpretable explanations. An example is medical diagnosis, where the input examples (e.g. histopathological images) come with various artifacts (e.g. stemming from image quality or suboptimal annotations) that have in principle nothing to do with the diagnostic task, yet, due to the limited amount of available data, the ML model may harvest specifically these spurious correlations with the prediction target (e.g. [50], [138]). Here interpretability could point at anomalous or awkward decision behavior before harm is caused in a later usage as a diagnostic tool.\nSimilarly essential when using ML in the sciences is again interpretabilty, since ideally, the transparent ML model \u2014 having learned from data \u2014 may have embodied scientific knowledge that would subsequently provide insight to the scientist, occasionally this can even be novel scientific insight (see e.g. [127]). \u2014 Note that in numerous scientific applications it has been most common so far to use linear models [118], favoring interpretabilty often at the expense of predictivity (see e.g. [51], [93]).\nTo summarize, there is a strong push toward better understanding ML systems that are being used and in consequence blackbox algorithms are more and more abandoned for many applications. This growing consensus has led to a strong growth of a subfield of ML, namely explainable AI (XAI) that strives to produce transparent nonlinear learning methods, and supplies novel theoretical perspectives on machine learning models, along with powerful practical tools for a better understanding and interpretation of AI systems.\nIn this review paper, we will summarize the recent exciting developments, present different classes of XAI methods, provide theoretical insights, and highlight the current best practices when applying interpretability. Note finally, that we do not attempt an encyclopedic treatment of all available XAI\nar X\niv :2\n00 3.\n07 63\n1v 1\n[ cs\n.L G\n] 1\n7 M\nar 2\n02 0\n2 literature, rather, we present a slightly biased point of view illustrating the main ideas (and in doing so we often draw from the work of the authors) and providing \u2014 to the best of our knowledge \u2014 reference to related work for further reading."
    },
    {
      "heading": "II. TOWARDS EXPLAINING DEEP NEURAL NETWORKS",
      "text": "To introduce basic concepts of interpretable machine learning, in particular, what is an explanation, and how to produce it, we will consider as a starting point a fairly general class of machine learning models. The model will be assumed to have been fully trained and its prediction behavior to be describable in an abstract manner by a function\nf : Rd \u2192 R.\nThis function receives as input a vector of real-valued features x = (x1, . . . , xd) typically corresponding to various sensor measurements. The function produces as an output a realvalued score on which the decision is based. Classification results are then obtained by verifying whether the output is above a certain threshold or larger than the output of other functions representing the remaining classes. The function output can be interpreted as the amount of evidence for / against deciding in favor of a certain class. A sketch of such function receiving two features x1 and x2 as input is given in Fig. 1.\nIn a medical scenario, the function may receive as input an array of clinical variables, and the output of the function may be a prediction of the medical condition [77]. In an engineering setting, the input could be the composition of some compound material, and the output could be a prediction of its strength [156] or stability.\nSuppose a given instance is predicted by the machine learning model to be healthy, or a compound material is predicted to have high strength. We may choose to trust the prediction and go ahead with next step within an application scenario. However, we may benefit from taking a closer look at that prediction, e.g. to verify that the prediction \u2018healthy\u2019 is associated to relevant clinical information, and not some spurious features that accidentally correlate with the predicted quantity in the dataset [82], [85]. Such problem can often be identified by building an explanation of the ML prediction [85].\nConversely, suppose that another instance is predicted by the machine learning model to be of low health or low strength. Here, an explanation could prove equally useful as it could hint at actions to be taken on the sample to improve its predicted\nscore [149], e.g. possible therapies in a medical setting, or small adjustments of the compound design that lead to higher strength."
    },
    {
      "heading": "A. How to Explain: Global vs. Local",
      "text": "Numerous approaches have emerged to shed light onto machine learning predictions. Certain approaches such as activation-maximization [135], [108], [106] aim at a global interpretation of the model, by identifying prototypical cases for the output quantity x? = argmaxx f(x) and allowing in principle to verify that the function has a high value only for the valid cases. While these prototypical cases may be interesting per se, both for model validation or knowledge discovery, such prototypes will be of little use to understand for a given example x (say, near the decision boundary) what features play in favor or against the model output f(x)\nSpecifically, we would like to know for that very example what input features contribute positively or negatively to the given prediction. These local analyses of the decision function have received growing attention and many approaches have been proposed [14], [159], [13], [116], [142]. For simple models with limited nonlinearity, the decision function can be approximated locally as the linear function [13]:\nf(x) \u2248 d\u2211 i=1 [\u2207f(x\u0303)]i \u00b7 (xi \u2212 x\u0303i)\ufe38 \ufe37\ufe37 \ufe38 Ri\n(1)\nwhere x\u0303 is some nearby root point (cf. Fig. 1). This expansion takes the form of a weighted sum over the input features, where the summand Ri can be interpreted as the contribution of feature i to the prediction. Specifically, an inspection of the summands reveals that a feature xi will be attributed strong relevance if the following two conditions are met: (1) the feature must be expressed in the data, i.e. it differs from the reference value x\u0303i, and (2) the model output should be sensitive to the presence of that feature, i.e. [\u2207f(x\u0303)]i 6= 0. An explanation for the prediction can then be formed by the vector of relevance scores (Ri)i. It can be given to the user as a histogram over the input features or as a heatmap.\nFor illustration, consider the problem of explaining a prediction for a data point from the Concrete Compressive Strength Data Set [156]. For this data point, a simple two-layer neural network model predicts a low compressive strength. Applying the analysis above gives an explanation for this prediction, which we show in Fig. 2.\n3 For this example low cement concentration and below average age are factors of low compressive strength, although this is partly compensated by a high quantity of blast furnace slag.\nFurthermore, for an explanation to be interpretable by its receiver, the latter must be able to make sense of the input features. Some features such as \u2018cement\u2019, \u2018water\u2019, and \u2018age\u2019, are understandable to everyone, however, more technical terms such as \u2018blast furnace slag\u2019 or \u2018superplaticizer\u2019 may only be accessible to a domain expert. Therefore, when using these explanation techniques, we make the implicit assumption that those input features are interpretable to the receiver."
    },
    {
      "heading": "B. Deep Networks and the Difficulty of Explaining Them",
      "text": "In practice, linear models or shallow neural networks may not be sufficiently expressive to predict the task optimally. Deep neural networks have been proposed as a way of producing more predictive models. They can be abstracted as a sequence of layers\nf(x) = fL \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f1(x),\nwhere each layer applies a linear transformation followed by an element-wise nonlinearity. Combining a large number of these layers endows the model with high prediction power. DNNs have proven especially successful on computer vision tasks [80], [136], [52]. However, DNN models are also much more complex and nonlinear, and quantities entering into the simple explanation model of Eq. (1) become considerably harder to compute and to estimate reliably.\nA first difficulty comes from the multiscale and distributed nature of neural network representations. Some neurons are activated for only a few data points, whereas others apply more globally. The prediction is thus a sum of local and global effects, which makes it difficult (or impossible) to find a root point x\u0303 that linearly expands to the prediction for the data point of interest. The transition from the global to local effect indeed introduces a nonlinearity, which Eq. (1) cannot capture.\nA second source of instability arises from the high depth of recent neural networks, where a \u2018shattered gradient\u2019 effect was observed [15], noting that the gradient locally resembles white noise. In particular, it can be shown that for deep rectifier networks, the number of discontinuities of the gradient can grow in the worst case exponentially with depth [101]. The shattered gradient effect is illustrated in Fig. 3 (left) for the well-established VGG-16 network [136]: The network is fed multiple consecutive video frames of an athlete lifting a barbell, and we observe the prediction for the output neuron \u2018barbell\u2019. The gradient of the prediction is changing its value much more quickly than the prediction itself. An explanation based on such gradient would therefore inherit this noise.\nA last difficulty comes from the challenge of searching for a root point x\u0303 on which to base the explanation, that is both close to the data and not an adversarial example [44], [107]. The problem is illustrated in Fig. 3 (right), where we showcase a reference point x\u0303 that does not carry any meaningful visual difference to the original data x, but for which function output has changed dramatically. The problem\n...\nFig. 3. Two difficulties encountered with DNNs. Left: Shattered gradient effect causing gradients to be highly varying and too noisy to be used for explanation. Right: Pathological minima in the function, making it difficult to search for meaningful reference points.\nof adversarial examples can be explained by the gradient noise, that causes the model to \u2018overreact\u2019 to certain pixel-wise perturbations, and also by the high dimensionality of the data (224 \u00d7 224 = 50176 pixels for the ImageNet dataset) where many small pixel-wise effects cumulate into a large effect on the model output."
    },
    {
      "heading": "III. PRACTICAL METHODS FOR EXPLAINING DNNS",
      "text": "In view of the multiple challenges posed by analyzing deep neural network functions, building robust and practical methods to explain their decisions has developed into an own research area [100], [48], [123] and an abundance of methods have been proposed. In parallel, efficient software (cf. Appendix C for a list) makes these newly developed methods readily usable in practice, and allows researchers to perform systematic comparisons between them on reference models and datasets.\nIn this section, we focus on four families of explanation techniques: Interpretable Local Surrogates, Occlusion Analysis, Gradient-based techniques, and Layer-Wise Relevance Propagation. In our view, these techniques exemplify the current diversity of possible approaches to explaining predictions in terms of input features, and taken together provide a broad coverage of the types of models to explain and the practical use cases. We give references to further related methods in the corresponding subsections. Table II in Appendix C provides a glossary of all referenced methods.\nA. Interpretable Local Surrogates [116] This category of methods aims to replace the decision function by a local surrogate model that is structured in a way that it is self-explanatory (an example of a self-explanatory model is the linear model). This approach is embodied in the LIME algorithm [116], which was successfully applied to DNN classifiers for images and text. Explanation can be achieved by first defining some local distribution px(\u03be) around our data point x, learning the parameter v of the linear model that best matches the function locally:\nmin v\n\u222b [ f(\u03be)\u2212 v>\u03be ]2 \u00b7 dpx(\u03be)\n4 and then extracting local feature contributions, e.g. Ri = vixi. Because the method does not rely on the gradient of the original DNN model, it avoids some of the difficulties discussed in Section II-B. The LIME method also covers the incorporation of sparsity or simple decision tree to the surrogate model to further enhance interpretability. Additionally, the learned surrogate model may be based on its own set of interpretable features, allowing to produce explanations in terms of features that are maximally interpretable to the human. Interpretable structures are also contained in much more complex models. For example, the CAM architecture [163] is formed by a sequence of convolutional layers followed by a top-level Global Average Pooling [89] that aggregates class features at various spatial locations. Relevant spatial locations are then readily given by the activations that enter into this top-level pooling layer."
    },
    {
      "heading": "B. Occlusion Analysis [159]",
      "text": "Occlusion Analysis is a particular type of perturbation analysis where we repeatedly test the effect on the neural network output, of occluding patches or individual features in the input image [159], [165]:\nRi = f(x)\u2212 f(x (1\u2212mi))\nwhere mi is an indicator function for the patch or feature to remove, and \u2018 \u2019 denotes the element-wise product. A heatmap (Ri)i can be built from these scores highlighting locations where the occlusion has caused the strongest decrease of the function. Because occlusion may produce visual artefacts, inpainting occluded patterns (e.g. using a generative model [2]) rather than setting them to gray was proposed as an enhancement. A further extension of occlusion analysis is Meaningful Perturbation [38], where an occluding pattern is synthesized, subject to a sparsity constraint, in order to engender the maximum drop of the function value f . The explanation is then readily given by the synthesized pattern. The perturbation-based approach was latter embedded in a rate distortion theoretical framework [94].\nC. Integrated Gradients / SmoothGrad [142], [137]\nThe first method we consider is Integrated Gradients [142]. It explains by integrating the gradient \u2207f(x) along some trajectory in input space connecting some root point x\u0303 to the data point x. The integration process addresses the problem of locality of the gradient information (cf. II-B), making it wellsuited for explaining functions that have multiple scales. In the simplest form, the trajectory is chosen to be the segment [x\u0303,x] connecting some root point to the data. Integrated gradients defines feature-wise scores as:\nRi(x) = (xi \u2212 x\u0303i) \u00b7 \u222b 1 0 [\u2207f(x\u0303+ t \u00b7 (x\u2212 x\u0303))]i dt\nIt can be shown that these scores satisfy \u2211 iRi(x) = f(x) and thus constitute a complete explanation. If necessary, the method can be easily extended to any trajectories in input space. For implementation purposes, integrated gradients must\nbe discretized. Specifically, the continuous trajectory is approximated by a sequence of data points x(1), . . . ,x(N). Integrated gradients is then implemented as shown in Algorithm 1.\nAlgorithm 1 Integrated Gradients R = 0 for n = 1 to N \u2212 1 do R = R+\u2207f(x(n)) (x(n+1) \u2212 x(n))\nend for return R\nThe gradient can easily be computed using automatic differentiation. The operation \u2018 \u2019 denotes the element-wise product. The larger the number of discretization steps, the closer the output gets to the integral form, but the more computationally expensive the procedure gets.\nAnother popular gradient-based explanation method is SmoothGrad [137]. The function\u2019s gradient is averaged over a large number of locations corresponding to small random perturbations of the original data point x:\n\u2207smoothf(x) = E\u03b5\u223cN (0,\u03c32I)[\u2207f(x+ \u03b5)]\nLike the method\u2019s name suggests, the averaging process \u2018smoothes\u2019 the explanation, and in turn also addresses the shattered gradient problem described in Section II-B. (See also [102], [14], [135] for earlier gradient-based explanation techniques).\nIn Section IV, we experiment with a combination of Integrated Gradients and SmoothGrad [137], where relevance scores obtained from Integrated Gradients are averaged over several integration paths that are drawn from some random distribution. The resulting method preserves the advantages of Integrated Gradients and further reduces the gradient noise."
    },
    {
      "heading": "D. Layer-Wise Relevance Propagation [13]",
      "text": "The Layer-wise Relevance Propagation (LRP) method [13] makes explicit use of the layered structure of the neural network and operates in an iterative manner to produce the explanation. Consider the neural network\nf(x) = fL \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f1(x)\nFirst, activations at each layer of the neural network are computed until we reach the output layer. The activation score in the output layer forms the prediction. Then, a reverse propagation pass is applied, where the output score is progressively redistributed, layer after layer, until the input variables are reached. The redistribution process follows a conservation principle analogous to Kirchoff\u2019s laws in electrical circuits. Specifically, all \u2018relevance\u2019 that flows into a neuron at a given layer flows out towards the neurons of the layer below. At a high-level, the LRP procedure can be implemented as a forward-backward loop, as shown in Algorithm 2.\nThe function relprop performs redistribution from one layer to the layer below and is based on \u2018propagation rules\u2019 defining the exact redistribution policy. Examples of propagation rules are given later in this section, and their implementation is provided in Appendix B. The LRP procedure is shown graphically in Fig. 4.\n5 Algorithm 2 Layer-wise Relevance Propagation a(0) = x for l = 1 . . . L do a(l) = fl(a\n(l\u22121)) end for R(L) = a(L) for l = L . . . 1 do R(l\u22121) = relprop(a(l\u22121),R(l), fl) end for return R(0)\nLRP-\u03f5Box LRP-\u03b3 LRP-0\nRj\nR = (Ri)i\nRkRj\u2190k\nFig. 4. Illustration of the LRP propagation procedure applied to a neural network. The prediction at the output is propagated backward in the network, using various propagation rules, until the input features are reached. The propagation flow is shown in red.\nWhile LRP can in principle be performed in any forward computational graph, a class of neural networks which is often encountered in practice, and for which LRP comes with efficient propagation rules that can be theoretically justified (cf. Section V) is deep rectifier networks [42]. The latter can be in large part abstracted as an interconnection of neurons of the type:\nak = max ( 0 , \u2211 0,j ajwjk ) ,\nwhere aj denotes some input activation, and wjk is the weight connecting neuron j to neuron k in the layer above. The notation \u2211 0,j indicates that we sum over all neurons j in the lower layer plus a bias term w0k with a0 = 1. For this class of networks, various propagation rules have been proposed (cf. Fig. 4). For example, the LRP-\u03b3 rule [98] defined as\nRj = \u2211 k aj(wjk + \u03b3w + jk)\u2211 0,j aj(wjk + \u03b3w + jk) Rk (2)\nredistributes based on the contribution of lower-layer neurons to the given neuron activation, with a preference for positive contributions over negative contributions. This makes it particularly robust and suitable for the lower-layer convolutions. Other propagation rules such as LRP- or LRP-0 are suitable for other layers [98]. Additional propagation rules have been\nproposed for special layers such as min/max pooling [13], [99], [68] and LSTM blocks [11], [9]. Furthermore, a number of other propagation techniques have been proposed [133], [132], [81], [160] with some of the rules overlapping with LRP for certain choices of parameters. For a technical overview of LRP including a discussion of the various propagation rules and further recent heuristics, see [98].\nAn inspection of Eq. (2) shows an important property of LRP, that of conserving relevance from layer to layer, in particular, we can show that in absence of bias terms,\u2211 j Rj = \u2211 k Rk. A further interesting property of this propagation rule is \u2018smoothing\u2019. Consider relevance can be written as Rj = ajcj and Rk = akck a product of activations and factors. Those factors can be directly related by the equation\ncj = \u2211 k (wjk + \u03b3w + jk)\nmax(0, \u2211\n0,j ajwjk)\u2211 0,j aj(wjk + \u03b3w + jk) ck. (3)\nThis equation can be interpreted as a smooth variant of the chain rule for derivatives used for computing the neural network gradient [97]. Thus, analogous to SmoothGrad [137], LRP also performs some gradient smoothing, however, it embeds it tightly into the deep architecture, so that only a single backward pass is required. In addition to smoothing, Eq. (3) can also be interpreted as a gradient that has been biased to positive values, an idea also found in methods such as DeconvNet [159] or Guided Backprop [139]. This modified gradient view on LRP can also be leveraged to achieve a simpler and more general implementation of LRP based on \u2018forward hooks\u2019, which we describe in the second part of Appendix B, and which we use to apply LRP on VGG-16 [136] and ResNet-50 [52] in Section IV."
    },
    {
      "heading": "IV. COMPARING EXPLANATION METHODS",
      "text": "The methods presented in Section III highlight the variety of approaches available for attributing the prediction of a deep neural network to its input features. This variety of techniques also translates into a variety of qualities of explanations. Illustrative examples of images and the explanation of predicted evidence for the ground truth class as produced by the different explanation methods are shown in Fig. 5. Occlusion Analysis is performed by occluding patches of size 32\u00d7 32 pixels with stride 16. Integrated Gradients performs 5 integration steps starting from 5 random points near the origin in order to add smoothing (cf. Appendix A), resulting in 25 function evaluations. LRP explanations are obtained by applying the same LRP rules as in [98]. We observe the following qualitative properties of the explanations: Occlusion-based explanations are coarse and are indicating relevant regions rather than the relevant pixel features. Integrated Gradients produces very fine pixel-wise explanations containing both substantial amount of evidence in favor and against the prediction (red and blue pixels). LRP preserves the fine explanation structure but tends to produce less negative scores and attributes relevance to whole features rather than individual pixels.\nIn practice, it is important to reach an objective assessment of how good an explanation is. Unfortunately, evaluating\n6\nexplanations is made difficult by the fact that it is generally impossible to collect \u2018ground truth\u2019 explanations. Building such ground truth explanations would indeed require the expert to understand how the deep neural network decides.\nStandard machine learning models are usually evaluated by the utility (expected risk) of their decision behavior (e.g. [150]). Transposing this concept of maximizing utility to the domain of explanation, quantifying the utility of the explanation would first require to define what is the ultimate target task (the explanation being only an intermediate step), and then assessing by how much the use of explanation by the human increases its performance on the target task, compared to not using it (see e.g. [14], [34], [123]). Because such endto-end evaluation schemes are hard to set up in practice, general desiderata for ML explanations have been proposed [143], [95]. Common ones include (1) faithfulness (2) humaninterpretability, and (3) possibility to practically apply it to an ML model or an ML task (e.g. algorithmic efficiency of the explanation algorithm)."
    },
    {
      "heading": "A. Faithfulness",
      "text": "Faithfulness is a property of the explanation to reliably and comprehensively represent the decision structure of the analyzed ML model. A practical technique to quantify faithfulness is \u2018pixel-flipping\u2019 [122]. The pixel-flipping procedure tests the faithfulness of an explanation by verifying whether removing the features highlighted by the explanation (as most relevant) leads to a strong decay of the network prediction abilities. The procedure is summarized in Algorithm 3.\nPixel-flipping runs from the most to the least relevant input features, iteratively removing them and monitoring the evolution of the neural network output. The series of recorded decaying prediction scores can be plotted as a curve. The faster the curve decreases, the more faithful the explanation method is w.r.t. the decision of the neural network. The pixel-flipping\nAlgorithm 3 Pixel-Flipping pfcurve = [ ] for p in argsort(\u2212R) do x\u2190 x\u2212 {xp} (remove pixel p from the image). pfcurve.append(f(x)).\nend for return pfcurve\ncurve can be computed for a single example, or averaged over a whole dataset in order to get a global estimate of the faithfulness of an explanation algorithm under study.\nFig. 6 applies pixel-flipping to the three considered explanation methods and on two models: VGG-16 [136] and ResNet-50 [52]. At each step of pixel-flipping, removed pixels are imputed using a simple inpainting algorithm, which avoids introducing visual artefacts in the image.\nWe observe that for all explanation methods, removing relevant features quickly destroys class evidence. In particular, they perform much better than a random explanation baseline. Fine differences can however be observed between the methods: For example, LRP performs better on VGG-16 than on ResNet-50. This can be explained by VGG-16 having a more explicit structure (standard pooling operations for VGG-16 vs. strided convolution for ResNet-50), which better supports the process of relevance propagation (see also [117] for a discussion of the effect of structure on the performance of explanation methods).\nA second observation in Fig. 6 is that Integrated Gradients has by far the highest decay rate initially but stagnates in the later phase of the pixel-flipping procedure. The reason for this effect is that IG focuses on pixels to which the network is the most sensitive, without however being able to identify fully comprehensively the relevant pattern in the image. This effect is illustrated in Fig. 6 (middle) on a zoomed-in exemplary image of class \u2018greater swiss mountain dog\u2019, where the image after 10% flipping has lost most of its prediction score, but visually appears almost intact. Effectively, IG has built an\n7 adversarial example [144], [107], i.e. an example whose visual content clearly disagrees with the prediction at the output of the network. We note that Occlusion and LRP do not run into such adversarial examples. For these methods, pixelflipping steadily and comprehensively removes features until class evidence has totally disappeared.\nOverall, the pixel-flipping algorithm characterizes various aspects of the faithfulness of an explanation method. We note however that faithfulness of an explanation does not tell us how easy it will be for a human to make sense of that explanation. We address this other key requirement of an explanation in the following section."
    },
    {
      "heading": "B. Human Interpretability",
      "text": "Here, we discuss whether the presented explanation techniques deliver results that are meaningful to the human, i.e. whether the human can gain understanding into the classifier\u2019s decision strategy from the explanation. Human interpretability is hard to define in general [95]. Different users may have different capabilities at reading explanations and at making sense of the features that support them [116], [105]. For example, the layman may wish for a visual interpretation, even approximate, whereas the expert may prefer an explanation supported by a larger vocabulary, including precise scientific or technical terms [14].\nFor the image classification setting, interpretability can be quantified in terms of the amount of information contained in the heatmap (e.g. as measured by the file size). An explanation with a small associated file size is more likely to be interpretable by a human. The table below shows average file sizes (in bytes1) associated to the various explanation techniques and for two neural networks.\nOcc IG LRP\nVGG-16 698.4 5795.0 1828.3 ResNet-50 693.6 5978.0 2928.2\nWe observe that occlusion produces the lowest file size and is therefore the most \u2018interpretable\u2019. It indeed only presents to the user rough localization information without going into the details of which exact feature has supported the decision as done e.g. by LRP. On the other side of the interpretability spectrum we find Integrated Gradients. In the explanations this last method produces, every single pixel contains information, and this makes it clearly overwhelming to the human.\nIn practice, neural networks do not need to be explained in terms of input features. For example, the TCAV method [71] considers directional derivatives in the space of activations (where the directions correspond to higher-level humaninterpretable concepts) in place of the input gradient. Similar higher-level interpretations are also possible using the Occlusion and LRP methods, respectively by perturbing groups of activations corresponding at a given layer to a certain concept, or by stopping the LRP procedure at the same layer and pooling scores on some group of neurons representing the desired concept.\n1JPEG compression using the Pillow image processing library for python with a quality setting of 75/100 (standard settings)."
    },
    {
      "heading": "C. Applicability and Runtime",
      "text": "Faithfulness and interpretability do not fully characterize the overall usefulness of an explanation method. To characterize usefulness, we also need to determine whether the explanation method is applicable to a range models that is sufficient large to include the neural network model of interest, and whether explanations can be obtained quickly with finite compute resources.\nOcclusion-based explanations are the easiest to implement. These explanations can be obtained for any neural network even those that are not differentiable. This also includes networks for which we do not have the source code and where we can only access their prediction through some online server. Technically, occlusion can therefore be used to understand the predictions of third-party models such as https://cloud.google.com/vision/ and https://www.clarifai.com/models. Integrated gradients requires instead for each prediction an access to the neural network gradient. Given that most machine learning models are differentiable, this method is widely applicable also for neural networks with complex structures, such as ResNets [52] or SqueezeNets [62]. Integrated Gradients is also easily implemented in state-of-the-art ML frameworks such as PyTorch or TensorFlow, where we can make use of automatic differentiation. Layer-wise Relevance Propagation assumes that the model is structured as (or can be converted to [67], [68]) a neural network with a canonical sequence of layers, for example, an alternation of linear/convolution layers, ReLU layers, and pooling layers. This stronger requirement and the implementation overhead caused by explicitly accessing the different layers (cf. Appendix B) will however be offset by a last characteristic we consider in this section, which is the computational cost associated producing the explanation. A runtime comparison2 of the three explanation methods studied here is given in the table below (measured in explanations per second).\nOcc IG LRP\nVGG-16 2.4 5.8 204.1 ResNet-50 4.0 8.7 188.7\nOcclusion is the slowest method as it requires to reevaluate the function for each occluded patch. For image data, the runtime of Occlusion increases quadratically with the step size, making the obtainment of high-resolution explanations with this method computationally prohibitive. Integrated Gradients inherits pixel-wise resolution from the gradient computation which is O(1) but requires multiple iterations for the integration. The runtime is further increased if performing an additional loop of smoothing. LRP is the fastest method in our benchmark by an order of magnitude. The LRP runtime is only approximately three times higher than that of computing a single forward pass. This makes LRP particularly convenient for the large-scale analyses we introduce in Section VI-B\n2Explanations are computed in batches of (up to) 16 samples on a GPU and with explanation techniques implemented in PyTorch. Results are averaged over 10 repetitions.\n8 where an explanation needs to be produced for every single example in the dataset."
    },
    {
      "heading": "V. UNIFYING VIEWS ON EXPLANATION METHODS",
      "text": "In parallel to developing explanation methods that address application requirements such as faithfulness, interpretability, usability and runtime, some works have focused on building theoretical foundations for the problem of explanation [99], [92] and establishing theoretical connections between the different methods [133], [5], [100].\nHere, we consider frameworks based on Taylor expansions. This includes the basic Taylor decomposition procedure [13], [17] and as well as an extension of it, the Deep Taylor Decomposition [99]. We then show how Occlusion, Integrated Gradients, or LRP intersect for certain choices of parameters with these mathematical approaches."
    },
    {
      "heading": "A. Taylor Decomposition",
      "text": "Taylor expansions are a well-known mathematical framework to decompose a function into a series of terms associated to different degrees and combinations of input variables. The Taylor expansion of some smooth and differentiable function f : Rd \u2192 R at some reference point x\u0303 is given by:\nf(x) = f(x\u0303) + \u2211 i[\u2207f(x\u0303)]i \u00b7 (xi \u2212 x\u0303i)\n+ 12 \u2211 ii\u2032 [\u22072f(x\u0303)]ii\u2032(xi \u2212 x\u0303i)(xi\u2032 \u2212 x\u0303i\u2032)\n+ . . .\nwhere \u2207f and \u22072f denote the gradient and the Hessian respectively. The zero-order term is the function value at the reference point and is zero if choosing a root point. There are as many first-order terms as there are dimensions and each of them is bound to a particular input variable. Thus, they offer a natural way of attributing a function value f(x) onto individual linear components. There are as many secondorder terms as there are pairs of ordered variables, and even more third-order and higher-order terms. When the function is approximately locally linear, second and higher-order terms can be ignored, and we get the following simple attribution scheme:\nRi = [\u2207f(x\u0303)]i \u00b7 (xi \u2212 x\u0303i) ,\na product of the gradient and the input relative to our root point. In the general case, there are no closed-form approach to find the root point and it is instead obtained using an optimization technique."
    },
    {
      "heading": "B. Deep Taylor Decomposition",
      "text": "An alternate way of formalizing the problem of attribution of a function onto input features is offered by the recent framework of Deep Taylor Decomposition [99]. Deep Taylor Decomposition assumes the function is structured as a deep neural network and seeks to attribute the prediction onto input features by performing a Taylor decomposition at every neuron of each layer instead of directly on the whole neural network function. Deep Taylor decomposition assumes the\noutput score have already been attributed onto some layer of activations (ak)k and attribution scores are denoted by Rk. Deep Taylor Decomposition then considers the function Rk(a) where a = (aj)j is the collection of neuron activations in the layer below. These quantities are illustrated in Fig. 7.\nThe function Rk(a) is typically very complex as it corresponds to a composition of multiple forward and backward computations. This function can however be approximated locally by some \u2018relevance model\u2019 R\u0302k(a), the choice of which will depend on the method we have used for computing Rk. We then compute a Taylor expansion of this function:\nR\u0302k(a) = R\u0302k(a\u0303) + \u2211 j [\u2207R\u0302k(a\u0303)]j \u00b7 (aj \u2212 a\u0303j)\n+ . . .\nThe linear terms define \u2018messages\u2019 Rj\u2190k that can be redistributed to neurons in the lower layer, and messages received by a given neuron at a certain layer are summed to form a total relevance score:\nRj = \u2211 k[\u2207R\u0302k(a\u0303 (k))]j \u00b7 (aj \u2212 a\u0303(k)j ) (4)\nhere, we have added an index {}(k) to the root point to make explicit that different root points can be used for expanding different neurons. The redistribution procedure is iterated from the top layer towards the lower layers, until the input features are reached."
    },
    {
      "heading": "C. Embedding Explanation Methods into the (Deep) Taylor Decomposition Framework",
      "text": "Having described the simple and Deep Taylor Decomposition frameworks, we now present some results from the literature showing how some explanation methods reduce for certain choices of parameters to these frameworks. The different connections we outline here are summarized in Fig. 8.\nWe start with a connection between occlusion-based explanation and Taylor decomposition.\nProposition 1. When applied to homogeneous linear models (of the type f(x) = w>x), occlusion with patch size 1 and replacement value 0 is equivalent to a Taylor decomposition with root point x\u0303 = 0.\nThis is shown by the chain of equations f(x) \u2212 f(x \u2212 {xi}) = wixi = [\u2207f(0)]i \u00b7 (xi \u2212 0). Integrated Gradients can\n9 Taylor decomposition DTD IGOcclusion occlude-1 linear LRP-0/\u03f5/\u03b3 deep rectifier LRP LRP-0\u03be = { t x , 0 < t < 1 } deep rectifier deep rectifier\nFig. 8. Relation between explanation methods and Taylor decomposition / Deep Taylor Decomposition (DTD), for certain choices of hyperparameters and models.\nalso be reduced to a Taylor decomposition and this connection also holds in particular for deep rectifier networks (without biases):\nProposition 2. When applied to deep rectifier networks of the type f(x) = \u03c1(WL \u03c1(. . . \u03c1(W2 \u03c1(W1 x)))), Integrated Gradients with integration path {tx; 0 < t \u2264 1} is equivalent to Taylor decomposition at x\u0303 = \u03b5x with \u03b5 almost zero.\nThis can be shown by making the preliminary observation that a deep rectifier network is linear with constant gradient on the segment (0,x] and then applying the chain of equations\u222b 1 \u03b5 xi[\u2207f(tx)]idt = (1 \u2212 \u03b5)xi[\u2207f(\u03b5x)]i = [\u2207f(\u03b5x)]i(xi \u2212 \u03b5xi). This connection, along with the observation that a single gradient evaluation of a deep network can be noisy (cf. Section II-B) speaks against integrating on the segment (0,x]. For this reason, we have opted in the experiments of Section IV to use a smoothed version of IG. A further result shows an equivalence between a \u2018naive\u2019 version of LRP-0 and Taylor decomposition.\nProposition 3. For deep rectifier nets of the type f(x) = \u03c1(WL \u03c1(. . . \u03c1(W2 \u03c1(W1 x)))), applying LRP-0 at each layer is equivalent to a Taylor decomposition at x\u0303 = \u03b5x with \u03b5 almost zero.\nThis result can be derived by taking the LRP formulation of Eq. (3) and setting \u03b3 = 0. This equation then reduces to:\ncj = \u2211 k wjkstep (\u2211 0,j ajwjk ) ck\nwhere step(t) = 1t>0. This equation is exactly the one that propagates gradients in a deep rectifier network. Hence, the input relevance computed by LRP becomes Ri = xici = xi[\u2207f(x)]i for which we have already shown the equivalence to simple Taylor decomposition in the proposition above.\nProposition 4. For deep rectifier networks of the type f(x) = \u03c1(WL \u03c1(. . . \u03c1(W2 \u03c1(W1 x)))), applying LRP-\u03b3 is equivalent to performing one step of deep Taylor decomposition and choosing the nearest root point on the line {a\u2212 ta (1+ \u03b3 \u00b7 1wk 0); t \u2208 R}.\nWe choose the relevance model R\u0302k(a) = ak(a) \u00b7ck with ck constant (cf. [98] for a justification). Injecting the root point in the first-order terms of DTD (summands of Eq. (4)) gives:\nRj\u2190k = wjk \u00b7 ck \u00b7 (aj \u2212 (aj \u2212 taj \u00b7 (1 + \u03b3 \u00b7 1wjk\u22650))) = aj \u00b7 (wjk + \u03b3w+jk) \u00b7 t \u00b7 ck\nwhere t is resolved using the conservation equation\u2211 j Rj\u2190k = Rk. LRP-0 is a special case of LRP-\u03b3 with \u03b3 = 0. A similar procedure with another choice of reference point gives LRP- (cf. [98])."
    },
    {
      "heading": "VI. EXPLANATIONS FOR UNSUPERVISED LEARNING AND BEYOND",
      "text": "Deep neural networks have been shown to perform extremely well on classification or regression tasks, however for other problems such as anomaly detection or clustering, k-means and kernel-based models such as one-class SVMs respectively have remained highly popular workhorses. As these models are not given in the form of a neural network, and furthermore are composed of strongly nonlinear functions such as the exponential, a direct application of methods designed in the context of linear models and DNNs is not feasible."
    },
    {
      "heading": "A. Neuralization",
      "text": "Neuralization [67], [68] was recently introduced as a framework for explainable machine learning, where non-neural architectures are translated into neural networks, in order to enhance their explanation properties. In other words, we identify a neural network structure that is functionally equivalent to the model to explain and ensure that this functional \u2018copy\u2019 is furthermore only composed of \u2018canonical\u2019 neural network functions, e.g. linear or pooling. This general concept of neuralization was first introduced in the context of explanation methods for unsupervised learning, namely, one-class SVMs [68] and k-means clustering models [67], where combinations of kernel RBF functions can be rewritten as pooling operations over linear or distance functions.\n1) Neuralizing Clustering: Consider a kernel k-means model of the type studied in [31]. For this type of model, and assuming a Gaussian kernel K(x,x\u2032) = exp(\u2212\u03b3\u2016x \u2212 x\u2032\u20162), the probability ratio in favor a given cluster \u03c9c can be expressed as:\nP (\u03c9c|x) 1\u2212 P (\u03c9c|x) =\n( Z\u22121c \u2211 i\u2208Cc K(x,xi) )\u03b2/\u03b3\u2211 k 6=c ( Z\u22121k \u2211 j\u2208Ck K(x,xj)\n)\u03b2/\u03b3 (5) This is a power-assignment model applied to the kernel density functions of each cluster. The sets Cc and Ck are the representatives for clusters c and k, and Zc, Zk are respective normalization factors. An example of decision function produced by this model for a three-cluster problem is shown in Fig. 9 (left). Clearly, Eq. (5) is a priori not composed of neurons. However, it can be reorganized into the following sequence of detection and pooling functions [67]:\nlog [ P (\u03c9c|x) 1\u2212 P (\u03c9c|x) ] = \u03b2min k 6=c \u03b2 { min j\u2208Ck \u03b3 { max i\u2208Cc \u03b3 { w>ijx+ bijk }}} with wij = 2(xi \u2212 xj) and bijk = \u2016xj\u20162 \u2212 \u2016xi\u20162 + \u03b3\u22121(logZk \u2212 logZc) are parameters of the first linear layer. This layer is followed by a hierarchy of log-sum-exp computations interpretable as canonical max- and min-pooling operations. The neuralized version of kernel k-means is depicted in Fig. 9 (right).\n10\n2) Neuralizing SoftMax Layers: The concept of neuralization can also be extended for the purpose of improving the explanation for deep neural networks. So far, we have explained quantities at the output of the last linear layer. Because these output quantities are unnormalized they may respond positively to several classes, thereby lacking selectivity. The problem of class selectivity was highlighted e.g. in [47], [63], [98] and practical solutions were proposed. Here, we present the \u2018neuralization\u2019 approach in [98], which first makes the observation that ratios of probabilities as given by the toplayer soft-assignment model can be expressed as:\nP (\u03c9c|x) 1\u2212 P (\u03c9c|x) = exp(w>c a)\u2211 k 6=c exp(w > k a)\nThis computation can then be reorganized in the two-layer neural network\nlog [ P (\u03c9c|x) 1\u2212 P (\u03c9c|x) ] = min k 6=c { (wc \u2212wk)>a } where min is a soft minimum implemented by a log-sum-exp computation. The DNN processing up to the output neuron or up to the output of the neuralized logit model is illustrated in Fig. 10 along with LRP explanations for these two quantities associated to the class \u2018passenger car\u2019.\nIn the first explanation, both the passenger car and the locomotive can be seen to contribute. In the second explanation, the locomotive turns blue. The latter is indeed speaking for the class locomotive, which mechanistically lowers the probability for the class \u2018passenger car\u2019 [98]. This example shows that it is important in presence of correlated features to precisely define what quantity (unnormalized score or logit) we would like to explain.\nWe note that while neuralization has served here to support LRP-type explanations, the concept could potentially be extended to other explanation frameworks. The identified neural network structure may help to gain further understanding of the model or provide intermediate representations that are potentially useful to solve related tasks."
    },
    {
      "heading": "B. Dataset-Wide Statistics on Explanations",
      "text": "In practice, we may not only be interested in explaining how the DNN predicts a single data point, but also in the statistics of them for a whole dataset. This may be useful to validate the model in a more complete manner. Let f : Rd \u2192 R be a function that takes a data point as input and predicts evidence for a certain class for each data point. Consider a dataset x1, . . . ,xN of such data points. The total class evidence can be represented as a function g : RN\u00d7d \u2192 R where:\ng(x1, . . . ,xN ) = \u2211N n=1 f(xn)\nThis composition of the neural network output and a sumpooling remains explainable by all methods surveyed here, however, the explanation is now high-dimensional (N \u00d7 d).\n1) Relevance Pooling: Practically, we may be not be interested in explaining every single data point in terms of every single input features. A more relevant information to the user would be the overall contribution of a subgroup of features I on a group of data points G (cf. [82], [100]). In particular the Integrated Gradient and LRP methods surveyed here produce explanations that satisfy the conservation property:\ng(x1, . . . ,xN ) \u2248 N\u2211 n=1 d\u2211 i=1 Ri,n\nand that can be converted to a coarse-grained explanation \u2248 \u2211 G \u2211 I \u2211 n\u2208G \u2211 i\u2208I\nRi,n\ufe38 \ufe37\ufe37 \ufe38 RI,G\nthat still satisfies the desired conservation property. As an illustration of the concept, we consider the \u2018Concrete Compression Strength\u2019 example of Section II. Data points are grouped in three k-means clusters, and features are grouped in two sets: the singleton {age}, and the set of all remaining features describing concrete composition. The pooled analysis is illustrated in Fig. 11.\nThis analysis gives further insight into our predictive model. We observe that most distinguishing factors, especially age, contribute negatively to strength. In other words, a \u2018typical\u2019 age and composition is a recipe for strength whereas high/low values tend to be explanatory for weakness. Notably, one data\n11\ncluster stands out by having composition features that are explanatory for strength.\n2) Spectral Relevance Analysis (SpRAy) [85]: While in Section VI-B1 we have reduced the dimensionality through pooling, other analyses are possible. For example, the SpRAy method [85] does not assume a fixed pooling structure (e.g. a partition of data points and a partition of features), and applies instead a clustering of explanations in order to identify protypical decision behaviors. Algorithm 4 outlines the three steps procedure used by SpRAy:\nAlgorithm 4 Spectral Relevance Analysis for n = 1 to N do R(n) \u2190 explain(x(n), f) R\n(n) \u2190 normalize(R(n)) end for clustering({R(1), . . . ,R(N)})\nThe method first produces an explanation for each data point. In principle, any explanation method can be used, e.g. occlusion, integrated gradients, or LRP. Explanations are then normalized (e.g. blurred and standardized) to become invariant to small pixel-wise or saliency variations. Finally, a clustering algorithm is applied to the normalized explanation, and examples with the same cluster index can be understood as being associated with some prototypical decision strategy, e.g. looking at the object, looking at the background, etc. Alternately, the clustering step can be replaced by a lowdimensional embedding step to produce a visual map of the overall decision structure of the ML model.\nAltogether, relevance pooling and SpRAy support a variety of dataset-wide analyses that are useful to explore and characterize the decision behavior of complex models trained on large datasets. Some successful applications are reviewed in Section VIII-A."
    },
    {
      "heading": "VII. WORKED-THROUGH EXAMPLES",
      "text": "In this paper, we have motivated the use of explanation in the context of deep learning models and showcased some\nmethods for obtaining explanations. Here, we aim to take a practical look for the user to assess when explanation is required, what are common issues with applying explanation techniques / setting their hyperparameters, and finally, how to make sure that the produced explanations deliver meaningful insights for the human."
    },
    {
      "heading": "A. Example 1: Validating a Face Classifier",
      "text": "In the first worked-through example we wish to train an accurate classifier for predicting a person\u2019s age from images of faces. We will show how to use explanation for this task, in particular, to verify that the model is not using \u201cwrong\u201d features for its decisions.\nLet us use for this the Adience benchmark dataset [35] providing 26,580 images captured \u2018in the wild\u2019 and labelled into eight ordinal groups of age ranges {(0-2), (4-6), (8-13), (15-20), (25-32), (38-43), (48-53), (60+)}.\nBecause the number of examples in this example is limited and likely not sufficient to extract good visual features, we adopt the common approach of starting with a generic pretrained classifier and fine-tune it on our task. We download a VGG-16 [136] neural network architecture pretrained on ImageNet [29] obtainable from modelzoo.co. First test results after training using Stochastic Gradient Descend (SGD) [87] report reasonable performance, with exact and 1-off [130], [35] prediction accuracy3 of 56.5% and 90.0%, respectively. Here, the 1-off accuracy considers predictions of (up to) one age group away from the true label as correct.\nIn order to understand the learned prediction strategies of our model and to verify that it uses meaningful features in the training data, we take an off-the-shelve explanation software, the LRP Toolbox [83] for Caffe [65], and choose the method LRP configured to perform \u2018LRP- \u2019 on all layers in a first attempt. Explanations are shown for a given image in Fig. 12 (first row).\nSome insight can be readily obtained from these explanations, e.g. the classifier has learned to ignore the background and makes his assessment mainly based on the actual person in the image. However, we also observe that explanations are\n3Results have been averaged over the official pre-selected five-fold dataset split [35].\n12\noverly complex with frequent local sign changes, making it hard to extract further insights, especially what are the features that contribute to different age groups. This leads to our first recommendation:\nChoose the explanation technique and its parameters\nSpecifically, we will now try an alternate LRP preset called \u2018LRP-CMP\u2019 that applies a composite strategy [84], [98], [75] where different rules are applied at different layers. Explanations obtained with this new rule are given in Figure 12 (bottom). The new explanations highlight features in a much more interpretable way and we also start to better understand what speaks \u2014 according to the model \u2014 in favor of or against certain age groups. For example, explanations amusingly reveal baldness as a feature corresponding to both age groups (0-2) and (60+). In the shown sample, baldness contributes evidence for the classes (0-2) and (60+), while it speaks against the age group (25-32). Relatedly, the expression of the man\u2019s chin and mouth area contradicts class (0-2) more than class (60+), but \u2018looks like\u2019 it would belong to a person aged (25-32).\nLet\u2019s now move back to the initial question, namely how to verify that the model is using the right features for predicting. While the decision structure of the model was meaningful in Fig. 12, we would like to verify it is also the case for other test cases. Figure 13 (top) shows further samples from the Adience dataset; a woman labelled (60+) and three images of the same male labelled (48-53) with smiles of varying intensities.\nWe apply LRP with the same preset \u2018LRP-CMP\u2019 on these images. LRP evidence for each image for the class (60+) is shown in Fig. 13 (middle). Surprisingly, according to the model, broad smiling contradicts the prediction of belonging to the age group (60+). Smiling is however clearly a confounding factor, which reliably predicts age group only to the extent that no such case is present in the training data. This predicting\nstrategy is related to the \u2018Clever Hans\u20194 effect [85] and we can therefore formulate our second recommendation:\nUnmask \u2018Clever Hans\u2019 examples\nAlternately, instead of screening through multiple images manually, we can also use techniques such as SpRAy [85], which perform such analysis systematically for large datasets such as ImageNet (see also Section VIII-A for successful applications).\nWhile for the examples showcased in Fig. 13 other features may compensate for such effect, \u2014 here almost all other features of the centered faces affect the decision towards this age group positively \u2014 this will cause errors for less clear-cut cases, and this may explain why the accuracy of the ImageNetbased model is not very high, and can point at the fact that the test set accuracy may drop dramatically on new datasets, e.g. comprising more old people smiling.\nNaturally, we would like our model to be robust to a subject\u2019s mood when predicting his or her age. We thus need to find a way to prevent Clever Hans behaviors, e.g., prevent the model to associate smiling with age. One reason the model has learned that connection in the first place is the extreme population imbalance among the age groups of the Adience dataset; a problem which is shared with many other datasets of face images, e.g. [61], [120]. We therefore add a second pre-training phase in between the ImageNet initialization and the actual training based on the Adience data, by using the considerably larger IMDB-WIKI [120] dataset. The IMDB-WIKI dataset consists of 523,051 images from 20,284 celebrities (460,723 images from the Internet Movie Data Base (Imdb) and 62,328 images from Wikipedia) at different ages, labelled with 101 labels (0-100 years, one label per year). The IMDB-WIKI dataset also suffers from highly imbalanced label populations. However, we follow [120] and re-normalize the age distribution by under-sampling the more frequent classes until approximately 260,000 samples are selected overall. Furthermore, we assume that since the IMDB-WIKI dataset is composed of photos of public figures (taken at publicized events) the ratio of expressed smiles in higher age groups will be more frequent than in the Adience dataset, which has been captured \u2018in the wild\u2019. A comparison of performance on the Adience benchmark of the original model \u2013 pretrained on ImageNet only \u2013 and the improved model is given in the table below.\naccuracy 1-off\nImageNet pretrained 56.5 90.0 IMDB-WIKI pretrained 63.0 96.0\nNot only did the additional (and more domain-specific) pretraining step improve the generalization performance of the VGG-16 model. It also prevented the model from associating\n4\u2018Clever Hans\u2019 was a famous horse at the beginning of the 20th century, which was believed by his trainer to be capable of performing arithmetic calculations. Subsequent analyses revealed that the horse was not performing arithmetic calculations but detecting cues on the face of his trainer to produce the right answers. In machine learning, the term \u2018Clever Hans\u2019 can be used to designate strategies that mimic the expected behavior but are based on unexpected correlations or artefacts in the data [85].\n13\nsmiling exclusively with younger age groups. Figure 13 (bottom) shows LRP heatmaps for all four samples and age label (60+). For the woman, the model has shifted its attention from the hair and clothes to the face region and neck, and no longer considers the smile as contradictory to the class. A similar effect can be observed for the samples showing the male person. The model\u2019s age prediction capabilities can no longer be attacked by just smiling into the camera. However, by introducing the IMDB-WIKI pretraining step, we have apparently replaced the smile-related Clever Hans strategy with another one, related to the presence of glasses in images of males in higher age groups. This leads to our third recommendation:\nIteratively validate and improve the model\nWe can do so until the model solely relies for its predictions on meaningful face features. For that, choosing a better pretraining may not be sufficient, and other more advanced interventions may be required."
    },
    {
      "heading": "B. Example 2: Identifying Gender-Specific Speech Features",
      "text": "After demonstrating how explanations can be used to unmask Clever Hans strategies, or more generally validate a classifier, we will now discuss another use case, where explanations are this time applied not to get a better model, but to gain new (scientific) insights. In this worked-through example, we will show that explanations can be used to identify genderspecific features in speech.\nBefore going into the analysis, let us first introduce the data and the model used for the speaker\u2019s gender classification task. As training data we use the recently recorded AudioMNIST [18] dataset, comprised of 30000 audio recordings of spoken digits from 60 different speakers, with 50 repetitions per digit and speaker, in a 48kHz sampling frequency. Next to annotations for spoken digit (0-9) and gender of speaker (48 male, 12 female), the dataset provides labels for speaker age, accent and origin. We begin by training a deep neural network model on the raw waveform data, which is first downsampled to 8kHz, and randomly padded with zeroes before and after the recorded signal to obtain a 8000 dimensional input vector per sample. A CNN architecture comprised of six 1d-convolution layers interleaved with maxpooling layers and topped of with three fully connected layers [18] and ReLU activation units after each weighted layer is prepared for optimization. In order to prevent the model from overfitting on the more frequent population of samples labelled as \u2018male\u2019, we (randomly) select 12 speakers from both classes. The model is then trained and evaluated in a 4-fold cross-validation setting, in which the 24 speakers are grouped into four sets of 3 male speakers and 3 female speakers. Each of the four splits thus contains 1000 waveform features. Two folds are used for training, while one of the remaining data splits are reserved for validation and testing. The model reaches an average test set accuracy (\u00b1 standard deviation) of 91.74% \u00b1 8.60% across all splits.\nWith the goal of understanding the data better by explaining the model, we consider two examples predicted by the network\nto be male and female and apply LRP to visualize those predictions. Here, the wave form is represented as a scatter plot where each time step is color-coded by its relevance. Results are shown in Fig. 14.\nThe explanations reveal that the model predominantly uses the outer hull of the waveform signal for decision making. For a human observer, however, these explanations are difficult to interpret due to the limited accessibility of the data representation in the first place (see Fig. 14). Although the model performs reasonably well on waveform data, it is hard to obtain a deeper understanding beyond the network\u2019s modus operandi based on relevance maps, due to the limitations imposed via the data representation itself.\nMake your input features interpretable\nWe therefore opt to change the data representation for improved interpretability. More precisely, we exchange the raw waveform representation of the data with a corresponding 228 \u00d7 230 (time \u00d7 frequency) shaped spectrogram representation by applying a short-time Fourier transform (time segment length of 455 samples, with 420 samples overlap), cropped to a 227 \u00d7 227 matrix by discarding the highest frequency bin and the last three time segments. Consequently we also exchange the neural network architecture and use an AlexNet [80] model, which is able to process the transformed input data using 2d-convolution operators.\nFigure 15 visualizes four input spectrograms, with corresponding relevance maps (only relevance values with more than 10% relative amplitude) drawn on top.\nHeatmap visualizations based on spectrogram input data are more informative than those for waveform data and reveal that the model has learned to distinguish between male and female speakers based on the lowest fundamental frequencies (male speakers, Fig. 15 (right)), and immediate harmonics (female speakers, Fig. 15 (left)) shown in the spectrogram. Many incorrectly classified samples with ground truth label \u2018male\u2019 show large gaps between frequency bands often occurring in samples from female speakers. Note that these insights are consistent with the literature [147].\n14\nfemale samples\nin co rre\nct\n0s 0.5s 1s 0k Hz 2k Hz 4k Hz gender | vp12 | digit 0 | rep. 14 | prediction 1 0s 0.5s 1s 0k Hz 2k Hz 4k Hz gender | vp2 | digit 0 | rep. 13 | prediction 0\n0s 0.5s 1s\n0k Hz\n2k Hz\n4k\nHz\ngender | vp56 | digit 0 | rep. 11 | prediction 0\n0s 0.5s 1s\n0k Hz\n2k Hz\n4k\nHz\ngender | vp25 | digit 0 | rep. 0 | prediction 1\nmale samples co rre ct\nFig. 15. Left: Spectrogram representations of digits \u2018zero\u2019 spoken by female speakers \u2018vp12\u2019 and \u2018vp56\u2019. Right: Spectrogram representations of digits \u2018zero\u2019 spoken by male speakers \u2018vp2\u2019 and \u2018vp25\u2019. Relevance maps are shown wrt. to the samples\u2019 true classes.\nGain insights by explaining predictions\nAs a noteworthy side effect, the increase in interpretability from switching from a waveform data representation to spectrogram data representation does not come at a price of model performance. On the contrary, model performance is even increased slightly from 91.74% to 95.87%."
    },
    {
      "heading": "VIII. SUCCESSFUL USE OF INTERPRETABLE ML",
      "text": "Interpretation methods can be applied for a variety of purposes. Some works have aimed to understand the model\u2019s prediction strategies, e.g., in order to validate the model [85]. Others visualize the learned representations and try to make the model itself more interpretable [58]. Finally, other works have sought to use explanations to learn about the data, e.g., by visualizing interesting input-prediction patterns extracted by a deep neural network model in scientific applications [146]. Technically, explanation methods have been applied to a broad range of models ranging from simple bag-ofwords-type classifiers or logistic regression [13], [24] to feedforward or recurrent deep neural networks [13], [133], [11], [9], and more recently also to unsupervised learning models [67], [68]. At the same time these methods were able to handle different types of data, including images [13], speech [18], text [10], and structured data such as molecules [127] or genetic sequences [151].\nSome of the first successes in interpreting deep neural networks have occurred in the context of image classification, where deep convolutional networks have also demonstrated very high predictive performance [80], [52]. Explanation methods have for the first time allowed to open these \u201cblack boxes\u201d and obtain insights into what the models have actually learned and how they arrive at their predictions. For instance, the works [135], [106]\u2014also known in this context as \u201cdeep dreams\u201d\u2014highlighted surprising effects when analyzing the inner behavior of deep image classification models by synthesizing meaningful preferred stimuli. They report that the preferred stimuli for the class \u2018dumbbell\u2019 would indeed contain a visual rendering of a dumbbell, but the latter would systematically come with an arm attached to it [103],\ndemonstrating that the output neurons do not only fire for the object of interest but also for correlated features.\nAnother surprising finding was reported in [82]. Here, interpretability\u2014more precisely the ability to determine which pixels are being used for prediction\u2014helped to reveal that the best performing ML model in a prestigious international competition, namely the PASCAL visual object classification (VOC) challenge, was actually relying partly on artefacts. The high performance of the model on the class \u201chorse\u201d could indeed be attributed to detecting a copyright tag present in the bottom left corner of many horse images of the dataset5, rather than detecting the actual horse in the image. Other effects of similar type have been reported for other classes and datasets in many other works, e.g., in [116] models were shown to distinguish between the class \u201cHusky\u201d and \u201cWolf\u201d solely based on the presence or absence of snow in the background.\nThese discoveries have been made rather accidentally by researchers carefully analysing suspicious explanations. It is clear that such laborious manual inspection of heatmaps does not scale to big datasets with millions of examples. Therefore, systematic approaches to the interpretation of ML models have recently gained increased attention."
    },
    {
      "heading": "A. Systematic Interpretation of ML Models on Big Data",
      "text": "This section describes two examples of a systematic analysis of a large number of heatmaps. In the first case, the goal of the analysis is to systematically find data artefacts picked up by the model (e.g., copyright tags in horse images), whereas the second analysis aims to carefully investigate the learning process of a deep model, in particular the emergence of novel prediction strategies during training.\nThe process of systematically extracting data artefacts was automated by a method called Spectral Relevance Analysis (SpRAy) [85], where after computing LRP-type explanations on a whole dataset (cf. Section VI-B), a cluster-based analysis was applied on the collection of produced explanations to extract prototypical decision behaviors. The SpRAy analysis would for example reveal for some shallow Fisher Vector model trained on Pascal VOC 2007 dataset that image of the \u2018horse\u2019 would be predicted as such using a finite number of prototypical decision behaviors ranging from detecting the horse itself to detecting weakly related features such as horse racing poles, or clear artefacts such as copyright tags [82]. The analysis was later on applied to the decisions of a state-of-the-art VGG-16 deep neural network classifier trained on ImageNet, and here again, interesting insight about the decision structure could be identified [6]. For example, certain predictions, e.g. for the class \u2018garbage truck\u2019 could be found by SpRAy to rely on some watermark in the bottom-left corner of the image (see Fig. 16). This watermark which is only present in specific images would thus be used by the model as a confounding factor (or artefact) to artificially improve prediction accuracy on this benchmark6.\n5The presence of these artifacts in the benchmark dataset had gone unnoticed for almost a decade.\n6Or in the case of [6] deteriorate model performance, as the identified confounding feature is exclusive to the training data.\n15\nSuch behavior of the ML classifier can be referred to as \u2018Clever Hans\u2019 behavior [85]. For machine learning models having implemented a Clever Hans behavior, an overconfident assessment of the true model accuracy would be produced by solely relying on the accuracy metric without an inspection of the model\u2019s decision structure. The model would have likely performed erratically once it is applied in a real-world setting, where, e.g., the copyright tag is decoupled from the concept of a horse or garbage truck respectively. Here, the ability to explain the decision-making of the model and to automatically analyse these explanations on a very large dataset, was therefore a key ingredient to more robustly assess the model\u2019s strength and weakness and potentially improving it.\nAnother example of a systematic interpretation of ML models can be found in the context of reinforcement learning, in particular board and video games. Here large amounts of data can be easily generated (simulated games) and used to carefully analyse the strategies of a ML model and how these strategies emerge during training. On games such as the arcade game Atari Breakout, the computer player would progressively learn strategies commonly employed by human players such as \u2018tunnel-digging\u2019 [96], [158]. The work of [85] analyzes the emergence of this advanced \u2018tunnel-digging\u2019 technique using interpretatable ML. First, LRP-type pixel-wise explanations the player\u2019s decision were produced at various time steps and training stages. The produced collection of explanations were then be pooled (cf. Section VI-B1) on bounding boxes representing some key visual elements of the game, specifically, the ball, the paddle, and the tunnel. Pooled quantities could then be easily and quantitatively monitored\nover the different stages of training. The analysis is shown in Fig. 17.\nWe observe that the neural network model would first learn to play conventionally by keeping track of the ball and the paddle, and only at a later stage of the training process would learn to focus on the tunnel area, allowing the ball to go past the wall and bounce repeatedly in the top area of the screen. This analysis highlights in a way that is easy interpretable to the human the multi-stage nature of learning, in particular, how the learning machine progressively develops increasingly sophisticated game playing strategies. Overall, this summarized information on the decision structure of the model and on the evolution of the learning process could prove to be crucial in learning improved models on purposely consolidated datasets. They could also prove useful for characterizing the different stages of learning and developing more efficient training procedures.\nB. Interpretable Deep Models in the Sciences\nIn the last subsection we demonstrated the use of explanation techniques for systematically analysing models and verifying that they have learned valid and meaningful prediction strategies. Once verified to not be Clever Hans predictors, nonlinear models offer a lot of potential for the sciences to detect new interesting patterns in the data, which may lead to an improved understanding of the underlying natural structures and processes \u2014 the primary goal of scientists. So far this was not possible, because non-linear models were actually considered to be \u201cblack boxes\u201d, i.e., scientists had to resort to the use of linear models (see e.g. [51], [93]), even if this came at the expense of predictivity. In the following we will show that interpretation methods remove this restriction and bring the full potential of non-linear methods to scientific disciplines.\nLet us start with the discussion of scientific problems, which concern images, thus can directly benefit from the advances made in image-based machine learning in the last years. Figure 18 (a) shows such an example: the task of predicting tissue type from histopathology imagery. The work of [20] addresses this problem using interpretable machine\n16\nlearning, more precisely it proposes an interpretable bag-ofwords prediction pipeline with invariances to rotation, shift and scale of the input data7. For the verification of the prediction results, relevance maps are computed, offering per-pixel scores which indicate the presence of tumorous structures. Figure 18 (a) demonstrates how LRP heatmaps computed for different target cell types can be combined for obtaining computationally predicted fluorescence images. The explanations are histopathologically meaningful and may potentially give interesting information about which tissue components are most indicative of cancer. Further analyses such as the identification, localization and counting of cells, i.e., lymphocytes, can be performed on these explanations (see [74]). In addition to visual explanations, [161] also generate a free-text pathology report to clarify the decision of the classifier.\nVarious other works apply interpretable machine learning to image-based analyses in the sciences, especially in medical applications [57]. For instance, [79] use deep multiple instance learning to classify and segment microscopy images using only whole image level annotations. The work of [39] introduces a model-agnostic interpretation method for the analysis of xray images, which not only visualizes the elements that have contributed to each decisions, but also produces descriptive sentences to clarify the decision of the classifier. The combined explanations are well adopted by doctors and are shown to be more informative than the visualisations or generated text alone. Interpretable and non-linear models have been successfully applied to many other tasks, including the detection of lesions in diabetic retinopathy data [115], the validation of predictions in dermatology [157], plant stress classification [41], or the analysis of galaxy morphologies [164]. The latter work aims to classify galaxy morphologies into five classes (completely round smooth, in-between smooth, cigar-shaped smooth, edge-on and spira) using a convolutional neural networks, and the convolution filters as well as activation patterns are analysed to gain insights into the features learned by the model to solve this task (see Fig. 18 (f)).\nInterpretable ML methods have also demonstrated their potential beyond the image domain, e.g., on scientific problems concerning time series data. For instance, the work of [140] presents one of the first uses of interpretable deep neural networks in cognitive neurosciences, specifically in brain computer interfacing [33] where linear methods are still the most widely used filtering methods [22], [51]. The results in [140] show that deep models achieve similar decoding performances8 and learn neurophysiologically plausible patterns (see Fig. 18 (b)), namely focus on the contralateral sensorimotor cortex \u2013 an area where the event-related desynchronization occurs during motor imagery. However, in contrast to the patterns computed with conventional approaches [22], [51], which only allow to visualize the aggregated information (average activity) per class, the explanations computed with LRP are available for every single input of the deep learning classifier, i.e., for every time point of individual trials (see\n7Note that recently interpretable deep neural networks have also been used for this task, e.g., [50].\n8Deep models usually require larger amounts of training data to have an advantage over linear techniques.\nFig. 18 (b)). This increased resolution (knowing which sources are relevant at each time point, instead of just having the average patterns) may contribute to a better understanding of cognitive processes in the brain.\nAnother application of interpretable machine learning in cognitive neuroscience is presented in [146], which applies deep learning to whole-brain fMRI data. The method, termed DeepLight, outperforms well-established local or linear decoding methods such as the generalized linear model and searchlight (see [146]). An adaption of LRP maintains interpretability and verifies that the model\u2019s predictions are based on physiologically appropriate brain areas for the classified cognitive states. Figure 18 (c) visualizes exemplar voxels, which are used by the deep model to accurately decode the state from the fMRI signal. These voxels of high relevance have been shown to correspond very well to the active areas described in the fMRI literature (see [146]). Note that also here the deep model not only gives an advantage in terms of performance (i.e., better decoding accuracy) compared to the local or linear baseline methods, but its explanations are provided for every single input, i.e., for every fMRI volume over time. This increased resolution allows to study the spatiotemporal dynamics of the fMRI signal and its impact on decoding, something which is not possible with classical decoding methods9.\nMany other studies use explanation methods to analyse time series signals in the sciences. For instance, [60] introduce interpretable machine learning methods to the domain of human gait recognition and show that non-linear learning models are not only the better predictors but that they can at the same time learn physiologically meaningful features for subject prediction which align with expected features used by linear models. Another work [78] applies Long ShortTerm Memory (LSTM) networks to the field of hydrology to predict the river discharge from meteorological observations. The authors apply the integrated gradients technique to analyse the internals of the network and obtain insights which are consistent with our understanding of the hydrological system.\nStructured data such as molecules or gene sequences are another very important domain for scientific investigations. Therefore, interpretable and non-linear ML methods have also attracted attention in scientific communities working with this type of data. One successful example of the use of interpretable ML methods in this domain has been reported in [114]. The authors train a deep model to predict molecular properties and bioactivities and report interesting insights when analysing what the model has learned (see Fig. 18 (d)). For instance, they show that single neurons play the role of pharmacophore detectors and demonstrate that the model uses pharmacophore-like features to reach its conclusions, which are consistent with pharmacologist literature. Another work [55] (see Figure 18 (e)) applies an extended version of LRP called CLRP to visualize how CNNs interpret individual protein-ligand complexes in molecular modeling. Also here the trained model learns meaningful features and has the\n9In classical fMRI analyses, p-values indicate the relevance of brain voxels. However, these p-values are usually obtained on a subject or group-level, not for single trials or single time points.\n17\nability to provide new insights into the mechanisms underlying protein-ligand interactions. Yet another work [155] applies LSTM predictors together with LRP for transparent therapy prediction on patients suffering from metastatic breast cancer. Clinical experts verify that the features used for prediction as revealed via LRP largely agree with established clinical guidelines and knowledge. The work by [69] uses interpretable ML to understand the activity prediction across chromosomes, whereas [27] uses these methods for understanding automated decisions on behavioral biometrics. Recently, also the physics community started to use interpretable machine learning for the task of energy prediction. The work of [127], [128] showed that accurate predictions are possible and obtained also physical meaningful insights from the model. Other works [91] showed that explanations in gene analysis lead to interpretable patterns consistent with literature knowledge."
    },
    {
      "heading": "IX. CHALLENGES AND OUTLOOK",
      "text": "While recent years have seen astonishing conceptual and technical progress in XAI, it is important to carefully discuss the current limits and the challenges that will need to be addressed by researchers to further establish the field and increase the usefulness of XAI systems.\nFoundational theoretical work in XAI has so far been limited. As discussed above in Section V, early works have established Taylor expansions and Deep Taylor Decomposition [99] as principled frameworks for describing the process of\nexplanation. Other frameworks such as Shapley values [131], [92] or rate distortion theory [94] have also emerged as ways of formalizing the task of explanation. Numerous theoretical questions however remain: For example, it remains unclear how to weigh the model and the data distribution into the explanation, in particular, whether an explanation should be based on any features the model locally reacts to, or only those that are expressed locally. Related to this question is that of causality, i.e. assuming a causal link between two input variables, it has not been answered yet whether the two variables, or only the source variable, must constitute the explanation. A deeper formalization and theoretical understanding of XAI will be instrumental for shedding light into these important questions.\nAnother central question in XAI is that of optimality of an explanation. So far, there is no well-agreed understanding of what should be an optimal explanation. Also, groundtruth explanations cannot be collected by humans as this would presuppose they are able to make sense of the complex ML model they would like to explain in the first place. Methods such as \u2018pixel-flipping\u2019 [122] assess explanation quality indirectly by testing how flipping relevant pixels affects the output score. The \u2018axiomatic approach\u2019 [142], [97] does not have this indirect step, however, axioms are usually too generic to evaluate an explanation comprehensively. The question of evaluating and comparing explanations becomes even more complex when integrating human factors such as\n18\ninterpretability, manageability, and overall utility of the XAI system [116], [105]. Application-driven evaluations account for those factors, however, they are also hard to implement in practice [34].\nFurther challenges arise when applying XAI on problems where different actors (e.g. the explainer and the explainee) have conflicting interests. Recent work has shown that an \u2018adversary\u2019 can modify the ML model in an imperceptible fashion so that the prediction behavior remains intact but the explanation of those predictions changes drastically [53]. Relatedly, even when the model remains unchanged, inputs could be perturbed imperceptibly to produce arbitrary explanations [32]. Interpretability may also find itself at odds with the constant quest for higher predicting accuracy. Because highly predictive models are typically complex and strongly engineered, XAI software must keep up with this ever increasing complexity [3], and at the same time, the human must also deal with explanations of increasingly subtler predictions. When designing new XAI-driven applications, adopting a holistic view that sets the right tradeoffs and delivers the optimal amount of information and range of action to the multiple and potentially conflicting actors, will constitute an important practical challenge.\nAnother question of utmost importance, especially, in safety critical domains, is whether we can fully trust the model after having explained some predictions. Here, we need to distinguish between model interpretation and model certification: While it is helpful to explain models for available input data, e.g. interpretable ML can detect erroneous decision strategies, certification would require to verify the model for all possible inputs, not only those included in the data. Furthermore, it must be remembered that explanations returned to the user are summaries of a potentially complex decision process, i.e. there may be different decision strategies, the wrong ones and the correct ones, mapping to the same explanation. Lastly, explanations are subject to their own biases and approximations, and they can be manipulated by an adversary to loose their informative content. Therefore, in order to ultimately establish a truly safe and trustworthy model, further steps are needed, potentially including the use of formal verification methods [19], [66].\nFinally, it may be worthwhile to explore new forms of explanations that are optimally suited to its user. Such explanations could for example leverage the user\u2019s prior knowledge or personal preferences. Novel approaches from knowledge engineering, cognitive sciences, and human-computer interfaces, will need to contribute. Also, while heatmaps provide a first intuition to users, they may not take advantage of the complex abstract reasoning capabilities of humans. An example would be to replace heatmaps by \u2018mathematical formulas\u2019 explaining the ML decision behavior. For example, the local extraction of polynomials or other interaction models would enable higher order explanations, specifically the automatic grouping of variables that jointly and combined nonlinearly constitute an explanation [148], [28]. In the neurosciences, von der Malsburg has coined the concept of \u2018binding\u2019, neural strategies that allow sets of variables (neurons) to synchronize collectively by learning [152]. In physics collective variables have been\nso far conceptualized manually giving rise to groundbreaking advances in solid state physics defining quasiparticles such as phonons, plasmons, polarons, magnons, exitons [73], etc. Ideally, collective variables in this sense would in the future be inferred from a learning model by e.g. automatically binding explanation variables in meaningful abstract ways."
    },
    {
      "heading": "X. CONCLUSION",
      "text": "Complex nonlinear ML models such as neural networks or kernel machines have become game changers in the sciences and industry. Fast progress in the field of explainable AI, has made virtually any of these complex models, supervised or unsupervised, interpretable to the user. Consequently, we no longer need to give up predictivity in favor of interpretability, and we can take full advantage of strong nonlinear machine learning in practical applications.\nIn this review we have made the attempt to provide a systematic path to bring XAI to the attention of an interested readership. This included an introduction to the technical foundations of XAI, a presentation of practical algorithms such as Occlusion, Integrated Gradients and LRP, concrete examples illustrating how to use explanation techniques in practice, and a discussion of successful applications. We would like to stress that the techniques introduced in this paper can be readily and broadly applied to the workhorses of supervised and unsupervised learning, e.g. clustering, anomaly detection, kernel machines, deep networks, as well as state-of-the-art pretrained convolutional networks and LSTMs.\nXAI techniques not only shed light into the inner workings of non-linear learning machines, explaining why they arrive at their successful predictions; they also help to discover biases and quality issues in large data corpora with millions of examples [6]. This is an increasingly relevant direction since modern machine learning relies more and more on reference datasets and reference pretrained models. Furthermore, initial steps have been taken to use XAI beyond validation to arrive at better and more predictive models e.g. [119], [7], [8], [6].\nWe would like to stress the importance of XAI, notably in safety critical operations such as medical assistance or diagnosis, where the highest level of transparency is required in order to avoid fatal outcomes.\nFinally as a versatile tool in the sciences, XAI has been allowing to gain novel insights (e.g. [127], [20], [55], [145], [36], [123], [112]) ultimately contributing to further our scientific knowledge.\nWhile XAI has seen an almost exponential rise in interest (and progress) with communities forming and many workshops emerging, there is a wealth of open problems and challenges with ample opportunities to contribute (see Section IX). Concluding, we firmly believe that XAI will in the future become an indispensable practical ingredient to obtain improved, transparent, safe, fair and unbiased learning models."
    },
    {
      "heading": "ACKNOWLEDGMENT",
      "text": "This research was supported in part by the Institute for Information & Communications Technology Promotion and funded by the Korea government (MSIT) (No. 2017-0-01779),\n19\nand was partly supported by the German Ministry for Education and Research (BMBF) under Grants 01IS14013A-E, 01GQ1115, 01GQ0850, 01IS18025A and 01IS18037A; the German Research Foundation (DFG) under Grant Math+, EXC 2046/1, Project ID 390685689. Correspondence to WS, GM, KRM."
    },
    {
      "heading": "APPENDIX A IMPLEMENTING SMOOTH INTEGRATED GRADIENTS",
      "text": "In this appendix, we give the algorithm combining SmoothGrad [137] and Integrated Gradients [142], which we use in Section IV in our comparison of explanation methods. Its implementation is shown in Algorithm 5.\nAlgorithm 5 Integrated Gradients with Smoothing R = 0 for s = 1 . . . S do x\u0303 \u223c N (0, \u03c3I) for t = 1 . . . T do R = R+ (x\u2212 x\u0303) \u2207f(x\u0303+ t\u22120.5T \u00b7 (x\u2212 x\u0303))\nend for end for return 1TS \u00b7R\nThe procedure consists of a simple nested loop of S smoothing and T integration steps, where each integration starts at some random location near the origin. Here, we note that these locations are not strict root points. However, in the context of image data, random noise does not change significantly evidence in favor or against a particular class. Thus, the explanation remains approximately complete."
    },
    {
      "heading": "APPENDIX B IMPLEMENTING LAYER-WISE RELEVANCE PROPAGATION",
      "text": "In this appendix, we outline two possible implementations of LRP [13], [98]. A first one that is intuitive and based on looping forward and backward over the multiple layers of the neural network. This procedure can be applied to simple sequential structures such as VGG-16 [136]. The second approach we present is based on \u2018forward hooks\u2019 and serves to extend the LRP method to more complex architectures such as ResNet [52]."
    },
    {
      "heading": "A. Standard Sequential Implementation",
      "text": "The standard implementation is based on the forwardbackward procedure outlined in Algorithm 2. We focus here on the relprop function of this procedure, which is called at each layer to propagate relevance to the layer below. We give an implementation for the LRP-0/ /\u03b3 rules [13], [98] and one for the zB-rule [99]. The first three rules can be seen as special cases of the more general rule\nRj = \u2211 k\naj\u03c1(wjk) + \u2211\n0,j aj\u03c1(wjk) Rk\nwhere \u03c1(wjk) = wjk + \u03b3w+jk. This propagation rule can be computed in four steps.\nAlgorithm 6 LRP-0/ /\u03b3 z = + f\u03c1l (a\n(l\u22121)) (Step 1) s = R(l) z (Step 2) c = \u2207\u3008z, [s]cst.\u3009 (Step 3) R(l\u22121) = a c (Step 4) return R(l\u22121)\nThe first step applies f\u03c1l , a forward evaluation of a copy of the layer whose parameters have gone through some function \u03c1, and also adds a small positive term . The third step is conveniently expressed as a gradient of some dot product \u3008z, [s]cst.\u3009 w.r.t. the input activations. The notation [\u00b7]cst. indicates that the term has been detached from the gradient computation and is therefore treated as a constant. In PyTorch, for example, this can be achieved by calling ().data. The relprop function implemented by Algorithm 6 is applicable for most linear and convolution layers of a deep rectifier network. For the pixel-layer, we use instead the zB-rule [99], [98]:\nRi = \u2211 j xiwij \u2212 liw+ij \u2212 hjw \u2212 ij\u2211 i xiwij \u2212 liw + ij \u2212 hjw \u2212 ij Rj\nwhere li and hi are the lowest/highest possible pixel values of xi. The corresponding implementation is shown in Algorithm 7 and again consists of four steps:\nAlgorithm 7 zB-rule z = f1(x)\u2212 f+1 (l)\u2212 f \u2212 1 (h) (Step 1)\ns = R(1) z (Step 2) c = \u2207x,l,h\u3008z, [s]cst.\u3009 (Step 3) R(0) = x c1 + l c2 + h c3 (Step 4) return R(0)\nThe functions f+1 and f \u2212 1 are forward passes on copies of the first layer whose parameters have been processed by the functions max(0, \u00b7) and min(0, \u00b7) respectively."
    },
    {
      "heading": "B. Forward-Hook Implementation",
      "text": "When the architecture has non-sequential components (e.g. ResNet [52]), it is more convenient to reuse the graph traversing procedures readily implemented by the model\u2019s existing forward pass and the automatically generated gradient propagation pass. To achieve this, we can implement \u2018forward hooks\u2019 at each linear and convolution layers. In this case, we leverage the \u2018smooth gradient\u2019 view of LRP (cf. Eq. (3)) and modify the implementation of the forward pass in a way that it keeps the forward pass functionally equivalent but modifies the local gradient computation. This is achieved by strategically detaching terms from the gradient in a way that calling the gradient becomes equivalent to computing Eq. (3) at each layer. Once the forward functions have been redefined at each layer, the explanation can be computed globally by calling the gradient of the whole function as shown in Algorithm 8. (Note that unlike the original function f(x) the new function that includes the hooks receives three arguments as input: the data point x, and the bounds l and h used by the first layer.)\n20\nAlgorithm 8 LRP implementation based on forward hooks\nForward hook for intermediate layers (LRP-0/ /\u03b3) z = + f\u03c1l (a\n(l\u22121)) return z [fl(a(l\u22121)) z]cst.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Forward hook for the first layer (zB-rule) z = f1(x)\u2212 f+1 (l)\u2212 f \u2212 1 (h)\nreturn z [f1(x) z]cst. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Global LRP computation y = f(x, l,h) c1, c2, c3 = \u2207\u0302y R = x c1 + l c2 + h c3 return R\nThe forward-hook implementation produces exactly the same output as the original function f(x), but its \u2018gradient\u2019, which we denote by \u2207\u0302 is no longer the same due to the detached terms. As a result, calling the gradient of this function, and recombining it with the input yields the same desired LRP explanation as one would get with the standard LRP implementation, but has now gained applicability to a broader set of neural network architectures."
    },
    {
      "heading": "APPENDIX C EXPLANATION SOFTWARE",
      "text": "The attention to interpretability in machine learning has grown frantically throughout the past decade alongside research on, and the development of computationally efficient deep learning frameworks. This attention in turn caused a strong demand for accessible and efficient software solutions for out-of-the-box applicability of XAI. In this section we briefly highlight a collection of software toolboxes released in recent years, providing convenient access to a plethora of methods of XAI and supporting various computational backends. A summarizing overview over the presented software solutions is given in Table I, alongside a glossary of methods with respective abbreviations used throughout our review in Table II.\nOne of the earlier and comprehensive XAI software packages is the LRP Toolbox [83], providing presently up to date implementations of LRP for the \u2014 until very recently \u2014 popular Caffe deep learning framework [65], as well as Matlab and Python via custom neural network interfaces. While support for Caffe is restricted to the C++ programming language and thus to CPU hardware, it provides functionality implementing DCN, GB, DTD, and SA and can be built and used as a stand-alone executable binary for predictors based on the Caffe neural network format. The sub-packages available for Matlab and Python provide out-of-the-box support for LRP and SA, while being easily extensible via custom neural network modules written with clarity and the methods\u2019 intelligibility in mind. The cupy [109] backend constitutes an alternative to the CPU-bound numpy [110] package, providing optional support for modern GPU hardware from NVIDIA.\nBoth the DeepExplain [4] and iNNvestigate [3] toolboxes built on top of the popular Keras [26] package for Python with TensorFlow backend for explaining Deep Neural Network models, and thus provide support for both CPU and GPU hardware and convenient access for users of Keras models. While the more recent iNNvestigate Toolbox implements a superset of the modified backpropagation methods available in DeepExplain, the latter also offers functionalty for perturbationbased attribution methods, i.e. the Occlusion method [159] and Shapley Value Resampling [25]. For explaining a model\u2019s prediction DeepExplain allows for an ad-hoc selection of the explanation method via pythonic context managers. The iNNvestigate package on the other hand operates by attaching and automatically configuring (several) modified backward graphs called \u201canalyzers\u201d to a model of interest \u2014 one per XAI method to compute attributions with.\nA present trend in the machine learning community is a migration to the PyTorch framework with its eager execution paradigm, away from other backends. Both the TorchRay [37] and Captum [76] packages for Python and PyTorch enable the use of interpretability methods for neural network models defined in context of PyTorch\u2019s high level neural network description modules. Captum can be understood as a rich selection of XAI methods based on modified backprop and is part of the PyTorch project itself. While not as extensive as Captum, the TorchRay package offers a series benchmarks for XAI alongside its selection of (benchmarked) interpretability methods."
    }
  ],
  "title": "Toward Interpretable Machine Learning: Transparent Deep Neural Networks and Beyond",
  "year": 2020
}
