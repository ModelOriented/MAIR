{
  "abstractText": "There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers\u2019 intuition of what constitutes a \u2018good\u2019 explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.",
  "authors": [
    {
      "affiliations": [],
      "name": "Tim Miller"
    }
  ],
  "id": "SP:9cecfd1391e808970272f2a387bea582e2711258",
  "references": [
    {
      "authors": [
        "D. Allemang",
        "M.C. Tanner",
        "T. Bylander",
        "J.R. Josephson"
      ],
      "title": "Computational Complexity of Hypothesis Assembly",
      "venue": "in: IJCAI, vol. 87, 1112\u20131117",
      "year": 1987
    },
    {
      "authors": [
        "C. Antaki",
        "I. Leudar"
      ],
      "title": "Explaining in conversation: Towards an argument model",
      "venue": "European Journal of Social Psychology 22 (2) ",
      "year": 1992
    },
    {
      "authors": [
        "A. Arioua",
        "M. Croitoru"
      ],
      "title": "Formalizing explanatory dialogues",
      "venue": "in: International Conference on Scalable Uncertainty Management, Springer, 282\u2013297",
      "year": 2015
    },
    {
      "authors": [
        "J.L. Aronson"
      ],
      "title": "On the grammar of \u2018cause",
      "venue": "Synthese 22 (3) ",
      "year": 1971
    },
    {
      "authors": [
        "D. Baehrens",
        "T. Schroeter",
        "S. Harmeling",
        "M. Kawanabe",
        "K. Hansen",
        "K.-R. M\u00c3\u017eller"
      ],
      "title": "How to explain individual classification decisions",
      "venue": "Journal of Machine Learning Research 11 (Jun) ",
      "year": 2010
    },
    {
      "authors": [
        "P. Besnard",
        "A. Hunter"
      ],
      "title": "Elements of argumentation",
      "venue": "vol. 47, MIT press Cambridge",
      "year": 2008
    },
    {
      "authors": [
        "O. Biran",
        "C. Cotton"
      ],
      "title": "Explanation and justification in machine learning: A survey",
      "venue": "in: IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI), 8\u201313",
      "year": 2017
    },
    {
      "authors": [
        "A. Boonzaier",
        "J. McClure",
        "R.M. Sutton"
      ],
      "title": "Distinguishing the effects of beliefs and preconditions: The folk psychology of goals and actions",
      "venue": "European Journal of Social Psychology 35 (6) ",
      "year": 2005
    },
    {
      "authors": [
        "R.I. Brafman"
      ],
      "title": "C",
      "venue": "Domshlak, From One to Many: Planning for Loosely Coupled Multi-Agent Systems., in: International Conference on Automated Planning and Scheduling, 28\u201335",
      "year": 2008
    },
    {
      "authors": [
        "J. Broekens",
        "M. Harbers",
        "K. Hindriks"
      ],
      "title": "K",
      "venue": "Van Den Bosch, C. Jonker, J.-J. Meyer, Do you get it? User-evaluated explainable BDI agents, in: German Conference on Multiagent System Technologies, Springer, 28\u201339",
      "year": 2010
    },
    {
      "authors": [
        "S. Bromberger"
      ],
      "title": "Why\u2013questions",
      "venue": "in: R. G. Colodny (Ed.), Mind and Cosmos: Essays in Contemporary Science and Philosophy, Pittsburgh University Press, Pittsburgh, 68\u2013111",
      "year": 1966
    },
    {
      "authors": [
        "B. Buchanan",
        "E. Shortliffe"
      ],
      "title": "Rule-based expert systems: the MYCIN experiments of the Stanford Heuristic Programming Project",
      "venue": "Addison-Wesley",
      "year": 1984
    },
    {
      "authors": [
        "A. Burguet",
        "D. Hilton"
      ],
      "title": "Effets de contexte sur l\u2019explication causale",
      "venue": "in: M. B. et A. Trognon (Ed.), Psychologie Sociale et Communication, Paris: Dunod, 219\u2013228",
      "year": 2004
    },
    {
      "authors": [
        "R.M. Byrne"
      ],
      "title": "The Construction of Explanations",
      "venue": "in: AI and Cognitive Science\u201990, Springer, 337\u2013 351",
      "year": 1991
    },
    {
      "authors": [
        "A. Cawsey"
      ],
      "title": "Generating Interactive Explanations",
      "venue": "in: AAAI,",
      "year": 1991
    },
    {
      "authors": [
        "A. Cawsey"
      ],
      "title": "Explanation and interaction: the computer generation of explanatory dialogues",
      "venue": "MIT press",
      "year": 1992
    },
    {
      "authors": [
        "A. Cawsey"
      ],
      "title": "Planning interactive explanations",
      "venue": "International Journal of Man-Machine Studies 38 (2) ",
      "year": 1993
    },
    {
      "authors": [
        "A. Cawsey"
      ],
      "title": "User modelling in interactive explanations",
      "venue": "User Modeling and User-Adapted Interaction 3 ",
      "year": 1993
    },
    {
      "authors": [
        "T. Chakraborti",
        "S. Sreedharan",
        "Y. Zhang",
        "S. Kambhampati"
      ],
      "title": "Plan explanations as model reconciliation: Moving beyond explanation as soliloquy",
      "venue": "in: Proceedings of IJCAI, URL https: //www.ijcai.org/proceedings/2017/0023.pdf",
      "year": 2017
    },
    {
      "authors": [
        "K. Chan",
        "T.-W. Lee",
        "P.A. Sample",
        "M.H. Goldbaum",
        "R.N. Weinreb",
        "T.J. Sejnowski"
      ],
      "title": "Comparison of machine learning and traditional classifiers in glaucoma diagnosis",
      "venue": "IEEE Transactions on Biomedical Engineering 49 (9) ",
      "year": 2002
    },
    {
      "authors": [
        "B. Chandrasekaran",
        "M.C. Tanner",
        "J.R. Josephson"
      ],
      "title": "Explaining control strategies in problem solving",
      "venue": "IEEE Expert 4 (1) ",
      "year": 1989
    },
    {
      "authors": [
        "E. Charniak",
        "R. Goldman"
      ],
      "title": "A probabilistic model of plan recognition",
      "venue": "in: Proceedings of the ninth National conference on Artificial intelligence-Volume 1, AAAI Press, 160\u2013165",
      "year": 1991
    },
    {
      "authors": [
        "J.Y. Chen",
        "K. Procci",
        "M. Boyce",
        "J. Wright",
        "A. Garcia",
        "M. Barnes"
      ],
      "title": "Situation awareness-based agent transparency",
      "venue": "Tech. Rep. ARL-TR-6905, U.S. Army Research Laboratory",
      "year": 2014
    },
    {
      "authors": [
        "Y. Chevaleyre",
        "U. Endriss",
        "J. Lang",
        "N. Maudet"
      ],
      "title": "A short introduction to computational social choice",
      "venue": "in: International Conference on Current Trends in Theory and Practice of Computer Science, Springer, 51\u201369",
      "year": 2007
    },
    {
      "authors": [
        "S. Chin-Parker",
        "A. Bradner"
      ],
      "title": "Background shifts affect explanatory style: how a pragmatic theory of explanation accounts for background effects in the generation of explanations",
      "venue": "Cognitive Processing 11 (3) ",
      "year": 2010
    },
    {
      "authors": [
        "S. Chin-Parker",
        "J. Cantelon"
      ],
      "title": "Contrastive Constraints Guide Explanation-Based Category Learning",
      "venue": "Cognitive science 41 (6) ",
      "year": 2017
    },
    {
      "authors": [
        "H. Chockler",
        "J.Y. Halpern"
      ],
      "title": "Responsibility and blame: A structural-model approach",
      "venue": "Journal of Artificial Intelligence Research 22 ",
      "year": 2004
    },
    {
      "authors": [
        "A. Cimpian",
        "E. Salomon"
      ],
      "title": "The inherence heuristic: An intuitive means of making sense of the world",
      "venue": "and a potential precursor to psychological essentialism, Behavioral and Brain Sciences 37 (5) ",
      "year": 2014
    },
    {
      "authors": [
        "A. Cooper"
      ],
      "title": "The inmates are running the asylum: Why high-tech products drive us crazy and how to restore the sanity",
      "venue": "Sams Indianapolis, IN, USA",
      "year": 2004
    },
    {
      "authors": [
        "G.C. Davey"
      ],
      "title": "Characteristics of individuals with fear of spiders",
      "venue": "Anxiety Research 4 (4) ",
      "year": 1991
    },
    {
      "authors": [
        "M.M. de Graaf",
        "B.F. Malle"
      ],
      "title": "How People Explain Action (and Autonomous Intelligent Systems Should Too)",
      "venue": "in: AAAI Fall Symposium on Artificial Intelligence for Human-Robot Interaction,",
      "year": 2017
    },
    {
      "authors": [
        "D.C. Dennett"
      ],
      "title": "The intentional stance",
      "venue": "MIT press",
      "year": 1989
    },
    {
      "authors": [
        "D.C. Dennett"
      ],
      "title": "From bacteria to Bach and back: The evolution of minds",
      "venue": "WW Norton & Company",
      "year": 2017
    },
    {
      "authors": [
        "F. Dignum",
        "R. Prada",
        "G.J. Hofstede"
      ],
      "title": "From autistic to social agents",
      "venue": "in: Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, IFAAMAS, 1161\u20131164",
      "year": 2014
    },
    {
      "authors": [
        "D.H. Dodd",
        "J.M. Bradshaw"
      ],
      "title": "Leading questions and memory: Pragmatic constraints",
      "venue": "Journal of Memory and Language 19 (6) ",
      "year": 1980
    },
    {
      "authors": [
        "P. Dowe"
      ],
      "title": "Wesley Salmon\u2019s process theory of causality and the conserved quantity theory",
      "venue": "Philosophy of Science 59 (2) ",
      "year": 1992
    },
    {
      "authors": [
        "T. Eiter",
        "T. Lukasiewicz"
      ],
      "title": "Complexity results for structure-based causality",
      "venue": "Artificial Intelligence 142 (1) ",
      "year": 2002
    },
    {
      "authors": [
        "T. Eiter",
        "T. Lukasiewicz"
      ],
      "title": "Causes and explanations in the structural-model approach: Tractable cases",
      "venue": "Artificial Intelligence 170 (6-7) ",
      "year": 2006
    },
    {
      "authors": [
        "R. Fagin",
        "J. Halpern",
        "Y. Moses",
        "M. Vardi"
      ],
      "title": "Reasoning about knowledge",
      "venue": "vol. 4, MIT press Cambridge",
      "year": 1995
    },
    {
      "authors": [
        "D. Fair"
      ],
      "title": "Causation and the Flow of Energy",
      "venue": "Erkenntnis 14 (3) ",
      "year": 1979
    },
    {
      "authors": [
        "G. Fischer"
      ],
      "title": "User modeling in human\u2013computer interaction",
      "venue": "User modeling and user-adapted interaction 11 (1-2) ",
      "year": 2001
    },
    {
      "authors": [
        "M. Fox",
        "D. Long",
        "D. Magazzeni"
      ],
      "title": "Explainable Planning",
      "venue": "in: IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI), URL https://arxiv.org/pdf/1709.10256",
      "year": 2017
    },
    {
      "authors": [
        "T. Gerstenberg",
        "D.A. Lagnado"
      ],
      "title": "Spreading the blame: The allocation of responsibility amongst multiple agents",
      "venue": "Cognition 115 (1) ",
      "year": 2010
    },
    {
      "authors": [
        "T. Gerstenberg",
        "M.F. Peterson",
        "N.D. Goodman",
        "D.A. Lagnado",
        "J.B. Tenenbaum"
      ],
      "title": "Eye-tracking causality",
      "venue": "Psychological science 28 (12) ",
      "year": 2017
    },
    {
      "authors": [
        "M. Ghallab",
        "D. Nau",
        "P. Traverso"
      ],
      "title": "Automated Planning: theory and practice",
      "venue": "Elsevier",
      "year": 2004
    },
    {
      "authors": [
        "D.T. Gilbert",
        "P.S. Malone"
      ],
      "title": "The correspondence bias",
      "venue": "Psychological bulletin 117 (1) ",
      "year": 1995
    },
    {
      "authors": [
        "C. Ginet"
      ],
      "title": "In defense of a non-causal account of reasons explanations",
      "venue": "The Journal of Ethics 12 (3-4) ",
      "year": 2008
    },
    {
      "authors": [
        "L. Giordano",
        "C. Schwind"
      ],
      "title": "Conditional logic of actions and causation",
      "venue": "Artificial Intelligence 157 (1- 61 2) ",
      "year": 2004
    },
    {
      "authors": [
        "V. Girotto",
        "P. Legrenzi",
        "A. Rizzo"
      ],
      "title": "Event controllability in counterfactual thinking",
      "venue": "Acta Psychologica 78 (1) ",
      "year": 1991
    },
    {
      "authors": [
        "M. Greaves",
        "H. Holmback",
        "J. Bradshaw"
      ],
      "title": "What is a conversation policy",
      "venue": "in: Issues in Agent Communication, Springer, 118\u2013131",
      "year": 2000
    },
    {
      "authors": [
        "H.P. Grice"
      ],
      "title": "Logic and conversation",
      "venue": "in: Syntax and semantics 3: Speech arts, New York: Academic Press, 41\u201358",
      "year": 1975
    },
    {
      "authors": [
        "J.Y. Halpern"
      ],
      "title": "Axiomatizing causal reasoning",
      "venue": "Journal of Artificial Intelligence Research 12 ",
      "year": 2000
    },
    {
      "authors": [
        "J.Y. Halpern",
        "J. Pearl"
      ],
      "title": "Causes and explanations: A structural-model approach",
      "venue": "Part I: Causes, The British Journal for the Philosophy of Science 56 (4) ",
      "year": 2005
    },
    {
      "authors": [
        "J.Y. Halpern",
        "J. Pearl"
      ],
      "title": "Causes and explanations: A structural-model approach",
      "venue": "Part II: Explanations, The British Journal for the Philosophy of Science 56 (4) ",
      "year": 2005
    },
    {
      "authors": [
        "R.J. Hankinson"
      ],
      "title": "Cause and explanation in ancient Greek thought",
      "venue": "Oxford University Press",
      "year": 2001
    },
    {
      "authors": [
        "N.R. Hanson"
      ],
      "title": "Patterns of discovery: An inquiry into the conceptual foundations of science",
      "venue": "CUP Archive",
      "year": 1965
    },
    {
      "authors": [
        "G.H. Harman"
      ],
      "title": "The inference to the best explanation",
      "venue": "The philosophical review 74 (1) ",
      "year": 1965
    },
    {
      "authors": [
        "H.L.A. Hart",
        "T. Honor\u00e9"
      ],
      "title": "Causation in the Law",
      "venue": "OUP Oxford",
      "year": 1985
    },
    {
      "authors": [
        "B. Hayes",
        "J.A. Shah"
      ],
      "title": "Improving Robot Controller Transparency Through Autonomous Policy Explanation",
      "venue": "in: Proceedings of the 12th ACM/IEEE International Conference on Human-Robot Interaction ",
      "year": 2017
    },
    {
      "authors": [
        "F. Heider"
      ],
      "title": "The psychology of interpersonal relations",
      "venue": "New York: Wiley",
      "year": 1958
    },
    {
      "authors": [
        "F. Heider",
        "M. Simmel"
      ],
      "title": "An experimental study of apparent behavior",
      "venue": "The American Journal of Psychology 57 (2) ",
      "year": 1944
    },
    {
      "authors": [
        "C.G. Hempel",
        "P. Oppenheim"
      ],
      "title": "Studies in the Logic of Explanation",
      "venue": "Philosophy of Science 15 (2) ",
      "year": 1948
    },
    {
      "authors": [
        "G. Hesslow"
      ],
      "title": "The problem of causal selection",
      "venue": "Contemporary science and natural explanation: Commonsense conceptions of causality ",
      "year": 1988
    },
    {
      "authors": [
        "D. Hilton"
      ],
      "title": "Social Attribution and Explanation",
      "venue": "in: Oxford Handbook of Causal Reasoning, Oxford University Press, 645\u2013676",
      "year": 2017
    },
    {
      "authors": [
        "D.J. Hilton"
      ],
      "title": "Logic and causal attribution",
      "venue": "in: Contemporary science and natural explanation: Commonsense conceptions of causality, New York University Press, 33\u201365",
      "year": 1988
    },
    {
      "authors": [
        "D.J. Hilton"
      ],
      "title": "Conversational processes and causal explanation",
      "venue": "Psychological Bulletin 107 (1) ",
      "year": 1990
    },
    {
      "authors": [
        "D.J. Hilton"
      ],
      "title": "Mental models and causal explanation: Judgements of probable cause and explanatory relevance",
      "venue": "Thinking & Reasoning 2 (4) ",
      "year": 1996
    },
    {
      "authors": [
        "D.J. Hilton",
        "J. McClure",
        "B. Slugoski"
      ],
      "title": "Counterfactuals",
      "venue": "conditionals and causality: A social psychological perspective, in: D. R. Mande, D. J. Hilton, P. Catellani (Eds.), The psychology of counterfactual thinking, London: Routledge, 44\u201360",
      "year": 2005
    },
    {
      "authors": [
        "D.J. Hilton",
        "J. McClure",
        "R.M. Sutton"
      ],
      "title": "Selecting explanations from causal chains: Do statistical principles explain preferences for voluntary causes",
      "venue": "European Journal of Social Psychology 40 (3) ",
      "year": 2010
    },
    {
      "authors": [
        "D.J. Hilton",
        "J.L. McClure",
        "R. Slugoski"
      ],
      "title": "Ben",
      "venue": "The Course of Events: Counterfactuals, Causal Sequences and Explanation, in: D. R. Mandel, D. J. Hilton, P. Catellani (Eds.), The Psychology of Counterfactual Thinking, Routledge",
      "year": 2005
    },
    {
      "authors": [
        "D.J. Hilton",
        "B.R. Slugoski"
      ],
      "title": "Knowledge-based causal attribution: The abnormal conditions focus model",
      "venue": "Psychological review 93 (1) ",
      "year": 1986
    },
    {
      "authors": [
        "R.R. Hoffman",
        "G. Klein"
      ],
      "title": "Explaining explanation",
      "venue": "part 1: theoretical foundations, IEEE Intelligent Systems 32 (3) ",
      "year": 2017
    },
    {
      "authors": [
        "D. Hume"
      ],
      "title": "An enquiry concerning human understanding: A critical edition",
      "venue": "vol. 3, Oxford University Press",
      "year": 2000
    },
    {
      "authors": [
        "J.M. Jaspars",
        "D.J. Hilton"
      ],
      "title": "Mental models of causal reasoning",
      "venue": "in: The social psychology of knowledge, Cambridge University Press, 335\u2013358",
      "year": 1988
    },
    {
      "authors": [
        "J.R. Josephson",
        "S.G. Josephson"
      ],
      "title": "Abductive inference: Computation",
      "venue": "philosophy, technology, Cambridge University Press",
      "year": 1996
    },
    {
      "authors": [
        "D. Kahneman"
      ],
      "title": "Thinking",
      "venue": "fast and slow, Macmillan",
      "year": 2011
    },
    {
      "authors": [
        "D. Kahneman",
        "A. Tversky"
      ],
      "title": "The simulation heuristic",
      "venue": "in: P. S. D. Kahneman, A. Tversky (Eds.), Judgment under Uncertainty: Heuristics and Biases, New York: Cambridge University Press",
      "year": 1982
    },
    {
      "authors": [
        "Y. Kashima",
        "A. McKintyre",
        "P. Clifford"
      ],
      "title": "The category of the mind: Folk psychology of belief",
      "venue": "desire, and intention, Asian Journal of Social Psychology 1 (3) ",
      "year": 1998
    },
    {
      "authors": [
        "A. Kass",
        "D. Leake"
      ],
      "title": "Types of Explanations",
      "venue": "Tech. Rep. ADA183253, DTIC Document",
      "year": 1987
    },
    {
      "authors": [
        "H.H. Kelley"
      ],
      "title": "Attribution theory in social psychology",
      "venue": "in: Nebraska symposium on motivation, University of Nebraska Press, 192\u2013238",
      "year": 1967
    },
    {
      "authors": [
        "H.H. Kelley"
      ],
      "title": "Causal schemata and the attribution process",
      "venue": "General Learning Press, Morristown, NJ",
      "year": 1972
    },
    {
      "authors": [
        "J. Knobe"
      ],
      "title": "Intentional action and side effects in ordinary language",
      "venue": "Analysis 63 (279) ",
      "year": 2003
    },
    {
      "authors": [
        "T. Kulesza",
        "M. Burnett",
        "W.-K. Wong",
        "S. Stumpf"
      ],
      "title": "Principles of explanatory debugging to personalize interactive machine learning",
      "venue": "in: Proceedings of the 20th International Conference on Intelligent User Interfaces, ACM, 126\u2013137",
      "year": 2015
    },
    {
      "authors": [
        "T. Kulesza",
        "S. Stumpf",
        "M. Burnett",
        "S. Yang",
        "I. Kwan",
        "W.-K. Wong"
      ],
      "title": "Too much",
      "venue": "too little, or just right? Ways explanations impact end users\u2019 mental models, in: Visual Languages and Human- Centric Computing (VL/HCC), 2013 IEEE Symposium on, IEEE, 3\u201310",
      "year": 2013
    },
    {
      "authors": [
        "T. Kulesza",
        "S. Stumpf",
        "W.-K. Wong",
        "M.M. Burnett",
        "S. Perona",
        "A. Ko",
        "I. Oberst"
      ],
      "title": "Why-oriented end-user debugging of naive Bayes text classification",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS) 1 (1) ",
      "year": 2011
    },
    {
      "authors": [
        "D.A. Lagnado",
        "S. Channon"
      ],
      "title": "Judgments of cause and blame: The effects of intentionality and foreseeability",
      "venue": "Cognition 108 (3) ",
      "year": 2008
    },
    {
      "authors": [
        "P. Langley",
        "B. Meadows",
        "M. Sridharan",
        "D. Choi"
      ],
      "title": "Explainable Agency for Intelligent Autonomous Systems",
      "venue": "in: Proceedings of the Twenty-Ninth Annual Conference on Innovative Applications of Artificial Intelligence, AAAI Press",
      "year": 2017
    },
    {
      "authors": [
        "D.B. Leake"
      ],
      "title": "Goal-Based Explanation Evaluation",
      "venue": "Cognitive Science 15 (4) ",
      "year": 1991
    },
    {
      "authors": [
        "D.B. Leake"
      ],
      "title": "Abduction",
      "venue": "experience, and goals: A model of everyday abductive explanation, Journal of Experimental & Theoretical Artificial Intelligence 7 (4) ",
      "year": 1995
    },
    {
      "authors": [
        "J. Leddo",
        "R.P. Abelson",
        "P.H. Gross"
      ],
      "title": "Conjunctive explanations: When two reasons are better than one",
      "venue": "Journal of Personality and Social Psychology 47 (5) ",
      "year": 1984
    },
    {
      "authors": [
        "H.J. Levesque"
      ],
      "title": "A knowledge-level account of abduction",
      "venue": "in: IJCAI, 1061\u20131067",
      "year": 1989
    },
    {
      "authors": [
        "D. Lewis"
      ],
      "title": "Causation",
      "venue": "The Journal of Philosophy 70 (17) ",
      "year": 1974
    },
    {
      "authors": [
        "D. Lewis"
      ],
      "title": "Causal explanation",
      "venue": "Philosophical Papers 2 ",
      "year": 1986
    },
    {
      "authors": [
        "B.Y. Lim",
        "A.K. Dey"
      ],
      "title": "Assessing demand for intelligibility in context-aware applications",
      "venue": "in: Proceedings of the 11th international conference on Ubiquitous computing, ACM, 195\u2013204",
      "year": 2009
    },
    {
      "authors": [
        "M.P. Linegang",
        "H.A. Stoner",
        "M.J. Patterson",
        "B.D. Seppelt",
        "J.D. Hoffman",
        "Z.B. Crittendon",
        "J.D. Lee"
      ],
      "title": "Human-automation collaboration in dynamic mission planning: A challenge requiring an ecological approach",
      "venue": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting 50 (23) ",
      "year": 2006
    },
    {
      "authors": [
        "P. Lipton"
      ],
      "title": "Contrastive explanation",
      "venue": "Royal Institute of Philosophy Supplement 27 ",
      "year": 1990
    },
    {
      "authors": [
        "T. Lombrozo"
      ],
      "title": "The structure and function of explanations",
      "venue": "Trends in Cognitive Sciences 10 (10) ",
      "year": 2006
    },
    {
      "authors": [
        "T. Lombrozo"
      ],
      "title": "Simplicity and probability in causal explanation",
      "venue": "Cognitive psychology 55 (3) ",
      "year": 2007
    },
    {
      "authors": [
        "T. Lombrozo"
      ],
      "title": "Explanation and categorization: How \u201cwhy?\u201d informs \u201cwhat?",
      "venue": "Cognition 110 (2) ",
      "year": 2009
    },
    {
      "authors": [
        "T. Lombrozo"
      ],
      "title": "Causal\u2013explanatory pluralism: How intentions",
      "venue": "functions, and mechanisms influence causal ascriptions, Cognitive Psychology 61 (4) ",
      "year": 2010
    },
    {
      "authors": [
        "T. Lombrozo"
      ],
      "title": "Explanation and abductive inference",
      "venue": "Oxford handbook of thinking and reasoning ",
      "year": 2012
    },
    {
      "authors": [
        "T. Lombrozo",
        "N.Z. Gwynne"
      ],
      "title": "Explanation and inference: mechanistic and functional explanations guide property generalization",
      "venue": "Frontiers in human neuroscience 8 ",
      "year": 2014
    },
    {
      "authors": [
        "J.L. Mackie"
      ],
      "title": "The cement of the universe",
      "venue": "Oxford",
      "year": 1980
    },
    {
      "authors": [
        "B.F. Malle"
      ],
      "title": "How people explain behavior: A new theoretical framework",
      "venue": "Personality and Social Psychology Review 3 (1) ",
      "year": 1999
    },
    {
      "authors": [
        "B.F. Malle"
      ],
      "title": "How the mind explains behavior: Folk explanations",
      "venue": "meaning, and social interaction, 63 MIT Press",
      "year": 2004
    },
    {
      "authors": [
        "B.F. Malle"
      ],
      "title": "Attribution theories: How people make sense of behavior",
      "venue": "Theories in Social Psychology ",
      "year": 2011
    },
    {
      "authors": [
        "B.F. Malle"
      ],
      "title": "Time to Give Up the Dogmas of Attribution: An Alternative Theory of Behavior Explanation",
      "venue": "Advances in Experimental Social Psychology 44 (1) ",
      "year": 2011
    },
    {
      "authors": [
        "B.F. Malle",
        "J. Knobe"
      ],
      "title": "The folk concept of intentionality",
      "venue": "Journal of Experimental Social Psychology 33 (2) ",
      "year": 1997
    },
    {
      "authors": [
        "B.F. Malle",
        "J. Knobe"
      ],
      "title": "M",
      "venue": "J. O\u2019Laughlin, G. E. Pearce, S. E. Nelson, Conceptual structure and social functions of behavior explanations: Beyond person\u2013situation attributions, Journal of Personality and Social Psychology 79 (3) ",
      "year": 2000
    },
    {
      "authors": [
        "B.F. Malle",
        "J.M. Knobe",
        "S.E. Nelson"
      ],
      "title": "Actor-observer asymmetries in explanations of behavior: new answers to an old question",
      "venue": "Journal of Personality and Social Psychology 93 (4) ",
      "year": 2007
    },
    {
      "authors": [
        "B.F. Malle",
        "G.E. Pearce"
      ],
      "title": "Attention to behavioral events during interaction: Two actor-observer gaps and three attempts to close them",
      "venue": "Journal of Personality and Social Psychology 81 (2) ",
      "year": 2001
    },
    {
      "authors": [
        "D. Marr"
      ],
      "title": "Vision: A computational investigation into the human representation and processing of visual information",
      "venue": "Inc., New York, NY",
      "year": 1982
    },
    {
      "authors": [
        "D. Marr",
        "T. Poggio"
      ],
      "title": "From understanding computation to understanding neural circuitry",
      "venue": "AI Memos AIM-357, MIT",
      "year": 1976
    },
    {
      "authors": [
        "R. McCloy",
        "R.M. Byrne"
      ],
      "title": "Counterfactual thinking about controllable events",
      "venue": "Memory & Cognition 28 (6) ",
      "year": 2000
    },
    {
      "authors": [
        "J. McClure"
      ],
      "title": "Goal-based explanations of actions and outcomes",
      "venue": "European Review of Social Psychology 12 (1) ",
      "year": 2002
    },
    {
      "authors": [
        "J. McClure",
        "D. Hilton"
      ],
      "title": "For you can\u2019t always get what you want: When preconditions are better explanations than goals",
      "venue": "British Journal of Social Psychology 36 (2) ",
      "year": 1997
    },
    {
      "authors": [
        "J. McClure",
        "D. Hilton",
        "J. Cowan",
        "L. Ishida",
        "M. Wilson"
      ],
      "title": "When rich or poor people buy expensive objects: Is the question how or why",
      "venue": "Journal of Language and Social Psychology 20 ",
      "year": 2001
    },
    {
      "authors": [
        "J. McClure",
        "D.J. Hilton"
      ],
      "title": "Are goals or preconditions better explanations? It depends on the question",
      "venue": "European Journal of Social Psychology 28 (6) ",
      "year": 1998
    },
    {
      "authors": [
        "J.L. McClure",
        "R.M. Sutton",
        "D.J. Hilton"
      ],
      "title": "The Role of Goal-Based Explanations",
      "venue": "in: Social judgments: Implicit and explicit processes, vol. 5, Cambridge University Press, 306",
      "year": 2003
    },
    {
      "authors": [
        "A.L. McGill",
        "J.G. Klein"
      ],
      "title": "Contrastive and counterfactual reasoning in causal judgment",
      "venue": "Journal of Personality and Social Psychology 64 (6) ",
      "year": 1993
    },
    {
      "authors": [
        "P. Menzies",
        "H. Price"
      ],
      "title": "Causation as a secondary quality",
      "venue": "The British Journal for the Philosophy of Science 44 (2) ",
      "year": 1993
    },
    {
      "authors": [
        "J.E. Mercado",
        "M.A. Rupp",
        "J.Y. Chen",
        "M.J. Barnes",
        "D. Barber",
        "K. Procci"
      ],
      "title": "Intelligent agent transparency in human\u2013agent teaming for Multi-UxV management",
      "venue": "Human Factors 58 (3) ",
      "year": 2016
    },
    {
      "authors": [
        "J.S. Mill"
      ],
      "title": "A system of logic: The collected works of John Stuart Mill",
      "venue": "vol. III",
      "year": 1973
    },
    {
      "authors": [
        "D.T. Miller",
        "S. Gunasegaram"
      ],
      "title": "Temporal order and the perceived mutability of events: Implications for blame assignment",
      "venue": "Journal of personality and social psychology 59 (6) ",
      "year": 1990
    },
    {
      "authors": [
        "T. Miller",
        "P. Howe",
        "L. Sonenberg"
      ],
      "title": "Explainable AI: Beware of Inmates Running the Asylum",
      "venue": "in: IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI), 36\u201342, URL http://people. eng.unimelb.edu.au/tmiller/pubs/explanation-inmates.pdf",
      "year": 2017
    },
    {
      "authors": [
        "T.M. Mitchell",
        "R.M. Keller",
        "S.T. Kedar-Cabelli"
      ],
      "title": "Explanation-based generalization: A unifying view",
      "venue": "Machine learning 1 (1) ",
      "year": 1986
    },
    {
      "authors": [
        "J.D. Moore",
        "C.L. Paris"
      ],
      "title": "Planning text for advisory dialogues: Capturing intentional and rhetorical information",
      "venue": "Computational linguistics 19 (4) ",
      "year": 1993
    },
    {
      "authors": [
        "C. Muise",
        "V. Belle",
        "P. Felli",
        "S. McIlraith",
        "T. Miller",
        "A.R. Pearce",
        "L. Sonenberg"
      ],
      "title": "Planning Over Multi-Agent Epistemic States: A Classical Planning Approach",
      "venue": "in: B. Bonet, S. Koenig (Eds.), Proceedings of AAAI 2015, 1\u20138",
      "year": 2015
    },
    {
      "authors": [
        "M.J. O\u2019Laughlin",
        "B.F. Malle"
      ],
      "title": "How people explain actions performed by groups and individuals, Journal of Personality and Social Psychology",
      "year": 2002
    },
    {
      "authors": [
        "J. Overton"
      ],
      "title": "Scientific Explanation and Computation",
      "venue": "in: D. B. L. Thomas Roth-Berghofer, Nava Tintarev (Ed.), Proceedings of the 6th International Explanation-Aware Computing (ExaCt) workshop, 41\u201350",
      "year": 2011
    },
    {
      "authors": [
        "J.A. Overton"
      ],
      "title": "Explanation in Science",
      "venue": "Ph.D. thesis, The University of Western Ontario",
      "year": 2012
    },
    {
      "authors": [
        "J.A. Overton"
      ],
      "title": "Explain\u201d in scientific discourse",
      "venue": "Synthese 190 (8) ",
      "year": 2013
    },
    {
      "authors": [
        "J. Pearl",
        "D. Mackenzie"
      ],
      "title": "The Book of Why: The New Science of Cause and Effect",
      "venue": "Hachette UK",
      "year": 2018
    },
    {
      "authors": [
        "C.S. Peirce"
      ],
      "title": "Harvard lectures on pragmatism",
      "venue": "Collected Papers v. 5",
      "year": 1903
    },
    {
      "authors": [
        "R. Petrick",
        "M.E. Foster"
      ],
      "title": "Using General-Purpose Planning for Action Selection in Human-Robot Interaction",
      "venue": "in: AAAI 2016 Fall Symposium on Artificial Intelligence for Human-Robot Interaction",
      "year": 2016
    },
    {
      "authors": [
        "D. Poole"
      ],
      "title": "Normality and Faults in logic-based diagnosis",
      "venue": "in: IJCAI,",
      "year": 1989
    },
    {
      "authors": [
        "H.E. Pople"
      ],
      "title": "On the mechanization of abductive logic",
      "venue": "in: IJCAI, vol. 73, 147\u2013152",
      "year": 1973
    },
    {
      "authors": [
        "K. Popper"
      ],
      "title": "The logic of scientific discovery",
      "venue": "Routledge",
      "year": 2005
    },
    {
      "authors": [
        "H. Prakken"
      ],
      "title": "Formal systems for persuasion dialogue",
      "venue": "The Knowledge Engineering Review 21 (02) ",
      "year": 2006
    },
    {
      "authors": [
        "S. Prasada"
      ],
      "title": "The scope of formal explanation",
      "venue": "Psychonomic Bulletin & Review ",
      "year": 2017
    },
    {
      "authors": [
        "S. Prasada",
        "E.M. Dillingham"
      ],
      "title": "Principled and statistical connections in common sense conception",
      "venue": "Cognition 99 (1) ",
      "year": 2006
    },
    {
      "authors": [
        "J. Preston",
        "N. Epley"
      ],
      "title": "Explanations versus applications: The explanatory power of valuable beliefs",
      "venue": "Psychological Science 16 (10) ",
      "year": 2005
    },
    {
      "authors": [
        "M. Ranney",
        "P. Thagard"
      ],
      "title": "Explanatory coherence and belief revision in naive physics",
      "venue": "in: Proceedings of the Tenth Annual Conference of the Cognitive Science Society, 426\u2013432",
      "year": 1988
    },
    {
      "authors": [
        "A.S. Rao"
      ],
      "title": "M",
      "venue": "P. Georgeff, BDI agents: From theory to practice., in: ICMAS, vol. 95, 312\u2013319",
      "year": 1995
    },
    {
      "authors": [
        "S.J. Read",
        "A. Marcus-Newhall"
      ],
      "title": "Explanatory coherence in social explanations: A parallel distributed processing account",
      "venue": "Journal of Personality and Social Psychology 65 (3) ",
      "year": 1993
    },
    {
      "authors": [
        "B. Rehder"
      ],
      "title": "A causal-model theory of conceptual representation and categorization",
      "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition 29 (6) ",
      "year": 2003
    },
    {
      "authors": [
        "B. Rehder"
      ],
      "title": "When similarity and causality compete in category-based property generalization",
      "venue": "Memory & Cognition 34 (1) ",
      "year": 2006
    },
    {
      "authors": [
        "R. Reiter"
      ],
      "title": "A theory of diagnosis from first principles",
      "venue": "Artificial intelligence 32 (1) ",
      "year": 1987
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why Should I Trust You?: Explaining the Predictions of Any Classifier",
      "venue": "in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 1135\u20131144",
      "year": 2016
    },
    {
      "authors": [
        "M. Robnik-\u0160ikonja",
        "I. Kononenko"
      ],
      "title": "Explaining classifications for individual instances",
      "venue": "IEEE Transactions on Knowledge and Data Engineering 20 (5) ",
      "year": 2008
    },
    {
      "authors": [
        "W.C. Salmon"
      ],
      "title": "Four decades of scientific explanation",
      "venue": "University of Pittsburgh press",
      "year": 2006
    },
    {
      "authors": [
        "J. Samland",
        "M. Josephs",
        "M.R. Waldmann",
        "H. Rakoczy"
      ],
      "title": "The role of prescriptive norms and knowledge in children\u2019s and adults\u2019 causal selection",
      "venue": "Journal of Experimental Psychology: General 145 (2) ",
      "year": 2016
    },
    {
      "authors": [
        "J. Samland",
        "M.R. Waldmann"
      ],
      "title": "Do Social Norms Influence Causal Inferences",
      "venue": "in: P. Bello, M. Guarini, M. McShane, B. Scassellati (Eds.), Proceedings of the 36th Annual Conference of the Cognitive Science Society, Cognitive Science Society, 1359\u20131364",
      "year": 2014
    },
    {
      "authors": [
        "M. Scriven"
      ],
      "title": "The concept of comprehension: From semantics to software",
      "venue": "in: J. B. Carroll, R. O. Freedle (Eds.), Language comprehension and the acquisition of knowledge, Washington: W. H. Winston & Sons, 31\u201339",
      "year": 1972
    },
    {
      "authors": [
        "Z. Shams"
      ],
      "title": "M",
      "venue": "de Vos, N. Oren, J. Padget, Normative Practical Reasoning via Argumentation and Dialogue, in: Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16), AAAI Press",
      "year": 2016
    },
    {
      "authors": [
        "R. Singh",
        "T. Miller",
        "J. Newn",
        "L. Sonenberg",
        "E. Velloso",
        "F. Vetere"
      ],
      "title": "Combining Planning with Gaze for Online Human Intention Recognition",
      "venue": "in: Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems",
      "year": 2018
    },
    {
      "authors": [
        "B.R. Slugoski",
        "M. Lalljee",
        "R. Lamb",
        "G.P. Ginsburg"
      ],
      "title": "Attribution in conversational context: Effect of mutual knowledge on explanation-giving",
      "venue": "European Journal of Social Psychology 23 (3) ",
      "year": 1993
    },
    {
      "authors": [
        "K. Stubbs",
        "P. Hinds",
        "D. Wettergreen"
      ],
      "title": "Autonomy and common ground in human-robot interaction: A field study",
      "venue": "IEEE Intelligent Systems 22 (2) ",
      "year": 2007
    },
    {
      "authors": [
        "J. Susskind",
        "K. Maurer",
        "V. Thakkar",
        "D.L. Hamilton",
        "J.W. Sherman"
      ],
      "title": "Perceiving individuals and groups: expectancies",
      "venue": "dispositional inferences, and causal attributions, Journal of Personality and Social Psychology 76 (2) ",
      "year": 1999
    },
    {
      "authors": [
        "W.R. Swartout",
        "J.D. Moore"
      ],
      "title": "Explanation in second generation expert systems",
      "venue": "in: Second 65 Generation Expert Systems, Springer, 543\u2013585",
      "year": 1993
    },
    {
      "authors": [
        "P.E. Tetlock",
        "R. Boettger"
      ],
      "title": "Accountability: a social magnifier of the dilution effect",
      "venue": "Journal of Personality and Social Psychology 57 (3) ",
      "year": 1989
    },
    {
      "authors": [
        "P.E. Tetlock",
        "J.S. Learner",
        "R. Boettger"
      ],
      "title": "The dilution effect: judgemental bias",
      "venue": "conversational convention, or a bit of both?, European Journal of Social Psychology 26 ",
      "year": 1996
    },
    {
      "authors": [
        "P. Thagard"
      ],
      "title": "Explanatory coherence",
      "venue": "Behavioral and Brain Sciences 12 (03) ",
      "year": 1989
    },
    {
      "authors": [
        "T. Trabasso",
        "J. Bartolone"
      ],
      "title": "Story understanding and counterfactual reasoning",
      "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition 29 (5) ",
      "year": 2003
    },
    {
      "authors": [
        "A. Tversky",
        "D. Kahneman"
      ],
      "title": "Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment",
      "venue": "Psychological Review 90 (4) ",
      "year": 1983
    },
    {
      "authors": [
        "K. Uttich",
        "T. Lombrozo"
      ],
      "title": "Norms inform mental state ascriptions: A rational explanation for the side-effect effect",
      "venue": "Cognition 116 (1) ",
      "year": 2010
    },
    {
      "authors": [
        "J. Van Bouwel",
        "E. Weber"
      ],
      "title": "Remote causes",
      "venue": "bad explanations?, Journal for the Theory of Social Behaviour 32 (4) ",
      "year": 2002
    },
    {
      "authors": [
        "B.C. Van Fraassen"
      ],
      "title": "The pragmatics of explanation",
      "venue": "American Philosophical Quarterly 14 (2) ",
      "year": 1977
    },
    {
      "authors": [
        "N. Vasilyeva",
        "D.A. Wilkenfeld"
      ],
      "title": "T",
      "venue": "Lombrozo, Goals Affect the Perceived Quality of Explanations., in: D. C. Noelle, R. Dale, A. S. Warlaumont, J. Yoshimi, T. Matlock, C. D. Jennings, P. P. Maglio (Eds.), Proceedings of the 37th Annual Conference of the Cognitive Science Society, Cognitive Science Society, 2469\u20132474",
      "year": 2015
    },
    {
      "authors": [
        "F.B. von der Osten",
        "M. Kirley",
        "T. Miller"
      ],
      "title": "The minds of many: opponent modelling in a stochastic game",
      "venue": "in: Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI), AAAI Press,",
      "year": 2017
    },
    {
      "authors": [
        "G.H. Von Wright"
      ],
      "title": "Explanation and understanding",
      "venue": "Cornell University Press",
      "year": 1971
    },
    {
      "authors": [
        "D. Walton"
      ],
      "title": "A new dialectical theory of explanation",
      "venue": "Philosophical Explorations 7 (1) ",
      "year": 2004
    },
    {
      "authors": [
        "D. Walton"
      ],
      "title": "Examination dialogue: An argumentation framework for critically questioning an expert opinion",
      "venue": "Journal of Pragmatics 38 (5) ",
      "year": 2006
    },
    {
      "authors": [
        "D. Walton"
      ],
      "title": "Dialogical Models of Explanation",
      "venue": "in: Proceedings of the International Explanation- Aware Computing (ExaCt) workshop, 1\u20139",
      "year": 2007
    },
    {
      "authors": [
        "D. Walton"
      ],
      "title": "A dialogue system specification for explanation",
      "venue": "Synthese 182 (3) ",
      "year": 2011
    },
    {
      "authors": [
        "D.N. Walton"
      ],
      "title": "Logical Dialogue \u2014 Games and Fallacies",
      "venue": "University Press of America, Lanham, Maryland",
      "year": 1984
    },
    {
      "authors": [
        "J. Weiner"
      ],
      "title": "BLAH",
      "venue": "a system which explains its reasoning, Artificial intelligence 15 (1-2) ",
      "year": 1980
    },
    {
      "authors": [
        "A. Wendt"
      ],
      "title": "On constitution and causation in international relations",
      "venue": "Review of International Studies 24 (05) ",
      "year": 1998
    },
    {
      "authors": [
        "D.A. Wilkenfeld",
        "T. Lombrozo"
      ],
      "title": "Inference to the best explanation (IBE) versus explaining for the best inference (EBI)",
      "venue": "Science & Education 24 (9-10) ",
      "year": 2015
    },
    {
      "authors": [
        "J.J. Williams",
        "T. Lombrozo",
        "B. Rehder"
      ],
      "title": "The hazards of explanation: Overgeneralization in the face of exceptions",
      "venue": "Journal of Experimental Psychology: General 142 (4) ",
      "year": 2013
    },
    {
      "authors": [
        "M. Winikoff"
      ],
      "title": "Debugging Agent Programs with Why?: Questions",
      "venue": "in: Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, AAMAS \u201917, IFAAMAS, 251\u2013259",
      "year": 2017
    },
    {
      "authors": [
        "J. Woodward"
      ],
      "title": "Making things happen: A theory of causal explanation",
      "venue": "Oxford University Press",
      "year": 2005
    },
    {
      "authors": [
        "J. Woodward"
      ],
      "title": "Sensitive and insensitive causation",
      "venue": "The Philosophical Review 115 (1) ",
      "year": 2006
    }
  ],
  "sections": [
    {
      "text": "There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers\u2019 intuition of what constitutes a \u2018good\u2019 explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.\nKeywords: Explanation, Explainability, Interpretability, Explainable AI, Transparency\nContents"
    },
    {
      "heading": "1 Introduction 3",
      "text": "1.1 Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Major Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.3 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.4 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"
    },
    {
      "heading": "2 Philosophical Foundations \u2014 What Is Explanation? 8",
      "text": "2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1.1 Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.1.2 Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.3 Explanation as a Product . . . . . . . . . . . . . . . . . . . . . . . 12 2.1.4 Explanation as Abductive Reasoning . . . . . . . . . . . . . . . . . 13 2.1.5 Interpretability and Justification . . . . . . . . . . . . . . . . . . . 14\nPreprint submitted to Journal Name August 16, 2018\nar X\niv :1\n70 6.\n07 26\n9v 3\n[ cs\n.A I]\n1 5\nA ug\n2 01\n2.2 Why People Ask for Explanations . . . . . . . . . . . . . . . . . . . . . . 14 2.3 Contrastive Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.4 Types and Levels of Explanation . . . . . . . . . . . . . . . . . . . . . . . 17 2.5 Structure of Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.6 Explanation and XAI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.6.1 Causal Attribution is Not Causal Explanation . . . . . . . . . . . . 20 2.6.2 Contrastive Explanation . . . . . . . . . . . . . . . . . . . . . . . . 20 2.6.3 Explanatory Tasks and Levels of Explanation . . . . . . . . . . . . 21 2.6.4 Explanatory Model of Self . . . . . . . . . . . . . . . . . . . . . . . 22 2.6.5 Structure of Explanation . . . . . . . . . . . . . . . . . . . . . . . 23"
    },
    {
      "heading": "3 Social Attribution \u2014 How Do People Explain Behaviour? 23",
      "text": "3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.2 Intentionality and Explanation . . . . . . . . . . . . . . . . . . . . . . . . 24 3.3 Beliefs, Desires, Intentions, and Traits . . . . . . . . . . . . . . . . . . . . 26\n3.3.1 Malle\u2019s Conceptual Model for Social Attribution . . . . . . . . . . 26 3.4 Individual vs. Group Behaviour . . . . . . . . . . . . . . . . . . . . . . . . 27 3.5 Norms and Morals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.6 Social Attribution and XAI . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.6.1 Folk Psychology . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.6.2 Malle\u2019s Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.6.3 Collective Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.6.4 Norms and Morals . . . . . . . . . . . . . . . . . . . . . . . . . . . 31"
    },
    {
      "heading": "4 Cognitive Processes \u2014 How Do People Select and Evaluate Explanations? 32",
      "text": "4.1 Causal Connection, Explanation Selection, and Evaluation . . . . . . . . . 32 4.2 Causal Connection: Abductive Reasoning . . . . . . . . . . . . . . . . . . 34\n4.2.1 Abductive Reasoning and Causal Types . . . . . . . . . . . . . . . 34 4.2.2 Background and Discounting . . . . . . . . . . . . . . . . . . . . . 35 4.2.3 Explanatory Modes . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.2.4 Inherent and Extrinsic Features . . . . . . . . . . . . . . . . . . . . 36\n4.3 Causal Connection: Counterfactuals and Mutability . . . . . . . . . . . . 37 4.3.1 Abnormality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.3.2 Temporality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.3.3 Controllability and Intent . . . . . . . . . . . . . . . . . . . . . . . 38 4.3.4 Social Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.4 Explanation Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.4.1 Facts and Foils . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.4.2 Abnormality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 4.4.3 Intentionality and Functionality . . . . . . . . . . . . . . . . . . . . 41 4.4.4 Necessity, Sufficiency and Robustness . . . . . . . . . . . . . . . . 41 4.4.5 Responsibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4.4.6 Preconditions, Failure, and Intentions . . . . . . . . . . . . . . . . 43 4.5 Explanation Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.5.1 Coherence, Simplicity, and Generality . . . . . . . . . . . . . . . . 43 4.5.2 Truth and Probability . . . . . . . . . . . . . . . . . . . . . . . . . 44\n4.5.3 Goals and Explanatory Mode . . . . . . . . . . . . . . . . . . . . . 46 4.6 Cognitive Processes and XAI . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.6.1 Abductive Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.6.2 Mutability and Computation . . . . . . . . . . . . . . . . . . . . . 47 4.6.3 Abnormality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 4.6.4 Intentionality and Functionality . . . . . . . . . . . . . . . . . . . . 48 4.6.5 Perspectives and Controllability . . . . . . . . . . . . . . . . . . . 48 4.6.6 Evaluation of Explanations . . . . . . . . . . . . . . . . . . . . . . 49"
    },
    {
      "heading": "5 Social Explanation \u2014 How Do People Communicate Explanations? 49",
      "text": "5.1 Explanation as Conversation . . . . . . . . . . . . . . . . . . . . . . . . . 50\n5.1.1 Logic and Conversation . . . . . . . . . . . . . . . . . . . . . . . . 50 5.1.2 Relation & Relevance in Explanation Selection . . . . . . . . . . . 51 5.1.3 Argumentation and Explanation . . . . . . . . . . . . . . . . . . . 53 5.1.4 Linguistic structure . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n5.2 Explanatory Dialogue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 5.3 Social Explanation and XAI . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n5.3.1 Conversational Model . . . . . . . . . . . . . . . . . . . . . . . . . 56 5.3.2 Dialogue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 5.3.3 Theory of Mind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 5.3.4 Implicature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 5.3.5 Dilution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 5.3.6 Social and Interactive Explanation . . . . . . . . . . . . . . . . . . 59"
    },
    {
      "heading": "6 Conclusions 59",
      "text": ""
    },
    {
      "heading": "1. Introduction",
      "text": "Recently, the notion of explainable artificial intelligence has seen a resurgence, after having slowed since the burst of work on explanation in expert systems over three decades ago; for example, see Chandrasekaran et al. [23], [168], and Buchanan and Shortliffe [14]. Sometimes abbreviated XAI (eXplainable artificial intelligence), the idea can be found in grant solicitations [32] and in the popular press [136]. This resurgence is driven by evidence that many AI applications have limited take up, or are not appropriated at all, due to ethical concerns [2] and a lack of trust on behalf of their users [166, 101]. The running hypothesis is that by building more transparent, interpretable, or explainable systems, users will be better equipped to understand and therefore trust the intelligent agents [129, 25, 65].\nWhile there are many ways to increase trust and transparency of intelligent agents, two complementary approaches will form part of many trusted autonomous systems: (1) generating decisions1 in which one of the criteria taken into account during the computation is how well a human could understand the decisions in the given context, which is often called interpretability or explainability ; and (2) explicitly explaining decisions\n1We will use decision as the general term to encompass outputs from AI systems, such as categorisations, action selection, etc.\nto people, which we will call explanation. Applications of explanation are considered in many sub-fields of artificial intelligence, such as justifying autonomous agent behaviour [129, 65], debugging of machine learning models [89], explaining medical decision-making [45], and explaining predictions of classifiers [157].\nIf we want to design, and implement intelligent agents that are truly capable of providing explanations to people, then it is fair to say that models of how humans explain decisions and behaviour to each other are a good way to start analysing the problem. Researchers argue that people employ certain biases [82] and social expectations [72] when they generate and evaluate explanation, and I argue that such biases and expectations can improve human interactions with explanatory AI. For example, de Graaf and Malle [34] argues that because people assign human-like traits to artificial agents, people will expect explanations using the same conceptual framework used to explain human behaviours.\nDespite the recent resurgence of explainable AI, most of the research and practice in this area seems to use the researchers\u2019 intuitions of what constitutes a \u2018good\u2019 explanation. Miller et al. [132] shows in a small sample that research in explainable AI typically does not cite or build on frameworks of explanation from social science. They argue that this could lead to failure. The very experts who understand decision-making models the best are not in the right position to judge the usefulness of explanations to lay users \u2014 a phenomenon that Miller et al. refer to (paraphrasing Cooper [31]) as \u201cthe inmates running the asylum\u201d. Therefore, a strong understanding of how people define, generate, select, evaluate, and present explanations seems almost essential.\nIn the fields of philosophy, psychology, and cognitive science, there is a vast and mature body of work that studies these exact topics. For millennia, philosophers have asked the questions about what constitutes an explanation, what is the function of explanations, and what are their structure. For over 50 years, cognitive and social psychologists have analysed how people attribute and evaluate the social behaviour of others. For over two decades, cognitive psychologists and scientists have investigated how people generate explanations and how they evaluate their quality.\nI argue here that there is considerable scope to infuse this valuable body of research into explainable AI. Building intelligent agents capable of explanation is a challenging task, and approaching this challenge in a vacuum considering only the computational problems will not solve the greater problems of trust in AI. Further, while some recent work builds on the early findings on explanation in expert systems, that early research was undertaken prior to much of the work on explanation in social science. I contend that newer theories can form the basis of explainable AI \u2014 although there is still a lot to learn from early work in explainable AI around design and implementation.\nThis paper aims to promote the inclusion of this existing research into the field of explanation in AI. As part of this work, over 250 publications on explanation were surveyed from social science venues. A smaller subset of these were chosen to be presented in this paper, based on their currency and relevance to the topic. The paper presents relevant theories on explanation, describes, in many cases, the experimental evidence supporting these theories, and presents ideas on how this work can be infused into explainable AI."
    },
    {
      "heading": "1.1. Scope",
      "text": "In this article, the term \u2018Explainable AI \u2019 loosely refers to an explanatory agent revealing underlying causes to its or another agent\u2019s decision making. However, it is important\nto note that the solution to explainable AI is not just \u2018more AI\u2019. Ultimately, it is a human-agent interaction problem. Human-agent interaction can be defined as the intersection of artificial intelligence, social science, and human-computer interaction (HCI); see Figure 1. Explainable AI is just one problem inside human-agent interaction.\nThis article highlights the top circle in Figure 1: the philosophy, social and cognitive psychology, and cognitive science views of explanation, and their relation to the other two circles: their impact on the design of both artificial intelligence and our interactions with them. With this scope of explainable AI in mind, the scope of this article is threefold:\n\u2022 Survey : To survey and review relevant articles on the philosophical, cognitive, and social foundations of explanation, with an emphasis on \u2018everyday\u2019 explanation.\n\u2022 Everyday explanation: To focus on \u2018everyday\u2019 (or local) explanations as a tool and process for an agent, who we call the explainer, to explain decisions made by itself or another agent to a person, who we call the explainee. \u2018Everyday\u2019 explanations are the explanations of why particular facts (events, properties, decisions, etc.) occurred, rather than explanations of more general relationships, such as those seen in scientific explanation. We justify this focus based on the observation from AI literature that trust is lost when users cannot understand traces of observed behaviour or decisions [166, 129], rather than trying to understand and construct generalised theories. Despite this, everyday explanations also sometimes refer to generalised theories, as we will see later in Section 2, so scientific explanation is relevant, and some work from this area is surveyed in the paper.\n\u2022 Relationship to Explainable AI : To draw important points from relevant articles to some of the different sub-fields of explainable AI.\nThe following topics are considered out of scope of this article:\n\u2022 Causality : While causality is important in explanation, this paper is not a survey on the vast work on causality. I review the major positions in this field insofar as they relate to the relationship with models of explanation.\n\u2022 Explainable AI : This paper is not a survey on existing approaches to explanation or interpretability in AI, except those that directly contribute to the topics in scope or build on social science. For an excellent short survey on explanation in machine learning, see Biran and Cotton [9]."
    },
    {
      "heading": "1.2. Major Findings",
      "text": "As part of this review, I highlight four major findings from the surveyed literature that I believe are important for explainable AI, but which I believe most research and practitioners in artificial intelligence are currently unaware:\n1. Explanations are contrastive \u2014 they are sought in response to particular counterfactual cases, which are termed foils in this paper. That is, people do not ask why event P happened, but rather why event P happened instead of some event Q . This has important social and computational consequences for explainable AI. In Sections 2\u20134, models of how people provide contrastive explanations are reviewed.\n2. Explanation are selected (in a biased manner) \u2014 people rarely, if ever, expect an explanation that consists of an actual and complete cause of an event. Humans are adept at selecting one or two causes from a sometimes infinite number of causes to be the explanation. However, this selection is influenced by certain cognitive biases. In Section 4, models of how people select explanations, including how this relates to contrast cases, are reviewed.\n3. Probabilities probably don\u2019t matter \u2014 while truth and likelihood are important in explanation and probabilities really do matter, referring to probabilities or statistical relationships in explanation is not as effective as referring to causes. The most likely explanation is not always the best explanation for a person, and importantly, using statistical generalisations to explain why events occur is unsatisfying, unless accompanied by an underlying causal explanation for the generalisation itself.\n4. Explanations are social \u2014 they are a transfer of knowledge, presented as part of a conversation2 or interaction, and are thus presented relative to the explainer\u2019s beliefs about the explainee\u2019s beliefs. In Section 5, models of how people interact regarding explanations are reviewed.\n2Note that this does not imply that explanations must be given in natural language, but implies that explanation is a social interaction between the explainer and the explainee.\nThese four points all converge around a single point: explanations are not just the presentation of associations and causes (causal attribution), they are contextual. While an event may have many causes, often the explainee cares only about a small subset (relevant to the context), the explainer selects a subset of this subset (based on several different criteria), and explainer and explainee may interact and argue about this explanation.\nI assert that, if we are to build truly explainable AI, especially intelligent systems that are able to offer explanations, then these three points are imperative in many applications."
    },
    {
      "heading": "1.3. Outline",
      "text": "The outline of this paper is as follows. Section 1.4 presents a motivating example of an explanatory agent that is used throughout the paper. Section 2 presents the philosophical foundations of explanation, defining what explanations are, what they are not, how to relate to causes, their meaning and their structure. Section 3 focuses on one specific type of explanation \u2014 those relating to human or social behaviour, while Section 4 surveys work on how people generate and evaluate explanations more generally; that is, not just social behaviour. Section 5 describes research on the dynamics of interaction in explanation between explainer and explainee. Section 6 concludes and highlights several major challenges to explanation in AI."
    },
    {
      "heading": "1.4. Example",
      "text": "This section presents a simple example, which is used to illustrate many important concepts through this paper. It is of a hypothetical system that categorises images of arthropods into several different types, based on certain physical features of the arthropods, such as number of legs, number of eyes, number of wings, etc. The algorithm is assumed to have been trained on a large set of valid data and is highly accurate. It is used by entomologists to do automatic classification of their research data. Table 1 outlines a simple model of the features of arthropods for illustrative purposes. An explanation function is available for the arthropod system.\nNow, consider the idealised and simple dialogue between a human user and \u2018ExplAgent\u2019, who is the interactive explanation agent, outlined in Table 2. This dialogue is not intended to be realistic, but is merely illustrative of how a particular explanatory agent may interact: responding to posed questions, using mixed modalities \u2014 in this case, language and visual images \u2014 and being able to answer a range of questions about its decision making. This example shows different types of questions being posed, and demonstrates that the explanatory agent will need to keep track of the state of the explanation; for example, by noting what it has already told the explainee, and may have to infer what the explainee has inferred themselves.\nWe will refer back to this example throughout the paper and link difference parts of work the different parts of the dialogue above."
    },
    {
      "heading": "2. Philosophical Foundations \u2014 What Is Explanation?",
      "text": "To explain an event is to provide some information about its causal history. In an act of explaining, someone who is in possession of some information about the causal history of some event \u2014 explanatory information, I shall call it \u2014 tries to convey it to someone else. \u2013 Lewis [99, p. 217]\nIn this section, we outline foundational work in explanation, which helps to define causal explanation and how it differs from other concepts such as causal attribution and interpretability."
    },
    {
      "heading": "2.1. Definitions",
      "text": "There are several related concepts in explanation, which seem to be used interchangeably between authors and also within articles, often demonstrating some conflation of the terms. In particular, this section describes the difference between causal attribution and causal explanation. We will also briefly touch on the difference between explanation and interpretability."
    },
    {
      "heading": "2.1.1. Causality",
      "text": "The idea of causality has attracted much work, and there are several different accounts of what constitutes a cause of an event or property. The various definitions of causation can be broken into two major categories: dependence theories and transference theories.\nCausality and Counterfactuals. Hume [79, Section VII] is credited with deriving what is known as the regularity theory of causation. This theory states that there is a cause between two types of events if events of the first type are always followed by events of the second. However, as argued by Lewis [98], the definition due to Hume is in fact about\ncounterfactuals, rather than dependence alone. Hume argues that the co-occurrence of events C and E, observed from experience, do not give causal information that is useful. Instead, the cause should be understood relative to an imagined, counterfactual case: event C is said to have caused event E if, under some hypothetical counterfactual case the event C did not occur, E would not have occurred. This definition has been argued and refined, and many definitions of causality are based around this idea in one way or another; c.f. Lewis [98], Hilton [71].\nThis classical counterfactual model of causality is well understood but competing definitions exist. Interventionist theories of causality [191, 58] state that event C can be deemed a cause of event E if and only if any change to event E can be brought about solely by intervening on event C. Probabilistic theories, which are extensions of interventionist theories, state that event C is a cause of event E if and only if the occurrence of C increases the probability of E occurring [128].\nTransference theories [5, 43, 39], on the other hand, are not defined on dependence, but instead describe physical causation as the transference of energy between objects. In short, if E is an event representing the change of energy of an object O, then C causes E if object O is in contact with the object that causes C, and there is some quantity of energy transferred.\nWhile the aim here is not a detailed survey of causality, however, it is pertinent to note that the dependence theories all focus around the concept of counterfactuals: the state of affairs that would have resulted from some event that did not occur. Even transference theories, which are not explicitly defined as counterfactual, consider that causation is an unnatural transference of energy to the receiving object, implying what would have been otherwise. As such, the notion of \u2018counterfactual\u2019 is important in causality.\nGerstenberg et al. [49] tested whether people consider counterfactuals when making causal judgements in an experiment involving colliding balls. They presented experiment participants with different scenarios involving two balls colliding, with each scenario having different outcomes, such as one ball going through a gate, just missing the gate, or missing the gate by a long distance. While wearing eye-tracking equipment, participants were asked to determine what the outcome would have been (a counterfactual) had the candidate cause not occurred (the balls had not collided). Using the eye-gaze data from the tracking, they showed that their participants, even in these physical environments, would trace where the ball would have gone had the balls not collided, thus demonstrating that they used counterfactual simulation to make causal judgements.\nNecessary and Sufficient Causes. Kelley [87] proposes a taxonomy of causality in social attribution, but which has more general applicability, and noted that there are two main types of causal schemata for causing events: multiple necessary causes and multiple sufficient causes. The former defines a schema in which a set of events are all necessary to cause the event in question, while the latter defines a schema in which there are multiple possible ways to cause the event, and only one of these is required. Clearly, these can be interleaved; e.g. causes C1, C2, and C3 for event E, in which C1 is necessary and either of C2 or C3 are necessary, while both C2 and C3 are sufficient to cause the compound event (C2 or C3).\nInternal and External Causes. Heider [66], the grandfather of causal attribution in social psychology, argues that causes fall into two camps: internal and external. Internal causes\nof events are those due to the characteristics of an actor, while external causes are those due to the specific situation or the environment. Clearly, events can have causes that mix both. However, the focus of work from Heider was not on causality in general, but on social attribution, or the perceived causes of behaviour. That is, how people attribute the behaviour of others. Nonetheless, work in this field, as we will see in Section 3, builds heavily on counterfactual causality.\nCausal Chains. In causality and explanation, the concept of causal chains is important. A causal chain is a path of causes between a set of events, in which a cause from event C to event E indicates that C must occur before E. Any events without a cause are root causes.\nHilton et al. [76] define five different types of causal chain, outlined in Table 2, and note that different causal chains are associated with different types of explanations.\nPeople do not need to understand a complete causal chain to provide a sound explanation. This is evidently true: causes of physical events can refer back to events that occurred during the Big Bang, but nonetheless, most adults can explain to a child why a bouncing ball eventually stops.\nFormal Models of Causation. While several formal models of causation have been proposed, such as those based on conditional logic [53, 98], the model of causation that\nI believe would be of interest to many in artificial intelligence is the formalisation of causality by Halpern and Pearl [58]. This is a general model that should be accessible to anyone with a computer science background, has been adopted by philosophers and psychologists, and is accompanied by many additional results, such as an axiomatisation [57] and a series articles on complexity analysis [40, 41].\nHalpern and Pearl [58] define a model-based approach using structural causal models over two sets of variables: exogenous variables, whose values are determined by factors external to the model, and endogenous variables, whose values are determined by relationships with other (exogenous or endogenous) variables. Each endogenous variable has a function that defines its value from other variables. A context is an assignment of values to variables. Intuitively, a context represents a \u2018possible world\u2019 of the model. A model/context pair is called a situation. Given this structure, Halpern and Pearl define a actual cause of an event X = x (that is, endogenous variable X receiving the value x) as a set of events E (each of the form Y = y) such that (informally) the following three criteria hold:\nAC1 Both the event X = x and the cause E are true in the actual situation.\nAC2 If there was some counterfactual values for the variables of the events in E, then the event X = x would not have occurred.\nAC3 E is minimal \u2014 that is, there are no irrelevant events in the case.\nA sufficient cause is simply a non-minimal actual cause; that is, it satisfies the first two items above.\nWe will return later to this model in Section 5.1.2 to to discuss Halpern and Pearl\u2019s model of explanation."
    },
    {
      "heading": "2.1.2. Explanation",
      "text": "An explanation is an assignment of causal responsibility \u2014 Josephson and Josephson [81]\nExplanation is both a process and a product, as noted by Lombrozo [104]. However, I argue that there are actually two processes in explanation, as well as the product:\n1. Cognitive process \u2014 The process of abductive inference for \u2018filling the gaps\u2019 [27] to determine an explanation for a given event, called the explanandum, in which the causes for the event are identified, perhaps in relation to a particular counterfactual cases, and a subset of these causes is selected as the explanation (or explanans).\nIn social science, the process of identifying the causes of a particular phenomenon is known as attribution, and is seen as just part of the entire process of explanation.\n2. Product \u2014 The explanation that results from the cognitive process is the product of the cognitive explanation process.\n3. Social process \u2014 The process of transferring knowledge between explainer and explainee, generally an interaction between a group of people, in which the goal is that the explainee has enough information to understand the causes of the event; although other types of goal exists, as we discuss later.\nBut what constitutes an explanation? This question has created a lot of debate in philosophy, but accounts of explanation both philosophical and psychology stress the importance of causality in explanation \u2014 that is, an explanation refers to causes [159, 191, 107, 59]. There are, however, definitions of non-causal explanation [52], such as explaining \u2018what happened\u2019 or explaining what was meant by a particular remark [187]. These definitions out of scope in this paper, and they present a different set of challenges to explainable AI."
    },
    {
      "heading": "2.1.3. Explanation as a Product",
      "text": "We take the definition that an explanation is an answer to a why\u2013question [35, 138, 99, 102].\nAccording to Bromberger [13], a why-question is a combination of a whether\u2013question, preceded by the word \u2018why\u2019. A whether-question is an interrogative question whose correct answer is either \u2018yes\u2019 or \u2018no\u2019. The presupposition within a why\u2013question is the fact referred to in the question that is under explanation, expressed as if it were true (or false if the question is a negative sentence). For example, the question \u201cwhy did they do that?\u201d is a why-question, with the inner whether-question being \u201cdid they do that?\u201d, and the presupposition being \u201cthey did that\u201d. However, as we will see in Section 2.3, why\u2013questions are structurally more complicated than this: they are contrastive.\nHowever, other types of questions can be answered by explanations. In Table 3, I propose a simple model for explanatory questions based on Pearl and Mackenzie\u2019s Ladder of Causation [141]. This model places explanatory questions into three classes: (1) what\u2013 questions, such as \u201cWhat event happened?\u201d; (2) how -questions, such as \u201cHow did that event happen?\u201d; and (3) why\u2013questions, such as \u201cWhy did that event happen?\u201d. From the perspective of reasoning, why\u2013questions are the most challenging, because they use the most sophisticated reasoning. What-questions ask for factual accounts, possibly using associative reasoning to determine, from the observed events, which unobserved events also happened. How questions are also factual, but require interventionist reasoning to determine the set of causes that, if removed, would prevent the event from happening. This may also require associative reasoning. We categorise what if \u2013questions has how\u2013 questions, as they are just a contrast case analysing what would happen under a different situation. Why\u2013questions are the most challenging, as they require counterfactual reasoning to undo events and simulate other events that are not factual. This also requires associative and interventionist reasoning.\nDennett [36] argues that \u201cwhy\u201d is ambiguous and that there are two different senses of why\u2013question: how come? and what for?. The former asks for a process narrative, without an explanation of what it is for, while the latter asks for a reason, which implies some intentional thought behind the cause. Dennett gives the examples of \u201cwhy are planets spherical?\u201d and \u201cwhy are ball bearings spherical?\u201d. The former asks for an explanation based on physics and chemistry, and is thus a how-come\u2013question, because planets are not round for any reason. The latter asks for an explanation that gives the reason what the designer made ball bearings spherical for : a reason because people design them that way.\nGiven a why\u2013question, Overton [138] defines an explanation as a pair consisting of: (1) the explanans: which is the answer to the question; and (2) and the explanandum; which is the presupposition."
    },
    {
      "heading": "2.1.4. Explanation as Abductive Reasoning",
      "text": "As a cognitive process, explanation is closely related to abductive reasoning. Peirce [142] was the first author to consider abduction as a distinct form of reasoning, separate from induction and deduction, but which, like induction, went from effect to cause. His work focused on the difference between accepting a hypothesis via scientific experiments (induction), and deriving a hypothesis to explain observed phenomenon (abduction). He defines the form of inference used in abduction as follows:\nThe surprising fact, C, is observed; But if A were true, C would be a matter of course, Hence, there is reason to suspect that A is true.\nClearly, this is an inference to explain the fact C from the hypothesis A, which is different from deduction and induction. However, this does not account for competing hypotheses. Josephson and Josephson [81] describe this more competitive-form of abduction as:\nD is a collection of data (facts, observations, givens). H explains D (would, if true, explain D). No other hypothesis can explain D as well as H does. Therefore, H is probably true.\nHarman [62] labels this process \u201cinference to the best explanation\u201d. Thus, one can think of abductive reasoning as the following process: (1) observe some (presumably unexpected or surprising) events; (2) generate one or more hypothesis about these events; (3) judge the plausibility of the hypotheses; and (4) select the \u2018best\u2019 hypothesis as the explanation [78].\nResearch in philosophy and cognitive science has argued that abductive reasoning is closely related to explanation. In particular, in trying to understand causes of events, people use abductive inference to determine what they consider to be the \u201cbest\u201d explanation. Harman [62] is perhaps the first to acknowledge this link, and more recently, experimental evaluations have demonstrated it [108, 188, 109, 154]. Popper [146] is perhaps the most influential proponent of abductive reasoning in the scientific process. He argued strongly for the scientific method to be based on empirical falsifiability of hypotheses, rather than the classic inductivist view at the time.\nEarly philosophical work considered abduction as some magical process of intuition \u2014 something that could not be captured by formalised rules because it did not fit the standard deductive model. However, this changed when artificial intelligence researchers began investigating abductive reasoning to explain observations, such as in diagnosis (e.g. medical diagnosis, fault diagnosis) [145, 156], intention/plan recognition [24], etc. The necessity to encode the process in a suitable computational form led to axiomatisations, with Pople [145] seeming to be the first to do this, and characterisations of how to implement such axiomatisations; e.g. Levesque [97]. From here, the process of abduction as a principled process gained traction, and it is now widely accepted that abduction, induction, and deduction are different modes of logical reasoning.\nIn this paper, abductive inference is not equated directly to explanation, because explanation also refers to the product and the social process; but abductive reasoning does fall into the category of cognitive process of explanation. In Section 4, we survey the cognitive science view of abductive reasoning, in particular, cognitive biases in hypothesis formation and evaluation."
    },
    {
      "heading": "2.1.5. Interpretability and Justification",
      "text": "Here, we briefly address the distinction between interpretability, explainability, justification, and explanation, as used in this article; and as they seem to be used in artificial intelligence.\nLipton [103] provides a taxonomy of the desiderata and methods for interpretable AI. This paper adopts Lipton\u2019s assertion that explanation is post-hoc interpretability. I use Biran and Cotton [9]\u2019s definition of interpretability of a model as: the degree to which an observer can understand the cause of a decision. Explanation is thus one mode in which an observer may obtain understanding, but clearly, there are additional modes that one can adopt, such as making decisions that are inherently easier to understand or via introspection. I equate \u2018interpretability\u2019 with \u2018explainability\u2019.\nA justification explains why a decision is good, but does not necessarily aim to give an explanation of the actual decision-making process [9].\nIt is important to understand the similarities and differences between these terms as one reads this article, because some related research discussed is relevant to explanation only, in particular, Section 5, which discusses how people present explanations to one another; while other sections, in particular Sections 3 and 4 discuss how people generate and evaluate explanations, and explain behaviour of others, so are broader and can be used to create more explainable agents."
    },
    {
      "heading": "2.2. Why People Ask for Explanations",
      "text": "There are many reasons that people may ask for explanations. Curiosity is one primary criterion that humans use, but other pragmatic reasons include examination \u2014 for example, a teacher asking her students for an explanation on an exam for the purposes of testing the students\u2019 knowledge on a particular topic; and scientific explanation \u2014 asking why we observe a particular environmental phenomenon.\nIn this paper, we are interested in explanation in AI, and thus our focus is on how intelligent agents can explain their decisions. As such, this section is primarily concerned with why people ask for \u2018everyday\u2019 explanations of why specific events occur, rather than explanations for general scientific phenomena, although this work is still relevant in many cases.\nIt is clear that the primary function of explanation is to facilitate learning [104, 189]. Via learning, we obtain better models of how particular events or properties come about, and we are able to use these models to our advantage. Heider [66] states that people look for explanations to improve their understanding of someone or something so that they can derive stable model that can be used for prediction and control. This hypothesis is backed up by research suggesting that people tend to ask questions about events or observations that they consider abnormal or unexpected from their own point of view [77, 73, 69].\nLombrozo [104] argues that explanations have a role in inference learning precisely because they are explanations, not necessarily just due to the causal information they reveal. First, explanations provide somewhat of a \u2018filter\u2019 on the causal beliefs of an event. Second, prior knowledge is changed by giving explanations; that is, by asking someone to provide an explanation as to whether a particular property is true or false, the explainer changes their perceived likelihood of the claim. Third, explanations that offer fewer causes and explanations that explain multiple observations are considered more believable and more valuable; but this does not hold for causal statements. Wilkenfeld and Lombrozo [188] go further and show that engaging in explanation but failing to arrive at a correct explanation can improve ones understanding. They describe this as \u201cexplaining for the best inference\u201d, as opposed to the typical model of explanation as \u201cinference to the best explanation\u201d.\nMalle [112, Chapter 3], who gives perhaps the most complete discussion of everyday explanations in the context of explaining social action/interaction, argues that people ask for explanations for two reasons:\n1. To find meaning : to reconcile the contradictions or inconsistencies between elements of our knowledge structures.\n2. To manage social interaction: to create a shared meaning of something, and to change others\u2019 beliefs & impressions, their emotions, or to influence their actions.\nCreating a shared meaning is important for explanation in AI. In many cases, an explanation provided by an intelligent agent will be precisely to do this \u2014 to create a shared understanding of the decision that was made between itself and a human observer, at least to some partial level.\nLombrozo [104] and Wilkenfeld and Lombrozo [188] note that explanations have several functions other than the transfer of knowledge, such as persuasion, learning, or assignment of blame; and that in some cases of social explanation, the goals of the explainer and explainee may be different. With respect to explanation in AI, persuasion is surely of interest: if the goal of an explanation from an intelligent agent is to generate trust from a human observer, then persuasion that a decision is the correct one could in some case be considered more important than actually transferring the true cause. For example, it may be better to give a less likely explanation that is more convincing to the explainee if we want them to act in some positive way. In this case, the goals of the explainer (to generate trust) is different to that of the explainee (to understand a decision)."
    },
    {
      "heading": "2.3. Contrastive Explanation",
      "text": "The key insight is to recognise that one does not explain events per se, but that one explains why the puzzling event occurred in the target cases but not in some counterfactual contrast case. \u2014 Hilton [72, p. 67]\nI will dedicate a subsection to discuss one of the most important findings in the philosophical and cognitive science literature from the perspective of explainable AI: contrastive explanation. Research shows that people do not explain the causes for an event per se, but explain the cause of an event relative to some other event that did not occur; that is, an explanation is always of the form \u201cWhy P rather than Q?\u201d, in which P is the target event and Q is a counterfactual contrast case that did not occur, even if the Q is implicit in the question. This is called contrastive explanation.\nSome authors refer to Q as the counterfactual case [108, 69, 77]. However, it is important to note that this is not the same counterfactual that one refers to when determining causality (see Section 2.1.1). For causality, the counterfactuals are hypothetical \u2018noncauses\u2019 in which the event-to-be-explained does not occur \u2014 that is a counterfactual to cause C \u2014, whereas in contrastive explanation, the counterfactuals are hypothetical outcomes \u2014 that is, a counterfactual to event E [127].\nLipton [102] refers to the two cases, P and Q , as the fact and the foil respectively; the fact being the event that did occur, and the foil being the event that did not. To avoid confusion, throughout the remainder of this paper, we will adopt this terminology and use counterfactual to refer to the hypothetical case in which the cause C did not occur, and foil to refer to the hypothesised case Q that was expected rather than P .\nMost authors in this area argue that all why\u2013questions ask for contrastive explanations, even if the foils are not made explicit [102, 77, 69, 72, 110, 108], and that people are good at inferring the foil; e.g. from language and tone. For example, given the question, \u201cWhy did Elizabeth open the door?\u201d, there are many, possibly an infinite number, of foils; e.g. \u201cWhy did Elizabeth open the door, rather than leave it closed?\u201d, \u201cWhy did Elizabeth open the door rather than the window?\u201d, or \u201cWhy did Elizabeth open the door rather than Michael opening it?\u201d. These different contrasts have different explanations, and there is no inherent one that is certain to be the foil for this question. The negated presupposition not(Elizabeth opens the door) refers to an entire class of foils, including all those listed already. Lipton [102] notes that \u201ccentral requirement for a sensible contrastive question is that the fact and the foil have a largely similar history, against which the differences stand out. When the histories are disparate, we do not know where to begin to answer the question.\u201d This implies that people could use the similarity of the history of facts and possible foils to determine what the explainee\u2019s foil truly is.\nIt is important that the explainee understands the counterfactual case [69]. For example, given the question \u201cWhy did Elizabeth open the door?\u201d, the answer \u201cBecause she was hot\u201d is a good answer if the foil is Elizabeth leaving the door closed, but not a good answer if the foil is \u201crather than turning on the air conditioning\u201d, because the fact that Elizabeth is hot explains both the fact and the foil.\nThe idea of contrastive explanation should not be controversial if we accept the argument outlined in Section 2.2 that people ask for explanations about events or observations that they consider abnormal or unexpected from their own point of view [77, 73, 69]. In such cases, people expect to observe a particular event, but then observe another, with the observed event being the fact and the expected event being the foil.\nVan Bouwel and Weber [175] define four types of explanatory question, three of which are contrastive:\nPlain fact : Why does object a have property P? P-contrast : Why does object a have property P , rather than property Q? O-contrast : Why does object a have property P , while object b has property Q? T-contrast : Why does object a have property P at time t, but property Q at time t\u2032?\nVan Bouwel and Weber note that differences occur on properties within an object (P-contrast), between objects themselves (O-contrast), and within an object over time (T-contrast). They reject the idea that all \u2018plain fact\u2019 questions have an implicit foil, proposing that plain-fact questions require showing details across a \u2018non-interrupted\u2019 causal chain across time. They argue that plain-fact questions are typically asked due to curiosity, such as desiring to know how certain facts fit into the world, while contrastive questions are typically asked when unexpected events are observed.\nLipton [102] argues that contrastive explanations between a fact P and a foil Q are, in general, easier to derive than \u2018complete\u2019 explanations for plain-fact questions about P . For example, consider the arthropod classification algorithm in Section 1.4. To be a beetle, an arthropod must have six legs, but this does not cause an arthropod to be a beetle \u2013 other causes are necessary. Lipton contends that we could answer the P-contrast question such as \u201cWhy is image J labelled as a Beetle instead of a Spider?\u201d by citing the fact that the arthropod in the image has six legs. We do not need information about eyes, wings, or stingers to answer this, whereas to explain why image J is a spider in a non-contrastive way, we must cite all causes.\nThe hypothesis that all causal explanations are contrastive is not merely philosophical. In Section 4, we see several bodies of work supporting this, and these provide more detail as to how people select and evaluate explanations based on the contrast between fact and foil."
    },
    {
      "heading": "2.4. Types and Levels of Explanation",
      "text": "The type of explanation provided to a question is dependent on the particular question asked; for example, asking why some event occurred is different to asking under what circumstances it could have occurred; that is, the actual vs. the hypothetical [159]. However, for the purposes of answering why\u2013questions, we will focus on a particular subset of philosophical work in this area.\nAristotle\u2019s Four Causes model, also known as the Modes of Explanation model, continues to be foundational for cause and explanation. Aristotle proposed an analytic scheme, classed into four different elements, that can be used to provide answers to why\u2013questions [60]:\n1. Material : The substance or material of which something is made. For example, rubber is a material cause for a car tyre.\n2. Formal : The form or properties of something that make it what it is. For example, being round is a formal cause of a car tyre. These are sometimes referred to as categorical explanations.\n3. Efficient : The proximal mechanisms of the cause something to change. For example, a tyre manufacturer is an efficient cause for a car tyre. These are sometimes referred to as mechanistic explanations.\n4. Final : The end or goal of something. Moving a vehicle is an efficient cause of a car tyre. These are sometimes referred to as functional or teleological explanations.\nA single why\u2013question can have explanations from any of these categories. For example, consider the question: \u201cWhy does this pen contain ink?\u201d. A material explanation is based on the idea that the pen is made of a substance that prevents the ink from leaking out. A formal explanation is that it is a pen and pens contain ink. An efficient explanation is that there was a person who filled it with ink. A final explanation is that pens are for writing, and so require ink.\nSeveral other authors have proposed models similar to Aristotle\u2019s, such as Dennett [35], who proposed that people take three stances towards objects: physical, design, and intention; and Marr [119], building on earlier work with Poggio [120], who define the computational, representational, and hardware levels of understanding for computational problems.\nKass and Leake [85] define a categorisation of explanations of anomalies into three types: (1) intentional ; (2) material ; and (3) social. The intentional and material categories correspond roughly to Aristotle\u2019s final and material categories, however, the social category does not correspond to any particular category in the models of Aristotle, Marr [119], or Dennett [35]. The social category refers to explanations about human behaviour that is not intentionally driven. Kass and Leake give the example of an increase in crime rate in a city, which, while due to intentional behaviour of individuals in that city, is not a phenomenon that can be said to be intentional. While individual crimes are committed with intent, it cannot be said that the individuals had the intent of increasing the crime rate \u2014 that is merely an effect of the behaviour of a group of individuals."
    },
    {
      "heading": "2.5. Structure of Explanation",
      "text": "As we saw in Section 2.1.2, causation is a major part of explanation. Earlier accounts of explanation from Hempel and Oppenheim [68] argued for logically deductive models of explanation. Kelley [86] subsequently argued instead that people consider co-variation in constructing explanations, and proposed a statistical model of explanation. However, while influential, subsequent experimental research uncovered many problems with these models, and currently, both the deductive and statistical models of explanation are no longer considered valid theories of everyday explanation in most camps [114].\nOverton [140, 139] defines a model of scientific explanation. In particular, Overton [139] defines the structure of explanations. He defines five categories of properties or objects that are explained in science: (1) theories: sets of principles that form building blocks for models; (2) models: an abstraction of a theory that represents the relationships between kinds and their attributes; (3) kinds: an abstract universal class that supports counterfactual reasoning; (4) entities: an instantiation of a kind; and (5) data: statements about activities (e.g. measurements, observations). The relationships between these is shown in Figure 3.\nFrom these categories, Overton [139] provides a crisp definition of the structure of scientific explanations. He argues that explanations of phenomena at one level must be relative to and refer to at least one other level, and that explanations between two such levels must refer to all intermediate levels. For example, an arthropod (Entity) has eight legs (Data). Entities of this Kind are spiders, according to the Model of our Theory of arthropods. In this example, the explanation is constructed by appealing to the Model\n.\nof insects, which, in turn, appeals to a particular Theory that underlies that Model. Figure 4 shows the structure of a theory-data explanation, which is the most complex because it has the longest chain of relationships between any two levels.\nWith respect to social explanation, Malle [112] argues that social explanation is best understood as consisting of three layers:\n1. Layer 1: A conceptual framework that outlines the assumptions people make about\nhuman behaviour and explanation.\n2. Layer 2: The psychological processes that are used to construct explanations.\n3. Layer 3: Language layer that specifies the type of linguistic structures people use in giving explanations.\nI will present Malle\u2019s views of these three layers in more detail in the section on social attribution (Section 3), cognitive processes (Section 4, and social explanation (Section 5). This work is collated into Malle\u2019s 2004 book [112]."
    },
    {
      "heading": "2.6. Explanation and XAI",
      "text": "This section presents some ideas on how the philosophical work outlined above affects researchers and practitioners in XAI."
    },
    {
      "heading": "2.6.1. Causal Attribution is Not Causal Explanation",
      "text": "An important concept is the relationship between cause attribution and explanation. Extracting a causal chain and displaying it to a person is causal attribution, not (necessarily) an explanation. While a person could use such a causal chain to obtain their own explanation, I argue that this does not constitute giving an explanation. In particular, for most AI models, it is not reasonable to expect a lay-user to be able to interpret a causal chain, no matter how it is presented. Much of the existing work in explainable AI literature is on the causal attribution part of explanation \u2014 something that, in many cases, is the easiest part of the problem because the causes are well understood, formalised, and accessible by the underlying models. In later sections, we will see more on the difference between attribution and explanation, why existing work in causal attribution is only part of the problem of explanation, and insights of how this work can be extended to produce more intuitive explanations."
    },
    {
      "heading": "2.6.2. Contrastive Explanation",
      "text": "Perhaps the most important point in this entire section is that explanation is contrastive (Section 2.3). Research indicates that people request only contrastive explanations, and that the cognitive burden of complete explanations is too great.\nIt could be argued that because models in AI operate at a level of abstraction that is considerably higher than real-world events, the causal chains are often smaller and less cognitively demanding, especially if they can be visualised. Even if one agrees with this, this argument misses a key point: it is not only the size of the causal chain that is important \u2014 people seem to be cognitively wired to process contrastive explanations, so one can argue that a layperson will find contrastive explanations more intuitive and more valuable.\nThis is both a challenge and an opportunity in AI. It is a challenge because often a person may just ask \u201cWhy X?\u201d, leaving their foil implicit. Eliciting a contrast case from a human observer may be difficult or even infeasible. Lipton [102] states that the obvious solution is that a non-contrastive question \u201cWhy P?\u201d can be interpreted by default to \u201cWhy P rather than not-P?\u201d. However, he then goes on to show that to answer \u201cWhy P rather than not-P?\u201d is equivalent to providing all causes for P\u2014 something that is not so useful. As such, the challenge is that the foil needs to be determined. In some\napplications, the foil could be elicited from the human observer, however, in others, this may not be possible, and therefore, foils may have to be inferred. As noted later in Section 4.6.3, concepts such as abnormality could be used to infer likely foils, but techniques for HCI, such as eye gaze [164] and gestures could be used to infer foils in some applications.\nIt is an opportunity because, as Lipton [102] argues, explaining a contrastive question is often easier than giving a full causal attribution because one only needs to understand what is different between the two cases, so one can provide a complete explanation without determining or even knowing all of the causes of the fact in question. This holds for computational explanation as well as human explanation.\nFurther, it can be beneficial in a more pragmatic way: if a person provides a foil, they are implicitly pointing towards the part of the model they do not understand. In Section 4.4, we will see research that outlines how people use contrasts to select explanations that are much simpler than their full counterparts.\nSeveral authors within artificial intelligence flag the importance of contrastive questions. Lim and Dey [100] found via a series of user studies on context-aware applications that \u201cWhy not . . . ?\u201d questions were common questions that people asked. Further, several authors have looked to answer contrastive questions. For example, Winikoff [190] considers the questions of \u201cWhy don\u2019t you believe . . . ?\u201d and \u201cWhy didn\u2019t you do . . . ?\u201d for BDI programs, or Fox et al. [46] who have similar questions in planning, such as \u201cWhy didn\u2019t you do something else (that I would have done)?\u201d. However, most existing work considers contrastive questions, but not contrastive explanations; that is, finding the differences between the two cases. Providing two complete explanations does not take advantage of contrastive questions. Section 4.4.1 shows that people use the difference between the fact and foil to focus explanations on the causes relevant to the question, which makes the explanations more relevant to the explainee."
    },
    {
      "heading": "2.6.3. Explanatory Tasks and Levels of Explanation",
      "text": "Researchers and practitioners in explainable AI should understand and adopt a model of \u2018levels of explanation\u2019 \u2014 either one of those outlined above, or some other sensible model. The reason is clear: the answer that is provided to the why\u2013question is strongly linked to the level at which the question is posed.\nTo illustrate, let\u2019s take a couple of examples and apply them to Aristotle\u2019s modes of explanation model outlined in Section 2.4. Consider our earlier arthropod classification algorithm from Section 1.4. At first glance, it may seem that such an algorithm resides at the formal level, so should offer explanations based on form. However, this would be erroneous, because that given categorisation algorithm has both efficient/mechanistic components, a reason for being implemented/executed (the final mode), and is implemented on hardware (the final mode). As such, there are explanations for its behaviour at all levels. Perhaps most why\u2013questions proposed by human observers about such an algorithm would indeed by at the formal level, such as \u201cWhy is image J in group A instead of group B?\u201d, for which an answer could refer to the particular form of image and the groups A and B. However, in our idealised dialogue, the question \u201cWhy did you infer that the insect in image J had eight legs instead of six?\u201d asks a question about the underlying algorithm for counting legs, so the cause is at the efficient level; that is, it does not ask for what constitutes a spider in our model, but from where the inputs for that model came. Further, the final question about classifying the spider as an octopus\nrefers to the final level, referring to the algorithms function or goal. Thus, causes in this algorithm occur at all four layers: (1) the material causes are at the hardware level to derive certain calculations; (2) the formal causes determine the classification itself; (3) the efficient causes determine such concepts as how features are detected; and (4) final causes determine why the algorithm was executed, or perhaps implemented at all.\nAs a second example, consider an algorithm for planning a robotic search and rescue mission after a disaster. In planning, programs are dynamically constructed, so different modes of cause/explanation are of interest compared to a classification algorithm. Causes still occur at the four levels: (1) the material level as before describes the hardware computation; (2) the formal level describes the underlying model passed to the planning tool; (3) the mechanistic level describes the particular planning algorithm employed; and (4) the final level describes the particular goal or intention of a plan. In such a system, the robot would likely have several goals to achieve; e.g. searching, taking pictures, supplying first-aid packages, returning to re-fuel, etc. As such, why\u2013questions described at the final level (e.g. its goals) may be more common than in the classification algorithm example. However, questions related to the model are relevant, or why particular actions were taken rather than others, which may depend on the particular optimisation criteria used (e.g. cost vs. time), and these require efficient/mechanistic explanations.\nHowever, I am not arguing that we, as practitioners, must have explanatory agents capable of giving explanations at all of these levels. I argue that these frameworks are useful for analysing the types of questions explanatory agents one may receive. In Sections 3 and 4, we will see work that demonstrates that for explanations at these different levels, people expect different types of explanation. Thus, it is important to understand which types of questions refer to which levels in particular instances of technology, that different levels will be more useful/likely than others, and that, in research articles on interpretability, it is clear at which level we are aiming to provide explanations."
    },
    {
      "heading": "2.6.4. Explanatory Model of Self",
      "text": "The work outlined in this section demonstrates that an intelligent agent must be able to reason about its own causal model. Consider our image classification example. When posed with the question \u201cWhy is image J in group A instead of group B?\u201d, it is non-trivial, in my view, to attribute the cause by using the algorithm that generated the answer. A cleaner solution would be to have a more abstract symbolic model alongside this that records information such as when certain properties are detected and when certain categorisations are made, which can be reasoned over. In other words, the agent requires a model of it\u2019s own decision making \u2014 a model of self \u2014 that exists merely for the purpose of explanation. This model may be only an approximation of the original model, but more suitable for explanation.\nThis idea is not new in XAI. In particular, researchers have investigated machine learning models that are uninterpretable, such as neural nets, and have attempted to extract model approximations using more interpretable model types, such as Bayesian networks [63], decision trees [47], or local approximations [157]. However, my argument here is not only for the purpose of interpretability. Even models considered interpretable, such as decision trees, could be accompanied by another model that is specifically used for explanation. For example, to explain control policies, Hayes and Shah [65] select and annotate particular important state variables and actions that are relevant for explanation only. Langley et al. notes that \u201cAn agent must represent content in a way that\nsupports the explanations\u201d [93, p. 2]. Thus, to generate meaningful and useful explanations of behaviour, models based on the our understanding of explanation must sit alongside and work with the decisionmaking mechanisms."
    },
    {
      "heading": "2.6.5. Structure of Explanation",
      "text": "Related to the \u2018model of self\u2019 is the structure of explanation. Overton\u2019s model of scientific explanation [139] defines what I believe to be a solid foundation for the structure of explanation in AI. To provide an explanation along the chain outlined in Figure 4, one would need an explicit explanatory model (Section 2.6.4) of each of these different categories for the given system.\nFor example, the question from our dialogue in Section 1.4 \u201cHow do you know that spiders have eight legs?\u201d, is a question referring not to the causal attribution in the classification algorithm itself, but is asking: \u201cHow do you know this?\u201d, and thus is referring to how this was learnt \u2014 which, in this example, was learnt via another algorithm. Such an approach requires an additional part of the \u2018model of self\u2019 that refers specifically to the learning, not the classification.\nOverton\u2019s model [139] or one similar to it seems necessary for researchers and practitioners in explainable AI to frame their thoughts and communicate their ideas."
    },
    {
      "heading": "3. Social Attribution \u2014 How Do People Explain Behaviour?",
      "text": "Just as the contents of the nonsocial environment are interrelated by certain lawful connections, causal or otherwise, which define what can or will happen, we assume that there are connections of similar character between the contents of the social environment. \u2013 Heider [66, Chapter 2, pg. 21]\nIn this section, we outline work on social attribution, which defines how people attribute and (partly) explain behaviour of others. Such work is clearly relevant in many areas of artificial intelligence. However, research on social attribution laid the groundwork for much of the work outlined in Section 4, which looks at how people generate and evaluate events more generally. For a more detailed survey on this, see McClure [122] and Hilton [70]."
    },
    {
      "heading": "3.1. Definitions",
      "text": "Social attribution is about perception. While the causes of behaviour can be described at a neurophysical level, and perhaps even lower levels, social attribution is concerned not with the real causes of human behaviour, but how other attribute or explain the behaviour of others. Heider [66] defines social attribution as person perception.\nIntentions and intentionality is key to the work of Heider [66], and much of the recent work that has followed his \u2014 for example, Dennett [35], Malle [112], McClure [122], Boonzaier et al. [10], Kashima et al. [84]. An intention is a mental state of a person in which they form a commitment to carrying out some particular action or achieving some particular aim. Malle and Knobe [115] note that intentional behaviour therefore is always contrasted with unintentional behaviour, citing that laws of state, rules in sport, etc. all treat intentional actions different from unintentional actions because intentional\nrule breaking is punished more harshly than unintentional rule breaking. They note that, while intentionality can be considered an objective fact, it is also a social construct, in that people ascribe intentions to each other whether that intention is objective or not, and use these to socially interact.\nFolk psychology, or commonsense psychology, is the attribution of human behaviour using \u2018everyday\u2019 terms such as beliefs, desires, intentions, emotions, and personality traits. This field of cognitive and social psychology recognises that, while such concepts may not truly cause human behaviour, these are the concepts that humans use to model and predict each others\u2019 behaviours [112]. In other words, folk psychology does not describe how we think; it describes how we think we think.\nIn the folk psychological model, actions consist of three parts: (1) the precondition of the action \u2014 that is, the circumstances under which it can be successfully executed, such as the capabilities of the actor or the constraints in the environment; (2) the action itself that can be undertaken; and (3) the effects of the action \u2014 that is, the changes that they bring about, either environmentally or socially.\nActions that are undertaken are typically explained by goals or intentions. In much of the work in social science, goals are equated with intentions. For our discussions, we define goals as being the end to which a mean contributes, while we define intentions as short-term goals that are adopted to achieve the end goals. The intentions have no utility themselves except to achieve positive utility goals. A proximal intention is a near-term intention that helps to achieve some further distal intention or goal. In the survey of existing literature, we will use the term used by the original authors, to ensure that they are interpreted as the authors expected."
    },
    {
      "heading": "3.2. Intentionality and Explanation",
      "text": "Heider [66] was the first person to experimentally try to identify how people attribute behaviour to others. In their now famous experiment from 1944, Heider and Simmel [67], showed a video containing animated shapes \u2014 a small triangle, a large triangle, and a small circle \u2014 moving around a screen3, and asked experiment participants to observe the video and then describe the behaviour of the shapes. Figure 5 shows a captured screenshot from this video in which the circle is opening a door to enter into a room. The participants\u2019 responses described the behaviour anthropomorphically, assigning actions, intentions, emotions, and personality traits to the shapes. However, this experiment was not one on animation, but in social psychology. The aim of the experiment was to demonstrate that people characterise deliberative behaviour using folk psychology.\nHeider [66] argued then that, the difference between object perception \u2014 describing causal behaviour of objects \u2014 and person perception was the intentions, or motives, of the person. He noted that behaviour in a social situation can have two types of causes: (1) personal (or dispositional) causality; and (2) impersonal causality, which can subsequently be influenced by situational factors, such as the environment. This interpretation lead to many researchers reflecting on the person-situation distinction and, in Malle\u2019s view [114], incorrectly interpreting Heider\u2019s work for decades.\nHeider [66] contends that the key distinction between intentional action and nonintentional events is that intentional action demonstrates equifinality, which states that\n3See the video here: https://www.youtube.com/watch?v=VTNmLt7QX8E.\nwhile the means to realise an intention may vary, the intention itself remains equa-final. Thus, if an actor should fail to achieve their intention, they will try other ways to achieve this intention, which differs from physical causality. Lombrozo [107] provides the example of Romeo and Juliet, noting that had a wall been placed between them, Romeo would have scaled the wall or knocked in down to reach his goal of seeing Juliet. However, iron filaments trying to get to a magnet would not display such equifinality \u2014 they would instead be simply blocked by the wall. Subsequent research confirms this distinction [35, 112, 122, 10, 84, 108].\nMalle and Pearce [118] break the actions that people will explain into two dimensions: (1) intentional vs. unintentional ; and (2) observable vs. unobservable; thus creating four different classifications (see Figure 6).\nMalle and Pearce [118] performed experiments to confirm this model. As part of these experiments, participants were placed into a room with another participant, and were left for 10 minutes to converse with each other to \u2018get to know one another\u2019, while their conversation was recorded. Malle and Pearce coded participants responses to questions with regards to observability and intentionality. Their results show that actors tend to explain unobservable events more than observable events, which Malle and Pearce argue is because the actors are more aware of their own beliefs, desires, feelings, etc., than of their observable behaviours, such as facial expressions, gestures, postures, etc.). On the other hand, observers do the opposite for the inverse reason. Further, they showed that actors tend to explain unintentional behaviour more than intentional behaviour, again because (they believe) they are aware of their intentions, but not their \u2018unplanned\u2019 unintentional behaviour. Observers tend to find both intentional and unintentional behaviour difficult\nto explain, but will tend to find intentional behaviour more relevant. Such a model accounts for the correspondence bias noted by Gilbert and Malone [51], which is the tendency for people to explain others\u2019 behaviours based on traits rather than situational factors, because the situational factors (beliefs, desires) are invisible."
    },
    {
      "heading": "3.3. Beliefs, Desires, Intentions, and Traits",
      "text": "Further to intentions, research suggest that other factors are important in attribution of social behaviour; in particular, beliefs, desires, and traits.\nKashima et al. [84] demonstrated that people use the folk psychological notions of belief, desire, and intention to understand, predict, and explain human action. In particular, they demonstrated that desires hold preference over beliefs, with beliefs being not explained if they are clear from the viewpoint of the explainee. They showed that people judge that explanations and behaviour \u2018do not make sense\u2019 when belief, desires, and intentions were inconsistent with each other. This early piece of work is one of the first to re-establish Heider\u2019s theory of intentional behaviour in attribution [66].\nHowever, it is the extensive body of work from Malle [111, 112, 113] that is the most seminal in this space."
    },
    {
      "heading": "3.3.1. Malle\u2019s Conceptual Model for Social Attribution",
      "text": "Malle [112] proposes a model based on Theory of Mind, arguing that people attribute behaviour of others and themselves by assigning particular mental states that explain the behaviour. He offers six postulates (and sub-postulates) for the foundation of people\u2019s folk explanation of behaviour, modelled in the scheme in Figure 7. He argues that these six postulates represent the assumptions and distinctions that people make when attributing behaviour to themselves and others:\n1. People distinguish between intentional and unintentional behaviour.\n2. For intentional behaviour, people use three modes of explanation based on the specific circumstances of the action:\n(a) Reason explanations are those explanations that link to the mental states (typically desires and beliefs, but also values) for the act, and the grounds on which they formed an intention.\n(b) Causal History of Reason (CHR) explanations are those explanations that use factors that \u201clay in the background\u201d of an agent\u2019s reasons (note, not the background of the action), but are not themselves reasons. Such factors can include unconscious motives, emotions, culture, personality, and the context. CHR explanations refer to causal factors that lead to reasons.\nCHR explanations do not presuppose either subjectivity or rationality. This has three implications. First, they do not require the explainer to take the perspective of the explainee. Second, they can portray the actor as less rationale, by not offering a rational and intentional reason for the behaviour. Third, they allow the use of unconscious motives that the actor themselves would typically not use. Thus, CHR explanations can make the agent look less rationale and in control than reason explanations.\n(c) Enabling factor (EF) explanations are those explanations that explain not the intention of the actor, but instead explain how the intentional action achieved the outcome that it did. Thus, it assumes that the agent had an intention, and then refers to the factors that enabled the agent to successfully carry out the action, such as personal abilities or environmental properties. In essence, it relates to why preconditions of actions were enabled.\n3. For unintentional behaviour, people offer just causes, such as physical, mechanistic, or habitual cases.\nAt the core of Malle\u2019s framework is the intentionality of an act. For a behaviour to be considered intentional, the behaviour must be based on some desire, and a belief that the behaviour can be undertaken (both from a personal and situational perspective) and can achieve the desire. This forms the intention. If the agent has the ability and the awareness that they are performing the action, then the action is intentional.\nLinguistically, people make a distinction between causes and reasons; for example, consider \u201cWhat were her reasons for choosing that book?\u201d, vs. \u201cWhat were his causes for falling over?\u201d. The use of \u201chis causes\u201d implies that the cause does not belong to the actor, but the reason does.\nTo give a reason explanation is to attribute intentionality to the action, and to identify the desires, beliefs, and valuings in light of which (subjectivity assumption) and on the grounds of which (rationality assumption) the agent acted. Thus, reasons imply intentionality, subjectivity, and rationality."
    },
    {
      "heading": "3.4. Individual vs. Group Behaviour",
      "text": "Susskind et al. [167] investigated how people ascribe causes to groups rather than individuals, focusing on traits. They provided experimental participants with a set of\nstatements describing behaviours performed by individuals or groups, and were then asked to provide ratings of different descriptions of these individuals/groups, such as their intelligence (a trait, or CHR in Malle\u2019s framework), and were asked to judge the confidence of their judgements. Their results showed that as with individuals, participants freely assigned traits to groups, showing that groups are seen as agents themselves. However, they showed that when explaining an individual\u2019s behaviour, the participants were able to produce explanations faster and more confidently than for groups, and that the traits that they assigned to individuals were judged to be less \u2018extreme\u2019 than those assigned to to groups. In a second set of experiments, Susskind et al. showed that people expect more consistency in an individual\u2019s behaviour compared to that of a group. When presented with a behaviour that violated the impression that participants had formed of individuals or groups, the participants were more likely to attribute the individual\u2019s behaviour to causal mechanisms than the groups\u2019 behaviour.\nO\u2019Laughlin and Malle [137] further investigated people\u2019s perception of group vs. individual behaviour, focusing on intentionality of explanation. They investigated the relative agency of groups that consist of \u2018unrelated\u2019 individuals acting independently (aggregate groups) compared to groups acting together (jointly acting groups). In their study, participants were more likely to offer CHR explanations than intention explanations for aggregate groups, and more likely to offer intention explanations than CHR explanations for jointly acting groups. For instance, to explain why all people in a department store came to that particular store, participants were more likely offer a CHR explanation, such as that there was a sale on at the store that day. However, to answer the same question for why a group of friends came to the same store place, participants were more likely to offer an explanation that the group wanted to spend the day together shopping \u2013 a desire. This may demonstrate that people cannot attribute intentional behaviour to the individuals in an aggregate group, so resort to more causal history explanations.\nO\u2019Laughlin and Malle\u2019s [137] finding about using CHRs to explain aggregate group behaviour is consistent with the earlier work from Kass and Leake [85], whose model of explanation explicitly divided intentional explanations from social explanations, which are explanations about human behaviour that is not intentionally driven (discussed in more detail in Section 2.4). These social explanations account for how people attribute deliberative behaviour to groups without referring to any form of intention.\nAn intriguing result from O\u2019Laughlin and Malle [137] is that while people attribute less intentionality to aggregate groups than to individuals, they attribute more intentionality to jointly acting groups than to individuals. O\u2019Laughlin and Malle reason that joint action is highly deliberative, so the group intention is more likely to have been explicitly agreed upon prior to acting, and the individuals within the group would be explicitly aware of this intention compared to the their own individual intentions."
    },
    {
      "heading": "3.5. Norms and Morals",
      "text": "Norms have been shown to hold a particular place in social attribution. Burguet and Hilton [15] (via Hilton [70]) showed that norms and abnormal behaviour are important in how people ascribe mental states to one another. For example, Hilton [70] notes that upon hearing the statement \u201cTed admires Paul\u201d, people tend to attribute some trait to Paul as the object of the sentence, such as that Paul is charming and many people would admire him; and even that Ted does not admire many people. However, a counternormative statement such as \u201cTed admires the rapist\u201d triggers attributions instead to\nTed, explained by the fact that it is non-normative to admire rapists, so Ted\u2019s behaviour is distinctive to others, and is more likely to require an explanation. In Section 4, we will see more on the relationship between norms, abnormal behaviour, and attribution.\nUttich and Lombrozo [174] investigate the relationship of norms and the effect it has on attributing particular mental states, especially with regard to morals. They offer an interesting explanation of the side-effect effect, or the Knobe effect [88], which is the effect for people to attribute particular mental states (Theory of Mind) based on moral judgement. Knobe\u2019s vignette from his seminal [88] paper is:\nThe vice-president of a company went to the chairman of the board and said, \u201cWe are thinking of starting a new program. It will help us increase profits, but it will also harm the environment\u201d. The chairman of the board answered, \u201cI don\u2019t care at all about harming the environment. I just want to make as much profit as I can. Let\u2019s start the new program.\u201d They started the new program. Sure enough, the environment was harmed.\nKnobe then produce a second vignette, which is exactly the same, but the side-effect of the program was in fact that the environment was helped. When participants were asked if the chairman had intentionally harmed the environment (first vignette), 82% of respondents replied yes. However, in the second vignette, only 23% thought that the chairman intentionally helped the environment.\nUttich and Lombrozo [174] hypothesis that the two existing camps aiming to explain this effect: the Intuitive Moralist and Biased Scientist, do not account for this. Uttich and Lombrozo hypothesise that it is the fact the norms are violated that account for this; that is, rather than moralist judgements influencing intentionality attribution, it is the more general relationship of conforming (or not) to norms (moral or not). In particular, behaviour that conforms to norms is less likely to change a person\u2019s Theory of Mind (intention) of another person compared to behaviour that violates norms.\nSamland and Waldmann [161] further investigate social attribution in the context of norms, looking at permissibility rather than obligation. They gave participants scenarios in which two actors combined to cause an outcome. For example, a department in which only administrative assistants are permitted to take pens from the stationary cupboard. One morning, Professor Smith (not permitted) and an assistant (permitted) each take a pen, and there are no pens remaining. Participants were tasked with rating how strongly each agent caused the outcome. Their results showed that participants rated the action of the non-permitted actor (e.g. Professor Smith) more than three times stronger than the other actor. However, if the outcome was positive instead of negative, such as an intern (not permitted) and a doctor (permitted) both signing off on a request for a drug for a patient, who subsequently recovers due to the double dose, participants rate the non-permitted behaviour only slightly stronger. As noted by Hilton [70, p. 54], these results indicate that in such settings, people seem to interpret the term cause as meaning \u201cmorally or institutionally responsible\u201d.\nIn a follow-up study, Samland et al. [160] showed that children are not sensitive to norm violating behaviour in the same way that adults are. In particular, while both adults and children correlate cause and blame, children do not distinguish between cases in which the person was aware of the norm, while adults do."
    },
    {
      "heading": "3.6. Social Attribution and XAI",
      "text": "This section presents some ideas on how the work on social attribution outlined above affects researchers and practitioners in XAI."
    },
    {
      "heading": "3.6.1. Folk Psychology",
      "text": "While the models and research results presented in this section pertain to the behaviour of humans, it is reasonably clear that these models have a place in explainable AI. Heider and Simmel\u2019s seminal experiments from 1944 with moving shapes [67] (Section 3.2) demonstrate unequivocally that people attribute folk psychological concepts such as belief, desire, and intention, to artificial objects. Thus, as argued by de Graaf and Malle [34], it is not a stretch to assert that people will expect explanations using the same conceptual framework used to explain human behaviours.\nThis model is particularly promising because many knowledge-based models in deliberative AI either explicitly build on such folk psychological concepts, such as belief-desireintention (BDI) models [152], or can be mapped quite easily to them; e.g. in classical-like AI planning, goals represent desires, intermediate/landmark states represent intentions, and the environment model represents beliefs [50].\nIn addition, the concepts and relationships between actions, preconditions, and proximal and distal intentions is similar to those in models such as BDI and planning, and as such, the work on the relationships between preconditions, outcomes, and competing goals, is useful in this area."
    },
    {
      "heading": "3.6.2. Malle\u2019s Models",
      "text": "Of all of the work outlined in this section, it is clear that Malle\u2019s model, culminating in his 2004 text book [112], is the most mature and complete model of social attribution to date. His three-layer models provides a solid foundation on which to build explanations of many deliberative systems, in particular, goal-based deliberation systems.\nMalle\u2019s conceptual framework provides a suitable framework for characterising different aspects of causes for behaviour. It is clear that reason explanations will be useful for goal-based reasoners, as discussed in the case of BDI models and goal-directed AI planning, and enabling factor explanations can play a role in how questions and in counterfactual explanations. In Section 4, we will see further work on how to select explanations based on these concepts.\nHowever, the causal history of reasons (CHR) explanations also have a part to play for deliberative agents. In human behaviour, they refer to personality traits and other unconscious motives. While anthropomorphic agents could clearly use CHRs to explain behaviour, such as emotion or personality, they are also valid explanations for nonanthropomorphic agents. For example, for AI planning agents that optimise some metric, such as cost, the explanation that action a was chosen over action b because it had lower cost is a CHR explanation. The fact that the agent is optimising cost is a \u2018personality trait\u2019 of the agent that is invariant given the particular plan or goal. Other types of planning systems may instead be risk averse, optimising to minimise risk or regret, or may be \u2018flexible\u2019 and try to help out their human collaborators as much as possible. These types of explanations are CHRs; even if they are not described as personality traits to the explainee. However, one must be careful to ensure these CHRs do not make their agent appear irrational \u2014 unless of course, that is the goal one is trying to achieve with the explanation process.\nBroekens et al. [12] describe algorithms for automatic generation of explanations for BDI agents. Although their work does not build on Malle\u2019s model directly, it shares a similar structure, as noted by the authors, in that their model uses intentions and enabling conditions as explanations. They present three algorithms for explaining behaviour: (a) offering the goal towards which the action contributes; (b) offering the enabling condition of an action; and (c) offering the next action that is to be performed; thus, the explanadum is explained by offering a proximal intention. A set of human behavioural experiments showed that the different explanations are considered better in different circumstances; for example, if only one action is required to achieve the goal, then offering the goal as the explanation is more suitable than offering the other two types of explanation, while if it is part of a longer sequence, also offering a proximal intention is evaluated as being a more valuable explanation. These results reflect those by Malle, but also other results from social and cognitive psychology on the link between goals, proximal intentions, and actions, which are surveyed in Section 4.4.3"
    },
    {
      "heading": "3.6.3. Collective Intelligence",
      "text": "The research into behaviour attribution of groups (Section 3.4) is important for those working in collective intelligence; areas such as in multi-agent planning [11], computational social choice [26], or argumentation [8]. Although this line of work appears to be much less explored than attributions of individual\u2019s behaviour, the findings from Kass and Leake [85], Susskind et al., and in particular O\u2019Laughlin and Malle [137] that people assign intentions and beliefs to jointly-acting groups, and reasons to aggregate groups, indicates that the large body of work on attribution of individual behaviour could serve as a solid foundation for explanation of collective behaviour."
    },
    {
      "heading": "3.6.4. Norms and Morals",
      "text": "The work on norms and morals discussed in Section 3.5 demonstrates that normative behaviour, in particular, violation of such behaviour, has a large impact on the ascription of a Theory of Mind to actors. Clearly, for anthropomorphic agents, this work is important, but as with CHRs, I argue here that it is important for more \u2018traditional\u2019 AI as well.\nFirst, the link with morals is important for applications that elicit ethical or social concerns, such as defence, safety-critical applications, or judgements about people. Explanations or behaviour in general that violate norms may give the impression of \u2018immoral machines\u2019 \u2014 whatever that can mean \u2014 and thus, such norms need to be explicitly considered as part of explanation and interpretability.\nSecond, as discussed in Section 2.2, people mostly ask for explanations of events that they find unusual or abnormal [77, 73, 69], and violation of normative behaviour is one such abnormality [73]. Thus, normative behaviour is important in interpretability \u2014 a statement that would not surprise those researchers and practitioners of normative artificial intelligence.\nIn Section 4, we will see that norms and violation of normal/normative behaviour is also important in the cognitive processes of people asking for, constructing, and evaluating explanations, and its impact on interpretability."
    },
    {
      "heading": "4. Cognitive Processes \u2014 How Do People Select and Evaluate Explanations?",
      "text": "There are as many causes of x as there are explanations of x. Consider how the cause of death might have been set out by the physician as \u2018multiple haemorrhage\u2019, by the barrister as \u2018negligence on the part of the driver\u2019, by the carriage-builder as \u2018a defect in the brakelock construction\u2019, by a civic planner as \u2018the presence of tall shrubbery at that turning\u2019. None is more true than any of the others, but the particular context of the question makes some explanations more relevant than others. \u2013 Hanson [61, p. 54]\nMill [130] is one of the earliest investigations of cause and explanation, and he argued that we make use of \u2018statistical\u2019 correlations to identify cause, which he called the Method of Difference. He argued that causal connection and explanation selection are essentially arbitrary and the scientifically/philosophically it is \u201cwrong\u201d to select one explanation over another, but offered several cognitive biases that people seem to use, including things like unexpected conditions, precipitating causes, and variability. Such covariation models ideas were dominant in causal attribution, in particular, the work of Kelley [86]. However, many researchers noted that the covariation models failed to explain many observations; for example, people can identify causes between events from a single data point [127, 75]; and therefore, more recently, new theories have displaced them, while still acknowledging that the general idea that people using co-variations is valid.\nIn this section, we look at these theories, in particular, we survey three types of cognitive processes used in explanation: (1) causal connection, which is the process people use to identify the causes of events; (2) explanation selection, which is the process people use to select a small subset of the identified causes as the explanation; and (3) explanation evaluation, which is the processes that an explainee uses to evaluate the quality of an explanation. Most of this research shows that people have certain cognitive biases that they apply to explanation generation, selection, and evaluation."
    },
    {
      "heading": "4.1. Causal Connection, Explanation Selection, and Evaluation",
      "text": "Malle [112] presents a theory of explanation, which breaks the psychological processes used to offer explanations into two distinct groups, outlined in Figure 8:\n1. Information processes \u2014 processes for devising and assembling explanations. The present section will present related work on this topic.\n2. Impression management processes \u2013 processes for governing the social interaction of explanation. Section 5 will present related work on this topic.\nMalle [112] further splits these two dimensions into two further dimensions, which refer to the tools for constructing and giving explanations, and the explainer\u2019s perspective or knowledge about the explanation.\nTaking the two dimensions, there are four items:\n1. Information requirements \u2014 what is required to give an adequate explanation; for example, one must knows the causes of the explanandum, such as the desires and beliefs of an actor, or the mechanistic laws for a physical cause.\n2. Information access \u2014 what information the explainer has to give the explanation, such as the causes, the desires, etc. Such information can be lacking; for example, the explainer does not know the intentions or beliefs of an actor in order to explain their behaviour.\n3. Pragmatic goals \u2014 refers to the goal of the the explanation, such as transferring knowledge to the explainee, making an actor look irrational, or generating trust with the explainee.\n4. Functional capacities \u2014 each explanatory tool has functional capacities that constrain or dictate what goals can be achieved with that tool.\nMalle et al. [117] argue that this theory accounts for apparent paradoxes observed in attribution theory, most specifically the actor-observer asymmetries, in which actors and observers offer different explanations for the same action taken by an actor. They hypothesise that this is due to information asymmetry ; e.g. an observer cannot access the intentions of an actor \u2014 the intentions must be inferred from the actor\u2019s behaviour.\nIn this section, we first look specifically at processes related to the explainer: information access and pragmatic goals. When requested for an explanation, people typically do not have direct access to the causes, but infer them from observations and prior knowledge. Then, they select some of those causes as the explanation, based on the goal of the explanation. These two process are known as causal connection (or causal inference), which is a processing of identifying the key causal connections to the fact; and explanation selection (or casual selection), which is the processing of selecting a subset of those causes to provide as an explanation.\nThis paper separates casual connection into two parts: (1) abductive reasoning, the cognitive process in which people try to infer causes that explain events by making assumptions about hypotheses and testing these; and (2) simulation, which is the cognitive\nprocess of simulating through counterfactuals to derive a good explanation. These processes overlap, but can be somewhat different. For example, the former requires the reasoner to make assumptions and test the validity of observations with respect to these assumptions, while in the latter, the reasoner could have complete knowledge of the causal rules and environment, but use simulation of counterfactual cases to derive an explanation. From the perspective of explainable AI, an explanatory agent explaining its decision would not require abductive reasoning as it is certain of the causes of its decisions. An explanatory agent trying to explain some observed events not under its control, such as the behaviour of another agent, may require abductive reasoning to find a plausible set of causes.\nFinally, when explainees receive explanations, they go through the process of explanation evaluation, through which they determine whether the explanation is satisfactory or not. A primary criteria is that the explanation allows the explainee to understand the cause, however, people\u2019s cognitive biases mean that they prefer certain types of explanation over others."
    },
    {
      "heading": "4.2. Causal Connection: Abductive Reasoning",
      "text": "The relationship between explanation and abductive reasoning is introduced in Section 2.1.4. This section surveys work in cognitive science that looks at the process of abduction. Of particular interest to XAI (and artificial intelligence in general) is work demonstrating the link between explanation and learning, but also other processes that people use to simplify the abductive reasoning process for explanation generation, and to switch modes of reasoning to correspond with types of explanation."
    },
    {
      "heading": "4.2.1. Abductive Reasoning and Causal Types",
      "text": "Rehder [154] looked specifically at categorical or formal explanations. He presents the causal model theory, which states that people infer categories of objects by both their features and the causal relationships between features. His experiments show that people categorise objects based their perception that the observed properties were generated by the underlying causal mechanisms. Rehder gives the example that people not only know that birds can fly and bird have wings, but that birds can fly because they have wings. In addition, Rehder shows that people use combinations of features as evidence when assigning objects to categories, especially for features that seem incompatible based on the underlying causal mechanisms. For example, when categorising an animal that cannot fly, yet builds a nest in trees, most people would consider it implausible to categorise it as a bird because it is difficult to build a nest in a tree if one cannot fly. However, people are more likely to categorise an animal that does not fly and builds nests on the ground as a bird (e.g. an ostrich or emu), as this is more plausible; even though the first example has more features in common with a bird (building nests in trees).\nRehder [155] extended this work to study how people generalise properties based on the explanations received. When his participants were ask to infer their own explanations using abduction, they were more likely to generalise a property from a source object to a target object if they had more features that were similar; e.g. generalise a property from one species of bird to another, but not from a species of bird to a species of plant. However, given an explanation based on features, this relationship is almost completely eliminated: the generalisation was only done if the features detailed in the explanation\nwere shared between the source and target objects; e.g. bird species A and mammal B both eat the same food, which is explained as the cause for an illness, for example. Thus, the abductive reasoning process used to infer explanations were also used to generalise properties \u2013 a parallel seen in machine learning [133].\nHowever, Williams et al. [189] demonstrate that, at least for categorisation in abductive reasoning, the properties of generalisation that support learning can in fact weaken learning by overgeneralising. They gave experimental participants a categorisation task to perform by training themselves on exemplars. They asked one group to explain the categorisations as part of the training, and another to just \u2018think aloud\u2019 about their task. The results showed that the explanation group more accurately categorised features that had similar patterns to the training examples, but less accurately categorised exceptional cases and those with unique features. Williams et al. argue that explaining (which forces people to think more systematically about the abduction process) is good for fostering generalisations, but this comes at a cost of over-generalisation.\nChin-Parker and Cantelon [28] provide support for the contrastive account of explanation (see Section 2.3) in categorisation/classification tasks. They hypothesise that contrast classes (foils) are key to providing the context to explanation. They distinguish between prototypical features of categorisation, which are those features that are typical of a particular category, and diagnostic features, which are those features that are relevant for a contrastive explanation. Participants in their study were asked to either describe particular robots or explain why robots were of a particular category, and then follow-up on transfer learning tasks. The results demonstrated that participants in the design group mentioned significantly more features in general, while participants in the explanation group selectively targeted contrastive features. These results provide empirical support for contrastive explanation in category learning."
    },
    {
      "heading": "4.2.2. Background and Discounting",
      "text": "Hilton [73] discusses the complementary processes of backgrounding and discounting that affect the abductive reasoning process. Discounting is when a hypothesis is deemed less likely as a cause because additional contextual information is added to a competing hypothesis as part of causal connection. It is actually discounted as a cause to the event. Backgrounding involves pushing a possible cause to the background because it is not relevant to the goal, or new contextual information has been presented that make it no longer a good explanation (but still a cause). That is, while it is the cause of an event, it is not relevant to the explanation because e.g. the contrastive foil also has this cause.\nAs noted by Hilton [73], discounting occurs in the context of multiple possible causes \u2014 there are several possible causes and the person is trying to determine which causes the fact \u2014, while backgrounding occurs in the context of multiple necessary events \u2014 a subset of necessary causes is selected as the explanation. Thus, discounting is part of causal connection, while backgrounding is part of explanation selection."
    },
    {
      "heading": "4.2.3. Explanatory Modes",
      "text": "As outlined in Section 2.4, philosophers and psychologists accept that different types of explanations exist; for example, Aristotle\u2019s model: material, formal, efficient, and final. However, theories of causality have typically argued for only one type of cause, with the two most prominent being dependence theories and transference theories.\nLombrozo [107] argues that both dependence theories and transference theories are at least psychologically real, even if only one (or neither) is the true theory. She hypothesises that people employ different modes of abductive reasoning for different modes of cognition, and thus both forms of explanation are valid: functional (final) explanations are better for phenomena that people consider have dependence relations, while mechanistic (efficient) explanations are better for physical phenomena.\nLombrozo [107] gave experimental participants scenarios in which the explanatory mode was manipulated and isolated using a mix of intentional and accidental/incidental human action, and in a second set of experiments, using biological traits that provide a particular function, or simply cause certain events incidentally. Participants were asked to evaluate different causal claims. The results of these experiments show that when events were interpreted in a functional manner, counterfactual dependence was important, but physical connections were not. However, when events were interpreted in a mechanistic manner, both counterfactual dependence and physical dependence were both deemed important. This implies that there is a link between functional causation and dependence theories on the one hand, and between mechanistic explanation and transference theories on the other. The participants also rated the functional explanation stronger in the case that the causal dependence was intentional, as opposed to accidental.\nLombrozo [106] studied at the same issue of functional vs. mechanistic explanations for inference in categorisation tasks specifically. She presented participants with tasks similar to the following (text in square brackets added):\nThere is a kind of flower called a holing. Holings typically have brom compounds in their stems and they typically bend over as they grow. Scientists have discovered that having brom compounds in their stems is what usually causes holings to bend over as they grow [mechanistic cause]. By bending over, the holing\u2019s pollen can brush against the fur of field mice, and spread to neighboring areas [functional cause].\nExplanation prompt: Why do holings typically bend over?\nThey then gave participants a list of questions about flowers; for example: Suppose a flower has brom compounds in its stem. How likely do you think it is that it bends over?\nTheir results showed that participants who provided a mechanistic explanation from the first prompt were more likely to think that the flower would bend over, and viceversa for functional causes. Their findings shows that giving explanations influences the inference process, changing the importance of different features in the understanding of category membership, and that the importance of features in explanations can impact the categorisation of that feature. In extending work, Lombrozo and Gwynne [109] argue that people generalise better from functional than mechanistic explanations."
    },
    {
      "heading": "4.2.4. Inherent and Extrinsic Features",
      "text": "Prasada and Dillingham [149] and Prasada [148] discuss how people\u2019s abductive reasoning process prioritises certain factors in the formal mode. Prasada contends that \u201cIdentifying something as an instance of a kind and explaining some of its properties in terms of its being the kind of thing it is are not two distinct activities, but a single cognitive activity.\u201d [148, p. 2]\nPrasada and Dillingham [149] note that people represent relationships between the kinds of things and the properties that they posses. This description conforms with Overton\u2019s model of the structure of explanation [139] (see Section 2.6.5). Prasada and Dillingham\u2019s experiments showed that people distinguish between two types of properties for a kind: k-properties, which are the inherent properties of a thing that are due to its kind, and which they call principled connections; and t-properties, which are the extrinsic properties of a thing that are not due to its kind, which they call factual connections. Statistical correlations are examples of factual connections. For instance, a queen bee has a stinger and five legs because it is a bee (k-property), but the painted mark seen on almost all domesticated queen bees is because a bee keeper has marked it for ease of identification (t-property). K-properties have both principled and factual connections to their kind, whereas t-properties have mere factual connections. They note that kproperties have a normative aspect, in that it is expected that instances of kinds will have their k-properties, and when they do not, they are considered abnormal; for instance, a bee without a stinger.\nIn their experiments, they presented participants with explanations using different combinations of k-properties and t-properties to explain categorisations; for example, \u201cwhy is this a dog?\u201d Their results showed that for formal modes, explanations involving k-properties were considered much better than explanations involving t-properties, and further, that using a thing\u2019s kind to explain why it has a particular property was considered better for explaining k-properties than for explaining t-properties.\nUsing findings from previous studies, Cimpian and Salomon [30] argue that, when asked to explain a phenomenon, such as a feature of an object, people\u2019s cognitive biases make them more likely to use inherent features (k-properties) about the object to explain the phenomenon, rather than extrinsic features (t-properties), such as historical factors. An inherent feature is one that characterises \u201chow an object is constituted\u201d [30, p. 465], and therefore they tend to be stable and enduring features. For example, \u201cspiders have eight legs\u201d is inherent, while \u201chis parents are scared of spiders\u201d is not. Asked to explain why they find spiders scary, people are more likely to refer to the \u201clegginess\u201d of spiders rather than the fact that their parents have arachnophobia, even though studies show that people with arachnophobia are more likely to have family members who find spiders scary [33]. Cimpian and Salomon argue that, even if extrinsic information is known, it is not readily accessible by the mental shotgun [82] that people use to retrieve information. For example, looking at spiders, you can see their legs, but not your family\u2019s fear of them. Therefore, this leads to people biasing explanations towards inherent features rather than extrinsic. This is similar to the correspondence bias discussed in Section 3.2, in which people are more likely to describe people\u2019s behaviour on personality traits rather than beliefs, desires, and intentions, because the latter are not readily accessible while the former are stable and enduring. The bias towards inherence is affected by many factors, such as prior knowledge, cognitive ability, expertise, culture, and age."
    },
    {
      "heading": "4.3. Causal Connection: Counterfactuals and Mutability",
      "text": "To determine the causes of anything other than a trivial event, it is not possible for a person to simulate back through all possible events and evaluate their counterfactual cases. Instead, people apply heuristics to select just some events to mutate. However, this process is not arbitrary. This section looks at several biases used to assess the mutability of events; that is, the degree to which the event can be \u2018undone\u2019 to consider\ncounterfactual cases. It shows that abnormality (including social abnormality), intention, time and controllability of events are key criteria."
    },
    {
      "heading": "4.3.1. Abnormality",
      "text": "Kahneman and Tversky [83] performed seminal work in this field, proposing the simulation heuristic. They hypothesise that when answering questions about past events, people perform a mental simulation of counterfactual cases. In particular, they show that abnormal events are mutable: they are the common events that people undo when judging causality. In their experiments, they asked people to identity primary causes in causal chains using vignettes of a car accident causing the fatality of Mr. Jones, and which had multiple necessary causes, including Mr. Jones going through a yellow light, and the teenager driver of the truck that hit Mr. Jones\u2019 car while under the influence of drugs. They used two vignettes: one in which Mr. Jones the car took an unusual route home to enjoy the view along the beach (the route version); and one in which he took the normal route home but left a bit early (the time version). Participants were asked to complete an \u2018if only\u2019 sentence that undid the fatal accident, imagining that they were a family member of Mr. Jones. Most participants in the route group undid the event in which Mr. Jones took the unusual route home more than those in the time version, while those in the time version undid the event of leaving early more often than those in the route version. That is, the participants tended to focus more on abnormal causes. In particular, Kahneman and Tversky note that people did not simply undo the event with the lowest prior probability in the scenario.\nIn their second study, Kahneman and Tversky [83] asked the participants to empathise with the family of the teenager driving the truck instead of with Mr. Jones, they found that people more often undid events of the teenage driver, rather Mr. Jones. Thus, the perspective or the focus is important in what types of events people undo."
    },
    {
      "heading": "4.3.2. Temporality",
      "text": "Miller and Gunasegaram [131] show that the temporality of events is important, in particular that people undo more recent events than more distal events. For instance, in one of their studies, they asked participants to play the role of a teacher selecting exam questions for a task. In one group, the teacher-first group, the participants were told that the students had not yet studied for their exam, while those in the another group, the teacher-second group, were told that the students had already studied for the exam. Those in the teacher-second group selected easier questions than those in the first, showing that participants perceived the degree of blame they would be given for hard questions depends on the temporal order of the tasks. This supports the hypothesis that earlier events are considered less mutable than later events."
    },
    {
      "heading": "4.3.3. Controllability and Intent",
      "text": "Girotto et al. [54] investigated mutability in causal chains with respect to controllability. They hypothesised that actions controllable by deliberative actors are more mutable than events that occur as a result of environmental effects. They provided participants with a vignette about Mr. Bianchi, who arrived late home from work to find his wife unconscious on the floor. His wife subsequently died. Four different events caused Mr. Bianchi\u2019s lateness: his decision to stop at a bar for a drink on the way home, plus three non-intentional causes, such as delays caused by abnormal traffic. Different\nquestionnaires were given out with the events in different orders. When asked to undo events, participants overwhelmingly selected the intentional event as the one to undo, demonstrating that people mentally undo controllable events over uncontrollable events, irrelevant of the controllable events position in the sequence or whether the event was normal or abnormal. In another experiment, they varied whether the deliberative actions were constrained or unconstrained, in which an event is considered as constrained when they are somewhat enforced by other conditions; for example, Mr. Bianchi going to the bar (more controllable) vs. stopping due to an asthma attack (less controllable). The results of this experiment show that unconstrained actions are more mutable than constrained actions."
    },
    {
      "heading": "4.3.4. Social Norms",
      "text": "McCloy and Byrne [121] investigated the mutability of controllable events further, looking at the perceived appropriateness (or the socially normative perception) of the events. They presented a vignette similar to that of Girotto et al. [54], but with several controllable events, such as the main actor stopping to visit his parents, buy a newspaper, and stopping at a fast-food chain to get a burger. Participants were asked to provide causes as well as rate the \u2018appropriateness\u2019 of the behaviour. The results showed that participants were more likely to indicate inappropriate events as causal; e.g. stopping to buy a burger. In a second similar study, they showed that inappropriate events are traced through both normal and other exceptional events when identifying cause."
    },
    {
      "heading": "4.4. Explanation Selection",
      "text": "Similar to causal connection, people do not typically provide all causes for an event as an explanation. Instead, they select what they believe are the most relevant causes. Hilton [70] argues that explanation selection is used for cognitive reasons: causal chains are often too large to comprehend. He provides an example [70, p. 43, Figure 7] showing the causal chain for the story of the fatal car accident involving \u2018Mr. Jones\u2019 from Kahneman and Tversky [83]. For a simple story of a few paragraphs, the causal chain consists of over 20 events and 30 causes, all relevant to the accident. However, only a small amount of these are selected as explanations [172].\nIn this section, we overview key work that investigates the criteria people use for explanation selection. Perhaps unsurprisingly, the criteria for selection look similar to that of mutability, with temporality (proximal events preferred over distal events), abnormality, and intention being important, but also the features that are different between fact and foil."
    },
    {
      "heading": "4.4.1. Facts and Foils",
      "text": "As noted in Section 2, why\u2013questions are contrastive between a fact and a foil. Research shows that the two contrasts are the primary way that people select explanations. In particular, to select an explanation from a set of causes, people look at the difference between the cases of the fact and foil.\nMackie [110] is one of the earliest to argue for explanation selection based on contrastive criteria, however, the first crisp definition of contrastive explanation seems to come from Hesslow [69]:\nThis theory rests on two ideas. The first is that the effect or the explanandum, i.e. the event to be explained, should be construed, not as an object\u2019s having a certain property, but as a difference between objects with regard to a certain property. The second idea is that selection and weighting of causes is determined by explanatory relevance. [Emphasis from the original source] \u2014 Hesslow [69, p. 24]\nHesslow [69] argues that criteria for selecting explanations are clearly not arbitrary, because people seem to select explanations in similar ways to each other. He defines an explanan as a relation containing an object a (the fact in our terminology), a set of comparison objects R, called the reference class (the foils), and a property E, which a has but the objects in reference class R does not. For example, a = Spider, R = Beetle, and E = eight legs. Hesslow argues that the contrast between the fact and foil is the primary criteria for explanation selection, and that the explanation with the highest explanatory power should be the one that highlights the greatest number of differences in the attributes between the target and reference objects.\nLipton [102], building on earlier work in philosophy from Lewis [99], derived similar thoughts to Hesslow [69], without seeming to be aware of his work. He proposed a definition of contrastive explanation based on what he calls the Difference Condition:\nTo explain why P rather than Q , we must cite a causal difference between P and not-Q , consisting of a cause of P and the absence of a corresponding event in the history of not-Q . \u2013 Lipton [102, p. 256]\nFrom an experimental perspective, Hilton and Slugoski [77] were the first researchers to both identify the limitations of covariation, and instead propose that contrastive explanation is best described as the differences between the two events (discussed further in Section 4.4.2). More recent research in cognitive science from Rehder [154, 155] supports the theory that people perform causal inference, explanation, and generalisation based on contrastive cases.\nReturning to our arthropod example, for the why\u2013question between image J categorised as a fly and image K a beetle, image J having six legs is correctly determined to have no explanatory relevance, because it does not cause K to be categorised as a beetle instead of a fly. Instead, the explanation would cite some other cause, which according to Table 1, would be that the arthropod in image J has five eyes, consistent with a fly, while the one in image K has two, consistent with a beetle."
    },
    {
      "heading": "4.4.2. Abnormality",
      "text": "Related to the idea of contrastive explanation, Hilton and Slugoski [77] propose the abnormal conditions model, based on observations from legal theorists Hart and Honore\u0301 [64]. Hilton and Slugoski argue that abnormal events play a key role in causal explanation. They argue that, while statistical notions of co-variance are not the only method employed in everyday explanations, the basic idea that people select unusual events to explain is valid. Their theory states that explainers use their perceived background knowledge with explainees to select those conditions that are considered abnormal. They give the example of asking why the Challenger shuttle exploded in 1986 (rather than not exploding, or perhaps why most other shuttles do not explode). The explanation that\nit exploded \u201cbecause of faulty seals\u201d seems like a better explanation than \u201cthere was oxygen in the atmosphere\u201d. The abnormal conditions model accounts for this by noting that an explainer will reason that oxygen is present in the atmosphere when all shuttles launch, so this is not an abnormal condition. On the other hand, most shuttles to not have faulty seals, so this contributing factor was a necessary yet abnormal event in the Challenger disaster.\nThe abnormal conditions model has been backed up by subsequent experimental studies, such as those by McClure and Hilton [125], McClure et al. [126], and Hilton et al. [76], and more recently, Samland and Waldmann [161], who show that a variety of non-statistical measures are valid foils."
    },
    {
      "heading": "4.4.3. Intentionality and Functionality",
      "text": "Other features of causal chains have been demonstrated to be more important than abnormality.\nHilton et al. [76] investigate the claim from legal theorists Hart and Honore\u0301 [64] that intentional action takes priority of non-intentional action in opportunity chains. Their perspective builds on the abnormal conditions model, noting that there are two important contrasts in explanation selection: (1) normal vs. abnormal; and (2) intentional vs. nonintentional. They argue further that causes will be \u201ctraced through\u201d a proximal (more recent) abnormal condition if there is a more distal (less recent) event that is intentional. For example, to explain why someone died, one would explain that the poison they ingested as part of a meal was the cause of death; but if the poison as shown to have been deliberately placed in an attempt to murder the victim, the intention of someone to murder the victim receives priority. In their experiments, they gave participants different opportunity chains in which a proximal abnormal cause was an intentional human action, an unintentional human action, or a natural event, depending on the condition to which they were assigned. For example, a cause of an accident was ice on the road, which was enabled by either someone deliberative spraying the road, someone unintentionally placing water on the road, or water from a storm. Participants were asked to rate the explanations. Their results showed that: (1) participants rated intentional action as a better explanation than the other two causes, and non-intentional action better than natural cases; and (2) in opportunity chains, there is little preference for proximal over distal events if two events are of the same type (e.g. both are natural events) \u2014 both are seen as necessary.\nLombrozo [107] argues further that this holds for functional explanations in general; not just intentional action. For instance, citing the functional reason that an object exists is preferred to mechanistic explanations."
    },
    {
      "heading": "4.4.4. Necessity, Sufficiency and Robustness",
      "text": "Several authors [102, 107, 192] argue that necessity and sufficiency are strong criteria for preferred explanatory causes. Lipton [102] argues that necessary causes are preferred to sufficient causes. For example, consider mutations in the DNA of a particular species of beetle that cause its wings to grow longer than normal when kept in certain temperatures. Now, consider that there is two such mutations, M1 and M2, and either is sufficient to cause the mutation. To contrast with a beetle whose wings would not change, the explanation of temperature is preferred to either of the mutations M1 or M2, because neither M1 nor M2 are individually necessary for the observed event; merely that either\nM1 or M2. In contrast, the temperature is necessary, and is preferred, even if we know that the cause was M1.\nWoodward [192] argues that sufficiency is another strong criteria, in that people prefer causes that bring about the effect without any other cause. This should not be confused with sufficiency in the example above, in which either mutation M1 or M2 is sufficient in combination with temperature. Woodward\u2019s argument applies to uniquely sufficient causes, rather than cases in which there are multiple sufficient causes. For example, if it were found that are third mutation M3 could cause longer wings irrelevant of the temperature, this would be preferred over temperature plus another mutation. This is related to the notation of simplicity discussed in Section 4.5.1.\nFinally, several authors [107, 192] argue that robustness is also a criterion for explanation selection, in which the extend to which a cause C is considered robust is whether the effect E would still have occurred if conditions other than C were somewhat different. Thus, a cause C1 that holds only in specific situations has less explanatory value than cause C2, which holds in many other situations."
    },
    {
      "heading": "4.4.5. Responsibility",
      "text": "The notions of responsibility and blame are relevant to causal selection, in that an event considered more responsible for an outcome is likely to be judged as a better explanation than other causes. In fact, it relates closely to necessity, as responsibility aims to place a measure of \u2018degree of necessity\u2019 of causes. An event that is fully responsible outcome for an event is a necessary cause.\nChockler and Halpern [29] modified the structural equation model proposed by Halpern and Pearl [58] (see Section 2.1.1) to define responsibility of an outcome. Informally, they define the responsibility of cause C to event E under a situation based on the minimal number of changes required to the situation to make event E no longer occur. If N is the minimal number of changes required, then the responsibility of C causes E is 1N+1 . If N = 0, then C is fully responsible. Thus, one can see that an event that is considered more responsible than another requires less changes to prevent E than the other.\nWhile several different cognitive models of responsibility attribution have been proposed (c.f. [74, 92]), I focus on the model of Chockler and Halpern [29] because, as far I am aware aware, experimental evaluation of the model shows it to be stronger than existing models [48], and because it is a formal model that is more readily adopted in artificial intelligence.\nThe structural model approach defines the responsibility of events, rather than individuals or groups, but one can see that it can be used in group models as well. Gerstenberg and Lagnado [48] show that the model has strong predictive power at attributing responsibility to individuals in groups. They ran a set of experiments in which participants played a simple game in teams in which each individual was asked to count the number of triangles in an image, and teams won or lost depending on how accurate their collective counts were. After the game, participants rated the responsibility of each player to the outcome. Their results showed that the modified structural equation model Chockler and Halpern [29] was more accurate at predicting participants outcomes than simple counterfactual model and the so-called Matching Model, in which the responsibility is defined as the degree of deviation to the outcome; in the triangle counting game, this would be how far off the individual was to the actual number of triangles."
    },
    {
      "heading": "4.4.6. Preconditions, Failure, and Intentions",
      "text": "An early study into explanation selection in cases of more than one cause was undertaken by Leddo et al. [96]. They conducted studies asking people to rate the probability of different factors as causes of events. As predicted by the intention/goal-based theory, goals were considered better explanations than relevant preconditions. However, people also rated conjunctions of preconditions and goals as better explanations of why the event occurred. For example, for the action \u201cFred went to the restaurant\u201d, participants rated explanations such as \u201cFred was hungry\u201d more likely than \u201cFred had money in his pocket\u201d, but further \u201cFred was hungry and had money in his pocket\u201d as an even more likely explanation, despite the fact the cause itself is less likely (conjoining the two probabilities). This is consistent with the well-known conjunction fallacy [173], which shows that people sometimes estimate the probability of the conjunction of two facts higher than either of the individual fact if those two facts are representative of prior beliefs.\nHowever, Leddo et al. [96] further showed that for failed or uncompleted actions, just one cause (goal or precondition) was considered a better explanation, indicating that failed actions are explained differently. This is consistent with physical causality explanations [106]. Leddo et al. argue that to explain an action, people combine their knowledge of the particular situation with a more general understanding about causal relations. Lombrozo [107] argues similarly that this is because failed actions are not goal-directed, because people do not intend to fail. Thus, people prefer mechanistic explanations for failed actions, rather than explanations that cite intentions.\nMcClure and Hilton [123] and McClure et al. [124] found that people tend to assign a higher probability of conjoined goal and precondition for a successful action, even though they prefer the goal as the best explanation, except in extreme/unlikely situations; that is, when the precondition is unlikely to be true. They argue that is largely due to the (lack of) controllability of unlikely actions. That is, extreme/unlikely events are judged to be harder to control, and thus actors would be less likely to intentionally select that action unless the unlikely opportunity presented itself. However, for normal and expected actions, participants preferred the goal alone as an explanation instead of the goal and precondition.\nIn a follow-up study, McClure and Hilton [125] looked at explanations of obstructed vs. unobstructed events, in which an event is obstructed by its precondition being false; for example, \u201cFred wanted a coffee, but did not have enough money to buy one\u201d as an explanation for why Fred failed to get a coffee. They showed that while goals are important to both, for obstructed events, the precondition becomes more important than for unobstructed events."
    },
    {
      "heading": "4.5. Explanation Evaluation",
      "text": "In this section, we look at work that has investigated the criteria that people use to evaluate explanations. The most important of these are: probability, simplicity, generalise, and coherence with prior beliefs."
    },
    {
      "heading": "4.5.1. Coherence, Simplicity, and Generality",
      "text": "Thagard [171] argues that coherence is a primary criteria for explanation. He proposes the Theory for Explanatory Coherence, which specifies seven principles of how explanations relate to prior belief. He argues that these principles are foundational principles that explanations must observe to be acceptable. They capture properties such\nas if some set of properties P explain some other property Q , then all properties in P must be coherent with Q ; that is, people will be more likely to accept explanations if they are consistent with their prior beliefs. Further, he contends that all things being equal, simpler explanations \u2014 those that cite fewer causes \u2014 and more general explanations \u2014 those that explain more events \u2014, are better explanations. The model has been demonstrated to align with how humans make judgements on explanations [151].\nRead and Marcus-Newhall [153] tested the hypotheses from Thagard\u2019s theory of explanatory coherence [171] that people prefer simpler and more general explanations. Participants were asked to rate the probability and the \u2018quality\u2019 of explanations with different numbers of causes. They were given stories containing several events to be explained, and several different explanations. For example, one story was about Cheryl, who is suffering from three medical problems: (1) weight gain; (2) fatigue; and (3) nausea. Different participant groups were given one of three types of explanations: (1) narrow : one of Cheryl having stopped exercising (weight gain), has mononucleosis (explains fatigue), or a stomach virus (explains nausea); (2) broad : Cheryl is pregnant (explains all three); or (3) conjunctive: all three from item 1 as the same time. As predicted, participants preferred simple explanations (pregnancy) with less causes than more complex ones (all three conjunctions), and participants preferred explanations that explained more events."
    },
    {
      "heading": "4.5.2. Truth and Probability",
      "text": "Probability has two facets in explanation: the probability of the explanation being true; and the use of probability in an explanation. Neither has a much importance as one may expect.\nThe use of statistical relationships to explain events is considered to be unsatisfying on its own. This is because people desire causes to explain events, not associative relationships. Josephson and Josephson [81] give the example of a bag full of red balls. When selecting a ball randomly from the bag, it must be red, and one can ask: \u201cWhy is this ball red?\u201d. The answer that uses the statistical generalisation \u201cBecause all balls in the bag are red\u201d is not a good explanation, because it does not explain why that particular ball is red. A better explanation is someone painted it red. However, for the question: \u201cWhy did we observe a red ball coming out of the bag\u201d, it is a good explanation, because having only red balls in the bag does cause us to select a red one. Josephson and Josephson highlight that the difference between explaining the fact observed (the ball is red) and explaining the event of observing the fact (a red ball was selected). To explain instances via statistical generalisations, we need to explain the causes of those generalisations too, not the generalisations themselves. If the reader is not convinced, consider my own example: a student coming to their teacher to ask why they only received 50% on an exam. An explanation that most students scored around 50% is not going to satisfy the student. Adding a cause for why most students only scored 50% would be an improvement. Explaining to the student why they specifically received 50% is even better, as it explains the cause of the instance itself.\nThe truth of likelihood of an explanation is considered an important criteria of a good explanation. However, Hilton [73] shows that the most likely or \u2018true\u201d cause is not necessarily the best explanation. Truth conditions4 are a necessary but not sufficient\n4We use the term truth condition to refer to facts that are either true or considered likely by the explainee.\ncriteria for the generation of explanations. While a true or likely cause is one attribute of a good explanation, tacitly implying that the most probable cause is always the best explanation is incorrect. As an example, consider again the explosion of the Challenger shuttle (Section 4.4.2), in which a faulty seal was argued to be a better explanation than oxygen in the atmosphere. This is despite the fact the the \u2018seal\u2019 explanation is a likely but not known cause, while the \u2018oxygen\u2019 explanation is a known cause. Hilton argues that this is because the fact that there is oxygen in the atmosphere is presupposed ; that is, the explainer assumes that the explainee already knows this.\nMcClure [122] also challenges the idea of probability as a criteria for explanations. Their studies found that people tend not to judge the quality of explanations around their probability, but instead around their so-called pragmatic influences of causal behaviour. That is, people judge explanations on their usefulness, relevance, etc., including via Grice\u2019s maxims of conversation [56] (see Section 5.1.1 for a more detailed discussion of this). This is supported by experiments such as Read and Marcus-Newhall [153] cited above, and the work from Tversky and Kahneman [173] on the conjunction fallacy.\nLombrozo [105] notes that the experiments on generality and simplicity performed by Read and Marcus-Newhall [153] cannot rule out that participants selected simple explanations because they did not have probability or frequency information for events. Lombrozo argues that if participants assumed that the events of stopping exercising, having mononucleosis, having a stomach virus, and being pregnant are all equally likely, then the probability of the conjunction of any three is much more unlikely than any one combined. To counter this, she investigated the influence that probability has on explanation evaluation, in particular, when simpler explanations are less probable than more complex ones. Based on a similar experimental setup to that of Read and Marcus-Newhall [153], Lombrozo presented experimental participants with information about a patient with several symptoms that could be explained by one cause or several separate causes. In some setups, base rate information about each disease was provided, in which the conjunction of the separate causes was more likely than the single (simpler) cause. Without base-rate information, participants selected the most simple (less likely) explanations. When base-rate information was included, this still occurred, but the difference was less pronounced. However, the likelihood of the conjunctive scenario had to be significantly more likely for it to be chosen. Lombrozo\u2019s final experiment showed that this effect was reduced again if participants were explicitly provided with the joint probability of the two events, rather than in earlier experiments in which they were provided separately.\nPreston and Epley [150] show that the value that people assign to their own beliefs \u2013 both in terms of probability and personal relevance \u2013 correspond with the explanatory power of those beliefs. Participants were each given a particular \u2018belief\u2019 that is generally accepted by psychologists, but mostly unknown in the general public, and were then allocated to three conditions: (1) the applications condition, who were asked to list observations that the belief could explain; (2) the explanations condition, who were asked to list observations that could explain the belief (the inverse to the previous condition); and (3) a control condition who did neither. Participants were then asked to consider the probability of that belief being true, and to assign their perceived value of the belief to themselves and society in general. Their results show that people in the applications and explanations condition both assigned a higher probability to the belief being true, demonstrating that if people link beliefs to certain situations, the perceived probability increased. However, for value, the results were different: those in the applications condi-\ntion assigned a higher value than the other two conditions, and those in the explanations condition assigned a lower value than the other two conditions. This indicates that people assign higher values to beliefs that explain observations, but a lower value to beliefs that can be explained by other observations.\nKulesza et al. [90] investigate the balance between soundness and completeness of explanation. They investigated explanatory debugging of machine learning algorithms making personalised song recommendations. By using progressively simpler models with less features, they trained a recommender system to give less correct recommendations. Participants were given recommendations for songs on a music social media site, based on their listening history, and were placed into one of several treatments. Participants in each treatment would be given a different combination of soundness and completeness, where soundness means that the explanation is correct and completeness means that all of the underlying causes are identified. For example, one treatment had low soundness but high completeness, while another had medium soundness and medium completeness. Participants were given a list of recommended songs to listen to, along with the (possibly unsound and incomplete) explanations, and were subsequently asked why the song had been recommended. The participants\u2019 mental models were measured. The results show that sound and complete models were the best for building a correct mental model, but at the expense of cost/benefit. Complete but unsound explanations improved the participants\u2019 mental models more than soundness, and gave a better perception of cost/benefit, but reduced trust. Sound but incomplete explanations were the least preferred, resulting in higher costs and more requests for clarification. Overall, Kulesza et al. concluded that completeness was more important than soundness. From these results, Kulesza et al. [89] list three principles for explainability: (1) Be sound ; (2) Be complete; but (3) Don\u2019t overwhelm. Clearly, principles 1 and 2 are at odds with principle 3, indicating that careful design must be put into explanatory debugging systems."
    },
    {
      "heading": "4.5.3. Goals and Explanatory Mode",
      "text": "Vasilyeva et al. [177] show that the goal of explainer is key in how the evaluated explanations, in particular, in relation to the mode of explanation used (i.e. material, formal, efficient, final). In their experiments, they gave participants different tasks with varying goals. For instance, some participants were asked to assess the causes behind some organisms having certain traits (efficient), others were asked to categorise organisms into groups (formal), and the third group were asked for what reason organisms would have those traits (functional). They provided explanations using different modes for parts of the tasks and then asked participants to rate the \u2018goodness\u2019 of an explanation provided to them. Their results showed that the goals not only shifted the focus of the questions asked by participants, but also that participants preferred modes of explanation that were more congruent with the goal of their task. This is further evidence that being clear about the question being asked is important in explanation."
    },
    {
      "heading": "4.6. Cognitive Processes and XAI",
      "text": "This section presents some ideas on how the work on the cognitive processes of explanation affects researchers and practitioners in XAI.\nThe idea of explanation selection is not new in XAI. Particularly in machine learning, in which models have many features, the problem is salient. Existing work has primarily\nlooked at selecting which features in the model were important for a decision, mostly built on local explanations [158, 6, 157] or on information gain [90, 89]. However, as far as the authors are aware, there are currently no studies that look at the cognitive biases of humans as a way to select explanations from a set of causes."
    },
    {
      "heading": "4.6.1. Abductive Reasoning",
      "text": "Using abductive reasoning to generate explanations has a long history in artificial intelligence [97], aiming to solve problems such as fault diagnosis [144], plan/intention recognition [24], and generalisation in learning [133]. Findings from such work has parallels with many of the results from cognitive science/psychology outlined in this section. Leake [95] provides an excellent overview of the challenges of abduction for everyday explanation, and summarises work that addresses these. He notes three of the main tasks that an abductive reasoner must perform are: (1) what to explain about a given situation (determining the question); (2) how to generate explanations (abductive reasoning); and (3) how to evaluate the \u201cbest\u201d explanation (explanation selection and evaluation). He stresses that determining the goal of the explanation is key to providing a good explanation; echoing the social scientists\u2019 view that the explainee\u2019s question is important, and that such questions are typically focused on anomalies or surprising observations.\nThe work from Rehder [154, 155] and Lombrozo [108] show that that explanation is good for learning and generalisation. This is interesting and relevant for XAI, because it shows that individual users should require less explanation the more they interact with a system. First, because they will construct a better mental model of the system and be able to generalise its behaviour (effectively learning its model). Second, as they see more cases, they should become less surprised by abnormal phenomena, which as noted in Section 4.4.2, is a primary trigger for requesting explanations. An intelligent agent that presents \u2014 unprompted \u2013 an explanation alongside every decision, runs a risk of providing explanations that become less needed and more distracting over time.\nThe work on inherent vs. extrinsic features (Section 4.2.4) is relevant for many AI applications, in particular classification tasks. In preliminary work, Bekele et al. [7] use the inherence bias [30] to explain person identification in images. Their re-identification system is tasked with determining whether two images contain the same person, and uses inherent features such as age, gender, and hair colour, as well as extrinsic features such as clothing or wearing a backpack. Their explanations use the inherence bias with the aim of improving the acceptability of the explanation. In particular, when the image is deemed to be of the same person, extrinsic properties are used, while for different people, intrinsic properties are used. This work is preliminary and has not yet been evaluated, but it is an excellent example of using cognitive biases to improve explanations."
    },
    {
      "heading": "4.6.2. Mutability and Computation",
      "text": "Section 4.3 studies the heuristics that people use to discount some events over others during mental simulation of causes. This is relevant to some areas of explainable AI because, in the same way that people apply these heuristics to more efficiently search through a causal chain, so to can these heuristics be used to more efficiently find causes, while still identifying causes that a human explainee would expect.\nThe notions of causal temporality and responsibility would be reasonably straightforward to capture in many models, however, if one can capture concepts such as ab-\nnormality, responsibility intentional, or controllability in models, this provides further opportunities."
    },
    {
      "heading": "4.6.3. Abnormality",
      "text": "Abnormality clearly plays a role in explanation and interpretability. For explanation, it serves as a trigger for explanation, and is a useful criteria for explanation selection. For interpretability, it is clear that \u2018normal\u2019 behaviour will, on aggregate, be judged more explainable than abnormal behaviour.\nAbnormality is a key criteria for explanation selection, and as such, the ability to identify abnormal events in causal chains could improve the explanations that can be supplied by an explanatory agent. While for some models, such as those used for probabilistic reasoning, identifying abnormal events would be straightforward, and for others, such as normative systems, they are \u2018built in\u2019, for other types of models, identifying abnormal events could prove difficult but valuable.\nOne important note to make is regarding abnormality and its application to \u201cnoncontrastive\u201d why\u2013questions. As noted in Section 2.6.2, questions of the form \u201cWhy P?\u201d may have an implicit foil, and determining this can improve explanation. In some cases, normality could be used to mitigate this problem. That is, in the case of \u201cWhy P?\u201d, we can interpret this as \u201cWhy P rather than the normal case Q?\u201d [72]. For example, consider the application of assessing the risk of glaucoma [22]. Instead of asking why they were given a positive diagnosis rather than a negative diagnosis, the explanatory again could provide one or more default foils, which would be \u2018stereotypical\u2019 examples of people who were not diagnosed and whose symptoms were more regular with respect to the general population. Then, the question becomes why was the person diagnosed with glaucoma compared to these default stereotypical cases without glaucoma."
    },
    {
      "heading": "4.6.4. Intentionality and Functionality",
      "text": "The work discussed in Section 4.4.3 demonstrates the importance of intentionality and functionality in selecting explanations. As discussed in Section 3.6.1, these concepts are highly relevant to deliberative AI systems, in which concepts such as goals and intentions are first-class citizens. However, the importance of this to explanation selection rather than social attribution must be drawn out. In social attribution, folk psychological concepts such as intentions are attributed to agents to identify causes and explanations, while in this section, intentions are used as part of the cognitive process of selecting explanations from a causal chain. Thus, even for a non-deliberative system, labelling causes as intentional could be useful. For instance, consider a predictive model in which some features represent that an intentional event has occurred. Prioritising these may lead to more intuitive explanations."
    },
    {
      "heading": "4.6.5. Perspectives and Controllability",
      "text": "The finding from Kahneman and Tversky [83] that perspectives change the events people mutate, discussed in Section 4.3, is important in multi-agent contexts. This implies that when explaining a particular agent\u2019s decisions or behaviour, the explanatory agent could focus on undoing actions of that particular agent, rather than others. This is also consistent with the research on controllability discussed in Section 4.3, in that, from the perspective of the agent in question, they can only control their own actions.\nIn interpretability, the impact of this work is also clear: in generating explainable behaviour, with all others things being equal, agents could select actions that lead to future actions being more constrained, as the subsequent actions are less likely to have counterfactuals undone by the observer."
    },
    {
      "heading": "4.6.6. Evaluation of Explanations",
      "text": "The importance of the research outlined in Section 4.5 is clear: likelihood is not everything. While likely causes are part of good explanations, they do not strongly correlate with explanations that people find useful. The work outlined in this section provides three criteria that are at least as equally important: simplicity, generality, and coherence.\nFor explanation, if the goal of an explanatory agent is to provide the most likely causes of an event, then these three criteria can be used to prioritise among the most likely events. However, if the goal of an explanatory agent is to generate trust between itself and its human observers, these criteria should be considered as first-class criteria in explanation generation beside or even above likelihood. For example, providing simpler explanations that increase the likelihood that the observer both understands and accepts the explanation may increase trust better than giving more likely explanations.\nFor interpretability, similarly, these three criteria can form part of decision-making algorithms; for example, a deliberative agent may opt to select an action that is less likely to achieve its goal, if the action helps towards other goals that the observer knows about, and has a smaller number of causes to refer to.\nThe selection and evaluation of explanations in artificial intelligence has been studied in some detail, going back to early work on abductive reasoning, in which explanations with structural simplicity, coherence, or minimality are preferred (e.g. [156, 97]) and the concept of explanatory power of a set of hypotheses is defined as the set of manifestations those hypotheses account for [1]. Other approaches use probability as the defining factor to determine the most likely explanation (e.g. [59]). In addition to the cognitive biases of people to discount probability, the probabilistic approaches have the problem that such fine-grained probabilities are not always available [95]. These selection mechanisms are context-independent and do not account for the explanations as being relevant to the question nor the explainee.\nLeake [94], on the other hand, argues for goal-directed explanations in abductive reasoning that explicitly aim to reduce knowledge gaps; specifically to explain why an observed event is \u201creasonable\u201d and to help identify faulty reasoning processes that led to it being surprising. He proposes nine evaluation dimensions for explanations: timeliness, knowability, distinctiveness, predictive power, causal force, independence, repairability, blockability, and desirability. Some of these correspond to evaluation criteria outlined in Section 4.5; for example, distinctiveness notes that a cause that is surprising is of good explanatory value, which equates to the criteria of abnormality."
    },
    {
      "heading": "5. Social Explanation \u2014 How Do People Communicate Explanations?",
      "text": "Causal explanation is first and foremost a form of social interaction. One speaks of giving causal explanations, but not attributions, perceptions, comprehensions, categorizations, or memories. The verb to explain is a three-\nplace predicate: Someone explains something to someone. Causal explanation takes the form of conversation and is thus subject to the rules of conversation. [Emphasis original] \u2014 Hilton [72]\nThis final section looks at the communication problem in explanation \u2014 something that has been studied little in explainable AI so far. The work outlined in this section asserts that the explanation process does not stop at just selecting an explanation, but considers that an explanation is an interaction between two roles: explainer and explainee (perhaps the same person/agent playing both roles), and that there are certain \u2018rules\u2019 that govern this interaction."
    },
    {
      "heading": "5.1. Explanation as Conversation",
      "text": "Hilton [72] presents the most seminal article on the social aspects of conversation, proposing a conversational model of explanation based on foundational work undertaken by both himself and others. The primary argument of Hilton is that explanation is a conversation, and this is how it differs from causal attribution. He argues that there are two stages: the diagnosis of causality in which the explainer determines why an action/event occurred; and the explanation, which is the social process of conveying this to someone. The problem is then to \u201cresolve a puzzle in the explainee\u2019s mind about why the event happened by closing a gap in his or her knowledge\u201d [72, p. 66].\nThe conversational model argues that good social explanations must be relevant. This means that they must answer the question that is asked \u2014 merely identifying causes does not provide good explanations, because many of the causes will not be relevant to the questions; or worst still, if the \u201cmost probable\u201d causes are selected to present to the explainee, they will not be relevant to the question asked. The information that is communicated between explainer and explainee should conform to the general rules of cooperative conversation [56], including being relevant to the explainee themselves, and what they already know.\nHilton [72] terms the second stage explanation presentation, and argues that when an explainer presents an explanation to an explainee, they are engaged in a conversation. As such, they tend to follow basic rules of conversation, which Hilton argues are captured by Grice\u2019s maxims of conversation [56]: (a) quality; (b) quantity; (c) relation; and (d) manner. Coarsely, these respectively mean: only say what you believe; only say as much as is necessary; only say what is relevant; and say it in a nice way.\nThese maxims imply that the shared knowledge between explainer and explainee are presuppositions of the explanations, and the other factors are the causes that should be explained; in short, the explainer should not explain any causes they think the explainee already knows (epistemic explanation selection).\nPrevious sections have presented the relevant literature about causal connection (Sections 3 and 4) and explanation selection (Sections 4). In the remainder of this subsection, we describe Grice\u2019s model and present related research that analyses how people select explanations relative to subjective (or social) viewpoints, and present work that supports Hilton\u2019s conversational model of explanation [72]."
    },
    {
      "heading": "5.1.1. Logic and Conversation",
      "text": "Grice\u2019s maxims [56] (or the Gricean maxims) are a model for how people engage in cooperative conversation. Grice observes that conversational statements do not occur in\nisolation: they are often linked together, forming a cooperative effort to achieve some goal of information exchange or some social goal, such as social bonding. He notes then that a general principle that one should adhere to in conversation is the cooperative principle: \u201cMake your conversational contribution as much as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged\u201d [56, p. 45].\nFor this, Grice [56] distinguishes four categories of maxims that would help to achieve the cooperative principle:\n1. Quality : Make sure that the information is of high quality \u2013 try to make your contribution one that is true. This contains two maxims: (a) do not say things that you believe to be false; and (b) do not say things for which you do not have sufficient evidence.\n2. Quantity : Provide the right quantity of information. This contains two maxims: (a) make your contribution as informative as is required; and (b) do not make it more informative than is required.\n3. Relation: Only provide information that is related to the conversation. This consists of a single maxim: (a) Be relevant. This maxim can be interpreted as a strategy for achieving the maxim of quantity.\n4. Manner : Relating to how one provides information, rather than what is provided. This consists of the \u2018supermaxim\u2019 of \u2018Be perspicuous\u2019, but according to Grice, is broken into \u2018various\u2019 maxims such as: (a) avoid obscurity of expression; (b) avoid ambiguity; (c) be brief (avoid unnecessary prolixity); and (d) be orderly.\nGrice [56] argues that for cooperative conversation, one should obey these maxims, and that people learn such maxims as part of their life experience. He further links these maxims to implicature, and shows that it is possible to violate some maxims while still being cooperative, in order to either not violate one of the other maxims, or to achieve some particular goal, such as to implicate something else without saying it. Irony and metaphors are examples of violating the quality maxims, but other examples, such as: Person A: \u201cWhat did you think of the food they served?\u201d; Person B: \u201cWell, it was certainly healthy\u201d, violates the maxim of manner, but is implying perhaps that Person B did not enjoy the food, without them actually saying so.\nFollowing from the claim that explanations are conversations, Hilton [72] argues that explanations should follow these maxims. The quality and quantity categories present logical characterisations of the explanations themselves, while the relation and manner categories define how they explanations should be given."
    },
    {
      "heading": "5.1.2. Relation & Relevance in Explanation Selection",
      "text": "Of particular interest here is research to support these Gricean maxims; in particular, the related maxims of quantity and relevance, which together state that the speaker should only say what is necessary and relevant. In social explanation, research has shown that people select explanations to adhere to these maxims by considering the particular question being asked by the explainee, but also by giving explanations that the explainee does not already accept as being true.: To quote Hesslow:\nWhat are being selected are essentially questions, and the causal selection that follows from this is determined by the straightforward criterion of explanatory relevance. \u2014 [69, p. 30]\nIn Section 4.4.1, we saw evidence to suggest that the difference between the fact and foil for contrastive why\u2013questions are the relevant causes for explanation. In this section, we review work on the social aspects of explanation selection and evaluation.\nEpistemic Relevance. Slugoski et al. [165] present evidence of Gricean maxims in explanation, and of support for the idea of explanation as conversation. They argue that the form of explanation must take into account its function as an answer to a specified why\u2013 question, and that this should take part within a conversational framework, including the context of the explainee. They gave experimental participants information in the form of a police report about an individual named George who had been charged with assault after a school fight. This information contained information about George himself, and about the circumstances of the fight. Participants were then paired with another \u2018participant\u2019 (played by a researcher), were told that the other participant had either: (a) information about George; (2) the circumstances of the fight; or (c) neither; and were asked to answer why George had assaulted the other person. The results showed participants provided explanations that are tailored to their expectations of what the hearer already knows, selecting single causes based on abnormal factors of which they believe the explainee is unaware; and that participants change their explanations of the same event when presenting to explainees with differing background knowledge.\nJaspars and Hilton [80] and Hilton [73] both argue that such results demonstrate that, as well as being true or likely, a good explanation must be relevant to both the question and to the mental model of the explainee. Byrne [16] offers a similar argument in her computational model of explanation selection, noting that humans are model-based, not proof-based, so explanations must be relevant to a model.\nHalpern and Pearl [59] present an elegant formal model of explanation selection based on epistemic relevance. This model extends their work on structural causal models [59], discussed in Section 2.1.1. They define an explanation as a fact that, if found to be true, would constitute an actual cause of a specific event.\nRecall from Section 2.1.1 structural causal models [58] contain variables and functions between these variables. A situation is a unique assignment from variables to values. Halpern and Pearl [59] then define an epistemic state as a set of situations, one for each possible situation that the explainee considers possible. Explaining the causes of an event then becomes providing the values for those variables that remove some situations from the epistemic state such that the cause of the event can be uniquely identified. They then further show how to provide explanations that describe the structural model itself, rather than just the values of variables, and how to reason when provided with probability distributions over events. Given a probabilistic model, Halpern and Pearl formally define the explanatory power of partial explanations. Informally, this states that explanation C1 has more explanatory power explanation C2 for explanandum E if and only if providing C1 to the explainee increases the prior probability of E being true more than providing C2 does.\nDodd and Bradshaw [38] demonstrates that the perceived intention of a speaker is important in implicature. Just as leading questions in eyewitness reports can have an\neffect on the judgement of the eyewitness, so to it can affect explanation. They showed that the meaning and presuppositions that people infer from conversational implicatures depends heavily on the perceived intent or bias of the speaker. In their experiments, they asked participants to assess, among other things, the causes of a vehicle accident, with the account of the accident being given by different parties: a neutral bystander vs. the driver of the vehicle. Their results show that the bystander\u2019s information is more trusted, but also that incorrect presuppositions are recalled as \u2018facts\u2019 by the participants if the account was provided by the neutral source, but not the biased source; even if they observed the correct facts to begin with. Dodd and Bradshaw argue that this is because the participants filtered the information relative to their perceived intention of the person providing the account.\nThe Dilution Effect. Tetlock and Boettger [169] investigated the effect of implicature with respect to the information presented, particularly its relevance, showing that when presented with additional, irrelevant information, people\u2019s implicatures are diluted. They performed a series of controlled experiments in which participants were presented with information about an individual David, and were asked to make predictions about David\u2019s future; for example, what his grade point average (GPA) would be. There were two control groups and two test groups. In the control groups, people were told David spent either 3 or 31 hours studying each week (which we will call groups C3 and C31), while in the diluted group test groups, subjects were also provided with additional irrelevant information about David (groups T3 and T31). The results showed that those in the diluted T3 group predicted a higher GPA than those in the undiluted C3 group, while those in the diluted T31 group predicted a lower GPA than those in the undiluted C31 group. Tetlock and Boettger argued that this is because participants assumed the irrelevant information may have indeed been relevant, but its lack of support for prediction led to less extreme predictions. This study and studies on which it built demonstrate the importance of relevance in explanation.\nIn a further study, Tetlock et al. [170] explicitly controlled for conversational maxims, by informing one set of participants that the information displayed to them was chosen at random from the history of the individual. Their results showed that the dilution effect disappeared when conversational maxims were deactivated, providing further evidence for the dilution effect.\nTogether, these bodies of work and those on which they build demonstrate that Grice\u2019s maxims are indeed important in explanation for several reasons; notably that they are a good model for how people expect conversation to happen. Further, while it is clear that providing more information than necessary not only would increase the cognitive load of the explainee, but that it dilutes the effects of the information that is truly important."
    },
    {
      "heading": "5.1.3. Argumentation and Explanation",
      "text": "Antaki and Leudar [3] extend Hilton\u2019s conversational model [72] from dialogues to arguments. Their research shows that a majority of statements made in explanations are actually argumentative claim-backings; that is, justifying that a particular cause indeed did hold (or was thought to have held) when a statement is made. Thus, explanations are used to both report causes, but also to back claims, which is an argument rather than just a question-answer model. They extend the conversational model to a wider class of contrast cases. As well as explaining causes, one must be prepared to defend a particular\nclaim made in a causal explanation. Thus, explanations extend not just to the state of affairs external to the dialogue, but also to the internal attributes of the dialogue itself.\nAn example on the distinction between explanation and argument provided by Antaki and Leudar [3, p. 186] is \u201cThe water is hot because the central heating is on\u201d. The distinction lies on whether the speaker believes that the hearer believes that the water is hot or not. If it is believed that the speaker believes that the water is hot, then the central heating being on offers an explanation: it contrasts with a case in which the water is not hot. If the speaker believes that the hearer does not believe the water is hot, then this is an argument that the water should indeed be hot; particularly if the speaker believes that the hearer believes that the central heating is on. The speaker is thus trying to persuade the hearer that the water is hot. However, the distinction is not always so clear because explanations can have argumentative functions."
    },
    {
      "heading": "5.1.4. Linguistic structure",
      "text": "Malle et al. [116] argue that the linguistic structure of explanations plays an important role in interpersonal explanation. They hypothesise that some linguistic devices are used not to change the reason, but to indicate perspective and to manage impressions. They asked experimental participants to select three negative and three positive intentional actions that they did recently that were outside of their normal routine. They then asked participants to explain why they did this, and coded the answers. Their results showed several interesting findings.\nFirst, explanations for reasons can be provided in two different ways: marked or unmarked. An unmarked reason is a direct reason, while a marked reason has a mental state marker attached. For example, to answer the question \u201cWhy did she go back into the house\u201d, the explanations \u201cThe key is still in the house\u201d and \u201cShe thinks the key is still in the house\u201d both give the same reason, but with different constructs that are used to give different impressions: the second explanation gives an impression that the explainee may not be in agreement with the actor.\nSecond, people use belief markers and desire markers; for example, \u201cShe thinks the key is in the house\u201d and \u201cShe wants the key to be in her pocket\u201d respectively. In general, dropping first-person markings, that is, a speaker dropping \u201cI/we believe\u201d, is common in conversation and the listeners automatically infer that this is a belief of the speaker. For example, \u201cThe key is in the house\u201d indicates a belief on the behalf of the speaker and inferred to mean \u201cI believe the key is in the house\u201d [116]5.\nHowever, for third-person perspective, this is not the case. The unmarked version of explanations, especially belief markers, generally imply some sort of agreement from the explainer: \u201cShe went back in because the key is in the house\u201d invites the explainee to infer that the actor and the explainer share the belief that the key is in the house. Whereas, \u201cShe went back in because she believes the key is in the house\u201d is ambiguous \u2014 it does not (necessarily) indicate the belief of the speaker. The reason: \u201cShe went back in because she mistakenly believes the key is in the house\u201d offers no ambiguity of the speaker\u2019s belief.\nMalle [112, p. 169, Table 6.3] argues that different markers sit on a scale between being distancing to being embracing. For example, \u201cshe mistakenly believes\u201d is more\n5Malle [112, Chapter 4] also briefly discusses valuings as markers, such as \u201cShe likes\u201d, but notes that these are rarely dropped in reasons.\ndistancing than \u201cshe jumped to the conclusion\u2019 \u2019, while \u201cshe realises\u201d is embracing. Such constructs aim not to provide different reasons, but merely allow the speaker to form impressions about themselves and the actor."
    },
    {
      "heading": "5.2. Explanatory Dialogue",
      "text": "If we accept the model of explanation as conversation, then we may ask whether there are particular dialogue structures for explanation. There has been a collection of such articles ranging from dialogues for pragmatic explanation [176] to definitions based on transfer of understanding [179]. However, the most relevant for the problem of explanation in AI is a body of work lead largely by Walton.\nWalton [180] proposed a dialectical theory of explanation, putting forward similar ideas to that of Antaki and Leudar [3] in that some parts of an explanatory dialogue require the explainer to provide backing arguments to claims. In particular, he argues that such an approach is more suited to \u2018everyday\u2019 or interpersonal explanation than models based on scientific explanation. He further argues that such models should be combined with ideas of explanation as understanding, meaning that social explanation is about transferring knowledge from explainer to explainee. He proposes a series of conditions on the dialogue and its interactions as to when and how an explainer should transfer knowledge to an explainee.\nIn a follow-on paper, Walton [182] proposes a formal dialogue model called CE, based on an earlier persuasion dialogue [184], which defines the conditions on how a explanatory dialogue commences, rules for governing the locutions in the dialogue, rules for governing the structure or sequence of the dialogue, success rules and termination rules.\nExtending on this work further [182], Walton [183] describes an improved formal dialogue system for explanation, including a set of speech act rules for practical explanation, consisting of an opening stage, exploration stage, and closing stage. In particular, this paper focuses on the closing stage to answer the question: how do we know that an explanation has \u2018finished\u2019? Scriven [162] argues that to test someone\u2019s understanding of a topic, merely asking them to recall facts that have been told to them is insufficient \u2014 we should also be able to answer new questions that demonstrate generalisation of and inference from what has been learnt: an examination.\nTo overcome this, Walton proposes the use of examination dialogues [181] as a method for the explainer to determine whether the explainee has correctly understood the explanation \u2014 that is, the explainer has a real understanding, not merely a perceived (or claimed) understanding. Walton proposes several rules for the closing stage of the examination dialogue, including a rule for terminating due to \u2018practical reasons\u2019, which aim to solve the problem of the failure cycle, in which repeated explanations are requested, and thus the dialogue does not terminate.\nArioua and Croitoru [4] formalise Walton\u2019s work on explanation dialogue [183], grounding it in a well-known argumentation framework [147]. In addition, they provide formalisms of commitment stores and understanding stores for maintaining what each party in the dialogue is committed to, and what they already understand. This is necessary to prevent circular arguments. They further define how to shift between different dialogues in order to enable nested explanations, in which an explanation produces a new why\u2013question, but also to shift from an explanation to an argumentation dialogue, which supports nested argument due to a challenge from an explainee, as noted by Antaki and\nLeudar [3]. The rules define when this dialectical shift can happen, when it can return to the explanation, and what the transfer of states is between these; that is, how the explanation state is updated after a nested argument dialogue."
    },
    {
      "heading": "5.3. Social Explanation and XAI",
      "text": "This section presents some ideas on how research from social explanation affects researchers and practitioners in XAI."
    },
    {
      "heading": "5.3.1. Conversational Model",
      "text": "The conversational model of explanation according to Hilton [72], and its subsequent extension by Antaki and Leudar [3] to consider argumentation, are appealing and useful models for explanation in AI. In particular, they are appealing because of its generality \u2014 they can be used to explain human or agent actions, emotions, physical events, algorithmic decisions, etc. It abstracts away from the cognitive processes of causal attribution and explanation selection, and therefore does not commit to any particular model of decision making, of how causes are determined, how explanations are selected, or even any particular mode of interaction.\nOne may argue that in digital systems, many explanations would be better done in a visual manner, rather than a conversational manner. However, the models of Hilton [72], Antaki and Leudar [3], and Walton [183] are all independent of language. They define interactions based on questions and answers, but these need not be verbal. Questions could be asked by interacting with a visual object, and answers could similarly be provided in a visual way. While Grice\u2019s maxim are about conversation, they apply just as well to other modes of interaction. For instance, a good visual explanation would display only quality explanations that are relevant and relate to the question \u2014 these are exactly Grice\u2019s maxims.\nI argue that, if we are to design and implement agents that can truly explain themselves, in many scenarios, the explanation will have to be interactive and adhere to maxims of communication, irrelevant of the media used. For example, what should an explanatory agent do if the explainee does not accept a selected explanation?"
    },
    {
      "heading": "5.3.2. Dialogue",
      "text": "Walton\u2019s explanation dialogues [180, 182, 183], which build on well-accepted models from argumentation, are closer to the notion of computational models than that of Hilton [72] or Antaki and Leudar [3]. While Walton also abstracts away from the cognitive processes of causal attribution and explanation selection, his dialogues are more idealised ways of how explanation can occur, and thus make certain assumptions that may be reasonable for a model, but of course, do not account for all possible interactions. However, this is appealing from an explainable AI perspective because it is clear that the interactions between an explanatory agent and an explainee will need to be scoped to be computationally tractable. Walton\u2019s models provide a nice step towards implementing Hilton\u2019s conversational model.\nArioua and Croitoru\u2019s formal model for explanation [4] not only brings us one step closer to a computational model, but also nicely brings together the models of Hilton [72] and Antaki and Leudar [3] for allowing arguments over claims in explanations. Such formal models of explanation could work together with concepts such as conversation policies [55] to implement explanations.\nThe idea of interactive dialogue XAI is not new. In particular, a body of work by Cawsey [17, 18, 19] describes EDGE: a system that generates natural-language dialogues for explaining complex principles. Cawsey\u2019s work was novel because it was the first to investigate discourse within an explanation, rather than discourse more generally. Due to the complexity of explanation, Cawsey advocates context-specific, incremental explanation, interleaving planning and execution of an explanation dialogue. EDGE separates content planning (what to say) from dialogue planning (organisation of the interaction). Interruptions attract their own sub-dialog. The flow of the dialogue is context dependent, in which context is given by: (1) the current state of the discourse relative to the goal/sub-goal hierarchy; (2) the current focus of the explanation, such as which components of a device are currently under discussion; and (3) assumptions about the user\u2019s knowledge. Both the content and dialogue are influenced by the context. The dialogue is planned using a rule-based system that break explanatory goals into sub-goals and utterances. Evaluation of EDGE [19] is anecdotal, based on a small set of people, and with no formal evaluation or comparison.\nAt a similar time, Moore and Paris [134] devised a system for explanatory text generation within dialogues that also considers context. They explicitly reject the notion that schemata can be used to generate explanations, because they are too rigid and lack the intentional structure to recover from failures or misunderstandings in the dialogue. Like Cawsey\u2019s EDGE system, Moore and Paris explicitly represent the user\u2019s knowledge, and plan dialogues incrementally. The two primary differences from EDGE is that Moore and Paris\u2019s system explicitly models the effects that utterances can have on the hearer\u2019s mental state, providing flexibility that allows recovery from failure and misunderstanding; and that the EDGE system follows an extended explanatory plan, including probing questions, which are deemed less appropriate in Moore and Paris\u2019s application area of advisory dialogues. The focus of Cawsey\u2019s and Moore and Paris\u2019s work are in applications such as intelligent tutoring, rather than on AI that explains itself, but many of the lessons and ideas generalise.\nEDGE and other related research on interactive explanation considers only verbal dialogue. As noted above, abstract models of dialogue such as those proposed by Walton [183] may serve as a good starting point for multi-model interactive explanations."
    },
    {
      "heading": "5.3.3. Theory of Mind",
      "text": "In Section 2.6.4, I argue that an explanation-friendly model of self is required to provide meaningful explanations. However, for social explanation, a Theory of Mind is also required. Clearly, as part of a dialog, an explanatory agent should at least keep track of what has already been explained, which is a simple model of other and forms part of the explanatory context. However, if an intelligent agent is operating with a human explainee in a particular environment, it could may have access to more complete models of other, such as the other\u2019s capabilities and their current beliefs or knowledge; and even the explainee\u2019s model of the explanatory agent itself. If it has such a model, the explanatory agent can exploit this by tailoring the explanation to the human observer. Halpern and Pearl [59] already considers a simplified idea of this in their model of explanation, but other work on epistemic reasoning and planning [42, 135] and planning for interactive dialogue [143] can play a part here. These techniques will be made more powerful if they are aligned with user modelling techniques used in HCI [44].\nWhile the idea of Theory of Mind in AI is not new; see for example [178, 37]; it\u2019s application to explanation has not been adequately explored. Early work on XAI took the idea of dialogue and user modelling seriously. For example, Cawsey\u2019s EDGE system, described in Section 5.3.2, contains a specific user model to provide better context for interactive explanations [20]. Cawsey argues that the user model must be integrated closely with explanation model to provide more natural dialogue. The EDGE user model consists of two parts: (1) the knowledge that the user has about a phenomenon; and (2) their \u2018level of expertise\u2019; both of which can be updated during the dialogue. EDGE uses dialogues questions to build a user model, either explicitly, using questions such as \u201cDo you known X?\u201d or \u201cWhat is the value of Y?\u201d, or implicitly, such as when a user asks for clarification. EDGE tries to guess other indirect knowledge using logical inference from this direct knowledge. This knowledge is then used to tailor explanation to the specific person, which is an example of using epistemic relevance to select explanations. Cawsey was not the first to consider user knowledge; for example, Weiner\u2019s BLAH system [185] for incremental explanation also had a simple user model for knowledge that is used to tailor explanation, and Weiner refers to Grice\u2019s maxim of quality to justify this.\nMore recently, Chakraborti et al. [21] discuss preliminary work in this area for explaining plans. Their problem definition consists of two planning models: the explainer and the explainee; and the task is to align the two models by minimising some criteria; for example, the number of changes. This is an example of using epistemic relevance to tailor an explanation. Chakraborti et al. class this as contrastive explanation, because the explanation contrasts two models. However, this is not the same use of the term \u2018contrastive\u2019 as used in social science literature (see Section 2.3), in which the contrast is an explicit foil provided by the explainee as part of a question."
    },
    {
      "heading": "5.3.4. Implicature",
      "text": "It is clear that in some settings, implicature can play an important role. Reasoning about implications of what the explainee says could support more succinct explanations, but just as importantly, those designing explanatory agents must also keep in mind what people could infer from the literal explanations \u2014 both correctly and incorrectly.\nFurther to this, as noted by Dodd and Bradshaw [38], people interpret explanations relative to the intent of the explainer. This is important for explainable AI because one of the main goals of explanation is to establish trust of people, and as such, explainees will be aware of this goal. It is clear that we should quite often assume from the outset that trust levels are low. If explainees are sceptical of the decisions made by a system, it is not difficult to imagine that they will also be sceptical of explanations provided, and could interpret explanations as biased."
    },
    {
      "heading": "5.3.5. Dilution",
      "text": "Finally, it is important to focus on dilution. As noted in the introduction of this paper, much of the work in explainable AI is focused on causal attributions. The work outlined in Section 4 shows that this is only part of the problem. While presenting a casual chain may allow an explainee to fill in the gaps of their own knowledge, there is still a likely risk that the less relevant parts of the chain will dilute those parts that are crucial to the particular question asked by the explainee. Thus, this again emphasises the importance of explanation selection and relevance."
    },
    {
      "heading": "5.3.6. Social and Interactive Explanation",
      "text": "The recent surge in explainable AI has not (yet) truly adopted the concept sociallyinteractive explanation, at least, relative to the first wave of explainable AI systems such as that by Cawsey [20] and Moore and Paris [134]. I hypothesise that this is largely due to the nature of the task being explained. Most recent research is concerned with explainable machine learning, whereas early work explained symbolic models such as expert systems and logic programs. This influences the research in two ways: (1) recent research focuses on how to abstract and simplify uninterpretable models such as neural nets, whereas symbolic approaches are relatively more interpretable and need less abstraction in general; and (2) an interactive explanation is a goal-based endeavour, which lends itself more naturally to symbolic approaches. Given that early work on XAI was to explain symbolic approaches, the authors of such work would have more intuitively seen the link to interaction. Despite this, others in the AI community have recently re-discovered the importance of social interaction for explanation; for example, [186, 163], and have noted that this is a problem that requires collaboration with HCI researchers."
    },
    {
      "heading": "6. Conclusions",
      "text": "In this paper, I have argued that explainable AI can benefit from existing models of how people define, generate, select, present, and evaluate explanations. I have reviewed what I believe are some of the most relevant and important findings from social science research on human explanation, and have provide some insight into how this work can be used in explainable AI.\nIn particular, we should take the four major findings noted in the introduction into account in our explainable AI models: (1) why\u2013questions are contrastive; (2) explanations are selected (in a biased manner); (3) explanations are social; and (4) probabilities are not as important as causal links. I acknowledge that incorporating these ideas are not feasible for all applications, but in many cases, they have the potential to improve explanatory agents. I hope and expect that readers will also find other useful ideas from this survey.\nIt is clear that adopting this work into explainable AI is not a straightforward step. From a social science viewpoint, these models will need to be refined and extended to provide good explanatory agents, which requires researchers in explainable AI to work closely with researchers from philosophy, psychology, cognitive science, and human-computer interaction. Already, projects of this type are underway, with impressive results; for example, see [91, 89, 157]."
    },
    {
      "heading": "Acknowledgements",
      "text": "The author would like to thank Denis Hilton for his review on an earlier draft of this paper, pointers to several pieces of related work, and for his many insightful discussions on the link between explanation in social sciences and artificial intelligence. The author would also like to thank several others for critical input of an earlier draft: Natasha Goss, Michael Winikoff, Gary Klein, Robert Hoffman, and the anonymous reviewers; and Darryn Reid for his discussions on the link between self, trust, and explanation.\nThis work was undertaken while the author was on sabbatical at the Universite\u0301 de Toulouse Capitole, and was partially funded by Australian Research Council DP160104083\nCatering for individuals\u2019 emotions in technology development and and a Sponsored Research Collaboration grant from the Commonwealth of Australia Defence Science and Technology Group and the Defence Science Institute, an initiative of the State Government of Victoria."
    }
  ],
  "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences",
  "year": 2018
}
