{
  "abstractText": "Explainable arti\u0080cially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, arti\u0080cial tasks such as how well humans predict the AI\u0080s decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. \u008ce results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our \u0080eld may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform be\u008aer than humans or AIs alone.",
  "authors": [
    {
      "affiliations": [],
      "name": "Zana Bu\u00e7inca"
    },
    {
      "affiliations": [],
      "name": "Phoebe Lin"
    },
    {
      "affiliations": [],
      "name": "Krzysztof Z. Gajos"
    },
    {
      "affiliations": [],
      "name": "Elena L. Glassman"
    }
  ],
  "id": "SP:8a9622715cb07f09680fbae4be5b09a928a60333",
  "references": [
    {
      "authors": [
        "Kenneth C. Arnold",
        "Krysta Chaunce",
        "Krzysztof Z. Gajos"
      ],
      "title": "Predictive Text Encourages Predictable Writing",
      "venue": "In Proceedings of the 25th International Conference on Intelligent User Interfaces (IUI \u201920)",
      "year": 2020
    },
    {
      "authors": [
        "Gagan Bansal",
        "Besmira Nushi",
        "Ece Kamar",
        "Walter S Lasecki",
        "Daniel S Weld",
        "Eric Horvitz"
      ],
      "title": "Beyond Accuracy: \u008ce Role of Mental Models in Human-AI Team Performance",
      "venue": "In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,",
      "year": 2019
    },
    {
      "authors": [
        "Jennifer S Blumenthal-Barby",
        "Heather Krieger"
      ],
      "title": "Cognitive biases and heuristics in medical decision making: a critical review using a systematic search strategy",
      "venue": "Medical Decision Making 35,",
      "year": 2015
    },
    {
      "authors": [
        "Jonathan Chang",
        "Sean Gerrish",
        "Chong Wang",
        "Jordan L Boyd-Graber",
        "David M Blei"
      ],
      "title": "Reading tea leaves: How humans interpret topic models",
      "venue": "In Advances in neural information processing systems",
      "year": 2009
    },
    {
      "authors": [
        "Pat Croskerry"
      ],
      "title": "Cognitive forcing strategies in clinical decisionmaking",
      "venue": "Annals of emergency medicine 41,",
      "year": 2003
    },
    {
      "authors": [
        "Louis Deslauriers",
        "Logan S McCarty",
        "Kelly Miller",
        "Kristina Callaghan",
        "Greg Kestin"
      ],
      "title": "Measuring actual learning versus feeling of learning in response to being actively engaged in the classroom",
      "venue": "Proceedings of the National Academy of Sciences",
      "year": 2019
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Been Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608",
      "year": 2017
    },
    {
      "authors": [
        "Mary T Dzindolet",
        "Sco\u008a A Peterson",
        "Regina A Pomranky",
        "Linda G Pierce",
        "Hall P Beck"
      ],
      "title": "\u008ce role of trust in automation reliance",
      "venue": "International journal of human-computer studies 58,",
      "year": 2003
    },
    {
      "authors": [
        "K Anders Ericsson",
        "Herbert A Simon"
      ],
      "title": "Protocol analysis: Verbal reports",
      "year": 1984
    },
    {
      "authors": [
        "Ellen C Garbarino",
        "Julie A Edell"
      ],
      "title": "Cognitive e\u0082ort, a\u0082ect, and choice",
      "venue": "Journal of consumer research 24,",
      "year": 1997
    },
    {
      "authors": [
        "Francisco Javier Chiyah Garcia",
        "David A Robb",
        "Xingkun Liu",
        "Atanas Laskov",
        "Pedro Patron",
        "Helen Hastie"
      ],
      "title": "Explainable autonomy: A study of explanation styles for building clear mental models",
      "venue": "In Proceedings of the 11th International Conference on Natural Language Generation",
      "year": 2018
    },
    {
      "authors": [
        "Leilani H Gilpin",
        "David Bau",
        "Ben Z Yuan",
        "Ayesha Bajwa",
        "Michael Specter",
        "Lalana Kagal"
      ],
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "venue": "IEEE 5th International Conference on data science and advanced analytics (DSAA)",
      "year": 2018
    },
    {
      "authors": [
        "George Anthony Gorry",
        "Michael S Sco\u008a Morton"
      ],
      "title": "A framework for management information systems",
      "year": 1971
    },
    {
      "authors": [
        "Ben Green",
        "Yiling Chen"
      ],
      "title": "Disparate interactions: An algorithm-in-theloop analysis of fairness in risk assessments",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Ben Green",
        "Yiling Chen"
      ],
      "title": "\u008ce principles and limits of algorithm-in-theloop decision making",
      "venue": "Proceedings of the ACM on Human-Computer Interaction",
      "year": 2019
    },
    {
      "authors": [
        "Renate H\u00e4uslschmid",
        "Max von Buelow",
        "Bastian P\u0083eging",
        "Andreas Butz"
      ],
      "title": "Supportingtrust in autonomous driving",
      "venue": "In Proceedings of the 22nd international conference on intelligent user interfaces",
      "year": 2017
    },
    {
      "authors": [
        "Robert R Ho\u0082man",
        "Shane T Mueller",
        "Gary Klein",
        "Jordan Litman"
      ],
      "title": "Metrics for explainable AI: Challenges and prospects",
      "year": 2018
    },
    {
      "authors": [
        "Mary E Johnston",
        "Karl B Langton",
        "R Brian Haynes",
        "Alix Mathieu"
      ],
      "title": "E\u0082ects of computer-based clinical decision support systems on clinician performance and patient outcome: a critical appraisal of research",
      "venue": "Annals of internal medicine 120,",
      "year": 1994
    },
    {
      "authors": [
        "Ece Kamar"
      ],
      "title": "Directions in Hybrid Intelligence: Complementing AI Systems with Human Intelligence",
      "venue": "In IJCAI",
      "year": 2016
    },
    {
      "authors": [
        "Ece Kamar",
        "Severin Hacker",
        "Eric Horvitz"
      ],
      "title": "Combining human and machine intelligence in large-scale crowdsourcing. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems IUI \u201920",
      "venue": "March 17\u201320,",
      "year": 2012
    },
    {
      "authors": [
        "Jon Kleinberg",
        "Himabindu Lakkaraju",
        "Jure Leskovec",
        "Jens Ludwig",
        "Sendhil Mullainathan"
      ],
      "title": "Human Decisions and Machine Predictions",
      "venue": "\u008ae \u0089arterly Journal of Economics 133,",
      "year": 2017
    },
    {
      "authors": [
        "Wouter Kool",
        "Ma\u008ahew Botvinick"
      ],
      "title": "Mental labour",
      "venue": "Nature human behaviour 2,",
      "year": 2018
    },
    {
      "authors": [
        "Wouter Kool",
        "Joseph T McGuire",
        "Zev B Rosen",
        "Ma\u008ahew M Botvinick"
      ],
      "title": "Decision making and the avoidance of cognitive demand",
      "venue": "Journal of Experimental Psychology: General 139,",
      "year": 2010
    },
    {
      "authors": [
        "Todd Kulesza",
        "Margaret Burne",
        "Weng-Keen Wong",
        "Simone Stumpf"
      ],
      "title": "Principles of explanatory debugging to personalize interactive machine learning",
      "venue": "In Proceedings of the 20th international conference on intelligent user interfaces",
      "year": 2015
    },
    {
      "authors": [
        "Isaac Lage",
        "Emily Chen",
        "Je\u0082rey He",
        "Menaka Narayanan",
        "Been Kim",
        "Samuel J Gershman",
        "Finale Doshi-Velez"
      ],
      "title": "Human Evaluation of Models Built for Interpretability",
      "venue": "In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,",
      "year": 2019
    },
    {
      "authors": [
        "Vivian Lai",
        "Chenhao Tan"
      ],
      "title": "On human predictions with explanations and predictions of machine learning models: A case study on deception detection",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Stephen H Bach",
        "Jure Leskovec"
      ],
      "title": "Interpretable decision sets: A joint framework for description and prediction",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining",
      "year": 2016
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Osbert Bastani"
      ],
      "title": "How do I fool you?\u201d: Manipulating User Trust via Misleading Black Box Explanations",
      "year": 2019
    },
    {
      "authors": [
        "Kathryn Ann Lambe",
        "Gary O\u2019Reilly",
        "Brendan D. Kelly",
        "Sarah Curristan"
      ],
      "title": "Dual-process cognitive interventions to enhance diagnostic reasoning: A systematic review",
      "venue": "BMJ \u0089ality and Safety 25,",
      "year": 2016
    },
    {
      "authors": [
        "John D Lee",
        "Katrina A See"
      ],
      "title": "Trust in automation: Designing for appropriate reliance",
      "venue": "Human factors 46,",
      "year": 2004
    },
    {
      "authors": [
        "Bonnie M Muir"
      ],
      "title": "Trust between humans and machines, and the design of decision aids. International journal of man-machine studies",
      "year": 1987
    },
    {
      "authors": [
        "Forough Poursabzi-Sangdeh",
        "Daniel G Goldstein",
        "Jake M Hofman",
        "Jennifer Wortman Vaughan",
        "Hanna Wallach"
      ],
      "title": "Manipulating and measuring model interpretability",
      "year": 2018
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should I trust you?: Explaining the predictions of any classi\u0080er",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining",
      "year": 2016
    },
    {
      "authors": [
        "Ramprasaath R Selvaraju",
        "Michael Cogswell",
        "Abhishek Das",
        "Ramakrishna Vedantam",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "venue": "In Proceedings of the IEEE International Conference on Computer Vision",
      "year": 2017
    },
    {
      "authors": [
        "Anuj K Shah",
        "Daniel M Oppenheimer"
      ],
      "title": "Heuristics made easy: An e\u0082ort-reduction framework",
      "venue": "Psychological bulletin 134,",
      "year": 2008
    },
    {
      "authors": [
        "Jonathan Sherbino",
        "Kulamakan Kulasegaram",
        "Elizabeth Howey",
        "Geo\u0082rey Norman"
      ],
      "title": "Ine\u0082ectiveness of cognitive forcing strategies to reduce biases in diagnostic reasoning: A controlled trial",
      "venue": "Canadian Journal of Emergency Medicine 16,",
      "year": 2014
    },
    {
      "authors": [
        "R. David"
      ],
      "title": "A General Inductive Approach for Analyzing \u008balitative Evaluation Data",
      "venue": "\u008comas",
      "year": 2006
    },
    {
      "authors": [
        "Amos Tversky",
        "Daniel Kahneman"
      ],
      "title": "Judgment under uncertainty: Heuristics and biases",
      "year": 1974
    },
    {
      "authors": [
        "Danding Wang",
        "Qian Yang",
        "Ashraf Abdul",
        "Brian Y Lim"
      ],
      "title": "Designing \u008ceory-Driven User-Centric Explainable AI",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Yingxu Wang"
      ],
      "title": "\u008ce theoretical framework of cognitive informatics",
      "venue": "International Journal of Cognitive Informatics and Natural Intelligence (IJCINI)",
      "year": 2007
    },
    {
      "authors": [
        "Yingxu Wang",
        "Ying Wang",
        "Shushma Patel",
        "Dilip Patel"
      ],
      "title": "A layered reference model of the brain (LRMB)",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",
      "year": 2006
    },
    {
      "authors": [
        "Katharina Weitz",
        "Dominik Schiller",
        "Ruben Schlagowski",
        "Tobias Huber",
        "Elisabeth Andr\u00e9"
      ],
      "title": "Do you trust me?: Increasing User-Trust by Integrating Virtual Agents in Explainable AI Interaction Design",
      "venue": "In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents",
      "year": 2019
    },
    {
      "authors": [
        "Robert Andrew Wilson",
        "Frank C Keil"
      ],
      "title": "\u008ae MIT encyclopedia of the cognitive sciences",
      "year": 2001
    },
    {
      "authors": [
        "Ming Yin",
        "Jennifer Wortman Vaughan",
        "Hanna Wallach"
      ],
      "title": "Understanding the E\u0082ect of Accuracy on Trust in Machine Learning Models",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "year": 2019
    },
    {
      "authors": [
        "John Zeleznikow"
      ],
      "title": "Building intelligent legal decision support systems: Past practice and future challenges",
      "venue": "In Applied Intelligent Systems",
      "year": 2004
    },
    {
      "authors": [
        "Bolei Zhou",
        "Yiyou Sun",
        "David Bau",
        "Antonio Torralba"
      ],
      "title": "Interpretable Basis Decomposition for Visual Explanation",
      "venue": "In ECCV",
      "year": 2018
    }
  ],
  "sections": [
    {
      "text": "CCS CONCEPTS \u2022Human-centered computing\u2192 Interaction design; Empirical studies in interaction design;\nKEYWORDS explanations, arti cial intelligence, trust\nACM Reference format: Zana Buc\u0327inca*, Phoebe Lin*, Krzysztof Z. Gajos, and Elena L. Glassman. 2020. Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating\n* equal contribution. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. IUI \u201920, Cagliari, Italy \u00a9 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-7118-6/20/03. . .$15.00 DOI: 10.1145/3377325.3377498\nExplainable AI Systems. In Proceedings of 25th International Conference on Intelligent User Interfaces, Cagliari, Italy, March 17\u201320, 2020 (IUI \u201920), 11 pages. DOI: 10.1145/3377325.3377498"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Because people and AI-powered systems have complementary strengths, many expected that human+AI teams would perform be er on decision-making tasks than either people or AIs alone [1, 21, 22]. However, there is mounting evidence that human+AI teams o en perform worse than AIs alone [16, 17, 28, 34].\nWe hypothesize that this mismatch between our eld\u2019s aspirations and the current reality can be a ributed, in part, to several pragmatic decisions we frequently make in our research practice. Speci cally, although our aspiration is formulated at the level of sociotechnical systems , i.e., human+AI teams working together to make complex decisions, we o en make one of two possible critical mistakes: (1) Rather than evaluating how well the human+AI team performs together on a decision-making task, we evaluate by using proxy tasks, how accurately a human can predict the decision or decision boundaries of the AI [13, 27, 29, 34]. (2) We rely on subjective measures of trust and preference, e.g., [35, 36, 44], instead of objective measures of performance. We consider each of these two concerns in turn.\nFirst, evaluations that use proxy tasks force study participants to pay a ention to the AI and the accompanying explanations\u2014 something that they are unlikely to do when performing a realistic decision-making task. Cognitive science provides compelling evidence that people treat cognition like any other form of labor [24] and favor less demanding forms of cognition, i.e., heuristics over analytical thinking, even in high stakes contexts like medical diagnosis [31]. erefore, we hypothesize that user performance and preference on proxy tasks may not accurately predict their performance and preference on the actual decision-making tasks where their cognitive focus is elsewhere and they can choose whether and how much to a end to the AI.\nSecond, subjective measures such as trust and preference have been embraced as the focal point for the evaluation of explainable systems [35, 36, 44], but we hypothesize that subjective measures may also be poor predictors of the ultimate performance of people\nar X\niv :2\n00 1.\n08 29\n8v 1\n[ cs\n.A I]\n2 2\nJa n\nperforming realistic decision-making tasks while supported by explainable AI-powered systems. Preference and trust are important facets of explainable AI systems: they may predict users\u2019 intent to a end to the AI and its explanations in realistic tasks se ings and adhere to the system s recommendations. However, the goal of explainable interfaces should be instilling in users the right amount of trust [10, 32, 33]. is remains a remarkable challenge, as on one end of the trust spectrum users might over-rely on the system and remain oblivious of its errors, whereas on the other end they might exhibit self-reliance and ignore the system s correct recommendations. Furthermore, evaluating an AI\u2019s decision, its explanation of that decision, and incorporating that information into the decisionmaking process requires cognitive e ort and the existing evidence suggests that preference does not predict performance on cognitive tasks [8, 12, 37].\nTo evaluate these two hypotheses, we conducted two online experiments and one in-person study of an AI-powered decision support system for a nutrition-related decision-making task. In one online study we used a proxy task, in which participants were asked to predict the AI\u2019s recommendations given the explanations produced by the explainable AI system. In the second online study, participants completed an actual decision-making task: actually making decisions assisted by the same explainable AI system as in the rst study. In both studies, we measured participants\u2019 objective performance and collected subjective measures of trust, preference, mental demand, and understanding. In the in-person study, we used a think-aloud method to gain insights into how people reason while making decisions assisted by an explainable AI system. In each study, we presented participants with two substantially distinct explanation types eliciting either deductive or inductive reasoning.\ne results of these studies indicate that (1) subjective measures from the proxy task do not generalize to the actual decision-making task, and (2) when using actual decision-making tasks, subjective results do not predict objective performance results. Speci cally, participants trusted and preferred inductive explanations in the proxy task, whereas they trusted and preferred the deductive explanations in the actual task. Second, in the actual decision-making task, participants recognized AI errors be er with inductive explanations, yet they preferred and trusted the deductive explanations more. e in-person think-aloud study revealed insights about why participants preferred and trusted one explanation type over another, but we found that by thinking aloud during an actual decision-making task, participants may be induced to exert additional cognitive e ort, and behave di erently than they would during an actual decision-making task when they are, more realistically, not thinking aloud.\nIn summary, we show that the results of evaluating explainable AI systems using proxy tasks may not predict the results of evaluations using actual decision-making tasks. Users also do not necessarily perform be er with systems that they prefer and trust more. To draw correct conclusions from empirical studies, explainable AI researchers should be wary of evaluation pitfalls, such as proxy tasks and subjective measures. us, as we recognize that explainable AI technology forms part of sociotechnical systems, and as we increasingly use these technologies in high-stakes scenarios, our evaluation methodologies need to reliably demonstrate\nhow the entire sociotechnical systems (i.e., human+AI teams) will perform on real tasks."
    },
    {
      "heading": "2 RELATEDWORK",
      "text": ""
    },
    {
      "heading": "2.1 Decision-making and Decision Support Systems",
      "text": "Decision-making is a fundamental cognitive process that allows humans to choose one option or course of action from among a set of alternatives [42, 43, 45]. Since it is an undertaking that requires cognitive e ort, people o en employ mental shortcuts, or heuristics, when making decisions [40]. ese heuristics save time and e ort, and frequently lead to good outcomes, but in some situations they result in cognitive biases that systematically lead to poor decisions (see, e.g., [4]).\nTo help people make good decision reliably, computer-based Decision Support Systems (DSS) have been used across numerous disciplines (e.g., management [15], medicine [20], justice [47]). While DSS have been around for a long time, they are now increasingly being deployed because the recent advancements in AI enabled these systems to achieve high accuracy. But since humans are the nal arbiters in decisions made with DSS, the overall sociotechincal system\u2019s accuracy depends both on the system\u2019s accuracy and on the humans and their underlying cognitive processes. Research shows that even when supported by a DSS, people are prone to insert bias into the decision-making process [16].\nOne approach for mitigating cognitive biases in decision-making is to use cognitive forcing strategies, which introduce self-awareness and self-monitoring of decision-making [7]. Although not universally e ective [38], these strategies have shown promising results as they improve decision-making performance, both if the human is assisted [17, 34] or is not assisted by a DSS [31]. To illustrate, Green & Chen [17] showed that across di erent AI-assisted decision-making treatments, humans performed best when they had to make the preliminary decision on their own rst before being shown the system recommendation (which forced them to engage analytically with the system\u2019s recommendation and explanation if their own preliminary decision di ered from that o ered by the system). Even though conceptual frameworks that consider cognitive processes in decision-making with DSS have been proposed recently [41], further research is needed to thoroughly investigate how to incorporate DSS into human decision-making and the e ect of cognitive processes while making system-assisted decisions."
    },
    {
      "heading": "2.2 Evaluating AI-Powered Decision Support Systems",
      "text": "Motivated by the growing number of studies in interpretable and explainable AI-powered decision support systems, researchers have called for more rigorous evaluation of explainable systems [9, 14, 19]. Notably, Doshi-Velez & Kim [9] proposed a taxonomy for evaluation of explainable AI systems, composed of the following categories: application grounded evaluation (i.e., domain experts evaluated on actual tasks), human grounded evaluation (i.e., lay humans evaluated on simpli ed tasks) and functionally grounded evaluation (i.e., no humans, proxy tasks). To put our work into context, our de nition of the actual task falls into application grounded\nevaluation, where people for whom the system is intended (i.e., not necessarily experts) are evaluated on the intended task. Whereas, the the proxy task is closer to human grounded evaluation but addresses both domain experts and lay people evaluated on simpli ed tasks, such as the simulation of model\u2019s prediction given an input and an explanation.\nStudies using actual tasks evaluate the performance of human and the system, as a whole, on the decision-making task [3, 17, 23, 46]. In these studies, participants are told to focus on making good decisions and it is up to them to decide whether and how to use the AI\u2019s assistance to accomplish the task. In contrast, studies that use proxy tasks evaluate how well users are able to simulate the model\u2019s decisions [6, 13, 27, 34] or decision boundaries [29]. In such studies, participants are speci cally instructed to pay a ention to the AI. ese studies evaluate the human\u2019s mental model of the system when the human is actively a ending to the system\u2019s predictions and explanations, but do not necessarily evaluate how well the human is able to perform real decision-making tasks with the system. For example, to identify which factors make a model more interpretable, Lage et al. ask participants to simulate the interpretable model\u2019s predictions [27].\nIn addition to the evaluation task, the choice of evaluation metrics is a critical one for the correct evaluation of intelligent systems [2]. In explainable AI literature, subjective measures, such as user trust and experience, have been largely embraced as the focal point for the evaluation of explainable systems [35, 36, 44, 48]. Ho man et al. [19] proposed metrics for explainable systems that are grounded in the subjective evaluation of a system (e.g., user satisfaction, trust, and understanding). ese may take the form of questionnaires on a itude and con dence in the system [18] and helpfulness of the system [5, 26]. However, while these measures are informative, evidence suggests they do not necessarily predict user\u2019s performance with the system. For example, Green & Chen [16] discovered that self-reported measures could be misleading, since participant\u2019s con dence in their performance was negatively associated with their actual performance. Similarly, Lai & Tan [28] found that humans cannot accurately estimate their own performance. More closely related to our ndings, PoursabziSangdeh et al. [34] observed that even though participants were signi cantly more con dent on the predictions of one model over the other, their decisions did not re ect the stated con dence. Furthermore, Lakkaraju & Bastani [30] demonstrated that participants trusted the same underlying biased model almost 10 times more when they were presented with misleading explanations compared to the truthful explanations that revealed the model\u2019s bias. ese ndings indicate that not only are subjective measures poor predictors of performance, but they can easily be manipulated and lead users to adhere to biased or malicious systems."
    },
    {
      "heading": "3 EXPERIMENTS",
      "text": "We conducted experiments with two di erent evaluation tasks and explanation designs to test the following hypotheses: H1: Results of widely accepted proxy tasks, where the user is asked to explicitly engage with the explanations, may not predict the results of realistic se ings where the user\u2019s focus is on the actual decision-making task.\nH2: Subjective measures, such as self-reported trust and preference with respect to di erent explanation designs, may not predict the ultimate human+AI performance."
    },
    {
      "heading": "3.1 Proxy Task",
      "text": "3.1.1 Task Description. We designed the task around nutrition because it is generally accessible and plausibly useful in explainable AI applications for a general audience. Participants were shown a series of 24 images of di erent plates of food. e ground truth of the percent fat content was also shown to them as a fact. Participants were then asked: \u201cWhat will the AI decide?\u201d given that the AI must decide \u201cIs X% or more of the nutrients on this plate fat?\u201d. As illustrated in Figure 1, each image was accompanied by explanations generated by the simulated AI. e participants chose which decision they thought the AI would make given the explanations and the ground truth.\nWe designed two types of explanations, eliciting either inductive or deductive reasoning. In inductive reasoning, one infers general pa ers from speci c observations. us, for the inductive explanations, we created example-based explanations that required participants to recognize the ingredients that contributed to fat content and draw their own conclusion about the given image. As shown in Figures 1a, the inductive explanations began with \u201cHere are examples of plates that the AI knows the fat content of and categorizes as similar to the one above.\u201d Participants then saw four additional images of plates of food. In deductive reasoning, in contrast, one starts with general rules and reaches a conclusion with respect to a speci c situation. us, for the deductive explanations, we provided the general rules that the simulated AI applied to generate its recommendations. For example, in Figure 1b, the deductive explanation begins with \u201cHere are ingredients the AI knows the fat content of and recognized as main nutrients:\u201d followed by a list of ingredients.\nWe chose a within-subjects study design, where for one half of the study session, participants saw inductive explanations and, for the other half of the study session, they saw deductive explanations. e order in which the two types of explanations were seen was counterbalanced. Each AI had an overall accuracy of 75%, which meant that in 25% of the cases the simulated AI misclassi ed the image or misrecognized ingredients (e.g., Figure 1b). e order of the speci c food images was randomized, but all participants encountered the AI errors at the same positions. We xed the errors at questions 4, 7, 11, 16, 22 and 23, though which food the error was associated to was randomized. We included the ground truth of the fat content of plates of food, because the main aim of the proxy task was to measure whether the user builds correct mental models of the AI and not to assess the actual nutrition expertise of the participant.\n3.1.2 Procedure. is study was conducted online, using Amazon Mechanical Turk. Participants were rst presented with brief information about the study and an informed consent form. Next, participants completed the main part of the study, in which they answered 24 nutrition-related questions, divided into two blocks of 12 questions. ey saw inductive explanations in one block and the deductive explanations in the other. e order of explanations was randomized across participants. Participants completed mid-study\nand end-study questionnaires so that they would provide a separate assessment for each of the two explanation types. ey were also asked to directly compare their experiences with the two simulated AIs in a questionnaire at the end of the study.\n3.1.3 Participants. We recruited 200 participants via Amazon Mechanical Turk (AMT). Participation was limited to adults in the US. Of the total 200 participants, 183 were retained for nal analyses, while 17 were excluded based on their answers to two commonsense questions included in the questionnaires (i.e., What color is the sky?). e study lasted 7 minutes on average. Each worker was paid 2 USD.\n3.1.4 Design and Analysis. is was a within-subjects design. e within-subjects factor was explanation type \u2014 inductive or deductive.\nWe collected the following measures: \u2022 Performance: Percentage of correct predictions of AI\u2019s\ndecisions \u2022 Appropriateness: Participants responded to the statement \u201c eAI based its decision on appropriate examples/ingredients.\u201d with either 0=No or 1=Yes (a er every question) \u2022 Trust: Participants responded to the statement \u201cI trust this\nAI to assess the fat content of food.\u201d on a 5-point Likert scale from 1=Strongly disagree to 5=Strongly agree (at the end of each block) \u2022 Mental demand: Participants answered the question \u201cHow mentally demanding was understanding how this AI makes decisions?\u201d on a 5-point Likert scale from 1=Very low to 5=Very high (every four questions)\n\u2022 Comparison between the two explanation types: Participants were asked at the end of the study to choose one AI over another on trust, preference, and mental demand.\nWe used repeated measures ANOVA for within-subjects analyses and the binomial test for the comparison questions."
    },
    {
      "heading": "3.2 Actual Decision-making Task",
      "text": "3.2.1 Task description. e actual decision-making task had a similar set up to the proxy task. Participants were shown the same series of 24 images of di erent plates of food, but were asked their own decision whether the percent fat content of nutrients on the plate is higher than a certain percentage. As illustrated in Figure 2, each image was accompanied by an answer recommended by a simulated AI, and an explanation provided by that AI. We introduced two more conditions to serve as baselines in the actual decision-making task depicted in Figure 3.\nere were three between-subjects conditions in this study: 1. the no-AI baseline (where no recommendations or explanations were provided), 2. the no-explanation baseline (where a recommendation was provided by a simulated AI, but no explanation was given), and 3. the main condition in which both recommendations and explanations were provided. In this last condition, two within-subjects sub-conditions were present: for one half of the study participants saw inductive explanations and for the other they saw deductive explanations. e order in which the two types of explanations were seen was counterbalanced. In the no-AI baseline, participants were not asked any of the questions relating to the performance of the AI.\ne explanations in this task di ered only slightly from the explanations in the proxy task, because they indicated the AI\u2019s recommendation. Inductive explanations started with: \u201cHere are examples of plates that the AI categorizes as similar to the one above and do (not) have X% or more fat.\u201d followed by four examples of images. Similarly, deductive explanations stated: \u201cHere are ingredients the AI recognized as main nutrients which do (not) make up X% or more fat on this plate:\u201d followed by a list of ingredients.\n3.2.2 Procedure. e procedure was the same as for the proxy task. e study was conducted online, using the Amazon Mechanical Turk. Participants were rst presented with a brief information about the study and an informed consent form. Next, participants\ncompleted the main part of the study, in which they answered 24 nutrition-related questions, divided into two blocks of 12 questions.\nAll participants also completed a questionnaire at the end of the study, providing subjective assessments of the system they interacted with. Participants who were presented with AI-generated recommendations accompanied by explanations also completed a mid-study questionnaire (so that they would provide separate assessment for each of the two explanation types) and they were also asked to directly compare their experiences with the two simulated AIs at the end of the study.\n3.2.3 Participants. We recruited 113 participants via Amazon Mechanical Turk (AMT). Participation was limited to adults in the US. Of the total 113 participants, 102 were retained for nal analyses, while 11 were excluded based on their answers to two common-sense questions included in the pre-activity and postactivity questionnaires (i.e., \u201cWhat color is the sky?\u201d ). e task lasted 10 minutes on average. Each worker was paid 5 USD per task.\n3.2.4 Design and Analysis. is was a mixed between- and within-subjects design. As stated before, the three between-subjects conditions were: 1. the no-AI baseline; 2. the no-explanation baseline, in which the AI-generated recommendations were provided but no explanations; 3. the main condition, in which both the AIgenerated recommendations and explanations were provided. e\nwithin-subjects factor was explanation type (inductive or deductive) and it was applied only for participants who were presented with AI-generated recommendations with explanations.\nWe collected the following measures: \u2022 Performance: Percentage of correct answers (overall for\neach AI, and speci cally for questions when AI presented incorrect explanations) \u2022 Understanding: Participants responded to the statement \u201cI understand how the AI made this recommendation.\u201d on a 5- point Likert scale from 1=Strongly disagree to 5=Strongly agree (a er every question) \u2022 Trust: Participants responded to the statement \u201cI trust this AI to assess the fat content of food.\u201d on a 5-point Likert scale from 1=Strongly disagree to 5=Strongly agree (every four questions) \u2022 Helpfulness: Participants responded to the statement \u201c is AI helped me assess the percent fat content.\u201d on a 5-point Likert scale from 1=Strongly disagree to 5=Strongly agree (at the end of each block) \u2022 Comparison between the two explanation types: Partic-\nipants were asked at the end of the study to choose one AI over another on trust, preference, understanding and helpfulness.\nWe used analysis of variance (ANOVA) for between-subjects analyses and repeated measures ANOVA for within-subjects analyses. We used the binomial test for the comparison questions."
    },
    {
      "heading": "4 RESULTS",
      "text": ""
    },
    {
      "heading": "4.1 Proxy Task Results",
      "text": "e explanation type had a signi cant e ect on participants\u2019 trust and preference in the system. Participants trusted the AI more when presented with inductive explanations (M = 3.55), rather than deductive explanations (M = 3.40, F1,182 = 5.37,p = .02). Asked to compare the two AIs, most of the participants stated they trusted more the inductive AI (58%,p = .04). When asked the hypothetical question: \u201cIf you were asked to evaluate fat content of plates of food, which AI would you prefer to interact with more?\u201d, again most of the participants (62%) chose the inductive AI over the deductive AI (p = .001).\ne inductive AI was also rated signi cantly higher (M = 0.83) than the deductive AI (M = 0.79) in terms of the appropriateness of examples (ingredients for the deductive condition) on which the AI based its decision (F (1, 182) = 13.68,p = 0.0003). When the AI presented incorrect examples/ingredients, there was no signi cant di erence among the inductive (M = 0.47) and deductive (M = 0.50) conditions (F (1, 182) = 1.02,p = .31,n.s .).\nWe observed no signi cant di erence in overall performance when participants were presented with inductive (M = 0.64) or deductive explanations (M = 0.64, F (1, 182) = 0.0009,n.s .). When either AI presented incorrect explanations, although the average performance dropped for both inductive (M = 0.40) and deductive (M = 0.41) conditions, there was also no signi cant di erence among them (F (1, 182) = .03,n.s .).\nIn terms of mental demand, there was a signi cant e ect of the explanation type. Participants rated the deductive AI (M = 2.94) as more mentally demanding than the inductive AI (M = 2.79,\nF (1, 182) = 7.75,p = .0006). e e ect was noticed also when they were asked: \u201cWhich AI required more thinking while choosing which decision it would make?\u201d, with 61% of participants choosing deductive over inductive (p = .005)."
    },
    {
      "heading": "4.2 Actual Decision-making Task Results",
      "text": "18 participants were randomized into the no-AI condition, 19 into the AI with no explanation condition, and 65 were presented with AI recommendations supported by explanations.\nWe observed a signi cant main e ect of the presence of explanations on participants\u2019 trust in the AI\u2019s ability to assess the fat content of food. Participants who saw either kind of explanation, trusted the AI more (M = 3.56) than those who received AI recommendations, but no explanations (M = 3.17, F1,483 = 11.28,p = .0008). Further, there was a signi cant main e ect of the explanation type on participants\u2019 trust: participants trusted the AI when they received deductive explanations more (M = 3.68) than when they received inductive explanations (M = 3.44, F1,64 = 5.96,p = .01). When asked which of the two AIs they trusted more, most participants (65%) said that they trusted the AI that provided deductive explanations more than the one that provided inductive explanations (p = .02).\nParticipants also found the AI signi cantly more helpful when explanations were present (M = 3.78) than when no explanations were o ered (M = 3.26, F1,147 = 4.88,p = .03). Further, participants reported that they found deductive explanations more helpful (M = 3.92) than inductive ones (M = 3.65) and this di erence was marginally signi cant (F1,64 = 3.66,p = .06). When asked which of the two AIs they found more helpful, most participants (68%) chose the AI that provided deductive explanations (p = .006).\nParticipants also reported that they understood how the AI made its recommendations be er when explanations were present (M = 3.84) than when no explanations were provided (M = 3.67, F1,2014 = 6.89p = .009). ere was no di erence in the perceived level of understanding between the two explanation types (F1,64 = 0.44,p = .51).\nAsked about their overall preference, most participants (63%) preferred the AI that provided deductive explanations over the AI that provided inductive explanations (p = .05).\nIn terms of actual performance on the task, participants who received AI recommendations (with or without explanations) provided a signi cantly larger fraction of accurate answers (M = 0.72) than those who did not receive AI recommendations (M = 0.46, F1,2446 = 118.07,p < .0001). Explanations further improved overall performance: participants who saw explanations of AI recommendations had a signi cantly higher proportion of correct answers (M = 0.74) than participants who did not receive explanations of AI recommendations (M = 0.68, F1,2014 = 5.10,p = .02) (depicted in Figure 4a). ere was no signi cant di erence between the two explanation types in terms of overall performance (F1,64 = 0.44,n.s .). However, we observed a signi cant interaction between explanation type and the correctness of AI recommendations (F2,2013 = 15.03p < .0001). When the AI made correct recommendations, participants performed similarly when they saw inductive (M = .78) and deductive (M = .81) explanations (F1,64 = 1.13,n.s .). When the AI made incorrect recommendations,\nhowever, participants were signi cantly more accurate when they saw inductive (M = 0.63) than deductive (M = 0.48) explanation (F1,64 = 7.02,p = .01) (depicted in Figure 4b).\nTo ensure the results of our studies were not random, we replicated both experiments with almost identical setup and obtained the same main results (in terms of signi cance) reported in this section."
    },
    {
      "heading": "5 QUALITATIVE STUDY",
      "text": "rough the qualitative study, we explored the user reasoning and sought to gain insight into the discrepancy between subjective measures and performance. We asked participants to think aloud during an in-person study in order to understand how and why people perceive AI the way they do, in addition to what factors go into making decisions when assisted by an AI."
    },
    {
      "heading": "5.1 Task",
      "text": "e same task design was used in this study as in the actual decisionmaking task, except that all participants were presented with the main condition (where both recommendations and explanations were provided). As in the actual decision-making task, each participant saw both inductive and deductive explanations."
    },
    {
      "heading": "5.2 Procedure",
      "text": "Upon arriving to the lab, participants were presented with an informed consent form, including agreeing to being screen- and audiorecorded, and instructions on the task. A erwards, the steps in this study were similar to those in the actual decision-making task, except that we added the think-aloud method [11]: as participants completed the task, they were asked to verbalize their thought process as they made each decision. At the end of the task, there\nwas a semi-structured interview, during which participants brie y discussed how they believed the two AIs were making their recommendations and why they did or did not trust them. Participants also discussed if and why they preferred one AI over the other."
    },
    {
      "heading": "5.3 Participants",
      "text": "We recruited 11 participants via community-wide emailing lists (8 female, 3 male, age range 23\u201329, M = 24.86, SD = 2.11). Participants were primarily graduate students with backgrounds from design, biomedical engineering, and education. Participants had varying levels of experience with AI and machine learning, ranging from 0\u20135 years of experience."
    },
    {
      "heading": "5.4 Design and Analysis",
      "text": "We transcribed the think-aloud comments and the post-task interviews. Transcripts were coded and analyzed for pa erns using an inductive approach [39]. We focused on comments about (1) how the AI made its recommendations; (2) trust in the AI; (3) erroneous recommendations; (4) why people preferred one explanation type over the other. From a careful reading of the transcripts, we discuss some of the themes and trends that emerged from the data."
    },
    {
      "heading": "5.5 Results",
      "text": "Preference of one explanation type over another. Eight out of the 11 participants preferred the inductive explanations. Participants who preferred inductive explanations perceived the four images as data. One participant stated that \u201cBecause [the AI] showed similar pictures, I knew that it had data backing it up\u201d (P3). On the other hand, participants who preferred deductive explanations perceived the listing of ingredients to be reliable, and that \u201cif the AI recognized that it\u2019s steak, then I would think, Oh the AI knows more\nabout steak fat than I do, so I\u2019m going to trust that since it identi ed the object correctly.\u201d (P6).\nIn our observations, we found that the way participants used the explanations was di erent depending on the explanation type. With inductive explanations, one participant o en rst made their own judgement before looking at the recommendation, and then used the recommendation to con rm their own judgement. In a cake example, one participant said, \u201cSo I feel it probably does have more than 30% because it\u2019s cake, and that\u2019s cream cheese. But these are all similar to that, and the AI also says that it does have more than 30% fat, so I agree\u201d (P2). With deductive explanations, participants evaluated the explanations and recommendation more before making any decision. In the same cake example, a di erent participant said, \u201c ere are [the AI recognizes] nuts, cream cheese, and cake. at seems to make sense. Nuts are high in fat, so is dairy, so I agree with that.\u201d (P6).\nCognitive Demand. At the end of the study, participants were asked which AI was easier to understand. Ten out of 11 participants felt the inductive explanations were easier to understand than the deductive explanations. Several participants stated that the deductive explanations forced them to think more, and that generally they spent more time making a decision with deductive explanations. One participant said, for example, \u201cI feel like with this one I have to think a bit more and and rely on my own experiences with food to see or understand to gauge what\u2019s fa y.\u201d (P2).\nErrors and Over-reliance. Nine out of 11 participants claimed to trust the inductive explanations more. We intentionally introduced erroneous recommendations because we expected participants to utilize them to calibrate their mental model of the AI. When participants understood the error and believed the error was reasonable for an AI to make, they expressed less distrust in subsequent questions. However, when participants perceived the error to be inconsistent with other errors, their trust in subsequent recommendations was hurt much more. For example, one participant stated, \u201cI think the AI makes the recommendation based on shape and color. But in some other dessert examples, it was able to identify the dessert as a dessert. So I wasn\u2019t sure why it was so di cult to understand this particular item\u201d (P5).\nWe found that there was also some observable correlation between explanation type and trust. Many participants claimed it was easier to identify errors from the inductive explanations, yet agreed with erroneous recommendations from inductive explanations more. In some of those instances, participants either did not realize the main food image was di erent from the other four or felt the main food image was similar enough though not exact. Lastly, one participant stated the inductive explanations were easier to understand because \u201cyou can visually see exactly why it would come to its decision,\u201d, but for deductive explanations \u201cyou can see what it\u2019s detecting but not why\u201d (P8), and yet this participant also stated that the deductive explanations seemed more trustworthy.\nImpact of the ink-Aloud method on participant behavior. In this study, we asked participants to perform the actual decision-making task and we expected to observe similar results to those obtained in the previous experiment when using the actual tasks. Yet, in this study, 8 out of the 11 participants preferred the inductive explanations and 10 out of 11 participants felt the inductive explanations were easier to understand than the deductive explanations. ese results are comparable to the results we obtained in the previous experiment when we used the proxy task rather than the actual task.\nWe believe that the use of the think-aloud method may have impacted participants\u2019 behavior in this study. Speci cally, because participants were instructed to verbalize their thoughts, they were more likely to engage in analytical thinking when considering the AI recommendations and explanations than they were in the previous experiment with the actual tasks, where their focus was primarily on making decisions.\nIt is possible that while the think-aloud method is part of standard research practice for evaluating interfaces, it is itself a form of cognitive forcing intervention [7], which impacts how people perform on cognitively-demanding tasks such as interacting with an explainable AI system on decision-making tasks. e act of talking about the explanations led participants to devote more of\ntheir a ention and cognition to the explanations, and thus made them behave more similarly to participants in working with the proxy task rather than those working with the actual task."
    },
    {
      "heading": "6 DISCUSSION",
      "text": "In this study, we investigated two hypotheses regarding the evaluation of AI-powered explainable systems:\n\u2022 H1: Results of widely accepted proxy tasks, where the user is asked to explicitly engage with the explanations, may not predict the results of realistic se ings where the user\u2019s focus is on the actual decision-making task. \u2022 H2: Subjective measures, such as self-reported trust and preference with respect to di erent explanation designs, may not predict the ultimate human+AI performance.\nWe examined these hypotheses in the context of a nutrition-related decision-making task, by designing two distinct evaluation tasks and two distinct explanation designs. e rst task was a proxy task, where the users had to simulate the AI\u2019s decision by examining the explanations. e second task was the more realistic, actual decision-making task, where the user had to make their own decisions about the nutritional content of meals assisted by AI-generated recommendations and explanations. Each of the tasks had two parts, where users interacted with substantially di erent explanation styles\u2014inductive and deductive.\nIn the experiment with the proxy task, participants preferred and trusted the AI that used inductive explanations signi cantly more. ey also reported that the AI that used inductive explanations based its decision on more accurate examples on average than the AI that used deductive explanations. When asked \u201cIf you were asked to evaluate fat content of plates of food, which AI would you prefer to interact with more?\u201d ), the majority of participants chose the AI that provided inductive explanations.\nIn contrast with the proxy task experiment, in the experiment with the actual decision-making task, participants rated the AI with deductive explanations as their preferred AI, and viewed it as more trustworthy and more helpful compared to the AI that used inductive explanations.\ne contrast in terms of performance measures was less pronounced. When a empting proxy tasks, participants demonstrated nearly identical accuracy regardless of explanation type. However, when a empting actual decision-making tasks and the AI provided an incorrect recommendation, participants ignored that incorrect recommendation and provided the correct answer signi - cantly more o en when they had access to inductive, not deductive, explanations for the AI\u2019s recommendation.\nese contradictory results produced by the two experiments indicate that results of evaluations that use proxy tasks may not correspond to results on actual tasks, thus supporting H1. is may be because in the proxy task the users cannot complete the task without engaging analytically with the explanations. Whereas, in the actual decision-making task, the user\u2019s primary goal is to make the most accurate decisions about the nutritional content of meals; she chooses whether and how deeply she engages with the AI\u2019s recommendations and explanations.\nis nding has implications for the explainable AI community, as there is a current trend to use proxy tasks to evaluate user mental\nmodels of the AI-powered systems, with the implicit assumption that the results will translate to the realistic se ings where users make decisions about an actual task while assisted by an AI.\nWe tested H2 on the actual decision-making task. e results show that participants preferred, trusted and found the AI with deductive explanations more helpful than the AI that used inductive explanations. Yet, they performed signi cantly be er with the AI that used inductive explanations when the AI made erroneous recommendations. erefore, H2 is also supported. is nding suggests that the design decisions for explainable interfaces should not be made by relying solely on user experience and subjective measures. Subjective measures of trust and preference are, of course, valuable and informative, but they should be used to complement rather than replace performance measures.\nOur research demonstrated that results from studies that use proxy tasks may not predict results from studies that use realistic tasks. Our results also demonstrated that user preference may not predict their performance. However, we recognize that evaluating novel AI advances through human subjects experiments that involve realistic tasks is expensive in terms of time and resources, and may negatively impact the pace of innovation in the eld. erefore, future research needs to uncover why these di erences exist so that we can develop low burden evaluation techniques that correctly predict the outcomes of deploying a system in a realistic se ing.\nWe believe that the reason why explainable AI systems are sensitive to the di erence between proxy task and actual task evaluation designs is that di erent AI explanation strategies require di erent kinds and amounts of cognition from the users (like our inductive and deductive explanations). However, people are reluctant to exert cognitive e ort [24, 25] unless they are motivated or forced to do so. ey also make substantially di erent decisions depending on whether they choose to exert cognitive e ort or not [12, 37]. In actual decision-making situations, people o en choose not to engage in e ortful analytical thinking, even in high-stakes situations like medical diagnosis [31]. Meanwhile, proxy tasks force participants to explicitly pay a ention to the behavior of the AI and the explanations produced. us, results observed when participants interact with proxy tasks do not accurately predict people\u2019s behavior in many realistic se ings. In our study, participants who interacted with the proxy task felt that the deductive explanations required signi cantly more thinking than the inductive explanations. erefore, in the proxy task where the participants were obliged to exert cognitive e ort to evaluate the explanations, they said they preferred and trusted the less cognitively demanding explanations more, the inductive explanations. In contrast, in the actual task the participants could complete the task even without engaging with the explanations. us, we suspect that in the deductive condition participants perceived the explanations as too mentally demanding, and chose to over-rely on the AI\u2019s recommendation, just to avoid cognitive e ort of examining those explanations. ey also might have perceived the AI that provided deductive explanations as more competent because it required more thinking.\nOne implication of our analysis is that the e ectiveness of explainable AI systems can be substantially impacted by the design of the interaction (rather than just the algorithms or explanations). For example, a recent study showed that a simple cognitive forcing strategy (having participants make their own preliminary decision\nbefore being shown the AI\u2019s decision) resulted in much higher accuracy of the nal decisions made by human+AI teams than any strategy that did not involve cognitive forcing [17].\nInadvertently, we uncovered an additional potential pitfall for evaluating explainable AI systems. As the results of our qualitative study demonstrated, the use of the think-aloud method\u2014a standard technique for evaluating interactive systems\u2014can also substantially impact how participants allocate their mental e ort. Because participants were asked to think aloud, we suspect that they exerted additional cognitive e ort to engage with the explanations and analyze their reasoning behind their decisions.\nTogether, these results indicate that cognitive e ort is an important aspect of explanation design and its evaluation. Explanations high in cognitive demand might be ignored by the users while simple explanations might not convey the appropriate amount of evidence that is needed to make informed decisions. At the same time, traditional methods of probing users\u2019 minds while using explainable interfaces should also be re-evaluated. By taking into account the cognitive e ort and cognitive processes that are employed during the evaluation of the explanations, we might generate explainable interfaces that optimize the performance of the sociotechnical (human+AI) system as a whole. Such interfaces would instill trust, and make the user aware of the system\u2019s errors."
    },
    {
      "heading": "7 CONCLUSION",
      "text": "To achieve the aspiration of human+AI teams that complement oneanother and perform be er than either the human or the AI alone, researchers need to be cautious about their pragmatic decisions. In this study, through online experiments and an in-person study, we showed how several assumptions researchers make about the evaluation of the explainable AI systems for decision-making tasks could lead to misleading results.\nFirst, choosing proxy tasks for the evaluation of explainable AI systems shi s the user\u2019s focus toward the AI, so the obtained results might not correspond to results of the user completing the actual decision-making task while assisted by the AI. In fact, our results indicate that users trust and prefer one explanation design (i.e. inductive) more in the proxy task, while they trust and prefer the other explanation design (i.e. deductive) more in the actual decision-making task.\nSecond, the subjective evaluation of explainable systems with measures such as trust and preference may not correspond to the ultimate user performance with the system. We found that people trusted and preferred the AI with deductive explanations more, but recognized AI errors be er with the inductive explanations.\nLastly, our results suggest that think-aloud studies may not convey how people make decisions with explainable systems in realistic se ings. e results from the think-aloud in-person study, which used the actual task design, aligned more with the results we obtained in the proxy task.\nese ndings suggest that to draw correct conclusions about their experiments, explainable AI researchers should be wary of the explainable systems\u2019 evaluation pitfalls and design their evaluation accordingly. Particularly, the correct and holistic evaluation of explainable AI interfaces as sociotechnical systems is of paramount\nimportance, as they are increasingly being deployed in critical decision-making domains with grave repercussions.\nAcknowledgements. We would like to thank Tianyi Zhang and Isaac Lage for helpful feedback."
    }
  ],
  "title": "Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems",
  "year": 2020
}
