{
  "abstractText": "Machine learning models deployed in real-world applications are often evaluated with precision-based metrics such as F1-score or AUC-PR (Area Under the Curve of Precision Recall). Heavily dependent on the class prior, such metrics make it difficult to interpret the variation of a model\u2019s performance over different subpopulations/subperiods in a dataset. In this paper, we propose a way to calibrate the metrics so that they can be made invariant to the prior. We conduct a large number of experiments on balanced and imbalanced data to assess the behavior of calibrated metrics and show that they improve interpretability and provide a better control over what is really measured. We describe specific real-world use-cases where calibration is beneficial such as, for instance, model monitoring in production, reporting, or fairness evaluation.",
  "authors": [
    {
      "affiliations": [],
      "name": "Wissam Siblini"
    },
    {
      "affiliations": [],
      "name": "Jordan Frry"
    },
    {
      "affiliations": [],
      "name": "Liyun He-Guelton"
    },
    {
      "affiliations": [],
      "name": "Yi-Qing Wang"
    }
  ],
  "id": "SP:28c83d0f933f9f5796fe67e3a5be0c2579c9f07c",
  "references": [
    {
      "authors": [
        "S. Barocas",
        "M. Hardt",
        "A. Narayanan"
      ],
      "title": "Fairness in machine learning",
      "venue": "NIPS Tutorial",
      "year": 2017
    },
    {
      "authors": [
        "P. Branco",
        "L. Torgo",
        "R.P. Ribeiro"
      ],
      "title": "A survey of predictive modeling on imbalanced domains",
      "venue": "ACM Computing Surveys (CSUR) 49(2), 31",
      "year": 2016
    },
    {
      "authors": [
        "D. Brzezinski",
        "J. Stefanowski",
        "R. Susmaga",
        "I. Szczech"
      ],
      "title": "On the dynamics of classification measures for imbalanced and streaming data",
      "venue": "IEEE transactions on neural networks and learning systems",
      "year": 2019
    },
    {
      "authors": [
        "A. Dal Pozzolo",
        "G. Boracchi",
        "O. Caelen",
        "C. Alippi",
        "G. Bontempi"
      ],
      "title": "Credit card fraud detection: a realistic modeling and a novel learning strategy",
      "venue": "IEEE transactions on neural networks and learning systems 29(8), 3784\u20133797",
      "year": 2018
    },
    {
      "authors": [
        "J. Davis",
        "M. Goadrich"
      ],
      "title": "The relationship between precision-recall and roc curves",
      "venue": "Proceedings of the 23rd international conference on Machine learning. pp. 233\u2013 240. ACM",
      "year": 2006
    },
    {
      "authors": [
        "T. Fawcett"
      ],
      "title": "An introduction to roc analysis",
      "venue": "Pattern recognition letters 27(8), 861\u2013874",
      "year": 2006
    },
    {
      "authors": [
        "P. Flach",
        "M. Kull"
      ],
      "title": "Precision-recall-gain curves: Pr analysis done right",
      "venue": "Advances in Neural Information Processing Systems. pp. 838\u2013846",
      "year": 2015
    },
    {
      "authors": [
        "V. Garc\u0131a",
        "J.S. S\u00e1nchez",
        "R.A. Mollineda"
      ],
      "title": "On the suitability of numerical performance measures for class imbalance problems",
      "venue": "International Conference in Pattern Recognition Applications and Methods. pp. 310\u2013313",
      "year": 2012
    },
    {
      "authors": [
        "J.A. Hanley",
        "B.J. McNeil"
      ],
      "title": "The meaning and use of the area under a receiver operating characteristic (roc) curve",
      "venue": "Radiology 143(1), 29\u201336",
      "year": 1982
    },
    {
      "authors": [
        "L.A. Jeni",
        "J.F. Cohn",
        "F. De La Torre"
      ],
      "title": "Facing imbalanced data\u2013recommendations for the use of performance metrics",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. pp. 245\u2013251. IEEE",
      "year": 2013
    },
    {
      "authors": [
        "J. Neyman",
        "E.S. Pearson"
      ],
      "title": "Ix",
      "venue": "on the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 231(694-706), 289\u2013 337",
      "year": 1933
    },
    {
      "authors": [
        "T. Saito",
        "M. Rehmsmeier"
      ],
      "title": "The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets",
      "venue": "PloS one 10(3), e0118432",
      "year": 2015
    },
    {
      "authors": [
        "M.S. Sajjadi",
        "O. Bachem",
        "M. Lucic",
        "O. Bousquet",
        "S. Gelly"
      ],
      "title": "Assessing generative models via precision and recall",
      "venue": "Advances in Neural Information Processing Systems. pp. 5228\u20135237",
      "year": 2018
    },
    {
      "authors": [
        "G. Santafe",
        "I. Inza",
        "J.A. Lozano"
      ],
      "title": "Dealing with the evaluation of supervised classification algorithms",
      "venue": "Artificial Intelligence Review 44(4), 467\u2013508",
      "year": 2015
    },
    {
      "authors": [
        "N. Tatbul",
        "T.J. Lee",
        "S. Zdonik",
        "M. Alam",
        "J. Gottschlich"
      ],
      "title": "Precision and recall for time series",
      "venue": "Advances in Neural Information Processing Systems. pp. 1920\u20131930",
      "year": 2018
    },
    {
      "authors": [
        "J. Vanschoren",
        "J.N. Van Rijn",
        "B. Bischl",
        "L. Torgo"
      ],
      "title": "Openml: networked science in machine learning",
      "venue": "ACM SIGKDD Explorations Newsletter 15(2), 49\u201360",
      "year": 2014
    },
    {
      "authors": [
        "G. Widmer",
        "M. Kubat"
      ],
      "title": "Effective learning in dynamic environments by explicit context tracking",
      "venue": "European Conference on Machine Learning. pp. 227\u2013243. Springer",
      "year": 1993
    }
  ],
  "sections": [
    {
      "text": "Keywords: Performance Metrics \u00b7 Class Imbalance \u00b7 Precision-Recall"
    },
    {
      "heading": "1 Introduction",
      "text": "In real-world machine learning systems, the predictive performance of a model is often evaluated on multiple datasets, and comparisons are made. These datasets can correspond to sub-populations in the data, or different periods in time [15]. Choosing the best suited metrics is not a trivial task. Some metrics may prevent a proper interpretation of the performance differences between the sets [8,14], especially because different datasets generally not only have a different likelihood P(x|y) but also a different class prior P(y). A metric dependent on the prior (e.g. precision) will be affected by both differences indiscernibly [3] but a practitioner could be interested in isolating the variation of performance due to likelihood which reflects the intrinsic model\u2019s performance (see illustration in Figure 1). Take the example of comparing the performance of a model across time periods: At time t, we receive data drawn from Pt(x, y) = Pt(x|y)Pt(y) where x are the features and y the label. Hence the optimal scoring function (i.e. model) for this dataset is the likelihood ratio [11]:\nst(x) := Pt(x|y = 1) Pt(x|y = 0)\n(1)\nIn particular, if Pt(x|y) does not vary with time, neither will st(x). In this case, even if the prior Pt(y) varies, it is desirable to have a performance metric M(\u00b7)\nar X\niv :1\n90 9.\n02 82\n7v 2\n[ cs\n.L G\n] 2\n8 A\npr 2\n02 0\nsatisfying M(st,Pt) = M(st+1,Pt+1),\u2200t so that the model maintains the same metric value over time. That being said, this does not mean that dependence to prior is an intrinsically bad behavior. Some applications seek this property as it reflects a part of the difficulty to classify on a given dataset (e.g. the performance of the random classifier evaluated with a prior-dependent metric is more or less high depending on the skew of the dataset).\nIn binary classification, researchers often rely on the AUC-ROC (Area Under the Curve of Receiver Operating Characteristic) to measure a classifier\u2019s performance [9,6]. While this metric has the advantage of being invariant to the class prior, many real-world applications, especially when data are imbalanced, have recently begun to favor precision-based metrics such as AUC-PR and FScore[12][13]. The reason is that AUC-ROC suffers from giving false positives too little importance [5] although the latter strongly deteriorate user experience and waste human efforts with false alerts. Indeed AUC-ROC considers a tradeoff between TPR and FPR whereas AUC-PR/F1-score consider a tradeoff between TPR (Recall) and Precision. With a closer look, the difference boils down to the fact that it normalizes the number of false positives with respect to the number of true negatives whereas precision-based metrics normalize it with respect to the number of true positives. In highly imbalanced scenarios (e.g. fraud/disease detection), the first is much more likely than the second because negative examples are in large majority.\nPrecision-based metrics give false positives more importance, but they are tied to the class prior [2,3]. A new definition of precision and recall into precision gain and recall gain has been recently proposed to correct several drawbacks of AUC-PR [7]. But, while the resulting AUC-PR Gain has some advantages of the AUC-ROC such as the validity of linear interpolation between points, it remains dependent on the class prior. Our study aims at providing metrics (i) that are precision-based to tackle problems where the class of interest is highly under-represented and (ii) that can be made independent of the prior for comparison purposes (e.g. monitoring the evolution of the performance of a classifier accross several time periods). To reach this objective, this paper\nprovides: (1) A formulation of calibration for precision-based metrics. It compute the value of precision as if the ratio \u03c0 of the test set was equal to a reference class ratio \u03c00. We give theoretical arguments to explain why it allows invariance to the class prior. We also provide a calibrated version for precision gain and recall gain [7]. (2) An empirical analysis on both synthetic and real-world data to confirm our claims and show that new metrics are still able to assess the model\u2019s performance and are easier to interpret. (3) A large scale experiments on 614 datasets using openML [16] to (a) give more insights on correlations between popular metrics by analyzing how they rank models, (b) explore the links between the calibrated metrics and the regular ones.\nNot only calibration solves the issue of dependence to the prior but also allows, with parameter \u03c00, anticipating a different ratio and controlling what the metric precisely reflects. This new property has several practical interests (e.g. for development, reporting, analysis) and we discuss them in realistic usecases in section 5."
    },
    {
      "heading": "2 Popular Metrics for Binary Classification: Advantages and Limits",
      "text": "We consider a usual binary classification setting where a model has been trained and its performance is evaluated on a test dataset of N instances. yi \u2208 {0, 1} is the ground-truth label of the ith instance and is equal to 1 (resp. 0) if the instance belongs to the positive (resp. negative) class. The model provides si \u2208 R, a score for the ith instance to belong to the positive class. For a given threshold \u03c4 \u2208 R, the predicted label is y\u0302i = 1 if si > \u03c4 and 0 otherwise. Predictive performance is generally measured using the number of true positives (TP = \u2211N i=1 1(y\u0302i =\n1, yi = 1)), true negatives (TN = \u2211N i=1 1(y\u0302i = 0, yi = 0)), false positives (FP =\u2211N\ni=1 1(y\u0302i = 1, yi = 0)), false negatives (FN = \u2211N i=1 1(y\u0302i = 0, yi = 1)). One can compute relevant ratios such as the True Positive Rate (TPR) also referred to as the Recall (Rec = TPTP+FN ), the False Positive Rate (FPR = FP TN+FP ) also referred to as the Fall-out and the Precision (Prec = TPTP+FP ). As these ratios are biased towards a specific type of error and can easily be manipulated with the threshold, more complex metrics have been proposed. In this paper, we discuss the most popular ones which have been widely adopted in binary classification: F1-Score, AUC-ROC, AUC-PR and AUC-PR Gain. F1-Score is the harmonic average between Prec and Rec:\nF1 = 2 \u2217 Prec \u2217Rec Prec+Rec . (2)\nThe three other metrics consider every threshold \u03c4 from the highest si to the lowest. For each one, they compute TP, FP, TN and FN. Then, they plot one ratio against another and compute the Area Under the Curve (Figure 2). AUC-ROC considers the Receiver Operating Characteristic curve where TPR is plotted against FPR. AUC-PR considers the Precision vs Recall curve. Finally,\nin AUC-PR Gain, the precision gain (PrecG) is plotted against the recall gain (RecG). They are defined in [7] as follows (\u03c0 = \u2211N i=1 yi N is the positive class ratio and we always consider that it is the minority class in this paper):\nPR Gain enjoys many properties of the ROC that the regular PR analysis does not (e.g. the validity of linear interpolations or the existence of universal baselines) [7]. However, AUC-PR Gain becomes hardly usable in extremely imbalanced settings. In particular, we can derive from (3) and (4) that PrecG/RecG will be mostly close to 1 if \u03c0 is close to 0 (see top right chart in Figure 2).\nAs explained in the introduction, precision-based metrics (F1, AUC-PR) are more adapted than AUC-ROC for problems with class imbalance. On the other hand, only AUC-ROC is invariant to the positive class ratio. Indeed, FPR and Rec are both unrelated to the class ratio because they only focus on one class but it is not the case for Prec. Its dependency on the positive class ratio \u03c0 is illustrated in Figure 3: when comparing a case (i) with a given ratio \u03c0 and another case (ii) where a randomly selected half of the positive examples has been removed, one can visually understand that both recall and false positive rate are the same but the precision is lower in the second case."
    },
    {
      "heading": "3 Calibrated Metrics",
      "text": "We seek a metric that is based on Prec to tackle problems where data are imbalanced and the minority (positive) class is the one of interest but we want it to be invariant w.r.t the class prior to be able to interpret its variation across different datasets (e.g. different time periods). To obtain such a metric, we will modify those based on Prec (AUC-PR, F1-Score and AUC-PR Gain) to make them independent of the positive class ratio \u03c0."
    },
    {
      "heading": "3.1 Calibration",
      "text": "The idea is to fix a reference ratio \u03c00 and to weigh the count of TP or FP in order to calibrate them to the value that they would have if \u03c0 was equal to \u03c00. \u03c00 can be chosen arbitrarily (e.g. 0.5 for balanced) but it is preferable to fix it according to the task at hand (we analyze the impact of \u03c00 in section 4 and describe simple guidelines to fix it in section 5).\nIf the positive class ratio is \u03c00 instead of \u03c0, the ratio between negative examples and positive examples is multiplied by \u03c0(1\u2212\u03c00)\u03c00(1\u2212\u03c0) . In this case, we expect the ratio between false positives and true positives to be multiplied by \u03c0(1\u2212\u03c00)\u03c00(1\u2212\u03c0) . Therefore, we define the calibrated precision Precc as follows:\nPrecc = TP\nTP + \u03c0(1\u2212\u03c00)\u03c00(1\u2212\u03c0)FP =\n1\n1 + \u03c0(1\u2212\u03c00)\u03c00(1\u2212\u03c0) FP TP\n(5)\nSince 1\u2212\u03c0\u03c0 is the imbalance ratio N\u2212 N+ where N+ (resp. N\u2212) is the number of\npositive (resp. negative) examples, we have: \u03c01\u2212\u03c0 FP TP = FP/N\u2212 TP/N+ = FPRTPR which is independent of \u03c0. Based on the calibrated precision, we can also define the calibrated F1-score, the calibrated PrecG and the calibrated RecG by replacing Prec by Precc and \u03c0 by \u03c00 in equations (2), (3) and (4). Note that calibration does not change precision gain. Indeed, calibrated precision gain Precc\u2212\u03c00(1\u2212\u03c00)Precc can be rewritten as Prec\u2212\u03c0(1\u2212\u03c0)Prec which is equal to the regular precision gain. Also, the interesting properties of the recall gain were proved independently of the ratio \u03c0 in [7] which means that calibration preserves them."
    },
    {
      "heading": "3.2 Robustness to Variations in \u03c0",
      "text": "In order to evaluate the robustness of the new metrics to variations in \u03c0, we create a synthetic dataset where the label is drawn from a Bernoulli distribution with parameter \u03c0 and the feature is drawn from Normal distributions:\np(x|y = 1;\u00b51) = N (x;\u00b51, 1), p(x|y = 0;\u00b50) = N (x;\u00b50, 1) (6)\nFor several values of \u03c0, data points are generated from (6) with \u00b51 = 2 and \u00b50 = 1.8. We consider a large number of points (10\n6) so that the empirical class ratio \u03c0 is approximately equal to the Bernouilli parameter \u03c0. We empirically study the evolution of several metrics (F1-score, AUC-PR, AUC-PR Gain and their calibrated version) for the optimal model (as defined in (1)) as \u03c0 decreases from \u03c0 = 0.5 (balanced) to \u03c0 = 0.001. We observe that the impact of the class prior on the regular metrics is important (Figure 4). It can be a serious issue for applications where \u03c0 sometimes vary by one order of magnitude from one day to another (see [4] for a real world example) as it leads to a significant variation of the measured performance (see the difference between AUC-PR when \u03c0 = 0.5 and when \u03c0 = 0.05) even if the optimal model remains the same. On the contrary, the calibrated versions remain very robust to changes in the class prior \u03c0 even for extreme values. Note that we here experiment with synthetic data to have a full control over the distribution/prior and make the analysis easier but the conclusions are exactly the same on real world data.1\n1 see appendix in https://figshare.com/articles/Calibrated_metrics_IDA_ Supplementary_material_pdf/11848146"
    },
    {
      "heading": "3.3 Assessment of the Model Quality",
      "text": "Besides the robustness of the calibrated metrics to changes in \u03c0, we also want them to be sensitive to the quality of the model. If this latter decreases regardless of the \u03c0 value, we expect all metrics, calibrated ones included, to decrease in value. Let us consider an experiment where we use the same synthetic dataset as defined the previous section. However, instead of changing the value of \u03c0 only, we change (\u00b51, \u00b50) to make the problem harder and harder and thus worsen the optimal model\u2019s performance. This can be done by reducing the distance between the two normal distributions in (6), because this would result in more overlapping between the classes and make it harder to discriminate between them. As a distance, we consider the KL-divergence that boils down to 12 (\u00b51 \u2212 \u00b50) 2.\nFigure 5 shows how the values of the metrics evolve as the KL-divergence gets closer to zero. For each run, we randomly chose the prior \u03c0 in the interval [0.001, 0.5]. As expected, all metrics globally decrease as the problem gets harder. However, we can notice an important difference: the variation in the calibrated metrics are smooth and monotonic compared to those of the original metrics which are affected by the random changes in \u03c0. In that sense, variations of the calibrated metrics across the different generated datasets are much easier to interpret than the original metrics."
    },
    {
      "heading": "4 Link Between Calibrated and Original Metrics",
      "text": ""
    },
    {
      "heading": "4.1 Meaning of \u03c00",
      "text": "Let us first remark that for test datasets in which \u03c0 = \u03c00, Precc is equal to the regular precision Prec since \u03c0(1\u2212\u03c00)\u03c00(1\u2212\u03c0) = 1 (this is observable in Figure 4 with the intersection of the metrics for \u03c0 = \u03c00 = 0.5).\nIf \u03c0 6= \u03c00, the calibrated metrics essentially have the value that the original ones would have if the positive class ratio \u03c0 was equal to \u03c00. To further demonstrate that, we compare our proposal for calibration (5) with the only proposal from the past [10] that was designed for the same objective: a heuristic-based calibration. The approach from [10] consists in randomly undersampling the test set to make the positive class ratio \u03c0 equal to a chosen ratio (let us refer to it as \u03c00 for the analogy) and then computing the regular metrics on the sampled set. Because of the randomness, sampling may remove more hard examples than easy examples so the performance can be over-estimated, and vice versa. To avoid that, the approach performs several runs and computes a mean estimation. In Figure 6, we compare the results obtained with our formula and with their heuristic, for several reference ratio \u03c00, on a highly unbalanced (\u03c0 = 0.0017) credit card fraud detection dataset available on Kaggle [4].\nWe can observe that our formula and the heuristic provide really close values. This can be theoretically explained (see appendix - footnote 1) and confirms that the calibration formula computes the value that the original metric would have if the ratio \u03c0 in the test set was \u03c00. Note that our closed-form calibration (5) can be seen as an improvement of the heuristic-based calibration from [10] as it provides the targeted value without running a costly Monte-Carlo simulation."
    },
    {
      "heading": "4.2 Do the Calibrated Metrics Rank Models in the Same Order as the Original Metrics ?",
      "text": "Calibration results in evaluating the metric for a different prior. In this section, we analyze how this impacts the task of selectioning the best model for a given dataset. To do this, we empirically analyze the correlation of several metrics in terms of model ordering. We use OpenML [16] to select the 602 supervised binary classification datasets on which at least 30 models have been evaluated with a 10-fold cross-validation. For each one, we randomly choose 30 models,\nfetch their predictions, and evaluate their performance with the metrics. This leaves us with 614 \u00d7 30 = 18, 420 different values for each metric. To analyze whether they rank the models in the same order, we compute the Spearman rank correlation coefficient between them for the 30 models for each of the 614 problems.2 Most datasets roughly have balanced classes (\u03c0 > 0.2 in more than 90% of the datasets). Therefore, to also specifically analyze the imbalance case, we run the same experiment with only the subset of 4 highly imbalanced datasets (\u03c0 < 0.01). The compared metrics are AUC-ROC, AUC-PR, AUC-PR Gain and the best F1-score over all possible thresholds. We also add the calibrated version of the last three. In order to understand the impact of \u03c00, we use two different values: the arbitrary \u03c00 = 0.5 and another value \u03c00 \u2248 \u03c0 (for the first experiment with all datasets, \u03c00 \u2248 \u03c0 corresponds to \u03c00 = 1.01\u03c0 and for the second experiment where \u03c0 is very small, we go further and \u03c00 \u2248 \u03c0 corresponds to \u03c00 = 10\u03c0 which remains closer to \u03c0 than 0.5). The obtained correlation matrices are shown in Figure 7. Each individual cell corresponds to the average Spearman correlation over all datasets between the row metric and the column metric.\nA general observation is that most metrics are less correlated with each other when classes are unbalanced (right matrix in Figure 7). We also note that the best F1-score is more correlated to AUC-PR than to AUC-ROC or AUC-PR Gain. In the balanced case (left matrix in Figure 7), we can see that metrics defined as area under curves are generally more correlated with each other than with the threshold sensitive classification metric F1-score. Let us now analyze the\n2 the implementation of the paper experiments can be found at https://github.com/ wissam-sib/calibrated_metrics\nimpact of calibration. As expected, in general, when \u03c00 \u2248 \u03c0, calibrated metrics have a behavior really close to that of the original metrics because \u03c0(1\u2212\u03c00)\u03c00(1\u2212\u03c0) \u2248 1 and therefore Precc \u2248 Prec. In the balanced case (left), since \u03c0 is close to 0.5, calibrated metrics with \u03c00 = 0.5 are also highly correlated with the original metrics. In the imbalanced case (on the right matrix of Figure 7), when \u03c00 is arbitrarily set to 0.5 the calibrated metrics seem to have a low correlation with the original ones. In fact, they are less correlated with them than with AUCROC. And this makes sense given the relative weights that each of the metric applies to FP and TP. The original precision gives the same weight to TP and FP , although false positives are 1\u2212\u03c0\u03c0 times more likely to occur ( 1\u2212\u03c0 \u03c0 > 100 if \u03c0 < 0.01). The calibrated precision with the arbitrary value \u03c00 = 0.5 boils down to TPTP+ \u03c0 (1\u2212\u03c0)FP and gives a weight 1\u2212\u03c0\u03c0 times smaller to false positives which counterbalances their higher likelihood. ROC, like the calibrated metrics with \u03c00 = 0.5, gives 1\u2212\u03c0 \u03c0 less weight to FP because it is computed from FPR and TPR which are linked to TP and FP with the relationship \u03c01\u2212\u03c0 FP TP = FPR TPR .\nTo sum up the results, we first emphasize that the choice of the metrics to rank classifiers when datasets are rather balanced seems to be much less sensitive than in the extremely imbalanced case. In the balanced case the least correlated metrics have an average rank correlation of 0.81. For the imbalanced datasets, on the other hand, many metrics have low correlations which means that they often disagree on the best model. The choice of the metric is therefore very important here. Our experiment also seems to reflect that rank correlations are mainly a matter of how much weight is given to each type of error. Choosing these \u201dweights\u201d generally depends on the application at hand. An this should be remembered when using calibration. To preserve the nature of a given metrics, \u03c00 has to be fixed to a value close to \u03c0 and not arbitrarily. The user still has the choice to fix it to another value if his purpose is to specifically place the results into a different reference with a different prior."
    },
    {
      "heading": "5 Guidelines and Use-Cases",
      "text": "Calibration could benefit ML practitioners when analyzing the performance of a model across different datasets/time periods. Without being exhaustive, we give four use-cases where it is beneficial (setting \u03c00 depends on the target use-case):\nComparing the performance of a model on two populations/classes: Consider a practitioner who wants to predict patients with a disease and evaluate the performance of his model on subpopulations of the dataset (e.g. children, adults and elderly people). If the prior is different from one population to another (e.g. elderly people are more likely to have the disease), precision will be affected, i.e. population with a higher disease ratio will be more likely to have a higher precision. In this case, the calibrated precision can be used to obtain the precision of each population set to the same reference prior (for instance, \u03c00 can be chosen as the average prior over all populations). This would provide an additional balanced point of view and make the analysis richer to draw more precise conclusions and perhaps study fairness [1].\nModel performance monitoring in an industrial context: in systems where a model\u2019s performance is monitored over time with precision-based metrics like F1-score, using calibration in addition to the regular metrics makes it easier to understand the evolution especially when the class prior can evolve (cf. application in Figure 1). For instance, it can be useful to analyze the drift (i.e. distinguish between variations linked to \u03c0 or P (X|y)) and design adapted solutions; either updating the threshold or completely retraining the model. To avoid denaturing too much the F1-score, here \u03c00 has to be fixed based on realistic values (e.g. average \u03c0 in historical data).\nEstablishing agreements with clients: As shown in previous sections, \u03c00 can be interpreted as the ratio to which we refer to compute the metric. This can be useful to establish a guarantee, in an agreement, that will be robust to uncontrollable events. Indeed, if we take the case of fraud detection, the real positive class ratio \u03c0 can vary extremely from one day to another and on particular events (e.g. fraudster attacks, holidays) which significantly affects the measured metrics (see Figure 4). Here, after having both parties to agree beforehand on a reasonable value for \u03c00 (based on their business knowledge), calibration will always compute the performance relative to this ratio and not the real \u03c0 and thus be easier to guarantee.\nAnticipating the deployment of a model in production: Imagine one collects a sample of data to develop an algorithm and reaches an acceptable AUCPR for production. If the prior in the collected data is different from reality, the non-calibrated metric might have given either a pessimistic or optimistic estimation of the post-deployment performance. This can be extremely harmful if the production has strict constraints. Here, if the practitioner uses calibration with \u03c00 equal to the minimal prior envisioned for the application at hand, he/she would be able to anticipate the worst case scenario."
    },
    {
      "heading": "6 Conclusion",
      "text": "In this paper, we provided a formula of calibration, empirical results, and guidelines to make the values of metrics across different datasets more interpretable. Calibrated metrics are a generalization of the original ones. They rely on a reference \u03c00 and compute the value that we would obtain if the positive class ratio \u03c0 in the evaluated test set was equal to \u03c00. If the user chooses \u03c00 = \u03c0, this does not change anything and he retrieves the regular metrics. But, with different choices, the metrics can serve several purposes such as obtaining robustness to variation in the class prior across datasets, or anticipation. They are useful in both academic and industrial applications as explained in the previous section: they help drawing more accurate comparisons between subpopulations, or study incremental learning on streams by providing a point of view agnostic to virtual concept drift [17]. They can be used to provide more controllable performance indicators (easier to guarantee and report), help preparing deployment in production, and prevent false conclusions about the evolution of a deployed model. However, \u03c00\nhas to be chosen with caution as it controls the relative weights given to FP and TP and, consequently, can affect the selection of the best classifier."
    }
  ],
  "title": "Master your Metrics with Calibration",
  "year": 2020
}
