{
  "abstractText": "In conventional prediction tasks, a machine learning algorithm outputs a single best model that globally optimizes its objective function, which typically is accuracy. Therefore, users cannot access the other models explicitly. In contrast to this, multiple model enumeration attracts increasing interests in non-standard machine learning applications where other criteria, e.g., interpretability or fairness, than accuracy are main concern and a user may want to access more than one nonoptimal, but suitable models. In this paper, we propose a K-best model enumeration algorithm for Support Vector Machines (SVM) that given a dataset S and an integer K > 0, enumerates the K-best models on S with distinct support vectors in the descending order of the objective function values in the dual SVM problem. Based on analysis of the lattice structure of support vectors, our algorithm efficiently finds the next best model with small latency. This is useful in supporting users\u2019s interactive examination of their requirements on enumerated models. By experiments on real datasets, we evaluated the efficiency and usefulness of our algorithm.",
  "authors": [
    {
      "affiliations": [],
      "name": "Kentaro Kanamori"
    },
    {
      "affiliations": [],
      "name": "Satoshi Hara"
    },
    {
      "affiliations": [],
      "name": "Masakazu Ishihata"
    },
    {
      "affiliations": [],
      "name": "Hiroki Arimura"
    }
  ],
  "id": "SP:64c5613099510b303065f1792b1b8d32f0af7304",
  "references": [
    {
      "authors": [
        "J. Adebayo"
      ],
      "title": "FairML: Auditing black-box predictive models",
      "venue": "https://github.com/adebayoj/fairml,",
      "year": 2018
    },
    {
      "authors": [
        "E. Angelino",
        "N. Larus-Stone",
        "D. Alabi",
        "M. Seltzer",
        "C. Rudin"
      ],
      "title": "Learning certifiably optimal rule lists",
      "venue": "In Proc. ACM KDD 2017, Halifax, August,",
      "year": 2017
    },
    {
      "authors": [
        "D. Avis",
        "K. Fukuda"
      ],
      "title": "Reverse search for enumeration",
      "venue": "Discrete Applied Mathematics,",
      "year": 1996
    },
    {
      "authors": [
        "J. Bien",
        "R. Tibshirani"
      ],
      "title": "Prototype selection for interpretable classification",
      "venue": "The Annals of Applied Statistics,",
      "year": 2011
    },
    {
      "authors": [
        "C.J. Burges",
        "D.J. Crisp"
      ],
      "title": "Uniqueness of the svm solution",
      "venue": "NIPS",
      "year": 1999
    },
    {
      "authors": [
        "T. Calders",
        "F. Kamiran",
        "M. Pechenizkiy"
      ],
      "title": "Building classifiers with independency constraints",
      "venue": "In IEEE ICDM 2009 Workshops,",
      "year": 2009
    },
    {
      "authors": [
        "T.H. Cormen",
        "C.E. Leiserson",
        "R.L. Rivest",
        "C. Stein"
      ],
      "title": "Introduction to Algorithms, Third Edition",
      "year": 2009
    },
    {
      "authors": [
        "K. Crawford"
      ],
      "title": "The trouble with bias",
      "venue": "NIPS 2017, invited talk, Long Beach, USA,",
      "year": 2017
    },
    {
      "authors": [
        "N. Cristianini",
        "J. Shawe-Taylor"
      ],
      "title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods",
      "year": 2000
    },
    {
      "authors": [
        "S. Hajian",
        "F. Bonchi",
        "C. Castillo"
      ],
      "title": "Algorithmic bias: From discrimination discovery to fairness-aware data mining",
      "venue": "In Proc. ACM KDD 2016,",
      "year": 2016
    },
    {
      "authors": [
        "S. Hara",
        "M. Ishihata"
      ],
      "title": "Approximate and exact enumeration of rule models",
      "venue": "In Proc. AAAI 2018,",
      "year": 2018
    },
    {
      "authors": [
        "S. Hara",
        "T. Maehara"
      ],
      "title": "Enumerate lasso solutions for feature selection",
      "venue": "In Proc. AAAI 2017, San Francisco,",
      "year": 2017
    },
    {
      "authors": [
        "T. Hastie",
        "R. Tibshirani",
        "J. Friedman"
      ],
      "title": "The Elements of Statistical Learning",
      "year": 2001
    },
    {
      "authors": [
        "T. Hastie",
        "S. Rosset",
        "R. Tibshirani",
        "J. Zhu"
      ],
      "title": "The entire regularization path for the support vector machine",
      "venue": "J. Mach. Learn. Res.,",
      "year": 2004
    },
    {
      "authors": [
        "B. Kim",
        "R. Khanna",
        "O. Koyejo"
      ],
      "title": "Examples are not enough, learn to criticize! criticism for interpretability",
      "venue": "In NIPS 2016,",
      "year": 2016
    },
    {
      "authors": [
        "E. Lawler"
      ],
      "title": "A procedure for computing the k best solutions to discrete optimization problems and its application to the shortest path problem",
      "venue": "Manag. Sci.,",
      "year": 1972
    },
    {
      "authors": [
        "Y. Mo",
        "E. Garone",
        "A. Casavola",
        "B. Sinopoli"
      ],
      "title": "False data injection attacks against state estimation in wireless sensor networks",
      "venue": "IEEE Conference on Decision and Control (CDC),",
      "year": 2010
    },
    {
      "authors": [
        "J.C. Platt"
      ],
      "title": "Fast training of support vector machines using sequential minimal optimization",
      "venue": "In Advances in Kernel Methods,",
      "year": 1999
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "why should I trust you?\u201d: Explaining the predictions of any classifier",
      "venue": "In Proc. ACM KDD 2016, San Francisco,",
      "year": 2016
    },
    {
      "authors": [
        "A. Ross",
        "W. Pan",
        "F. Doshi-Velez"
      ],
      "title": "Learning qualitatively diverse and interpretable rules for classification",
      "venue": "ICML Workshop on Human Interpretability in Machine Learning (WHI 2018),",
      "year": 2018
    },
    {
      "authors": [
        "S. Ruggieri"
      ],
      "title": "Enumerating distinct decision trees",
      "venue": "In Proc. ICML 2017, Sydney,",
      "year": 2017
    }
  ],
  "sections": [
    {
      "heading": "1. Introduction",
      "text": "Machine learning technologies are being widely applied to decision making in the real world. Recently, nonstandard learning problems with criteria, such as interpretability (Ribeiro et al., 2016; Angelino et al., 2017) and fairness (Hajian et al., 2016; Crawford, 2017), other than prediction accuracy attract increasing attention. In case that the predictions by a learning algorithm are not suitable to user\u2019s requirements, or violate critical constraints, it may no longer be usable in the actual world, even if it has high prediction accuracy.\n1Hokkaido University, Japan 2Osaka University, Japan 3NTT Communication Science Laboratories, Japan. Correspondence to: Kentaro Kanamori <kanamori@ist.hokudai.ac.jp>."
    },
    {
      "heading": "2019 ICML Workshop on Human in the Loop Learning (HILL",
      "text": "2019), Long Beach, USA. Copyright by the author(s).\nTo incorporate user\u2019s requirements into learning process, a new framework, called model enumeration, is recently proposed (Hara & Maehara, 2017; Ruggieri, 2017; Hara & Ishihata, 2018). In this framework, an algorithm enumerates several models with different structures, possibly with the same objective values, instead of finding a single, optimal model. It has a number of advantages to enumerate models. The previous work (Hara & Maehara, 2017) studied model enumeration focusing on enumeration of subsets of features. In contrast to this, we focus on enumeration of distinct models based on subsets of examples in a given dataset.\nIn this study, we propose an enumeration algorithm for Support Vector Machines (SVM) (Vapnik, 1998). In the dual form of the SVM learning problem, its decision boundary, i.e., its model is represented by a linear combination of the subset of a given dataset, which is called support vectors. Adopting the dual form of the SVM learning problem and extending the enumeration method for Lasso by (Hara & Maehara, 2017), we present an algorithm for enumerating SVM models that have distinct support vectors in the descending order of the dual form objective function values. Our approach has the following advantages:\n\u2022 Data understanding: A single model that optimizes its objective function is not necessarily the best model that can explain the data well, due to, e.g., label noise or data contamination. By enumerating many models, we have a chance to access better models from the user\u2019s interests. This can be seen as a multiple version of example-based explanation (Bien & Tibshirani, 2011).\n\u2022 Interactive learning: In a long-term prediction service, a single optimal model may not continue to be the best model forever due to change of a user\u2019s interests or requirements. Our framework can be used to provide the next best model by a user\u2019s request to interactively examine and select some of enumerated models.\nCONTRIBUTIONS\nIn this paper, we make the following contributions.\n1. We formulate a model enumeration problem for SVM as enumeration of SVM models with distinct support vectors in the descending order of the objective values.\n2. We propose an efficient exact algorithm for the SVM model enumeration problem by extending the approach\nar X\niv :1\n90 6.\n01 87\n6v 1\n[ cs\n.L G\n] 5\nJ un\n2 01\n9\nfor Lasso by (Hara & Maehara, 2017). Our algorithm can be extended to efficient top-K enumeration.\n3. By experiments on real datasets, we evaluate the efficiency and the effectiveness of our algorithm. We also show that there exist several models with different prediction results and fairness score although they have almost equal objective function values.\nRELATED WORK\nModel enumeration attracts increasing attention in recent years. Enumeration algorithms for several machine learning models, such as Lasso (Hara & Maehara, 2017), decision trees (Ruggieri, 2017), and rule models (Hara & Ishihata, 2018), have been proposed. In addition, a method for simultaneously learning multiple diverse classifiers has been proposed (Ross et al., 2018).\nExample-based explanations are widely used for interpreting the distribution of a dataset. Several methods for selecting representative examples from a dataset, such as prototypes (Bien & Tibshirani, 2011) and criticisms (Kim et al., 2016), have been proposed. However, our method is different from theirs since our method is based on support vectors that represent an SVM model, and enumerates them in the descending order of the objective value.\nIn the context of SVM, solution path (Hastie et al., 2004) is a method for tracing changes of obtained models by varying its regularization parameter monotonically. It is similar to our problem since it considers generation of different SVM models. However, our problem is different from it since our problem fixes the regularization parameter unlike a solution path algorithm varies it, and our algorithm outputs more various models. The uniqueness of the SVM solution were discussed by (Burges & Crisp, 2000)."
    },
    {
      "heading": "2. Preliminaries",
      "text": "Let R and and N = {1, 2, 3, . . . } be the sets of all real numbers and all positive integers, respectively. For any n \u2208 N, we denote by [n] := {1, . . . , n}. For any indexed set S = {s1, . . . , sn} and index subset I \u2286 [n], the subset of S indexed by I is defined by SI = {sj | j \u2208 I}. We denote by X and Y the input and output domains, respectively. In this paper, we assume the binary classification, i.e.,X = Rd and Y = {\u22121, 1} for some d \u2208 N. A dataset of size n \u2208 N is a finite set S = {(xj , yj)}nj=1 \u2286 X \u00d7 Y . A model is any function m : X \u2192 Y , and a model space is any set of models. For other definitions, see, e.g., (Hastie et al., 2001)."
    },
    {
      "heading": "2.1. Support Vector Machines (SVM)",
      "text": "In the following discussion, we assume as hyperparameters a positive definite kernel function K : Rd \u00d7 Rd \u2192 R and a positive number C > 0, called a regularization parameter.\nIn the following, we fix K, C, and S \u2286 X \u00d7 Y , and omit them if it is clear from context. Note that our results are independent of the choice of K and C.\nIn this paper, we consider the dual form of SVMs (Cristianini & Shawe-Taylor, 2000). We assume a given dataset S = {(xj , yj)}nj=1. For any n-dimensional vector \u03b1 = (\u03b11, . . . , \u03b1n) \u2208 Rn, the objective function of SVMs, f(\u03b1) := f(\u03b1 | S,K), is defined by\nf(\u03b1) := \u2211 j\u2208[n] \u03b1j \u2212 1 2 \u2211 i,j\u2208[n] \u03b1i\u03b1jyiyjK(xi, xj). (1)\nThe feasible solution space (or the model space) for SVMs, F = F(S,C), is defined by the set of all Lagrange multipliers \u03b1 \u2208 Rn satisfying the conditions (i) and (ii) below:\n(i) \u2211 j\u2208[n] \u03b1jyj = 0 and (ii) \u03b1j \u2208 [0, C],\u2200j \u2208 [n]. (2)\nNow, the (ordinary) SVM learning problem is stated as the following maximization problem:\n\u03b1\u2217 = arg max \u03b1\u2208F f(\u03b1). (3)\nSince the problem of Eq. (3) is a convex quadratic programming problem, the solution found is global, not necessarily unique, and one of them can be efficiently computed by various methods such as SMO (Platt, 1999).\nBy using \u03b1 \u2208 F , the prediction model (or the SVM model) m : X \u2192 Y associated to \u03b1 is given by\nm(x | \u03b1) = m(x | \u03b1, S,K) := sgn( \u2211 j\u2208[n] \u03b1jyjK(xj , x) + b), (4)\nwhere x \u2208 X , and a threshold b \u2208 R is determined by b = 1 \u2212 \u2211 j\u2208[n] \u03b1jyjK(xj , xi) for any i \u2208 [n] such that \u03b1i \u2208 (0, C). Since a model m(\u00b7 | \u03b1) is solely determined by \u03b1, we also call \u03b1 \u2208 F a model as well as m.\nIt is known that an optimal solution \u03b1\u2217 for an SVM tend to be a sparse vector. For any \u03b1 \u2208 F , we denote its support and support vectors by supp(\u03b1) := {j \u2208 [n] | \u03b1j 6= 0} and V (\u03b1 | S) := {(xj , yj) \u2208 S | j \u2208 supp(\u03b1)}, respectively. From Eq. (1), we have the next lemma, which says that the value of the objective function depends only on supp(\u03b1).\nLemma 1 For any \u03b1 \u2208 F such that supp(\u03b1) \u2286 I for some I \u2286 [n], f(\u03b1 | S,K) = f(\u03b1I | SI ,K).\nProof. Since \u03b1j = 0 for any j \u2208 ([n] \\ I), we have f(\u03b1 | S,K) = \u2211 j\u2208[n] \u03b1j \u2212 1 2 \u2211 i,j\u2208[n] \u03b1i\u03b1jyiyjK(xi, xj) =\u2211\nj\u2208I \u03b1j\u2212 1 2 \u2211 i,j\u2208I \u03b1i\u03b1jyiyjK(xi, xj) = f(\u03b1I | SI ,K).\nFrom Eq. (4), we also see that the prediction result of SVM model \u03b1 depends only on the set of its support vectors."
    },
    {
      "heading": "3. Problem Formulation",
      "text": "Before introducing our enumeration problem for SVMs, we define the constrained SVM learning problem below. For any index subset I \u2286 [n], the constrained problem associated to I is the problem (3) with the constraint supp(\u03b1) \u2286 I . Note that the problem (5) is equivalent to the problem (3) when the input is restricted to the subset SI \u2286 S.\nDefinition 1 For any given index subset I \u2286 [n], the constrained SVM learning problem with respect to I is expressed as the following maximization problem:\n\u03b1\u0302 = arg max \u03b1\u2208F(I) f(\u03b1) (5)\nwhere F(I) = F(I | S,C) is the constrained model space (or the feasible solution space) consisting of all Lagrange multipliers \u03b1 \u2208 Rn satisfying the conditions (i) and (ii) of Eq. (2) and the additional condition (iii) supp(\u03b1) \u2286 I .\nIn the above definition, the solution \u03b1\u0302 is called a support vector w.r.t. I . We remark that the value f(\u03b1) does not depend on the choice of I since f(\u03b1) = f(\u03b1 | S,K) = f(\u03b1I | SI ,K) by condition (iii) and Lemma 1.\nThen, we denote the set of globally optimal solutions by SVM(I) = SVM(I | S,C,K) := {\u03b1 \u2208 F(I) | f(\u03b1) = f\u0302}, where f\u0302 = max\u03b1\u2208F(I) f(\u03b1) is the optimum value for the objectives.\nThe following property plays a key role in the analysis of our algorithm proposed later.\nProposition 1 (key property of solutions) Let I \u2286 [n] be any index subset and \u03b1\u0302 \u2208 SVM(I) be any solution w.r.t. I . For any J \u2286 [n], supp(\u03b1\u0302) \u2286 J \u2286 I =\u21d2 \u03b1\u0302 \u2208 SVM(J).\nProof. By assumption, we have (a) supp(\u03b1\u0302) \u2286 J implies \u03b1\u0302 \u2208 F(J), and (b) J \u2286 I implies F(J) \u2286 F(I). From (a) and (b), we have that if \u03b1\u0302 \u2208 SVM(I) is optimal in F(I), it is also optimal in F(J). Thus, \u03b1\u0302 \u2208 SVM(J) is proved.\nAn algorithm for the constrained SVM problem is any deterministic algorithm ASVM : 2[n] \u2192 F that given I \u2286 [n] as well as S,K, and C, computes a solution \u03b1 = ASVM(I) \u2208 SVM(I) for the SVM problem. From Proposition 1, we make the following assumption on ASVM throughout this paper.\nAssumption 1 For any I, J \u2286 [n], ASVM satisfies that supp(ASVM(I)) \u2286 J \u2286 I implies ASVM(I) = ASVM(J).\nFor justification of Assumption 1, if the objective function f is strictly convex, the set SVM(I) has the unique solution (Burges & Crisp, 2000), and thus, the assumption\nholds. If f is not strictly convex, it is only known that SVM(I) is itself a convex set (Burges & Crisp, 2000). It remains open if a sort of greedy variable selection strategies in, e.g., the SMO (Platt, 1999) or the chunking (Vapnik, 1998) algorithms is sufficient to ensure Assumption 1.\nUnder the above assumption, the solution set for our enumeration problem on input (S,C,K) is the collection Mall = {\u03b1 \u2208 F | \u2203I \u2286 [n] : \u03b1 = ASVM(I)} of the distinct SVM models computed by ASVM for all possible index subsets of [n]. We observe that the corresponding set SVall = {supp(\u03b1) | \u03b1 \u2208 Mall} \u2286 2[n] of the supports is isomorphic to the quotient set \u03a0 w.r.t. the equivalence relation \u2261ASVM defined by I \u2261ASVM J \u21d0\u21d2 ASVM(I) = ASVM(J), where each representative [J ] \u2208 \u03a0 is written as [J ] = {I \u2208 2[n] | I \u2261ASVM J}.\nOur goal is to enumerate all models ofMall that have distinct support vectors in the descending order of their objective function values. Now, we state our problem as follows.\nProblem 1 (Enumeration problem for SVMs) Given any dataset S = {(xj , yj)}nj=1 \u2286 X \u00d7Y , parameter C > 0, and kernel function K, the task is to enumerate all distinct models \u03b1 inMall in the descending order of their objective function values f(\u03b1) without duplicates.\nNote that we fix the regularization parameter C unlike the solution path for SVMs (Hastie et al., 2004).\nTo solve Problem 1, a straightforward, but infeasible method is to simply collect ASVM(I) over all exponentially many subsets I in 2[n]. This has redundancy w.r.t. \u2261ASVM since some pair of subsets I and J may yield the same solution if they are equivalent. Hence, we seek for a more efficient method utilizing the sparseness of the SVM models inMall."
    },
    {
      "heading": "4. Algorithm",
      "text": "In this section, we propose an efficient algorithm ENUMSV for solving Problem 1. ENUMSV is based on Lawler\u2019s framework (Lawler, 1972) for top-K enumeration following the approach by Hara and Maehara to Lasso (Hara & Maehara, 2017)."
    },
    {
      "heading": "4.1. The outline of our algorithm",
      "text": "In Algorithm 1, we show the outline of our algorithm ENUMSV. It maintains as a data structure H , which is a priority queue (or a heap) (Cormen et al., 2009), to store triples \u03c4 = (\u03b1, I, B) consisting of\n\u2022 a discovered solution \u03b1 \u2208 F (a Lagrange multiplier),\n\u2022 an index set I \u2208 2[n] associated to \u03b1 by \u03b1 = ASVM(I), and\nAlgorithm 1 An enumeration algorithm ENUMSV for SVMs that, given a dataset S = {(xj , yj)}nj=1, C > 0, and a kernel function K, returns the set of all models in Mall = {ASVM(I | S,C,K) | I \u2286 [n]} in descending order of their objective function values without duplicates.\n1: Heap H \u2190 \u2205; 2: \u03b1\u0302\u2190 ASVM([n] | S,C,K); 3: Insert \u03c41 = (\u03b1\u0302, [n], \u2205) into the heap H; 4: M\u2190 \u2205; 5: while H 6= \u2205 do 6: Extract \u03c4 = (\u03b1, I, B) from the heap H; 7: if \u03b1 6\u2208 M thenM\u2190M\u222a {\u03b1}; 8: for j \u2208 supp(\u03b1) \\B do 9: I \u2032 \u2190 I \\ {j};\n10: \u03b1\u2032 \u2190 ASVM(I \u2032 | S,C,K); 11: Insert \u03c4 \u2032 = (\u03b1\u2032, I \u2032, B) into the heap H; 12: B \u2190 B \u222a {j}; 13: end for 14: end while 15: ReturnM;\n\u2022 a forbidden set B \u2208 2[n] to avoid searching redundant children of I .\nTriples are ordered in the descending order of their objective values f(\u03b1) as keys. For the heap H , we can insert to H any triple and extract (or deletemax) from H the triple \u03c4 with the maximum key each in O(log |H|) time (Cormen et al., 2009).\nIn Lawler\u2019s framework, we can compute an optimal solution for each subproblems avoiding subproblems that yields redundant solutions.\nBase Case: Initially, ENUMSV starts by inserting the first triple \u03c41 = (ASVM([n]), [n], \u2205) at Line 3, where \u03c41 corresponds to the solution for the ordinary SVM problem.\nInductive Case: While H is not empty, ENUMSV then repeats the following steps in the while-loop:\nStep 1 Extract a triple \u03c4 = (\u03b1, I, F ) from the heap H at Line 6, where \u03b1 is called a candidate. Insert vector \u03b1 toM as a solution at Line 7 if it has not been founded yet.\nStep 2 Repeat the following steps for any j \u2208 supp(\u03b1)\\B:\n1. Branch the search spaces as I \u2032 = I \\{j} at Line 9. 2. Compute \u03b1\u2032 = ASVM(I \u2032) at Line 10 and insert\nthe triple \u03c4 \u2032 = (\u03b1\u2032, I \u2032, F ), called a child of \u03c4 , into the heap H at Line 11. 3. Insert j into B\u2032 to avoid inserting the same index subset into the heap H twice at Line 12.\nStep 3 Back to step 1. if the heap H is not empty.\nThe most important step of ENUMSV in Algorithm 1 is Step 2 above. Based on Proposition 1, it branches a search on each index j \u2208 supp(\u03b1) \\ B. We can avoid redundant computations that yield the same solution that had already been output before. To avoid enumerating the same index subset I multiple times, we add the used index j into B."
    },
    {
      "heading": "4.2. The correctness",
      "text": "In this subsection, we show the correctness of ENUMSV in Algorithm 1 on input (S,C,K) through properties 1 and 2 below. For every k \u2265 1, \u03b1(k) denotes the k-th solution in M by ENUMSV. We first show a main technical lemma.\nLemma 2 For any feasible solution \u03b1 \u2208 F , there exists some \u03c4 (k) = (\u03b1(k), I(k), B(k)) extracted from H such that (i) supp(\u03b1(k)) \u2286 supp(\u03b1), (ii) supp(\u03b1) \u2286 I(k), and (iii) f(\u03b1(k)) \u2265 f(\u03b1), where k \u2265 1.\nProof. Let \u03b1 \u2208 F . Starting from the initial triple \u03c4 (1), we will go down the search space by visiting triples \u03c4 (k) from a parent to its child for k = 1, 2, . . . , while maintaining the invariant (ii). Base case: For k = 1, the first triple \u03c4 (1) clearly satisfies the invariant supp(\u03b1) \u2286 I(k). From Lemma 1, if \u03c4 (1) satisfies condition (i), the claim immediately follows.\nInduction case: Let k > 1. Suppose inductively that \u03c4 (k) satisfies (ii) supp(\u03b1) \u2286 I(k). Then, there are two cases (1) and (2) below on the inclusion supp(\u03b1(k)) \u2286 supp(\u03b1):\nCase (1): supp(\u03b1(k)) \u2286 supp(\u03b1) holds. By induction hypothesis, we have supp(\u03b1(k)) \u2286 supp(\u03b1) \u2286 I(k), and thus, \u03b1 \u2208 F(S | I(k)). Since \u03b1(k) is an optimal solution within F(S | I(k)), it follows that f(\u03b1(k)) \u2265 f(\u03b1). Case (2): supp(\u03b1(k)) 6\u2286 supp(\u03b1) holds. For any j \u2208 supp(\u03b1(k)) \\ supp(\u03b1), ENUMSV inserts into the heap the triple \u03c4 \u2032 = (\u03b1\u2032, I \u2032, B\u2032) at Line 11, and it will be eventually extracted as the m-th triple \u03c4 \u2032 = \u03c4 (m) = (\u03b1(m), I(m), F (m)) at some m > k. By induction hypothesis, supp(\u03b1) \u2286 I(k) and j 6\u2208 supp(\u03b1) hold, and thus, we have an invariant supp(\u03b1) \u2286 I(k) \\ {j} = I(m) for the child iteration with \u03c4 (m). By the above arguments, at every time following a path to a child in Case (2), the size of the difference \u2206(m) = |I(m) \\ supp(\u03b1)| decrements at least by one. Since \u2206(m) \u2265 0, this process must eventually halt at Case (1). This completes the proof.\nFrom Lemma 2, we can show the next lemma, saying that ENUMSV eventually outputs any solution.\nLemma 3 (Property 1) In ENUMSV, for any subset I \u2286 [n], there exists some k \u2265 1 such that \u03b1(k) = ASVM(I).\nProof. For \u03b1\u2032 = ASVM(I), it follows from Lemma 2 that there exists k \u2208 N such that supp(\u03b1(k)) \u2286 supp(\u03b1\u2032) and\nf(\u03b1(k)) \u2265 f(\u03b1\u2032). Since \u03b1(k) \u2208 F(I), we have \u03b1(k) = ASVM(I).\nAlso from Lemma 2, we have the next lemma for the top-K computation, which says that ENUMSV lists solutions \u03b1 exactly from larger to smaller values of f(\u03b1).\nLemma 4 (Property 2) ENUMSV enumerates solutions \u03b1 in the descending order of their objective function f(\u03b1), i.e., f(\u03b1(1)) \u2265 \u00b7 \u00b7 \u00b7 \u2265 f(\u03b1(k)) \u2265 . . . (k \u2265 1).\nProof. We show f(\u03b1(k)) \u2265 f(\u03b1(m)) for any m > k as follows. Suppose that \u03b1(k) is extracted by deletemax from the heap at step k. If \u03b1(m) is in the heap, then f(\u03b1(k)) \u2265 f(\u03b1(m)) immediately holds. Otherwise, there exists the triple (\u03b1(`), I(`), B(`)) where ` < m in the heap such that I(m) \u2286 I(`). Since \u03b1(m) \u2208 F(S | I(`)), f(\u03b1(`)) \u2265 f(\u03b1(m)) holds. From the definition of the heap, we have f(\u03b1(k)) \u2265 f(\u03b1(`)) \u2265 f(\u03b1(m)).\nBy combining Lemma 3 and Lemma 4, we show the main result of this paper.\nTheorem 1 ENUMSV in Algorithm 1 solves Problem 1.\nProof. From Lemmas 3 and 4, ENUMSV returns a collection of modelsM = {\u03b1(1),\u03b1(2), . . . } that satisfy Properties 1 and 2. Thus, ENUMSV solves Problem 1.\n4.3. Top-K Enumeration\nWe can modify Algorithm 1 to find the top-K models MK = {\u03b1(1), . . . ,\u03b1(K)} \u2286 Mall for a given positive integer K \u2208 N as follows. We simulate Algorithm 1, perform the enumeration of models in the descending order of their objectives, and then stop Algorithm 1 when |M| = K eventually holds. From Lemma 4, we see thatMK \u2286Mall andMK contains the top-K models.\nComplexity. For enumeration algorithms, it is the custom to analyze their time complexity in terms of the number of solutions, or in output-sensitive manner (Avis & Fukuda, 1996). However, it is difficult because more than one equivalent candidates I 6= J can result the model ASVM(I) = ASVM(J). Instead, we estimate its time complexity in terms of a candidate solution extracted in Line 6. The time complexity of Algorithm 1 for obtaining a candidate solution of k-th solution \u03b1(k) is O(TSVM \u00b7 | supp(\u03b1(k\u22121))|), where TSVM is the complexity of solving an SVM problem."
    },
    {
      "heading": "5. Experiments",
      "text": "In this section, we evaluate our algorithm by experiments on real datasets. All codes were implemented in Python 3.6\nwith scikit-learn. We used linear kernel K(xi, xj) = xTi xj as the kernel function in all experiments. All experiments were conducted on 64-bit Ubuntu 18.04.1 LTS with Intel Xeon E5-1620 v4 3.50GHz CPU and 62.8GiB Memory."
    },
    {
      "heading": "5.1. UCI Datasets",
      "text": "We first evaluated ENUMSV on three real datasets, German (n = 1000, d = 20), Ionosphere (n = 351, d = 34), and Sonar (n = 208, d = 60) from UCI ML repository (Dheeru & Karra Taniskidou, 2017). Their task is a binary classification. We randomly split each dataset into train (70%) and test (30%) samples, and evaluated the test loss by the hinge loss l(y, y\u0302) = max(0, 1\u2212 y \u00b7 y\u0302). For each dataset, the hyperparameter C was selected by 5-fold cross validation among 10\u22122, 10\u22121, . . . , 103 before enumeration.\nWe applied ENUMSV to these datasets, and enumerated\ntop-50 models. Figure 1 presents the values of the ratio f(\u03b1(k))/f(\u03b1(1)) of the objective function value and the ratio of the test loss of the k-th enumerated model \u03b1(k) to those of the best model \u03b1(1). Figure 1 (a) shows that the values of the objective function decreases as the rank k increases as expected from Theorem 1. For German dataset, the objective function values of top-50 were almost same within deviation of 6.0 \u00d7 10\u22124. It indicates that there are multiple models achieving the almost identical objective value. Figure 1 (b) shows that some enumerated model, such as \u03b1(32) for German, \u03b1(43) for Ionosphere, and \u03b1(41) for Sonar, had smaller test loss compared with the optimal model \u03b1(1). It means that an optimal model is not always the best model, and we obtained a better model with a lower test loss than an optimal model by enumerating models.\nFigure 2 presents the total time of enumerating top-1000 models. The total time seems almost linear in rank k. Consequently, we conclude that ENUMSV has small latency for outputting solutions independent of their ranks, and thus, is scalable in the number K of enumerated models."
    },
    {
      "heading": "5.2. Injected COMPAS Dataset",
      "text": "Next, we demonstrate an application of ENUMSV to a fair classification scenario under false data injection attacks. To evaluate the fairness of the model m for the sensitive attribute z \u2208 {\u22121, 1}, we used demographic parity (DP) (Calders et al., 2009) defined by\n\u03b4(m |z) := |P (m(x)=1 |z=1)\u2212 P (m(x)=1 |z=\u22121)|,\nwhere P is a probability on the joint distribution over (z,m(x)). We note that the larger the DP, the larger the discrimination of prediction.\nWe used COMPAS dataset (n = 6172, d = 12) related to recidivism risk prediction distributed at (Adebayo, 2018). The task is to predict whether individual people recidivate within two years from their criminal history. We used the attribute \u201dAfrican American\u201d as a sensitive attribute z. We assume a scenario of false data injection (Mo et al., 2010) that is a special kind of attacks to learning algorithms, which increases the DP of the learned model for the sensitive attribute z by flipping output labels y of a small subset of a training dataset. To reproduce this scenario, we generated injected subsets of the COMPAS by the following steps:\n1. Create a training dataset S by randomly sampling a subset of the COMPAS with 100 examples.\n2. Randomly choose a subset of S such that y 6= z with 10 examples, and replace these outputs y by \u2212y.\n3. Create a test dataset Stest by randomly sampling from the COMPAS with 50 examples.\nBy our preliminary experiments, we confirm that the above procedure increases the DP of SVM models on Stest.\nWe applied ENUMSV to the above injected COMPAS dataset, and measured objective function values, demographic parity (DP), and misclassification ratio of the top50 enumerated models. We observed that all the enumerated top-K models had the same objective value. However, these prediction results were mutually different.\nFigure 2 (a) presents the value of the DP of the enumerated models, where the dashed line indicates the reference DP value \u03b8\u2217 = 0.321 of the model learned by the non-injected subset of the input S. Figure 2 (b) presents the misclassification ratio of the enumerated models \u03b1(k) on Stest. From the figures, we observed that ENUMSV found the three fair models \u03b1(40), \u03b1(41), and \u03b1(44) achieving lower DP than \u03b8\u2217 and lower misclassification ratio than \u03b1(1). Consequently, ENUMSV successfully obtained several fair models against false data injection by enumerating models."
    },
    {
      "heading": "6. Conclusion",
      "text": "In this paper, we proposed an efficient algorithm to enumerate top-K SVM models with distinct support vectors in descending order of these objective function values. By experiments on real datasets, we demonstrated that our framework provides better models than one single optimal solution, and fair models against false data injection, which increases the unfairness of an optimal model. As future work, we will try to make theoretical or empirical justification of Assumption 1 for a particular class of SVM learning algorithms such as chunking (Vapnik, 1998) and SMO (Platt, 1999). It is also interesting future work to extend our algorithm to enumerate models taking their diversity into account so as to interactively help users to understand a dataset."
    },
    {
      "heading": "Acknowledgements",
      "text": "This work was partially supported by JSPS KAKENHI(S) 15H05711 and JSPS KAKENHI(A) 16H01743."
    }
  ],
  "title": "Enumeration of Distinct Support Vectors for Interactive Decision Making ",
  "year": 2019
}
