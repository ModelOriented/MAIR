{
  "abstractText": "In explainable artificial intelligence, there is increasing interest in understanding the behaviour of autonomous agents to build trust and validate performance. Modern agent architectures, such as those trained by deep reinforcement learning, are currently so lacking in interpretable structure as to effectively be black boxes, but insights may still be gained from an external, behaviourist perspective. Inspired by conceptual spaces theory, we suggest that a versatile first step towards general understanding is to discretise the state space into convex regions, jointly capturing similarities over the agent\u2019s action, value function and temporal dynamics within a dataset of observations. We create such a representation using a novel variant of the CART decision tree algorithm, and demonstrate how it facilitates practical understanding of black box agents through prediction, visualisation and rule-based explanation.",
  "authors": [
    {
      "affiliations": [],
      "name": "Tom Bewley"
    },
    {
      "affiliations": [],
      "name": "Jonathan Lawry"
    }
  ],
  "id": "SP:f981b2e0641576f1a24eb28d76d5d1932c660920",
  "references": [
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking inside the blackbox: A survey on Explainable Artificial Intelligence (XAI)",
      "venue": "IEEE Access 6: 52138\u201352160.",
      "year": 2018
    },
    {
      "authors": [
        "O. Bastani",
        "Y. Pu",
        "A. Solar-Lezama"
      ],
      "title": "Verifiable reinforcement learning via policy extraction",
      "venue": "Advances in neural information processing systems, 2494\u20132504.",
      "year": 2018
    },
    {
      "authors": [
        "T. Bewley",
        "J. Lawry",
        "A. Richards"
      ],
      "title": "Modelling Agent Policies with Interpretable Imitation Learning",
      "venue": "1st TAILOR workshop at ECAI 2020.",
      "year": 2020
    },
    {
      "authors": [
        "L. Breiman",
        "J. Friedman",
        "R. Olshen",
        "C. Stone"
      ],
      "title": "Classification and regression trees",
      "venue": "Wadsworth & Brooks. Cole Statistics/Probability Series .",
      "year": 1984
    },
    {
      "authors": [
        "G. Brockman",
        "V. Cheung",
        "L. Pettersson",
        "J. Schneider",
        "J. Schulman",
        "J. Tang",
        "W. Zaremba"
      ],
      "title": "OpenAI Gym",
      "year": 2016
    },
    {
      "authors": [
        "R. Carnap"
      ],
      "title": "The logical structure of the world",
      "venue": "Routledge London.",
      "year": 1967
    },
    {
      "authors": [
        "N. Chomsky"
      ],
      "title": "A review of BF Skinner\u2019s Verbal Behavior",
      "venue": "Language 35(1): 26\u201358.",
      "year": 1959
    },
    {
      "authors": [
        "Y. Coppens",
        "K. Efthymiadis",
        "T. Lenaerts",
        "A. Now\u00e9",
        "T. Miller",
        "R. Weber",
        "D. Magazzeni"
      ],
      "title": "Distilling deep reinforcement learning policies in soft decision trees",
      "venue": "Proceedings of the IJCAI 2019 Workshop on Explainable AI, 1\u20136.",
      "year": 2019
    },
    {
      "authors": [
        "G. De\u2019Ath"
      ],
      "title": "Multivariate regression trees: a new technique for modeling species\u2013environment relationships",
      "venue": "Ecology",
      "year": 2002
    },
    {
      "authors": [
        "E.W. Dijkstra"
      ],
      "title": "A note on two problems in connexion with graphs",
      "venue": "Numerische mathematik 1(1): 269\u2013271.",
      "year": 1959
    },
    {
      "authors": [
        "S. Edelman"
      ],
      "title": "Representation is representation of similarities",
      "venue": "The Behavioral and brain sciences 21(4): 449.",
      "year": 1998
    },
    {
      "authors": [
        "P. G\u00e4rdenfors"
      ],
      "title": "Conceptual spaces: The geometry of thought",
      "venue": "MIT press.",
      "year": 2004
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "F. Giannotti",
        "D. Pedreschi",
        "S. Ruggieri",
        "F. Turini"
      ],
      "title": "Factual and Counterfactual Explanations for Black Box Decision Making",
      "venue": "IEEE Intelligent Systems .",
      "year": 2019
    },
    {
      "authors": [
        "W.-C. Jiang",
        "K.-S. Hwang",
        "J.-L. Lin"
      ],
      "title": "An Experience Replay Method based on Tree Structure for Reinforcement Learning",
      "venue": "IEEE Transactions on Emerging Topics in Computing .",
      "year": 2019
    },
    {
      "authors": [
        "T. Kim",
        "Y. Yue",
        "S. Taylor",
        "I. Matthews"
      ],
      "title": "A decision tree framework for spatiotemporal sequence prediction",
      "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 577\u2013586.",
      "year": 2015
    },
    {
      "authors": [
        "A. Koul",
        "A. Fern",
        "S. Greydanus"
      ],
      "title": "Learning Finite State Representations of Recurrent Policy Networks",
      "venue": "International Conference on Learning Representations.",
      "year": 2018
    },
    {
      "authors": [
        "P. Lipton"
      ],
      "title": "Contrastive explanation",
      "venue": "Royal Institute of Philosophy Supplements 27: 247\u2013266.",
      "year": 1990
    },
    {
      "authors": [
        "G. Liu",
        "O. Schulte",
        "W. Zhu",
        "Q. Li"
      ],
      "title": "Toward interpretable deep reinforcement learning with linear model utrees",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 414\u2013429. Springer.",
      "year": 2018
    },
    {
      "authors": [
        "R. Poyiadzi",
        "K. Sokol",
        "R. Santos-Rodriguez",
        "T. De Bie",
        "P. Flach"
      ],
      "title": "FACE: feasible and actionable counterfactual explanations",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 344\u2013350.",
      "year": 2020
    },
    {
      "authors": [
        "L.D. Pyeatt"
      ],
      "title": "Reinforcement learning with decision trees",
      "venue": "21st IASTED International Multi-Conference on Applied Informatics, 26\u201331.",
      "year": 2003
    },
    {
      "authors": [
        "A. Raffin"
      ],
      "title": "RL Baselines Zoo",
      "venue": "https://github.com/ araffin/rl-baselines-zoo.",
      "year": 2018
    },
    {
      "authors": [
        "E. Rosch",
        "C.B. Mervis",
        "W.D. Gray",
        "D.M. Johnson",
        "P. Boyes-Braem"
      ],
      "title": "Basic objects in natural categories",
      "venue": "Cognitive psychology 8(3): 382\u2013439.",
      "year": 1976
    },
    {
      "authors": [
        "A.M. Roth",
        "N. Topin",
        "P. Jamshidi",
        "M. Veloso"
      ],
      "title": "Conservative Q-Improvement: Reinforcement Learning for an Interpretable Decision-Tree Policy",
      "venue": "arXiv:1907.01180 .",
      "year": 2019
    },
    {
      "authors": [
        "H.B. Saghezchi",
        "M. Asadpour"
      ],
      "title": "Multivariate decision tree function approximation for reinforcement learning",
      "venue": "International Conference on Neural Information Processing, 687\u2013694. Springer.",
      "year": 2010
    },
    {
      "authors": [
        "W. Samek",
        "T. Wiegand",
        "K.-R. M\u00fcller"
      ],
      "title": "Explainable AI: Understanding, visualizing and interpreting deep learning models",
      "venue": "arXiv:1708.08296 .",
      "year": 2017
    },
    {
      "authors": [
        "R.S. Sutton",
        "A.G. Barto"
      ],
      "title": "Reinforcement learning: An introduction",
      "venue": "MIT press.",
      "year": 2018
    },
    {
      "authors": [
        "P.S. Thomas",
        "B. Okal"
      ],
      "title": "A notation for Markov decision processes",
      "venue": "arXiv:1512.09075 .",
      "year": 2015
    },
    {
      "authors": [
        "W.T. Uther",
        "M.M. Veloso"
      ],
      "title": "Tree based discretization for continuous state space reinforcement learning",
      "venue": "AAAI/IAAI, 769\u2013774.",
      "year": 1998
    },
    {
      "authors": [
        "S. Wachter",
        "B. Mittelstadt",
        "C. Russell"
      ],
      "title": "Counterfactual explanations without opening the black box: Automated decisions and the GDPR",
      "venue": "Harv. JL & Tech. 31: 841.",
      "year": 2017
    },
    {
      "authors": [
        "A. Wan",
        "L. Dunlap",
        "D. Ho",
        "J. Yin",
        "S. Lee",
        "H. Jin",
        "S. Petryk",
        "S.A. Bargal",
        "J.E. Gonzalez"
      ],
      "title": "NBDT: NeuralBacked Decision Trees",
      "venue": "arXiv:2004.00221 .",
      "year": 2020
    }
  ],
  "sections": [
    {
      "heading": "Introduction",
      "text": "This paper explores representational tools for understanding the behaviour of extant autonomous agents while treating them and their environments as black boxes. In popular taxonomies of explainable artificial intelligence (XAI), this is categorised as post hoc, model-agnostic, global explanation (Adadi and Berrada 2018). While black box behaviourist analysis is inherently limited (Chomsky 1959), it can nonetheless serve the practical goals of XAI, which include building trust among human stakeholders and validating the performance of safety-critical systems. It is also viable in contexts where theoretical understanding of the agent\u2019s internal mechanism is lacking, as with modern deep learning systems (Samek, Wiegand, and Mu\u0308ller 2017), or where access to this mechanism is impractical or restricted.\nWe introduce a new data-driven model of black box agents, called TRIPLETREE, which builds on the flexible and interpretable architecture of a binary decision tree. As such, it provides a powerful tool for answering many meaningful questions about agent behaviour."
    },
    {
      "heading": "Problem Setup",
      "text": "Suppose that we need to understand the behaviour, performance and possible failure modes of an autonomous agent operating within a dynamic and complex environment. A\n\u2217Supported by an EPSRC/Thales industrial CASE award. Preprint. Under review.\npriori, we know nothing of the agent\u2019s provenance \u2013 its governing policy may be a product of reinforcement learning (RL), optimal control algorithms, evolution, or explicit manual design \u2013 but we assume that it can be analysed using the theoretical formalism of a Markov decision process (MDP). We refer readers to (Sutton and Barto 2018) for an overview of MDPs, and adopt the MDPNv1 notational standard (Thomas and Okal 2015).\nTo learn anything about the agent we must gather data, and in doing so we make the black box assumption (Wachter, Mittelstadt, and Russell 2017; Guidotti et al. 2019; Coppens et al. 2019). That is, we take the role of an observer of the agent-environment complex, with no access to the internal structure of either system, but the ability to record environment states s \u2208 S, agent actions a \u2208 A, and instantaneous rewards r \u2208 R, and their order of occurrence. We thereby assemble D, an ordered dataset of triplets (St, At, Rt). St, At andRt are the state, action and reward from one timestep, uniquely indexed by t. If the MDP is episodic (see (Sutton and Barto 2018)), we keep a record of which states are the initial and terminal ones in each episode. Importantly, we assume that states are represented by vectors of real-valued features, each with a straightforward semantic interpretation (such as a physical quantity). In doing so, we bypass a challenging phase of state representation learning.1\nThe data in D are generated by the interaction of opaque and complex mechanisms. How might they get us to a position of understanding? Following prior work across the academic spectrum (Carnap 1967; Rosch et al. 1976; Edelman 1998), we take the view that understanding arises by searching for similarities in observed data. A variant of this idea is Ga\u0308rdenfors\u2019 theory of conceptual spaces (Ga\u0308rdenfors 2004)."
    },
    {
      "heading": "Conceptual Spaces and Decision Trees",
      "text": "Ga\u0308rdenfors views sensory observations as embedded in high-dimensional mathematical spaces, and proposes that the building blocks of abstract reasoning are convex regions of such spaces, within which all contained observations are similar according to some salient measure. Such regions are deemed natural properties of the system being observed, and can be combined to form semantic concepts such as ob-\n1 We address the problem of interpretable feature construction and selection in (Bewley, Lawry, and Richards 2020).\nar X\niv :2\n00 9.\n04 74\n3v 2\n[ cs\n.A I]\n2 1\nSe p\n20 20\njects, categories, actions and events. For our purposes, the observations D do indeed lie within a mathematical space: the MDP state space S. We consider how these observations can be grouped into convex regions of S based on contextspecific measures of similarity. Rather than talking about the system on a state-by-state basis, we may then analyse such regions as meaningful entities in themselves, within which the agent behaves in predictable ways, and between which it moves in predictable patterns. This is a kind of MDP state abstraction, informed by observations of the agent itself.\nA central issue is choosing which attributes to use for measuring similarity within regions. An obvious criterion is the agent\u2019s action. If the agent takes the same action throughout a significant region of S, then it seems that region is worthy of being explicitly represented. Alternatively, we might be interested in the agent\u2019s performance as measured by the reward function, and thus measure similarity using the reward elements in D. In practice, it is likely more informative to invoke the notion of value, which is the expected sum of reward after the agent visits each state, temporally discounted by \u03b3 \u2208 [0, 1]. An empirical value estimate can be computed for each sample t \u2208 D using the rewards of successive samples: Vt = \u2211T k=0 \u03b3\nkRt+k. Here, T is the time until termination in an episodic MDP, and \u221e otherwise. A third valid option is to define similarity via the temporal dynamics of the MDP itself. These can be neatly captured by the time derivatives of state features, which in discrete time systems are equal to the change in state between timesteps. For each sample t \u2208 D, we define this as St+1 \u2212 St.\nOur key assertion in this paper is that there is no need to choose between these three sources of similarity, and that a powerful and versatile model results from identifying regions of S that are similar from all three perspectives. We complete our model by calculating transition probabilities between regions \u2013 the probability that having being observed at a state in one region, the agent will move to another region next. These effectively define a probabilistic finite state machine (FSM) model of the agent-environment dynamics, and can be estimated by harnessing the temporal ordering of D.\nFigure 1 summarises the proposed model. As we hope to demonstrate, it enables us to make sense of pertinent questions about the key invariances and changes points in the agent\u2019s behaviour, the environmental factors responsible for this behaviour and perturbations which would alter it, the regions of S most commonly visited, and the most likely trajectories between particular states of interest.\nWe now turn to the question of practical implementation. There exists an ideal computational tool for finding internally-similar convex regions of mathematical spaces: the humble decision tree. A decision tree is \u2018grown\u2019 to predict an output label by recursive binary partitioning of a space of input features. Typically, partitions are axis-aligned, so each resultant region (corresponding to a leaf of the tree) is a hyperrectangle. Partitions are chosen to greedily minimise a measure of impurity in the output labels of training data at each node, which is tantamount to finding and preserving similarity as conceived in our conceptual space model. Trees are popular in XAI, and often hailed as the gold standard of interpretability (Wan et al. 2020). Their hierarchical structure means that global functionality can be analysed as the concatenation of local effects, without higherorder interaction. Their alternative representation as rule sets in disjunctive normal form also enables factual and counterfactual explanation of their outputs (Guidotti et al. 2019).\nThe sub-field of agent explainability has seen enthusiastic uptake of decision trees (Bastani, Pu, and Solar-Lezama 2018; Coppens et al. 2019; Bewley, Lawry, and Richards 2020). However, these works focus exclusively on predicting and explaining a black box agent\u2019s single next action in any given state, effectively approximating its policy function, denoted by \u03c0. This approach uses only the first two elements of the triplets (St, At, Rt) in D, and ignores their ordering, thus foregoes an opportunity for far richer analysis. The allimportant notion of value, as well as the temporal dynamics of agent-environment interaction, are entirely absent from a policy-only model, which in isolation is insufficient for answering many reasonable questions about the target agent\u2019s performance and dynamical properties.\nThis lack of versatility is a product of how data are stored in the tree, and of the algorithm used to grow it, rather than an inherent limitation of the decision tree paradigm. We propose to extend the standard notion of impurity to capture multiple facets agent-environment interaction, making full use of the data in D, and providing a practical implementation of the conceptual model in figure 1. Concretely, we grow the tree using a hybrid of three impurity measures related to the agent\u2019s action, expected value and state feature time derivatives, hence our new model\u2019s name: TRIPLETREE. By modifying a weight vector \u03b8 \u2208 R3+, which sets the influence of the three measures, we can smoothly trade off between three types of interpretable model of the system: \u2022 \u03b8 = [1, 0, 0]: A conventional policy-only model. \u2022 \u03b8 = [0, 1, 0]: A value function. \u2022 \u03b8 = [0, 0, 1]: A model of the environment state dynamics. Any other weighting gives a blended combination of the three, allowing for multifactorial analysis."
    },
    {
      "heading": "The TRIPLETREE Model",
      "text": ""
    },
    {
      "heading": "Basic Structure: CART",
      "text": "TRIPLETREE is an extension of the CART algorithm (Breiman et al. 1984), which we briefly introduce first. We assume that given a datasetD, CART is being used to predict the agent\u2019s action given the state; a policy-only model.\nLet I = {1, ..., |D|} be the set of timestep indices in D, used to initialise the tree\u2019s root node before any partitions are made. Let IN be the subset of I at any given node N . To split this node in two, thereby growing the tree, CART searches over binary partitions IN = {I0, I1} such that for some state feature f and numerical threshold \u03c4 \u2208 R:\n(\u2200t \u2208 I0 S(f)t < \u03c4) \u2227 (\u2200t\u2032 \u2208 I1 S(f)t\u2032 \u2265 \u03c4). (1) Here, S(f)t is the f th element of the state vector St. For each candidate partition, we calculate the population-weighted reduction in a measure of action label impurity I induced by dividing the set into these two parts. For a discrete action space A, the Gini impurity is used:\nIN = 1 |IN |2 \u2211 a\u2208A count(IN , a)(1\u2212 count(IN , a)), (2)\nwhere count(IN , a) = |{t \u2208 IN : At = a}|. For continuous actions A = R, the impurity measure is the variance:\nIN = 1 2|IN |2 \u2211\nt\u2208IN\n\u2211\nt\u2032\u2208IN (At \u2212At\u2032)2. (3)\nThe quality Q of a candidate partition is defined as:\nQ(IN , {I0, I1}) = IN \u2212 I0|I0|+ I1|I1|\n|IN | . (4)\nCART selects the partition that maximises Q, and the corresponding feature f and threshold \u03c4 are recorded in the tree. I0 and I1 also define the members of two new child nodes of N . CART proceeds to search for the best binary partition of each child, and grows the tree depth-first up to a stopping condition, such as a depth limit.\nIn any tree, a subset of nodes, called the leaves L, remain childless. Every sample in D is a member of exactly one leaf L, whose key attribute is an action prediction a\u0303L \u2208 A. For discrete A, this is typically the modal action among the leaf\u2019s constituent samples: a\u0303L = argmaxa count(IL, a). For continuous A, the mean is used: a\u0303L = \u2211 t\u2208IL At/|IL|. To predict an action for an unseen state vector s, the tree propagates s down a path from the root by comparing its features to the thresholds encountered, then returns the prediction of the leaf which is ultimately reached."
    },
    {
      "heading": "Modifications and Extensions",
      "text": "Our fundamental divergence from CART is in the use of the entire content and strucure of D. TRIPLETREE accepts ordered triplet samples of the form (St, At, Rt), and prior to commencing growth evaluates two additional attributes, namely the value estimate Vt = \u2211T k=0 \u03b3\nkRt+k and state derivative vector Dt = St+1 \u2212 St. Rather than using only the agent\u2019s action as an output label, each leaf L is associated with three predictions: the action a\u0303L, a value estimate v\u0303L (the mean of the leaf\u2019s contituent samples), and a state derivative estimate d\u0303L (the elementwise mean).\nThe TRIPLETREE growth algorithm trades off the ability to make these three kinds of prediction by encouraging leaves to have low variability in all three attributes across their constituent samples. To achieve this, we compute three measures of the quality of candidate partitions:\n\u2022 Action quality Q(A): defined exactly as in equation 4. \u2022 Value quality Q(V ): defined equivalently, but using the\nvariance in value estimates as the impurity measure I(V ).\n\u2022 Derivative quality Q(D): defined equivalently, but using an impurity measure I(D) that sums the variance in derivatives across all d of the feature dimensions:\nI(D)N = 1 2|IN |2 d\u2211\nf=1\n1\n\u03c3(f)\n\u2211\nt\u2208IN\n\u2211\nt\u2032\u2208IN (D\n(f) t \u2212D(f)t\u2032 )2. (5)\n1/\u03c3(f) is a normalisation factor for each derivative \u2013 the reciprocal of its standard deviation across D \u2013 which prevents features with large magnitudes dominating the impurity calculation. This sum-of-variances impurity measure is similar to those used in prior work on multivariate regression trees (De\u2019Ath 2002; Kim et al. 2015).\nAfter computing Q(A), Q(V ) and Q(D), we aggregate them into a hybrid measure of partition quality Q\u2217. Having experimented with alternative methods in various MDP contexts, we find that a linear combination provides a good compromise of simplicity, robustness and flexibility:\nQ\u2217 = [ Q(A)N I(A)root , Q(V )N I(V )root , Q(D)N I(D)root ] \u00b7 \u03b8. (6)\nHere we have omitted the arguments of the quality terms for brevity. \u03b8 \u2208 R3+ is a weight vector, which trades off accurate modelling of the policy, value function and derivatives. Each quality term is normalised by the respective impurity at the root node (i.e. before any partitions are made). This brings the three measures onto equivalent scales.\nCART follows a depth-first growth strategy, which is known to lead to suboptimal allocation of partitions. We depart from this by adopting a simple best-first strategy for selecting which leaf node to partition at each stage of tree growth. Again taking a hybrid view of impurity, we identify the best current leaf to partition, Lbest, as follows:\nLbest = argmax L\u2208L\n|IL| [\nI(A)L I(A)root , I(V )L I(V )root , I(D)L I(D)root\n] \u00b7 \u03b8, (7)\nwhere \u03b8 is the same as in equation (6). This approach prioritises the partitioning of leaves with high total impurity, weighted by their sample counts. Our criterion for terminating tree growth is a limit on the number of leaves, |L|.\nThe final feature of TRIPLETREE is the calculation of leaf-to-leaf transition probabilities. Let leaf(t) = L \u2208 L : t \u2208 IL be the leaf at which a sample t resides. For terminal samples in episodic MDPs, we define leaf(t) = \u2205. Furthermore, let I\u2217L = {t \u2208 IL : leaf(t \u2212 1) 6= L} be the subset of IL whose predecessors are not themselves in IL: the first in each sequence of successive observations that reside at L. We perform our calculations at the level of sequences, rather than individual samples, to avoid a double-counting effect. For each sequence-starting sample t \u2208 I\u2217L, we find the length of its successor sequence, seqlen(t), and the leaf containing the sample that breaks it, nextleaf(t):\nseqlen(t) = min{k : leaf(t+ k) 6= leaf(t)} ; nextleaf(t) = leaf(t+ seqlen(t)).\n(8)\nFor a given source leaf L and destination leaf L\u2032, we are interested in the subset of I\u2217L whose successor sequences are followed by a transition to L\u2032:\nI\u2217L\u2192L\u2032 = {t \u2208 I\u2217L : nextleaf(t) = L\u2032}. (9) Note that in episodic MDPs, I\u2217L\u2192\u2205 is well-defined and meaningful; it contains the members of I\u2217L for whom the episode terminates before a transition to another leaf. We can now compute the empirical probability that any given sequence in L ends in a transition to L\u2032, and the mean length of such a sequence:\nPL(L \u2032) = |I\u2217L\u2192L\u2032 | |I\u2217L| ; TL(L \u2032) = \u2211\nt\u2208I\u2217 L\u2192L\u2032\nseqlen(t) |I\u2217L\u2192L\u2032 | . (10)\nTransition probabilities and times are stored at their respective source leaves (L here) as further attributes alongside the predictions a\u0303L, v\u0303L and d\u0303L.\nIn summary, the key features of TRIPLETREE are: \u2022 Acceptance of the triplet observations inD, calculation of\nvalue and state derivatives for each sample, and storage of predicted values of these variables at each leaf. \u2022 A hybrid measure of partition quality Q\u2217, mediated by a weight vector \u03b8, which trades off the tree\u2019s abilities to predict the target agent\u2019s action, value and state derivatives.\n\u2022 Calculation of PL and TL to encode information about temporal dynamics in terms of leaf-to-leaf transitions.\n\u2022 A best-first growth strategy. A Python implementation of TRIPLETREE is available on GitHub at https://github.com/tombewley/TripleTree."
    },
    {
      "heading": "Related Work",
      "text": "Before the widespread adoption of deep neural network function approximators in RL, decision trees were used to create discretised state abstractions for tabular Q-learning algorithms (Uther and Veloso 1998). Tree models have also been used to learn a value function with the aim of creating an interpretable agent that performs well in the task environment (Pyeatt 2003; Roth et al. 2019), and also to mimic the value function of an existing black box policy as a route to explainability (Liu et al. 2018). This latter model also keeps track of transition probabilities between tree leaves, similarly to our approach. In (Jiang, Hwang, and Lin 2019), a decision tree is grown to minimise the impurity of environment state derivatives as part of a model-based RL framework, and in (Kim et al. 2015) a tree is optimised for sequential prediction by jointly minimising loss on consecutive timesteps. Other work has looked at approximating a recurrent neural network policy as a finite state machine for visualisation and analysis (Koul, Fern, and Greydanus 2018).\nWe know of one work that considers a hybrid action- and value-based tree impurity measure (Saghezchi and Asadpour 2010), but the idea is tangential to the main topic of the paper and its implications left unconsidered. We are unaware of any prior work that jointly represents the policy, value function and temporal dynamics in one decision tree, or considers the benefits of doing so for interpretability."
    },
    {
      "heading": "Prediction Tradeoff Experiments",
      "text": "We initially validate TRIPLETREE in a simple MDP with 2 state features and 2 discrete actions. This can be interpreted as a straight road, down which a vehicle agent can drive in either direction. The state features are position pos \u2208 [0, 3] (increasing left-to-right) and speed speed \u2208 [\u22120.1, 0.1], and the agent\u2019s action is a small positive or negative acceleration acc \u2208 {\u22120.001, 0.001}. Walls lie at the left and right ends of the road; a collision with either yields a reward of Rleft and Rright respectively and instantly terminates the simulation episode. The agent also receives reward in each non-terminal state in proportion to its absolute speed: Rspeed \u00d7 |speed|. Figure 2 summarises this information.\nFor a given Rleft, Rright, Rspeed, discount factor \u03b3 (we use \u03b3 = 0.99), and suitable discretisation of S (we use a 30 \u00d7 30 grid) an optimal policy can be found by dynamic programming (DP). We use the DP policies for four reward function variants as the target agents in our experiments. For each, we create a dataset D with 104 samples, by running randomly-initialised episodes of 100 timesteps.\nFigure 3 shows the result of growing a TRIPLETREE of up to 200 leaves using these four datasets, with various impurity weightings \u03b8. The columns show predictive losses for action (proportion of incorrect predictions), value (RMS error) and derivatives (dot product of RMS error with normalisation factors 1/\u03c3) as a function of leaf count. Naturally, different trees result when different \u03b8 vectors are used, and in all cases the lowest loss of each type is obtained by exclusively using the corresponding partition quality measure. Crucially, however, using an equal weighting (\u03b8 = [1/3, 1/3, 1/3]; black curves) offers a strong compromise between the three modes of prediction. For action and derivatives, equal weighting converges slower than exclusive weighting, but to virtually the same asymptotic loss, with the greatest disparity for trees with around 50-100 leaves. For value the gap is more significant. This indicates that in this MDP, there tend to be regions of S in which value varies significantly but the agent\u2019s action and state derivatives do not, thereby creating a conflict as to which leaves are worthy of partitioning. This phenomenon is most pronounced for the policy on the bottom row. Another notable trend is that partitioning on derivatives alone does very well in terms of action loss. This makes perfect sense once we realise that the agent\u2019s action (acc) is exactly the time derivative of one of the state features (speed).\nThis analysis begs the question: what is the best \u03b8 for this MDP? Ultimately, the answer depends on the intended application, but if general versatility is important then we may wish to minimise the worst of the three loss types. Our analysis along these lines (see Appendix A) suggests that in this MDP, a good compromise is attained by placing increased weight on value impurity: \u03b8 = [0.2, 0.6, 0.2].\nMultiattribute Visualisation in State Space Recall that in a decision tree, each leaf is associated with a hyperrectangle in the d-dimensional state space S, whose boundaries correspond to the partitions of its ancestor nodes. If d \u2208 {1, 2}, hyperrectangles reduce to lines or rectangles, which can be directly shown on axes corresponding to S itself (we return to the d > 2 case later in this paper). Each leaf can be coloured according to some salient attribute including, but not limited to, one of its three predictions.\nFigure 4 demonstrates the rich information conveyed by such visualisations in the road MDP. Each row of plots is generated from a single TRIPLETREE with 200 leaves, grown using the compromise weighting \u03b8 = [0.2, 0.6, 0.2]. In the first column, leaves are coloured by predicted action, revealing the optimal DP policies. The decision boundaries have varying complexity; interesting features include the isolated \u2018island\u2019 of positive acceleration in the top policy, which occurs when a crash with the right wall is unavoidable but positive Rspeed can be obtained by accelerating in the meantime, and the Z-shaped feature in the bottom policy, which causes the agent to oscillate around the centre of the road to avoid hitting either wall (reward = \u2212100). In the second column, colours denote the predicted value, which intuitively reflects the differing reward components. In general, low value corresponds to an imminent crash into a low-reward wall. Value is high when the agent approaches a high-reward wall and/or has plenty of room to accumulate positive Rspeed. For the bottom policy, value is high within a boundary of stability for the oscillatory motion, and low elsewhere. The plots of predicted derivatives in the third column differ from the others. Since this is a vector quantity, we show it as a quiver plot with an arrow for each leaf, whose direction and magnitude reflect the mean change in state between successive timesteps. The system changes more rapidly at high speeds, hence the longer arrows in these areas. Quiver plots provide an excellent high-level overview\nof system dynamics, particularly the locations of directional changes, cycles and regions of constancy. The fourth column colours leaves by derivative impurity, showing where in S we should be most confident in the model\u2019s derivative predictions. We can also create equivalent plots for action and value impurity. The final colouring attribute is sample density, computed by dividing the population of each leaf by its volume in S: the product of boundary lengths, normalised by each feature\u2019s range across D. This reveals where the policies spend the most time: in narrow arcs for the top two, and a tight central patch for the bottom one."
    },
    {
      "heading": "Rule-based Explanation",
      "text": "A popular interpretability feature of decision trees is the generation of textual explanations of outputs in terms of the decision rules applied. The simplest type of rule-based explanation is a factual one. For any leaf L \u2208 L, simply enumerating the boundaries of the leaf\u2019s hyperrectangle describes the region of S within which a constant prediction holds. Figure 5a shows a portion of the action visualisation for one of the 200-leaf trees from the previous section. The action for state St can be explained factually as follows:\n\u201cAction = 0.001 because pos \u2208 [1.1, 1.32] and speed \u2208 [0.021, 0.045].\u201d\nIt is argued that a more natural (Lipton 1990) and legally persuasive (Wachter, Mittelstadt, and Russell 2017) form of explanation is the counterfactual, which provides reasons why an alternative outcome, known as a foil, does not occur instead. In our context, the foil is an action other than the one taken in St. After enumerating all leaves which predict the foil action, we choose one, then find the change in state required to move to a location s\u2032 in that leaf. There is much debate about how to select s\u2032 from many alternatives (Guidotti et al. 2019; Poyiadzi et al. 2020), which often hinges on the notion of a minimal change in state. In TRIPLETREE, we use a two-stage process based on the L0 and L2 norms (Appendix B), which gives the following minimal counterfactual for the action in figure 5a:\n\u201cAction would = \u22120.001 if speed \u2265 0.045.\u201d\nFor the ordered data in D, a third form of explanation is temporal, which explains changes over time such as the action change from St to St+1 in figure 5a. This again takes a counterfactual perspective, although a subtlety is that using St+1 directly as a foil does not produce a minimal explanation. Our method for resolving this (also in Appendix B) finds s\u2032\u2032, the minimal foil from St subject to the constraint that the minimum bounding box (MBB) of s\u2032\u2032 and St+1 only intersects leaves with the same action as St+1. This yields:\n\u201cAction changed 0.001\u2192 \u22120.001 because pos \u2265 1.48.\u201d Temporal explanation could be extended to a longer sequence of samples by identifying all timesteps at which the action changes, explaining each as above, and combining them into a behavioural story using the conjunction \u201cthen\u201d.\nThe TRIPLETREE model allows us to similarly explain value predictions v\u0303L. Since value is continuous, a counterfactual could explain why value is less than or greater than a threshold, rather than defining a precise numerical foil which will only ever be made by one leaf at most. In figure 5b, we can see that the foil condition v \u2264 0.3 leads to the following minimal counterfactual for the value at St:\n\u201cValue would \u2264 0.3 if pos \u2265 2.64 and speed \u2265 0.024.\u201d"
    },
    {
      "heading": "Trajectory Simulation",
      "text": "For each leaf L, the derivative prediction d\u0303L and transition probabilities PL both describe the agent\u2019s movement through a region of S. These can be combined to construct behavioural trajectories which may never have occurred in D, but are nonetheless realistic given the agent\u2019s policy. This could be useful for answering targeted queries about how it navigates before, after and between states of interest.\nHere we consider the problem of finding such a trajectory between a given start leaf LS and an end leaf LE . We start by using Dijkstra\u2019s algorithm (Dijkstra 1959) to find a sequence of leaves LS\u2192E = (LS , L1, L2, ..., LE) that the agent moves through with nonzero probability. We define the (inverse) cost of each transition L \u2192 L\u2032 according to the probability PL(L\u2032), and the cost of a full sequence as the product of its constituent transitions. If valid sequences exist between LS and LE , Dijkstra\u2019s algorithm is guaranteed to find the highest-probability one first. If no solutions exist the algorithm returns a null result, which still gives valuable information about the non-reachability of states.\nTo generate a realistic trajectory through LS\u2192E , we solve a constrained optimisation problem to build a piecewise linear path whose segments are well aligned with the leaves\u2019 predicted derivative vectors. Concretely, for each leaf Lj \u2208\nLS\u2192E we initialise a path node pj on the hyperrectangle boundary, then perform gradient descent updates on all node locations to minimise the squared angular deviation between the path segments and the derivative vectors. When calculating angles, we normalise derivatives by the vector of inverse standard deviations across D, 1/\u03c3. The unconstrained update to pj is proportional to the partial derivative of the squared deviations for the segments before and after:\n\u2202\n\u2202pj\n[( cos\u22121\nxj \u00b7 dj ||xj ||||dj ||\n)2 + ( cos\u22121\nxj+1 \u00b7 dj+1 ||xj+1||||dj+1||\n)2] ,\n(11) where xj = (pj \u2212 pj\u22121) \u25e6 1/\u03c3 and dj = d\u0303Lj \u25e6 1/\u03c3 (\u25e6 denotes the Hadamard product). Rather than applying the update directly, we constrain the node to remain on its respective boundary, and always \u2018visible\u2019 from the previous node (i.e. it never moves to the far side of the boundary).\nFigure 6 contains trajectories generated by this searchthen-align method from the 200-leaf TRIPLETREES. Derivative arrows from nearby leaves indicate that the trajectories align well with agents\u2019 true motion in each region of S. The optimisation converges reliably and generally yields highquality trajectories, but is rather expensive and can get stuck in local minima. We are exploring refinements to our approach, and how it may be combined with temporal explanation to provide a textual summary of simulated trajectories."
    },
    {
      "heading": "Experiments in a Higher-dimensional MDP",
      "text": "We now deploy TRIPLETREE in a more complex MDP: LUNARLANDERCONTINUOUS-V2 within OpenAI Gym (Brockman et al. 2016). Here, the state s is an 8-dimensional vector [x, y, vx, vy, \u03c6, v\u03c6, cL, cR], which are respectively the horizontal and vertical position and velocity, orientation, and angular velocity of a landing craft, and binary flags as to whether its left and right legs contact the ground. The action spaceA = [\u22121, 1]2 is bounded and 2D. The first component is the throttle for the lander\u2019s vertical engine (\u22121 is off) and the second is a left-right side engine (0 is off). The reward is +100 for a safe landing in a landing zone and \u2212100 for a crash, and there is additional shaping reward to disincentivise fuel burn. The black box target policy for our model is a Soft Actor-Critic deep RL agent from Baselines Zoo (Raffin 2018), the highest-performing policy on that repository.\nUsing a dataset of 105 observations, we grow a TRIPLETREE of up to 1000 leaves with \u03b8 = [1, 1, 1] (the multivariate action space requires us to slightly modify the action impurity measure). Figure 7 shows how the three losses vary during growth on both the training set and a validation set. In this more complex MDP the prediction problem is harder \u2013 particularly, it seems, for derivatives \u2013 and losses do not re-\nduce to near zero, but as we shall see, the model still captures enough of the statistical properties of the system to deliver significant insight. We use the validation losses to inform early stopping and select the 450-leaf tree for evaluation.\nWith an 8D state space, it is nontrivial to create visualisations like those in figure 4. We suggest two ways forward: projection and slicing, which are detailed in Appendix C. In the former, we project leaf hyperrectangles onto a plane defined by two feature axes. Where multiple projections overlap, we compute a marginal value for the colouring attribute as a weighted average. This creates a partial dependence plot (PDP) of the attribute over the two features. Figure 8 contains a diagram of the method, and results from the 450- leaf tree. The upper five plots are PDPs for the x-y plane (landing zone shown in red). Notice how the main engine fires less at high altitudes. Sample density is high in a column above the landing zone, and on the ground where the policy makes slow positional corrections. The value and derivatives plots reveal that despite the MDP being symmetric, the agent obtains higher value when landing from the left, and takes a less curved route when doing so. The side engine plot has weaker trends, but the dark band around y = 1 (indicating the engine tends to fire to the left) may explain the wider landing approaches on the right. PDPs for the y-vy plane show hard thresholds in main engine activation at vy = 0 and y \u2248 1, and a U-shaped vertical speed profile. In the \u03c6-v\u03c6 plane, we see that the side engine fires in an intuitive way to maintain stability, that value is highest when v\u03c6 \u2248 0, and that the lander has pendulum-like dynamics aside from several leaves (purple) where v\u03c6 jumps. These likely reflect rapid changes in side engine activation.\nSlice visualisation involves taking an axis-aligned planar cross-section of S, and displaying all intersected leaves as rectangles. This creates an individual conditional expectation (ICE) plot of the colouring attribute rather than a PDP, which is useful for illustrating counterfactual explanations for which the true state and minimal foil differ in \u2264 2 features. Examples are shown in figure 9. These plots not only display the minimal state change required to realise the foil condition, but reveal some of the surrounding state space, giving an indication of the counterfactual\u2019s robustness.\nWe can also simulate trajectories in this MDP and visualise them in 2D. Figure 10 contains some examples. Rather than showing a single trajectory between two leaves, we display all possible trajectories between leaves within a start zone (blue/orange) and end zone (red), demonstrating the distribution of paths taken by the agent. The first plot clarifies our prior observation that approaches from the right are wider, and shows that they occasionally miss the landing zone altogether. Thereafter, the lander must \u2018shuffle\u2019 along the ground into position; a major source of lost value. Similarly, the second plot confirms that the lander\u2019s vertical speed profile is U-shaped, and in fact very close to quadratic. The final plot is the most interesting. If rotated to the left (\u03c6 < 0.5), the lander\u2019s return to a stable, neutral orientation is direct and overdamped. From the right, trajectories back to neutrality tend to overshoot; a classic indicator of a poorly-tuned controller. This is further evidence that despite obtaining high reward, the policy is chronically asymmetric."
    },
    {
      "heading": "Conclusion",
      "text": "Ga\u0308rdenfors asserts that the aggregation of high-dimensional observational data into discrete convex regions, based similarity judgements, is a general route towards human understanding of complex systems. We consider TRIPLETREE to be a practical demonstration of this phenomenon; a versatile representational tool for delivering practical insight into the behaviour of black box autonomous agents through multivariate prediction, visualisation and rule-based explanation. In ongoing work, we continue to explore the potential of this representation, refining and expanding our methods of analysis and deploying the model in more challenging MDPs."
    },
    {
      "heading": "Appendix A: Worst-Case Loss Analysis",
      "text": "To more deeply understand the tradeoff between the three types of loss in the 2D road MDP, we consider 21 equallyspaced weighting vectors \u03b8. For each, we calculate which loss is worst as a ratio of the loss from a tree with just one leaf, which predicts the dataset average. The results are plotted in barycentric coordinates (simplex plots) in figure 1. The prevalence of red dots throughout the range of tree sizes shows that value prediction is weakest across most of the space of weightings. A notable exception is the righthand edge of the simplex, where derivative weight is 0 and that loss is accordingly the worst. The greyscale heatmaps show the magnitude of the worst loss ratio (interpolated using Matplotlib\u2019s LinearTriInterpolator), which as a general rule is lower towards the centre of the simplex, but also towards the right-hand side, where the value weight is higher. These results point to the conclusion that in this MDP, value should be up-weighted in the hybrid quality metric if minimising the worst-case loss ratio is important. From looking at the various heatmaps, we suggest that \u03b8 = [0.2, 0.6, 0.2] is close to the best possible compromise. We use this weighting throughout the rest of the main paper, aside from for the final section on LUNARLANDER.\n1. 5,\n0 , 1\n10 leaves\n20 leaves 50 leaves 100 leaves 200 leaves 1\n0\n[1,0,0]\n[0,1,0][0,0,1]"
    },
    {
      "heading": "Appendix B: Algorithms for Explanation",
      "text": ""
    },
    {
      "heading": "Finding the Minimal Counterfactual Foil",
      "text": "Let L\u2217 be the leaf where the observation to be explained, St, ends up after being propagated through the tree, and let z\u0303L\u2217 generically represent the leaf\u2019s predicted attribute. In a TRIPLETREE this attribute may be an action, value or derivative vector. For counterfactual explanation, we specify a foil condition F , which is used to identify a set of leaves in which the foil state may reside. If the attribute is a discrete\naction, the foil simply specifies another member of the action space. For continuous action or value explanation, the foil is an inequality which is not satisfied by z\u0303L\u22171. In either case, let L\u2032 = {L \u2208 L : F (L) = True} be the subset of leaves satisfying F .\nFor each L \u2208 L\u2032, let l(f)L and u (f) L be the lower and upper boundaries of that leaf\u2019s hyperrectangle along feature f , which correspond to partitions made at its ancestor nodes. One or both of these boundaries will be undefined (effectively infinite) if none of the ancestors partition along f . In such cases we replace a lower bound with the minimum value of the feature across all samples inD, denoted by s(f)min, and replace an upper bound with the maximum value s(f)max.\nThe location in L which is closest2 to the input state St, denoted by sL, lies on the boundary. It is defined on a feature-wise basis as follows:\ns (f) L =    u (f) L if S (f) t > u (f) L , l (f) L if S (f) t < l (f) L ,\nS (f) t otherwise.\n(1)\nLet \u03b4L = (sL \u2212 St) \u00b7 \u03b1 be the vector from St to sL, normalised by \u03b1, which is the vector of the reciprocal minmax ranges of the state features across the training setD. As discussed in the main paper, such normalisation is important to bring the state features onto equivalent scales.\nThe task of finding a minimal foil consists in selecting one element of the set {sL : L \u2208 L\u2032} by consideration of the corresponding \u03b4 vectors. Since this set is non-convex in general, there is not one choice which is unambiguously closest to St; this depends on which norm we apply to the \u03b4s. Different norms have different advantages. The 2-norm is the most common and intuitive distance metric in vector spaces, especially those with a physical interpretation, but does not incentivise sparsity, which would allow us to give a more compact explanation (fewer feature changes means fewer clauses in the textual summary). Conversely, the 0- norm only measures sparsity, and cannot differentiate between multiple options with the same number of nonzero elements. The 1-norm offers a popular compromise, but we do not use this in TRIPLETREE. Instead, we filter first by 0-norm\nS = {sL : ||\u03b4L||0 = inf{||\u03b4L\u2032 ||0 : L\u2032 \u2208 L\u2032}}, (2) then by 2-norm to find the minimal foil\ns\u2032 = sL \u2208 S : ||\u03b4L||2 = inf{||\u03b4L\u2032 ||2 : \u03b4L\u2032 \u2208 L\u2032}. (3) This two-stage approach enforces a strict priority of the 0- norm over the 2-norm; a foil state is only considered if its \u03b4 vector is at least as sparse as any of the others. We suggest that this is desirable behaviour, because it puts the strongest possible emphasis on compact explanations, while still punishing \u03b4 vectors with very large (Euclidean) magnitudes. It\n1For counterfactual explanation of derivative vectors we would need to specify an inequality for one or more features individually. We do not consider this case here.\n2Because hyperrectangles are convex, sL is unambiguously the closest point to St as measured by any p-norm.\nalso makes it simple, if required, to specify a hard threshold on sparsity, allowing us to answer questions of the form \u201ccan the foil condition be realised by changing \u2264 n features?\u201d. The method is summarised diagrammatically in figure 2, for which the generic textual explanation is\n\u201cF would be satisfied if f2 \u2264 s(f2)L4 .\u201d"
    },
    {
      "heading": "Adaptation for Temporal Explanation",
      "text": "In a temporal explanation scenario we are given two consecutive observations St and St+1 which fall into different leaves, so that St+1 satisfies some foil condition F (e.g. different discrete action, inequality of value) with respect to St. Simply using St+1 itself as the foil is uninformative and tautological, and we should again consider the notion of a minimal foil. However, merely following the method described above does not solve the problem, as illustrated in figure 3.\nHere L\u2032 = {L1, ..., L6}, but every one of sL1 , ..., sL6 is a suboptimal foil. In regular counterfactual explanation, sL1 would be minimal, but it implies moving in the opposite direction from St+1 so does not lie on any plausible path between the two observations. sL2 and sL3 are at least closer to St+1 than St is, but in both cases there is not an unbroken path of foil leaves connecting them to St+1, so they do not represent sufficient conditions for the change in attribute. More subtly, the same is true for sL4 ; the orange shading highlights a region of S below and to the right of sL4 , but which is not part of any L \u2208 L\u2032. The existence of this region means that a counterfactual of the form\n\u201cF became satisfied because f1 \u2265 s(f1)L4 and f2 \u2264 s (f2) L4 .\u201d\nwould be misleading, since it is possible to satisfy the stated conditions in a state between St and St+1 while not satisfying F . In contrast, both are sL5 and sL6 are acceptable according to this unbroken path criterion, and since the former is closer to St according to the 2-norm, it may initially appear that this is the best possible foil.\nHowever, we can do better than this. We propose that the best foil to use for temporal explanation, s\u2032\u2032, is the location\ninside one of the foil leaves which is minimal from St, subject to the constraint that the minimum axis-aligned bounding box (MBB) of s\u2032\u2032 and St+1 only intersects leaves in L\u2032. This constraint ensures that the generated counterfactual explanation is never misleading in the sense outlined above. Figure 4 demonstrates the best foil state in our example, for which the textual explanation is \u201cF became satisfied because f1 \u2265 s\u2032\u2032(f1) and f2 \u2264 s\u2032\u2032(f2).\u201d\nFinding s\u2032\u2032 in practice is not trivial, and we have currently only implemented a method for 2D state spaces, outlined in algorithm 1. This algorithm returns R, a set of rectangles tiling the region of S between St and St+1 which satisfies the MBB constraint. We then apply the equations of the preceding section3 toR instead of the underlying foil leaves L\u2032, thereby obtaining the desired foil state. In the simple example in figure 4, R has just a single element \u2013 the rectangle shown in blue \u2013 but in general the region satisfying the MBB constraint may be rather more complex."
    },
    {
      "heading": "Appendix C: Visualisation for High-dimensional MDPs",
      "text": ""
    },
    {
      "heading": "Projection",
      "text": "In the projection visualisation method, we choose two feature axes and project leaf hyperrectangles onto the plane de-\n3The placeholder function \u201crectangle\u201d in algorithm 1 creates a representation of each rectangle which can be fed into these equations in the same way as the leaves themselves.\nfined by these axes. By taking a population-weighted average of the attributes from overlapping leaf projections, we effectively create a partial dependence plot (PDP).\nLet B(f) denote the complete sequence of hyperrectangle boundaries in the tree along feature f , sorted by value:\nB(f) = ( bi \u2208 \u22c3\nL\u2208L {l(f)L , u (f) L }\n) ,\nsubject to \u2200i \u2208 {2..|B(f)|} B(f)i > B (f) i\u22121. (4)\nwhere l(f)L and u (f) L are defined as above, with any undefined boundaries again replaced by s(f)min or s (f) max.\nAssume without loss of generality that the two features onto which we are projecting are f1 and f2. The planar region s(f1)min \u2264 s(f1) \u2264 s (f1) max, s (f2) min \u2264 s(f2) \u2264 s (f2) max can be tiled by a (|B(f1)| \u2212 1) \u00d7 (|B(f2)| \u2212 1) grid of rectangles, each of which is constructed from two pairs of consecutive boundaries from B(f1) and B(f2). Steps 1 and 2 in figure 5 illustrate this reasoning.\nIf each rectangular area were to be simultaneously extruded along each of the d \u2212 2 orthogonal feature axes, the volume swept (which we call a core) would itself be a d-dimensional hyperrectangle which intersects with at least one of the leaf hyperrectangles. For each i \u2208 {2..|B(f1)|}, j \u2208 {2..|B(f2)|}, the set of intersected hyperrectangles can be identified as\nLi,j = { L \u2208 L :\n(l (f1) L \u2264 B (f1) i\u22121 ) \u2227 (u (f1) L \u2265 B (f1) i ) \u2227\n(l (f2) L \u2264 B (f2) j\u22121) \u2227 (u (f2) L \u2265 B (f2) j )\n} . (5)\nSteps 3 and 4 in figure 5 show the process of extruding a rectangle from the f1-f2 plane and identifying intersections. In this case the core intersects the three leaves labelled A, B and C.\nPrecisely how we proceed from this point depends which attribute we wish to visualise. For the sake of brevity, we assume we are visualising predicted actions a\u0303 and that the action space is continuous4. For each rectangle in the f1-f2 plane, identified by boundary indices i and j as above, we\n4The process is identical for visualising value predictions and impurities, and for derivative predictions we perform the aver-\neffectively marginalise out the d\u2212 2 orthogonal dimensions by taking a weighted mean of the predicted actions from the intersected leaves Li,j . The weight for each leaf L \u2208 Li,j , is jointly determined by its population |IL| and the degree to which its hyperrectangle overlaps with the core. We can therefore define the projected action prediction for rectangle i, j as\na\u0303i,j =\n\u2211 L\u2208Li,j w (i,j) L a\u0303L\n\u2211 L\u2208Li,j w (i,j) L\n(6)\nwhere\nw (i,j) L = |IL|\n[ B(f1)i \u2212 B (f1) i\u22121\nu (f1) L \u2212 l (f1) L\n][ B(f2)j \u2212 B f2) j\u22121\nu (f1) L \u2212 l (f1) L\n] . (7)\nDue to the way in which the underlying tiling of the f1-f2 plane is defined, the core must contain exactly zero boundaries along either f1 or f2, so both fractions in the formula for w(i,j)L are always \u2264 1. This part of the process is illustrated by steps 5 and 6 of figure 5, and step 7 shows the result of repeating for all remaining rectangles on the plane, thereby creating the 2D visualisation.\nThe key assumption behind this core-and-average approach to projection is that samples are close to uniformly distributed within leaf hyperrectangles, so that each w(i,j)L is an unbiased estimate of the number of samples from L within the core, and each a\u0303L is an unbiased estimate of the mean action for those samples."
    },
    {
      "heading": "Slicing",
      "text": "In the slicing visualisation method we again choose two features to visualise over, f1 and f2. For each remaining feature fi (where i \u2208 {3..d}), we specify a single threshold between s (fi) min and s (fi) max, denoted by s (fi) slice . The set of thresholds defines an axis-aligned planar cross-section through S, which aging on an elementwise basis. For discrete actions, we cannot take a weighted mean so instead add up the per-action counts count(IL, a) for overlapping leaves, weighted by their overlap proportions, then visualise the modal action for each rectangle. For density, we replace the population factor |IL| in equation 7 with the leaf\u2019s feature-scaled volume in S, as defined in the main paper. We also find that it is best to use a logarithmic colour map for density plots, since this attribute can vary over many orders of magnitude between leaves.\nintersects a subset of the leaves. Here we do not have to handle overlaps, and can display the rectangular cross-sections of the intersected leaves directly. This creates an individual conditional expectation (ICE) plot.\nThe slicing process is straightforward. Given the thresholds \u00b5(f3), ..., \u00b5(fd), we simply need to identify the subset of intersected leaves:\nLslice = d\u22c2\ni=3\n{ L \u2208 L : (l(fi)L \u2264 s (fi) slice \u2227 (u (f1) L \u2265 s (fi) slice } .\n(8) We then visualise each L \u2208 Lslice as a rectangle with boundaries at l(f1)L , u (f1) L , l (f2) L and u (f2) L , and coloured according to its attribute."
    },
    {
      "heading": "A Note on Unifying Projection and Slicing",
      "text": "In the preceding discussion, we have presented projection and slicing as two distinct visualisation methods, but in reality it is possible to smoothly transition between the two. Starting from the projection method as described, we achieve this by permitting the core extrusion along feature fi to be limited between two thresholds s (fi) low \u2265 s (fi) min and s (fi) high \u2264 s (fi) max. This allows use to visualise projections from only those leaves within a hyperrectangular subset of S rather than the entire space. From this point, it is easy to see that slicing results from the limiting case where\ns (fi) low = s (fi) high = s (fi) slice \u2200i \u2208 {3..d}. (9)\nAlgorithm 1: Finding the region of S satisfying the MBB constraint (for d = 2 only)\nInput: Observations St and St+1; the sets of all leaves L and foil leaves L\u2032. Result: A set of rectanglesR.\nInitialise s1 \u2190 St+1,R \u2190 {} while True do\ns2 \u2190 s1 // Expand rectangle along both\nfeature axes.\ns (1) 2 \u2190 extend(1, 2, s1, s2) s (2) 2 \u2190 extend(2, 1, s1, s2) // Break if no expansion.\nif s(2)1 = s (2) 2 break // Store rectangle and move s1 along axis 2 to reset R \u2190 R\u222a {rectangle(s1, s2)} s (2) 1 \u2190 s (2) 2\nend returnR\nFunction extend(a, b, s1, s2): /* Given a rectangle with opposite\ncorners at (s1, s2), move s2 along axis a until the rectangle either intersects a non-foil leaf \u2208 Lnf or extends beyond S(a)t . */\nl, u\u2190 inf{s(b)1 , s (b) 2 }, sup{s (b) 1 , s (b) 2 } Lnf \u2190 {L \u2208 L \\ L\u2032 : l(b)L \u2264 u \u2227 u (b) L \u2265 l} if S(a)t > S (a) t+1 then\n// Extend in the positive direction. return inf{l(a)L : L \u2208 Lnf \u2227 l (a) L \u2265 s (a) 1 } \u222a {S (a) t }\nelse // Extend in the negative\ndirection. return sup{u(a)L : L \u2208 Lnf \u2227u (a) L \u2264 s (a) 1 }\u222a{S (a) t }\nend"
    }
  ],
  "title": "TRIPLETREE: A Versatile Interpretable Representation of Black Box Agents and their Environments",
  "year": 2020
}
