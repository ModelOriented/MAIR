{"abstractText": "During the first step of practical reasoning, i.e. deliberation or goals selection, an intelligent agent generates a set of pursuable goals and then selects which of them he commits to achieve. Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions. In the context of goals selection, agents should be able to explain the reasoning path that leads them to select (or not) a certain goal. In this article, we use an argumentation-based approach for generating explanations about that reasoning path. Besides, we aim to enrich the explanations with information about emerging conflicts during the selection process and how such conflicts were resolved. We propose two types of explanations: the partial one and the complete one and a set of explanatory schemes to generate pseudo-natural explanations. Finally, we apply our proposal to the cleaner world scenario.", "authors": [{"affiliations": [], "name": "Mariela Morveli-Espinoza"}, {"affiliations": [], "name": "Cesar Augusto Tacla"}], "id": "SP:8861678a432c9ae0621a30b1837f0756a90bad3f", "references": [{"authors": ["Michael J Wooldridge"], "title": "Reasoning about rational agents", "venue": "MIT press,", "year": 2000}, {"authors": ["Cristiano Castelfranchi", "Fabio Paglieri"], "title": "The role of beliefs in goal dynamics: Prolegomena to a constructive theory of intentions", "year": 2007}, {"authors": ["Leila Amgoud", "Caroline Devred", "Marie-Christine Lagasquie-Schiex"], "title": "A constrained argumentation system for practical reasoning", "venue": "In Proceedings of the International Workshop on Argumentation in Multi-Agent Systems,", "year": 2008}, {"authors": ["M. Mariela Morveli-Espinoza", "Juan Carlos Nieves", "Ayslan Trevizan Possebom", "Josep Puyol-Gruart", "Cesar Augusto Tacla"], "title": "An argumentation-based approach for identifying and dealing with incompatibilities among procedural goals", "venue": "International Journal of Approximate Reasoning,", "year": 2019}, {"authors": ["John Thangarajah", "Lin Padgham", "Michael Winikoff"], "title": "Detecting and avoiding interference between goals in intelligent agents", "venue": "In Proceedings of the 23rd International Joint Conference on Artificial Intelligence. Morgan Kaufmann Publishers,", "year": 2003}, {"authors": ["Nick AM Tinnemeier", "Mehdi Dastani", "John-Jules Ch Meyer"], "title": "Goal selection strategies for rational agents", "venue": "In International Workshop on Languages, Methodologies and Development Tools for Multi-Agent Systems,", "year": 2007}, {"authors": ["Maicon Rafael Zatelli", "Jomi Fred H\u00fcbner", "Alessandro Ricci", "Rafael H Bordini"], "title": "Conflicting goals in agentoriented programming", "venue": "In Proceedings of the 6th International Workshop on Programming Based on Actors, Agents, and Decentralized Control,", "year": 2016}, {"authors": ["M Morveli-Espinoza", "Juan Carlos Nieves", "A Possebom", "Josep Puyol-Gruart", "Cesar Augusto Tacla"], "title": "An argumentation-based approach for identifying and dealing with incompatibilities among procedural goals", "venue": "International Journal of Approximate Reasoning,", "year": 2019}, {"authors": ["Diego C Mart\u0131nez", "Alejandro J Garc\u0131a", "Guillermo R Simari"], "title": "Progressive defeat paths in abstract argumentation frameworks", "venue": "In Proceedings of the 19th Canadian Conference on Artificial Intelligence,", "year": 2006}, {"authors": ["Sanjay Modgil", "Henry Prakken"], "title": "The ASPIC+ framework for structured argumentation: a tutorial", "venue": "Argument & Computation,", "year": 2014}, {"authors": ["Anthony Hunter"], "title": "Base logics in argumentation", "venue": "In COMMA, pages 275\u2013286,", "year": 2010}, {"authors": ["Philippe Besnard", "Anthony Hunter"], "title": "Argumentation based on classical logic. In Argumentation in artificial intelligence, pages 133\u2013152", "year": 2009}, {"authors": ["Phan Minh Dung"], "title": "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games", "venue": "Artificial intelligence,", "year": 1995}, {"authors": ["Esteban Guerrero", "Juan Carlos Nieves", "Helena Lindgren"], "title": "An activity-centric argumentation framework for assistive technology aimed at improving health", "venue": "Argument & Computation,", "year": 2016}, {"authors": ["Sule Anjomshoae", "Amro Najjar", "Davide Calvaresi", "Kary Fr\u00e4mling"], "title": "Explainable agents and robots: Results from a systematic literature review", "venue": "In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems,", "year": 2019}, {"authors": ["Joost Broekens", "Maaike Harbers", "Koen Hindriks", "Karel Van Den Bosch", "Catholijn Jonker", "John-Jules Meyer"], "title": "Do you get it? user-evaluated explainable bdi agents", "venue": "In German Conference on Multiagent System Technologies,", "year": 2010}, {"authors": ["Maaike Harbers", "Karel van den Bosch", "John-Jules Meyer"], "title": "Design and evaluation of explainable bdi agents", "venue": "In 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,", "year": 2010}, {"authors": ["Pat Langley", "Ben Meadows", "Mohan Sridharan", "Dongkyu Choi"], "title": "Explainable agency for intelligent autonomous systems", "venue": "In Twenty-Ninth IAAI Conference,", "year": 2017}, {"authors": ["Isabel Sassoon", "Elizabeth Sklar", "Nadin Kokciyan", "Simon Parsons"], "title": "Explainable argumentation for wellness consultation", "venue": "In Proceedings of 1st International Workshop on eXplanable TRansparent Autonomous Agents and Multi-Agent Systems (EXTRAAMAS2019),", "year": 2019}, {"authors": ["Mariela Morveli-Espinoza", "Ayslan Possebom", "Cesar Augusto Tacla"], "title": "Argumentation-based agents that explain their decisions", "venue": "In 2019 8th Brazilian Conference on Intelligent Systems (BRACIS),", "year": 2019}, {"authors": ["Mariela Morveli-Espinoza", "Ayslan Trevisam Possebom", "Josep Puyol-Gruart", "Cesar Augusto Tacla"], "title": "Argumentation-based intention formation process", "venue": "DYNA, 86(208):82\u201391,", "year": 2019}], "sections": [{"text": "Keywords Goal selection \u00b7 Explainable agents \u00b7 Formal argumentation"}, {"heading": "1 Introduction", "text": "Practical reasoning means reasoning directed towards actions, i.e. it is the process of figuring out what to do. According to Wooldridge [1], practical reasoning involves two phases: (i) deliberation, which is concerned with deciding what state of affairs an agent wants to achieve, thus, the outputs of deliberation phase are goals the agent intends to pursue, and (ii) means-ends reasoning, which is concerned with deciding how to achieve these states of affairs. The first phase is also decomposed in two parts: (i) firstly, the agent generates a set of pursuable goals1, and (ii) secondly, the agent chooses which goals he will be committed to bring about. In this paper, we focus on the first phase, that is, goals selection.\nGiven that an intelligent agent may generate multiple pursuable goals, some conflicts among these goals could arise, in the sense that it is not possible to pursue them simultaneously. Thus, a rational agent selects a set of non-conflicting goals based in a criterion or a set of criteria. There are many researches about identifying and resolving such conflict in order to determine the set of pursued goals (e.g., [3][4][5][6][7]). However, to the best of our knowledge, none of these\n1Pursuable goals are also known as desires and pursued goals as intentions. In this work, we consider that both are goals at different stages of processing, like it was suggested in [2].\nar X\niv :2\n00 9.\n06 13\n1v 1\napproaches gives explanations about the reasoning path to determine the final set of pursued goals. Thus, the returned outcomes can be negatively affected due to the lack of clarity and explainability about their dynamics and rationality.\nIn order to better understand the problem, consider the well-know \u201ccleaner world\u201d scenario, where a set of robots (intelligent agents) has the task of cleaning a dirty environment. The main goal of all the robots is to have the environment clean. Besides cleaning, the robots have other goals such as recharging their batteries or being fixed. Suppose that at a given moment one of the robots (let us call him BOB) detects dirt in slot (5,5); hence, the goal \u201ccleaning (5,5)\u201d becomes pursuable. On the other hand, BOB also auto-detects a technical defect; hence, the goal \u201cbe fixed\u201d also becomes pursuable. Suppose that BOB cannot commit to both goals at the same time because the plans adopted for each goal lead to an inconsistency. This means that only one of the goals will become pursued. Suppose that he decides to fix its technical defect instead of cleaning the perceived dirt. During the cleaning task or after the work is finished, the robot can be asked for an explanation about his decision. It is clear that it is important to endow the agents with the ability of explaining their decisions, that is, to explain how and why a certain pursuable goal became (or not) a pursued goal.\nThus, the research questions that are addressed in this paper are: (i) how to endow intelligent agents with the ability of generating explanations about their goals selection process? and (ii) how to improve the informational quality of the explanations?\nIn addressing the first question, we will use arguments to generate and represent the explanations. At this point, it is important to mention that in this article, argumentation is used in two different ways. Firstly, argumentation will be used in the goals selection process. The input to this process is a set of possible conflicting pursuable goals such that each one has a preference value and a set of plans that allow the agent to achieve them, and the output is a set of pursued goals. We will base on the work of Morveli-Espinoza et al. [4] for this process. One important contribution given in [4] is the computational formalization of three forms of conflicts, namely terminal incompatibility, resource incompatibility, and superfluity, which were conceptually defined in [2]. The identification of conflicts is done by using plans, which are represented by instrumental arguments2. These arguments are compared in order to determine the form of conflict that may exist between them. The set of instrumental arguments and the conflict relation between them make up an Argumentation Framework (AF). Finally, in order to resolve the conflicts, an argumentation semantics is applied. This semantics is a function that takes as input an AF and returns those non-conflicting goals the agent will commit to. Secondly, argumentation is used in the process of explanation generation. The input to this process is the AF mentioned above and the set of pursued goals and the output is a set of arguments that represent explanations. The arguments constructed in this part are not instrumental ones, that is, they do not represent plan but explanations. Regarding the second question, we will use the information in instrumental arguments for enriching explanations about the form(s) of conflict that exists between two goals.\nNext section focuses on the knowledge representation and the argumentation process for goal selection. Section 3 presents the argumentation process for generating explanations. Section 4 is devoted to the application of the proposal to the cleaner world scenario. Section 5 presents the main related work. Finally, Section 6 is devoted to conclusions and future work."}, {"heading": "2 Argumentation Process for Goals Selection", "text": "In this section, we will present part of the results of the article of Morveli-Espinoza et al. [4], on which we will base to construct the explanations. Since we want to enrich the explanations, we will increase the informational capacity of some of the results.\nFirstly, let L be a first-order logical language used to represent the mental states of the agent, ` denotes classical inference, and \u2261 the logical equivalence. Let G be the set of pursuable goals, which are represented by ground atoms of L and B be the set of beliefs of the agent, which are represented by ground literals3 of L. In order to construct instrumental arguments, other mental states are necessary (e.g. resources, actions, plan rules); however, they are not meaningful in this article. Therefore, we will assume that the knowledge base (denoted by K) of an agent includes such mental states, besides his beliefs.\nAccording to Castelfranchi and Paglieri [2], three forms of incompatibility could emerge during the goals selection: terminal, due to resources, and superfluity4. Morveli-Espinoza et al. [4] tackled the problems of identifying and\n2An instrumental argument is structured like a tree where the nodes are planning rules whose premise is made of a set of sub-goals, resources, actions, and beliefs and its conclusion or claim is a goal, which is the goal achieved by executing the plan represented by the instrumental argument.\n3Literals are atoms or negation of atoms (the negation of an atom a is denoted \u00aca). 4Hereafter, terminal incompatibility is denoted by t, resource incompatibility by r, and superfluity by s.\nresolving these three forms of incompatibilities. In order to identify these incompatibilities the plans that allow to achieve the goals in G are evaluated. Considering that in their proposal each plan is represented by means of instrumental arguments, as a result of the identification problem, they defined three AFs (one for each form of incompatibility) and a general AF that involves all of the instrumental arguments and attacks of the three forms of incompatibility.\nDefinition 1 (Argumentation frameworks) Let ARGins be the set of instrumental arguments that an agent can build from K5. A x-AF is a pair AFx = \u3008ARGx,Rx\u3009 (for x \u2208 {t, r, s}) where ARGx \u2286 ARGins andRx is the binary relation Rx \u2286 ARGins \u00d7 ARGins that represents the attack between two arguments of ARGins, so that (A,B) \u2208 Rx denotes that the argument A attacks the argument B.\nSince we want to improve the informational quality of explanations, we modify the general AF proposed in [4] by adding a function that returns the form of incompatibility that exists between two instrumental arguments. Thus, an agent will not only be able to indicate that there is an incompatibility between two goals but he will be able to indicate the form of incompatibility.\nDefinition 2 (General Argumentation Framework) Let ARGins be a set of instrumental arguments that an agent can build from K. A general AF is a tuple AFgen = \u3008ARGins,Rgen, f_INCOMP\u3009, where Rgen = Rt \u222a Rr \u222a Rs and f_INCOMP : Rgen \u2192 2{t,r,s}.\nExample 1 Recall the cleaner world scenario that was presented in Introduction where agent BOB has two pursuable goals, which can be expressed as clean(5, 5) and be(fixed) in language L. Consider that there are two instrumental arguments whose claim is clean(5, 5), namely A that has a sub-argument E whose claim is pickup(5, 5) and C that has a sub-argument D whose claim is mop(5, 5). Besides, there are two instrumental arguments whose claim is be(fixed), namely B that has a sub-argument H whose claim is be(in_workshop) and F that does not have any sub-argument.\nRecall also that terminal incompatibility was also exemplified. In order to exemplify the other forms of incompatibility and generate the general AF for this scenario, consider the following situations:\n\u2022 BOB has 90 units of battery. He needs 60 units for achieving C, he needs 70 units for achieving A, he needs 30 units for achieving B, and he does not need battery for achieving F because the mechanic can go to his position. We can notice that there is a conflict between A and B and consequently between their sub-arguments.\n\u2022 As can be noticed, there are two instrumental arguments whose claim is clean(5, 5) and two instrumental arguments whose plan is be(fixed). It would be redundant to perform more than one plan to achieve the same goal, this means that arguments with the same claim are conflicting due to superfluity. This conflict is also extended to their sub-arguments.\nWe can now generate the general AF for the cleaner world scenario: AFgen = \u3008{A,B,C,D,E, F,H},Rgen, f_INCOMP\u3009 where Rt = {(A,B), (B,A), (E,B), (B,E), (E,H), (H,E), (A,H), (H,A), (C,B), (B,C), (D,B), (B,D), (D,H), (H,D), (C,H), (H,C)}, Rr = {(A,B), (B,A), (E,B), (B,E), (A,H), (H,A), (E,H), (H,E)}, andRs = {(C,A), (A,C), (E,D), (D,E), (C,E), (E,C), (A,D), (D,A), (F,B), (B,F ), (F,H), (H,F )}. Figure 1 shows the graph representation.\nSo far, we have referred to instrumental arguments \u2013 which represent plans \u2013 however, since the selection is at goals level, it is necessary to generate an AF where arguments represent goals. In order to generate this framework, it is necessary to define when two goals attack each other. This definition is based on the general attack relation Rgen, which includes the three kinds of attacks that may exist between arguments. Thus, a goal g attacks another goal g\u2032 when all the instrumental arguments for g (that is, the plans that allow to achieve g) have a general attack relation with all the instrumental arguments for g\u2032. This attack relation between goals is captured by the binary relationRG \u2286 G \u00d7 G. We denote with (g, g\u2032) the attack relation between goals g and g\u2032. In other words, if (g, g\u2032) \u2208 RG means that goal g attacks goal g\u2032.\nDefinition 3 (Attack between goals) Let AFgen = \u3008ARGins,Rgen, f_INCOMP\u3009 be a general AF, g, g\u2032 \u2208 G be two pursuable goals, ARG_INS(g)6, ARG_INS(g\u2032) \u2286 ARGins be the set of arguments for g and g\u2032, respectively. Goal g attacks goal g\u2032 when \u2200A \u2208 ARG_INS(g) and \u2200A\u2032 \u2208 ARG_INS(g\u2032) it holds that (A,A\u2032) \u2208 Rgen or (A\u2032, A) \u2208 Rgen.\n5For further information about how instrumental arguments are built, the reader is referred to [4]. 6ARG_INS(g) denotes all the instrumental arguments that represent plans that allow to achieve g.\nOnce the attack relation between two goals was defined, it is also important to determine the forms of incompatibility that exist between any two conflicting goals. The function INCOMP_G(g, g\u2032) will return the set of forms of incompatibility between goals g and g\u2032. Thus, if (g, g\u2032) \u2208 RG, then \u2200(A,A\u2032) \u2208 Rgen and \u2200(A\u2032, A) \u2208 Rgen where A \u2208 ARG_INS(g) and A\u2032 \u2208 ARG_INS(g\u2032), INCOMP_G(g, g\u2032) = \u22c3 f_INCOMP((A,A\u2032)) \u222a f_INCOMP((A\u2032, A)). We can now define an AF where arguments represent goals.\nDefinition 4 (Goals AF) An argumentation-like framework for dealing with incompatibility between goals is a tuple GAF = \u3008G,RG, INCOMP_G, PREF\u3009, where: (i) G is a set of pursuable goals, (ii) RG \u2286 G \u00d7 G, (iii) INCOMP_G : RG \u2192 2{t,r,s}, and (iv) PREF : G \u2192 (0, 1] is a function that returns the preference value of a given goal such that 1 stands for the maximum value.\nHitherto, we have considered that all attacks are symmetrical. However, as can be noticed goals have a preference value, which indicates how valuable each goal is for the agent. Therefore, depending on this preference value, some attacks may be considered successful. This means that the symmetry of the relation attack may be broken.\nDefinition 5 (Successful attack)7 Let g, g\u2032 \u2208 G be two goals, we say that g successfully attacks g\u2032 when (g, g\u2032) \u2208 RG and PREF(g) > PREF(g\u2032).\nLet us denote with GAFsc = \u3008G,RGsc, INCOMP_G, PREF\u3009 the AF that results after considering the successful attacks.\nThe next step is to determine the set of goals that can be achieved without conflicts, which can also be called acceptable goals and in this article, they can be explicitly called pursued goals. With this aim, it has to be applied an argumentation semantics. Morveli-Espinoza et al. did an analysis about which semantics is more adequate for this problem. They reached to the conclusion that the best semantics is based on conflict-free sets, on which a function is applied. Next we present the definition given in [4] applied to the Goals AF.\nDefinition 6 (Semantics) Given a GAFsc = \u3008G,RGsc, INCOMP_G, PREF\u3009. Let SCF be a set of conflict-free sets calculated from GAFsc. MAX_UTIL : SCF \u2192 2SCF determines the set acceptable goals. This function takes as input a set of conflict-free sets and returns those with the maximum utility for the agent in terms of preference value.\nLet G \u2032 \u2286 G be the set of goals returned by MAX_UTIL. This means that G \u2032 is the set of goals the agent can commit to, which are called pursued goals or intentions.\nRegarding the function for determining acceptable goals, there may be many ways to make the calculations; for example, one way of characterizing MAX_UTIL is by summing up the preference value of all the goals in an extension. Another way may be by summing up the preference value of just the main goals without considering sub-goals. We will use the first characterization in our scenario.\n7In other works (e.g., [9] [10]), it is called a defeat relation.\nExample 2 Consider the general AF of Example 1, the agent generates: GAFsc = \u3008{clean(5, 5), pickup(5, 5), mop(5, 5), be(in_workshop), be(fixed)}, {(mop(5, 5), pickup(5, 5)), (clean(5, 5), be(in_workshop)), (mop(5, 5), be(in_workshop)), (pickup(5, 5), be(in_workshop))}, INCOMP_G, PREF\u3009. Figure 2 shows this GAF, the preference values of each goal, and the form of incompatibilities that exists between pairs of goals.\nFrom GAFsc, the number of conflict-free extensions is: |SCF | = 14. After applying MAX_UTIL, the extension with the highest preference is: {clean(5, 5),mop(5, 5), be(fixed)}. This means that G \u2032 = {clean(5, 5),mop(5, 5), be(fixed)} are compatible goals that can be achieved together without conflicts."}, {"heading": "3 Argumentation Process for Explanations Generation", "text": "In this section, we present explanatory arguments and the process for generating explanations for a goal become pursued or not.\nFirst of all, let us present the types of questions that can be answered:\n\u2022 WHY(g): it is required an explanation to justify why a goal g became pursued8. \u2022 WHY_NOT(g): it is required an explanation to justify why a goal g did not become pursued."}, {"heading": "3.1 Explanatory Arguments and Argumentation Framework", "text": "As a result of the above section, we obtain a Goals Argumentation Framework (GAF) and a set of pursued goals. Recall that in a GAF, the arguments represent goals; hence, in order to generate an explanation from a GAF, it is necessary to generate beliefs and rules \u2013 that reflect the knowledge contained in it \u2013 from which, explanatory arguments can be constructed. Before presenting the beliefs and rules, let us present some functions that will be necessary for the generation of beliefs:\n\u2022 COMPS(GAFsc) = {g | @(g, g\u2032) \u2208 RGsc (or (g\u2032, g) \u2208 RGsc), where g, g\u2032 \u2208 G} . This function returns the set of goals without conflicting relations.\n\u2022 EVAL_PREF(GAFsc) = {(g, g\u2032) | (g, g\u2032) \u2208 RGsc and (g\u2032, g) 6\u2208 RGsc}. This function returns all the pairs of goals inRG \u2032 that represent non-symmetrical relations between goals. When the relation is not symmetrical, it means that one of the goals is preferred to the other.\nUsing these functions, the set of beliefs generated from a GAFsc = \u3008G,RGsc, INCOMP_G, PREF\u3009 are the following:\n\u2022 \u2200g \u2208 COMPS(GAFsc) generate a belief \u00acincomp(g) \u2022 \u2200(g, g\u2032) \u2208 EVAL_PREF(GAFsc), if PREF(g) > PREF(g\u2032), then generate pref(g, g\u2032) and \u00acpref(g\u2032, g). \u2022 \u2200(g, g\u2032) \u2208 (RGsc \\ EVAL_PREF(GAFsc)) generate a belief eq_pref(g, g\u2032). These beliefs are created for\nthose pairs of goals with equal preference.\n\u2022 \u2200(g, g\u2032) \u2208 RGsc generate a belief incompat(g, g\u2032, ls) where ls = INCOMP_G(g, g\u2032) \u2022 \u2200g \u2208 G \u2032 generate a belief max_util(g) \u2022 \u2200g \u2208 G \\ G \u2032 generate a belief \u00acmax_util(g)\n8In order to better deal with goals, we map each goal to a constant in L.\nAll the beliefs that are generated have to be added to the set of beliefs B of the agent. These beliefs are necessary for triggering any of the following rules:\n\u2022 r1 : \u00acincomp(x)\u2192 pursued(x) \u2022 r2 : incompat(x, y, ls) \u2227 pref(x, y)\u2192 pursued(x) \u2022 r3 : incompat(x, y, ls) \u2227 \u00acpref(y, x)\u2192 \u00acpursued(y) \u2022 r4 : incompat(x, y, ls) \u2227 eq_pref(x, y)\u2192 pursued(x) \u2022 r5 : max_util(x)\u2192 pursued(x) \u2022 r6 : \u00acmax_util(x)\u2192 \u00acpursued(x)\nLet ER = {r1, r2, r3, r4, r5, r6} be the set of rules necessary for constructing explanatory arguments.\nDefinition 7 (Explanatory argument) Let B, ER, and g \u2208 G be the set of beliefs, set of rules, and a goal of an agent, respectively. An explanatory argument constructed from B and ER for determining the status of g is a pair A = \u3008S , h\u3009 such that (i) S \u2286 B \u222a ER, (ii) h \u2208 {pursued(g),\u00acpursued(g)}, (iii) S ` h, and (iv) S is consistent and minimal for the set inclusion9.\nLet ARGexp be the set of explanatory arguments that can be built from B and ER. We call S the support of an argument A (denoted by SUPPORT(A)) and h its claim (denoted by CLAIM(A)).\nWe can notice that rules in ER can generate conflicting arguments because they have inconsistent conclusions. Thus, we need to define the concept of attack. In this context, the attack that can exist between two explanatory arguments is the well-known rebuttal [12], where two explanatory arguments support contradictory claims. Formally:\nDefinition 8 (Rebuttal) Let \u3008S , h\u3009 and \u3008S \u2032, h\u2032\u3009 be two explanatory arguments. \u3008S , h\u3009 rebuts \u3008S \u2032, h\u2032\u3009 iff h \u2261 \u00ach\u2032.\nRebuttal attack has a symmetric nature, this means that two arguments rebut each other, that is, they mutually attack. Recall that the semantics for determining the set of pursued goals is based on conflict-free sets and on a function based on the preference value of the goals. This function is decisive in the selection of the extension that includes the goals the agent can commit to. Thus, it is natural to believe that arguments related to such function are stronger than other arguments. This difference in the strength of arguments turns out in a defeat relation between them, which breaks the previously mentioned symmetry.\nDefinition 9 (Defeat Relation - D) Let ER be the set of rules and A = \u3008S , h\u3009 and B = \u3008S \u2032, h\u2032\u3009 be two explanatory arguments such that A rebuts B and vice versa. A defeats B iff r5 \u2208 S (or r6 \u2208 S). We denote with (A,B) the defeat relation between A and B. In other words, if (A,B) \u2208 D, it means that A defeats B.\nOnce we have defined arguments and the defeat relation, we can generate the AF. It is important to make it clear that a different AF is generated for each goal.\nDefinition 10 (Explanatory Argumentation Framework) Let g \u2208 G be a pursuable goal. An Explanatory AF for g is a pair XAFg = \u3008ARGgexp,Dg\u3009 where:\n\u2022 ARGgexp \u2286 ARGexp such that \u2200A \u2208 ARGgexp, CLAIM(A) = pursued(g) or CLAIM(A) = \u00acpursued(g).\n\u2022 Dg \u2286 ARGgexp \u00d7 ARGgexp is a binary relation that captures the defeat relation between arguments in ARGgexp.\nThe next step is to evaluate the arguments that make part of the AF. This evaluation is important because it determines the set of non-conflicting arguments, which in turn determines if a goal becomes pursued or not. Recall that for obtaining such set, an argumentation semantics has to be applied. Unlike the semantics for goals selection, in this case we can use any of the semantics defined in literature. Next, the main semantics introduced by Dung [13] are recalled10.\nDefinition 11 (Semantics) Let XAFg = \u3008ARGgexp,Dg\u3009 be an explanatory AF and E \u2286 ARGgexp:\n\u2022 E is conflict-free if \u2200A,B \u2208 E , (A,B) /\u2208 Dg\n9Minimal means that there is no S \u2032 \u2282 S such that S ` h and consistent means that it is not the case that S ` pursued(g) and S ` \u00acpursued(g) [11].\n10It is not the scope of this article to study the most adequate semantics for this context or the way to select an extension when more than one is returned by a semantics.\n\u2022 E defends A iff \u2200B \u2208 ARGgexp, if (B,A) \u2208 Dg , then \u2203C \u2208 E s.t. (C,B) \u2208 Dg .\n\u2022 E is admissible iff it is conflict-free and defends all its elements.\n\u2022 A conflict-free E is a complete extension iff we have E = {A|E defends A}.\n\u2022 E is a preferred extension iff it is a maximal (w.r.t. the set inclusion) complete extension.\n\u2022 E is a grounded extension iff is a minimal (w.r.t. set inclusion) complete extension.\n\u2022 E is a stable extension iff E is conflict-free and \u2200A \u2208 ARGgexp, \u2203B \u2208 E such that (B,A) \u2208 Dg .\nFinally, a goal g becomes pursued when \u2203A \u2208 E such that CLAIM(A) = pursued(g)."}, {"heading": "3.2 Explanation Generation Process", "text": "In this article, an explanation is made up of a set of explanatory arguments that justify the fact that a pursuable goal becomes (or not) pursued. Recall that there is a different explanatory AF for each pursuable goal. Thus, we can say that an explanation for a given goal g is given by the explanatory AF generated for it, that is XAFg. Besides, if g \u2208 G \u2032, the explanation is required by using WHY(g); otherwise, the explanation is required by using WHY_NOT(g). Finally, we can differentiate between partial and complete explanations depending on the set of explanatory arguments that are employed for the justification:\n\u2022 A complete explanation for g is: CEg = XAFg \u2022 A partial explanation for g is: PEg = E , where E is an extension obtained by applying a semantics to XAFg .\nWe can now present the steps for generating explanations. Given a GAFsc = \u3008G,RGsc, INCOMP_G, PREF\u3009 and a set of pursued goals G \u2032, the steps for generating an explanation for a goal g \u2208 G are:\n1. From GAFsc generate the respective beliefs and add to B 2. Trigger the rules in ER that can be unified with the beliefs of B 3. Construct explanatory arguments based on the rules and beliefs of the two previous items\n4. \u2200g \u2208 G do (a) Generate the respective explanatory AF (that is, XAFg) with the arguments whose claim is pursued(g)\nor \u00acpursued(g) and the defeat relation (b) Calculate the extension E from XAFg"}, {"heading": "3.3 From Explanatory Arguments to Explanatory Sentences", "text": "Like it was done in [14], in this sub-section we present a pseudo-natural language for improving the understanding of the explanations when the agents are interacting with human users. Thus, we propose a set of explanatory schemes, one for each rule in ER. This means that depending on which rule an argument was constructed, the explanation scheme is different. In this first version of the scheme, we will generate explanatory sentences only for partial explanations.\nRecall that goals are mapped to constants of L, in order to improve the natural language let NAME(g) denote the original predicate of a given goal g. Besides, let RULE(A) denote which of the rules in ER was employed in order to construct A.\nDefinition 12 (Explanatory Schemes) Let A = \u3008S , h\u3009 be an explanatory argument. An explanatory scheme exp_sch for A is:11\n\u2022 If RULE(A) = r1 : \u00acincomp(x)\u2192 pursued(x), then exp_sch = \u3008NAME(x) has no incompatibility, so it became pursued.\u3009\n\u2022 If RULE(A) = r2 : incompat(x, y, ls) \u2227 pref(x, y)\u2192 pursued(x), then exp_sch = \u3008NAME(x) and NAME(y) have the following conflicts: ls. Since NAME(x) is more preferable than NAME(y), NAME(x) became pursued.\u3009\n11Underlined characters represent the variables of the schemes, which depend on the variables of rules.\n\u2022 If RULE(A) = r3 : incompat(x, y, ls) \u2227 \u00acpref(y, x)\u2192 \u00acpursued(y), then exp_sch = \u3008NAME(x) and NAME(y) have the following conflicts: ls. Since NAME(y) is less preferable than NAME(x), NAME(y) did not become pursued.\u3009\n\u2022 If RULE(A) = r4 : incompat(x, y, ls) \u2227 eq_pref(x, y)\u2192 pursued(x), then exp_sch = \u3008NAME(x) and NAME(y) have the following conflicts: ls. Since NAME(x) and NAME(y) have the same preference value, NAME(x) became pursued.\u3009\n\u2022 If RULE(A) = r5 : max_util(x)\u2192 pursued(x), then exp_sch = \u3008Since NAME(x) belonged to the set of goals that maximize the utility, it became pursued.\u3009\n\u2022 If RULE(A) = r6 : \u00acmax_util(x)\u2192 \u00acpursued(x), then exp_sch = \u3008Since NAME(x) did not belong to the set of goals that maximizes the utility, it did not become pursued.\u3009"}, {"heading": "4 Application: Cleaner World Scenario", "text": "Let us consider the GAFsc = \u3008G,RGsc, INCOMP_G, PREF\u3009 presented in Example 2, whose graph is depicted in Figure 2. Recall also that G \u2032 = {clean(5, 5),mop(5, 5), be(fixed)}. Firstly, we map the goals in G into constants of L in the following manner: g1 = clean(5, 5), g2 = pickup(5, 5), g3 = mop(5, 5), g4 = be(in_workshop), and g5 = be(fixed). We will also map the beliefs and rules to constants in L. We can now follow the steps to generate the explanations:"}, {"heading": "1. Generate beliefs", "text": "- b1 : \u00acincomp(g5) b10 : \u00acmax_util(g4) - b2 : incompat(g3, g2, \u2018s\u2019) b11 : pref(g3, g4) - b3 : incompat(g3, g4, \u2018t\u2019) b12 : \u00acpref(g4, g3) - b4 : incompat(g1, g4, \u2018t, r\u2019) b13 : pref(g1, g4) - b5 : incompat(g2, g4, \u2018t, r\u2019) b14 : \u00acpref(g4, g1) - b6 : max_util(g1) b15 : pref(g2, g4) - b7 : max_util(g3) b16 : \u00acpref(g4, g2) - b8 : max_util(g5) b17 : pref(g3, g2) - b9 : \u00acmax_util(g2) b18 : \u00acpref(g2, g3)"}, {"heading": "2. Trigger rules", "text": "- r1 : \u00acincomp(g5)\u2192 pursued(g5) - r2 : incompat(g3, g2, \u2018s\u2019) \u2227 pref(g3, g2)\u2192 pursued(g3) - r3 : incompat(g3, g2, \u2018s\u2019) \u2227 \u00acpref(g2, g3)\u2192 \u00acpursued(g2) - r4 : incompat(g3, g4, \u2018t\u2019) \u2227 pref(g3, g4)\u2192 pursued(g3) - r5 : incompat(g3, g4, \u2018t\u2019) \u2227 \u00acpref(g4, g3)\u2192 \u00acpursued(g4) - r6 : incompat(g1, g4, \u2018t, r\u2019) \u2227 pref(g1, g4)\u2192 pursued(g1) - r7 : incompat(g1, g4, \u2018t, r\u2019) \u2227 \u00acpref(g4, g1)\u2192 \u00acpursued(g4) - r8 : incompat(g2, g4, \u2018t, r\u2019) \u2227 pref(g2, g4)\u2192 pursued(g2) - r9 : incompat(g2, g4, \u2018t, r\u2019) \u2227 \u00acpref(g4, g2)\u2192 \u00acpursued(g4) - r10 : max_util(g1)\u2192 pursued(g1) - r11 : max_util(g3)\u2192 pursued(g3) - r12 : max_util(g5)\u2192 pursued(g5) - r13 : \u00acmax_util(g2)\u2192 \u00acpursued(g2) - r14 : \u00acmax_util(g4)\u2192 \u00acpursued(g4)"}, {"heading": "3. Construct explanatory arguments", "text": "- A1 = \u3008{b1, r1}, pursued(g5)}\u3009 - A2 = \u3008{b2, b17, r2}, pursued(g3)}\u3009 - A3 = \u3008{b2, b18, r3},\u00acpursued(g2)}\u3009 - A4 = \u3008{b3, b11, r4}, pursued(g3)}\u3009 - A5 = \u3008{b3, b12, r5},\u00acpursued(g4)}\u3009 - A6 = \u3008{b4, b13, r6}, pursued(g1)}\u3009 - A7 = \u3008{b4, b14, r7},\u00acpursued(g4)}\u3009 - A8 = \u3008{b5, b15, r8}, pursued(g2)}\u3009 - A9 = \u3008{b5, b16, r9},\u00acpursued(g4)}\u3009 - A10 = \u3008{b6, r10}, pursued(g1)}\u3009\n- A11 = \u3008{b7, r11}, pursued(g3)} \u3009 - A12 = \u3008{b8, r12}, pursued(g5)}\u3009 - A13 = \u3008{b9, r13},\u00acpursued(g2)}\u3009 - A14 = \u3008{b10, r14},\u00acpursued(g4)}\u3009"}, {"heading": "4. For each goal, generate an explanatory AF and extension", "text": "- For g1: XAFg1 = \u3008{A6, A10}, {}\u3009, E = {A6, A10} - For g2: XAFg2 = \u3008{A3, A8, A13}, {(A3, A8), (A13, A8)}\u3009, E = {A3, A13} - For g3: XAFg3 = \u3008{A2, A4, A11}, {}\u3009, E = {A2, A4, A11} - For g4: XAFg4 = \u3008{A5, A7, A9, A14}, {}\u3009, E = {A5, A7, A9, A14} - For g5: XAFg5 = \u3008{A1, A12}, {}\u3009, E = {A1, A12}\nThus, the \u2013 partial or complete \u2013 explanations for justifying the status of each goal were generated. Next, we present the query, set of arguments of the partial explanation, and the explanatory sentences for the status of each goal:\n\u2022 For the query WHY(g1), we have PE = {A6, A10}, which can be written: * clean(5, 5) and be(in_workshop) have the following conflicts: \u2018t, r\u2019. Since clean(5, 5) is more preferable than be(in_workshop), clean(5, 5) became pursued * Since clean(5, 5) belonged to the set of goals that maximizes the utility, it became pursued\n\u2022 For the query WHY_NOT(g2), we have PE = {A3, A13}, which can be written: * mop(5, 5) and pickup(5, 5) have the following conflicts: \u2018s\u2019. Since pickup(5, 5) is less preferable than mop(5, 5), pickup(5, 5) did not become pursued * Since pickup(5, 5) did not belong to the set of goals that maximizes the utility, it did not become pursued\n\u2022 For the query WHY(g3), we have PE = {A2, A4, A11}, which can be written: * mop(5, 5) and pickup(5, 5) have the following conflicts: \u2018s\u2019. Since mop(5, 5) is more preferable than pickup(5, 5), mop(5, 5) became pursued * mop(5, 5) and be(in_workshop) have the following conflicts: \u2018t\u2019. Since mop(5, 5) is more preferable than be(in_workshop), mop(5, 5) became pursued * Since mop(5, 5) belonged to the set of goals that maximizes the utility, it became pursued\n\u2022 For the query WHY_NOT(g4), we have PE = {A5, A7, A9, A14}, which can be written: * mop(5, 5) and be(in_workshop) have the following conflicts: \u2018t\u2019. Since be(in_workshop) is less preferable than mop(5, 5), be(in_workshop) did not become pursued * clean(5, 5) and be(in_workshop) have the following conflicts: \u2018t, r\u2019. Since be(in_workshop) is less preferable than clean(5, 5), be(in_workshop) did not become pursued * pickup(5, 5) and be(in_workshop) have the following conflicts: \u2018t, r\u2019. Since be(in_workshop) is less preferable than pickup(5, 5), be(in_workshop) did not become pursued * Since be(in_workshop) did not belong to the set of goals that maximizes the utility, it did not become pursued\n\u2022 For the query WHY(g5), we have PE = {A1, A12}, which can be written: * be_fixed has no incompatibility, so it became pursued * Since be_fixed belonged to the set of goals that maximizes the utility, it became pursued\nFor all the queries, except WHY_NOT(g2), the complete explanation is the same. In the case of WHY_NOT(g2), the complete explanation includes the attack relations between some of the arguments of its explanatory AF.\nWe are also working in a simulator \u2013 called ArgAgent12 \u2013 for generating explanations. In it first version, just partial explanations are generated. Figure 3 shows the explanation for query WHY(g1)."}, {"heading": "5 Related Work", "text": "Since XAI is a recently emerged domain in Artificial Intelligence, there are few reviews about the works in this area. In [15], Anjomshoae et al. make a Systematic Literature Review about goal-driven XAI, i.e., explainable agency for robots and agents. Their results show that 22% of the platforms and architectures have not explicitly indicate their\n12Available at: https://github.com/henriquermonteiro/BBGP-Agent-Simulator/\nmethod for generating explanations, 18% of papers relied on ad-hoc methods, 9% implemented their explanations in BDI architecture.\nSome works relied on the BDI model are the following. In [16] and [17], Broekens et al. and Harbers et al., respectively, focus on generating explanations for humans about how their goals were achieved. Unlike our proposal, their explanations do not focus on the goals selection. Langley et al. [18] focus on settings in which an agent receives instructions, performs them, and then describes and explains its decisions and actions afterwards.\nSassoon et al. [19] propose an approach of explainable argumentation based on argumentation schemes and argumentation-based dialogues. In this approach, an agent provides explanations to patients (human users) about their treatments. In this case, argumentation is applied in a different way than in our proposal and with other focus, they generate explanations for information seeking and persuasion. Finally, Morveli-Espinoza et al. [20] propose an argumentation-based approach for generating explanations about the intention formation process, that is, since a goal is a desire until it becomes an intention; however, the generated explanations about goals selection are not detailed and they do not present a pseudo-natural language."}, {"heading": "6 Conclusions and Future Work", "text": "In this article, we presented an argumentation-based approach for generating explanations about the goals selection process, that is, giving reasons to justify the transition of a set of goals from being pursuable (desires) to pursued (intentions). Such reasons are related to the conflicts that may exist between pursuable goals and how that conflicts were resolved. In the first part of the approach, argumentation was employed to deal with conflicts and in the second part it was employed to generate explanations. In order to improve the informational quality of explanations, we extended the results presented in [21]. Thus, explanations also include the form of incompatibility that exists between goals. Besides, we proposed a pseudo-natural language that is a first step to generate explanations for human users. Therefore, our proposal is able generate explanations for both intelligent agents and human-users.\nAs future work, we aim to further improve the informational quality of explanations by allowing information seeking about the exact point of conflict between two instrumental arguments (or plans) and information about the force of the arguments. The pseudo-natural language was only applied to partial explanations, we plan to extend such language in order to support complete explanations."}, {"heading": "Acknowledgment", "text": "This work is partially founded by CAPES (Coordena\u00e7\u00e3o de Aperfei\u00e7oamento de Pessoal de N\u00edvel Superior)."}], "title": "AN ARGUMENTATION-BASED APPROACH FOR EXPLAINING GOAL SELECTION IN INTELLIGENT AGENTS", "year": 2020}