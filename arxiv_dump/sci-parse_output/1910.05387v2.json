{
  "abstractText": "Causal inference is central to many areas of artificial intelligence, including complex reasoning, planning, knowledge-base construction, robotics, explanation, and fairness. An active community of researchers develops and enhances algorithms that learn causal models from data, and this work has produced a series of impressive technical advances. However, evaluation techniques for causal modeling algorithms have remained somewhat primitive, limiting what we can learn from experimental studies of algorithm performance, constraining the types of algorithms and model representations that researchers consider, and creating a gap between theory and practice. We argue for more frequent use of evaluation techniques that examine interventional measures rather than structural or observational measures, and that evaluate those measures on empirical data rather than synthetic data. We survey the current practice in evaluation and show that the techniques we recommend are rarely used in practice. We show that such techniques are feasible and that data sets are available to conduct such evaluations. We also show that these techniques produce substantially different results than using structural measures and synthetic data.",
  "authors": [
    {
      "affiliations": [],
      "name": "Amanda Gentzel"
    },
    {
      "affiliations": [],
      "name": "Dan Garant"
    }
  ],
  "id": "SP:e61bf2cb39280d331d940eb022a6ae8ea162d188",
  "references": [
    {
      "authors": [
        "David Maxwell Chickering"
      ],
      "title": "Optimal structure identification with greedy search",
      "venue": "Journal of Machine Learning Research,",
      "year": 2003
    },
    {
      "authors": [
        "Paul Cohen"
      ],
      "title": "Empirical Methods for Artificial Intelligence",
      "year": 1995
    },
    {
      "authors": [
        "Paul Cohen",
        "Adele Howe"
      ],
      "title": "Toward AI research methodology: Three case studies in evaluation",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics,",
      "year": 1989
    },
    {
      "authors": [
        "Thomas D. Cook",
        "William R. Shadish",
        "Vivian C. Wong"
      ],
      "title": "Three conditions under which experiments and observational studies produce comparable causal estimates: New findings from within-study comparisons",
      "venue": "Journal of Policy Analysis and Management: The Journal of the Association for Public Policy Analysis and Management,",
      "year": 2008
    },
    {
      "authors": [
        "Atray Dixit",
        "Oren Parnas",
        "Biyu Li",
        "Jenny Chen",
        "Charles P. Fulco",
        "Livnat Jerby-Arnon",
        "Nemanja D. Marjanovic",
        "Danielle Dionne",
        "Tyler Burks",
        "Raktima Raychowdhury",
        "Britt Adamson",
        "Thomas M. Norman",
        "Eric S. Lander",
        "Jonathan S. Weissman",
        "Nir Friedman",
        "Aviv Regev"
      ],
      "title": "Perturb-Seq: Dissecting molecular circuits with scalable single-cell RNA profiling of pooled genetic",
      "venue": "screens. Cell,",
      "year": 2016
    },
    {
      "authors": [
        "Vincent Dorie",
        "Jennifer Hill",
        "Uri Shalit",
        "Marc Scott",
        "Dan Cervone"
      ],
      "title": "Automated versus do-ityourself methods for causal inference: Lessons learned from a data analysis competition",
      "venue": "Statistical Science,",
      "year": 2019
    },
    {
      "authors": [
        "Dean Eckles",
        "Eytan Bakshy"
      ],
      "title": "Bias and high-dimensional adjustment in observational studies of peer effects",
      "venue": "arXiv preprint:1706.04692,",
      "year": 2017
    },
    {
      "authors": [
        "Dean Eckles",
        "Ren\u00e9 F. Kizilcec",
        "Eytan Bakshy"
      ],
      "title": "Estimating peer effects in networks with peer encouragement designs",
      "venue": "Proceedings of the National Academy of Sciences,",
      "year": 2016
    },
    {
      "authors": [
        "David Galles",
        "Judea Pearl"
      ],
      "title": "Testing identifiability of causal effects",
      "venue": "In Proceedings of the 11th International Conference on Uncertainty in Artificial Intelligence,",
      "year": 1995
    },
    {
      "authors": [
        "Brett R Gordon",
        "Florian Zettelmeyer",
        "Neha Bhargava",
        "Dan Chapsky"
      ],
      "title": "A comparison of approaches to advertising measurement: Evidence from big field experiments at facebook",
      "venue": "Marketing Science,",
      "year": 2019
    },
    {
      "authors": [
        "Clive WJ Granger"
      ],
      "title": "Investigating causal relations by econometric models and cross-spectral methods",
      "venue": "Econometrica: Journal of the Econometric Society,",
      "year": 1969
    },
    {
      "authors": [
        "Isabelle Guyon",
        "Constantin Aliferis",
        "Greg Cooper",
        "Peter Spirtes"
      ],
      "title": "Design and analysis of the causation and prediction challenge",
      "venue": "WCCI 2008 Workshop on Causality,",
      "year": 2008
    },
    {
      "authors": [
        "Isabelle Guyon",
        "Dominik Janzing",
        "Bernhard Sch\u00f6lkopf"
      ],
      "title": "Causality: Objectives and assessment",
      "venue": "NIPS 2008 Workshop on Causality,",
      "year": 2010
    },
    {
      "authors": [
        "P. Richard Hahn",
        "Vincent Dorie",
        "Jared S. Murray"
      ],
      "title": "Atlantic causal inference conference (ACIC) data analysis challenge",
      "year": 2017
    },
    {
      "authors": [
        "Michael P. Keane",
        "Kenneth I. Wolpin"
      ],
      "title": "Exploring the usefulness of a nonrandom holdout sample for model validation: Welfare effects on female",
      "venue": "behavior. International Economic Review,",
      "year": 2007
    },
    {
      "authors": [
        "Pat Langley"
      ],
      "title": "The changing science of machine learning",
      "venue": "Machine Learning,",
      "year": 2011
    },
    {
      "authors": [
        "Jianhua Lin"
      ],
      "title": "Divergence measures based on the Shannon entropy",
      "venue": "IEEE Transactions on Information Theory,",
      "year": 1991
    },
    {
      "authors": [
        "Clement J. McDonald",
        "Siu L. Hui",
        "William M. Tierney"
      ],
      "title": "Effects of computer reminders for influenza vaccination on morbidity during influenza",
      "venue": "epidemics. MD Computing,",
      "year": 1992
    },
    {
      "authors": [
        "Joris M Mooij",
        "Jonas Peters",
        "Dominik Janzing",
        "Jakob Zscheischler",
        "Bernhard Sch\u00f6lkopf"
      ],
      "title": "Distinguishing cause from effect using observational data: methods and benchmarks",
      "venue": "Journal of Machine Learning Research,",
      "year": 2016
    },
    {
      "authors": [
        "Jonas Peters",
        "Peter B\u00fchlmann"
      ],
      "title": "Structural intervention distance for evaluating causal graphs",
      "venue": "Neural Computation,",
      "year": 2015
    },
    {
      "authors": [
        "Jonas Peters",
        "Joris M. Mooij",
        "Dominik Janzing",
        "Bernhard Sch\u00f6lkopf"
      ],
      "title": "Causal discovery with continuous additive noise models",
      "venue": "Journal of Machine Learning Research,",
      "year": 2009
    },
    {
      "authors": [
        "Donald B. Rubin"
      ],
      "title": "Causal inference using potential outcomes: Design, modeling, decisions",
      "venue": "Journal of the American Statistical Association,",
      "year": 2005
    },
    {
      "authors": [
        "Karen Sachs",
        "Omar Perez",
        "Dana Pe\u2019er",
        "Douglas A. Lauffenburger",
        "Garry P. Nolan"
      ],
      "title": "Causal protein-signaling networks derived from multiparameter single-cell data",
      "year": 2005
    },
    {
      "authors": [
        "Thomas Schaffter",
        "Daniel Marbach",
        "Dario Floreano"
      ],
      "title": "GeneNetWeaver: In silico benchmark generation and performance profiling of network inference methods",
      "year": 2011
    },
    {
      "authors": [
        "William R. Shadish",
        "M.H. Clark",
        "Peter M. Steiner"
      ],
      "title": "Can nonrandomized experiments yield accurate answers? A randomized experiment comparing random and nonrandom assignments",
      "venue": "Journal of the American Statistical Association,",
      "year": 2008
    },
    {
      "authors": [
        "Yishai Shimoni",
        "Chen Yanover",
        "Ehud Karavani",
        "Yaara Goldschmnidt"
      ],
      "title": "Benchmarking framework for performance-evaluation of causal inference analysis",
      "venue": "arXiv preprint arXiv:1802.05046,",
      "year": 2018
    },
    {
      "authors": [
        "Joseph P. Simmons",
        "Leif D. Nelson",
        "Uri Simonsohn"
      ],
      "title": "False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant",
      "venue": "Psychological Science,",
      "year": 2011
    },
    {
      "authors": [
        "Peter Spirtes",
        "Clark Glymour",
        "Richard Scheines"
      ],
      "title": "Causation, Prediction and Search",
      "year": 2000
    },
    {
      "authors": [
        "Wei Sun",
        "Pengyuan Wang",
        "Dawei Yin",
        "Jian Yang",
        "Yi Chang"
      ],
      "title": "Causal Inference via Sparse Additive Models with Application to Online Advertising",
      "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence,",
      "year": 2015
    },
    {
      "authors": [
        "Ioannis Tsamardinos",
        "Laura E. Brown",
        "Constantin F. Aliferis"
      ],
      "title": "The max-min hill-climbing Bayesian network structure learning algorithm",
      "venue": "Journal of Machine Learning Research,",
      "year": 2006
    },
    {
      "authors": [
        "Qingyuan Zhao",
        "Luke J. Keele",
        "Dylan S. Small"
      ],
      "title": "Comment: Will competition-winning methods for causal inference also succeed in practice",
      "venue": "Statistical Science, 34(1):72\u201376,",
      "year": 2019
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Evaluation is central to research in artificial intelligence and machine learning [Cohen, 1995, Langley, 2011]. How we evaluate algorithms determines our perception of the relative effectiveness and usefulness of different approaches, and this knowledge guides choices about future research directions. As Cohen and Howe [1989] explained three decades ago: \u201cIdeally, evaluation should be a mechanism by which AI progresses both within and across individual research projects. It should be something we do as individuals to help our own research and, more importantly, on behalf of the field.\u201d\nAs fields develop, protocols for evaluation need to develop alongside them. In this paper, we offer an empirical analysis of the set of techniques typically used to evaluate algorithms for learning causal models, and we show that this set could be substantially enhanced. The ultimate goal of most algorithms for causal inference is to learn models capable of accurately estimating the effects of interventions in real-world systems. With this goal in mind, we would like to evaluate algorithms by comparing their estimates to actual interventional effects on data produced by a real-world system. In practice, though, many evaluations fall short of this ideal, most frequently using only synthetic data and structural or observational measures. Without the use of empirical data, our evaluations produce little information about whether our algorithms generalize to real-world systems, and this greatly reduces their likelihood of widespread adoption by others outside of the field. Without the use of interventional measures, our evaluations produce little information about whether learned models will accurately estimate the effects of interventions, limiting their real-world utility.\nNote that we do not argue for replacing the prevailing techniques for evaluation. These techniques have substantial value, both in assessing overall performance and in allowing fine-grained experiments\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n91 0.\n05 38\n7v 2\n[ cs\n.A I]\n1 N\nov 2\n01 9\nto diagnose specific performance issues. Rather, we argue for augmenting the current suite of evaluation techniques to gather experimental evidence that the prevailing techniques cannot. We also do not contend that interventional measures and empirical data are entirely absent from current studies. A very small minority of recent studies use these techniques in combination. Rather, we argue that interventional measures and empirical data should be used routinely, and should be used in combination, for any serious study of algorithms for learning causal models. Indeed, the conclusions of most studies that lack such evaluation techniques should be considered exploratory and would benefit from additional evaluation.\nWe make the following contributions:\nC1 Decomposition of Evaluation Techniques. We decompose evaluation techniques into three interacting components: the data source, the algorithm, and the evaluation measure, allowing for a modular discussion of the interacting components of an evaluation.\nC2 Survey of Current Techniques. We provide a detailed survey of recent literature in causal modeling to provide a quantitative understanding of current evaluation practices.\nC3 Critique of Current Practice. We provide evidence that increased adoption of both empirical data and interventional measures would be beneficial to the community."
    },
    {
      "heading": "2 Survey of Current Techniques",
      "text": "To assess how frequently different evaluation techniques are used in practice, we surveyed recent computer science publications on causal inference. We collected papers from the past five UAI, NeurIPS, AAAI, ICML, and KDD conferences, as well as causality workshops held at UAI. We examined papers whose titles contained the terms \u2018cause\u2019, \u2018causal\u2019, or \u2018causality\u2019 and then narrowed this selection of papers to those that describe, propose, or evaluate a causal modeling algorithm. This resulted in a final set of 111 papers, of which 82% (91) reported any sort of evaluation.1 Citations to all 111 papers are provided in the Supplementary Material.\nThe counts of papers included in the final survey are shown in Table 1. While some relevant papers may fall outside of our search parameters, this approach captures a reasonably representative sample of recent work within computer science on causal modeling, allowing us to infer which techniques are used in practice and how frequently these techniques are used.\nTable 1: Papers included in the survey\nVenue 2014 2015 2016 2017 2018 Total UAI 2 3 5 3 7 20\nNeurIPS 3 5 4 6 13 31 AAAI 1 6 2 4 5 18 ICML 1 5 1 3 5 15 KDD 0 2 3 0 2 7\nUAI-W 2 2 4 3 9 20 Total 9 23 19 19 41 111\nTable 2: Number of papers using different evaluation measures\nData Sources Synthetic Empirical\nE va\nlu at\nio n\nM ea\nsu re s Structural 44 23 Observational 22 14 Interventional 11 6\nVisual Inspection 0 19"
    },
    {
      "heading": "2.1 Survey Results",
      "text": "For ease of exposition, we decompose evaluation techniques into three components: (1) the data source; (2) the algorithm under evaluation; and (3) the evaluation measure. These dimensions are highly dependent\u2014a choice of one can determine feasible choices for the others. For example, models learned from observational macro-economic data often cannot be compared against a known structure because there exists no ground truth, and models consisting only of non-parameterized structure cannot be compared to interventional effects because the models cannot produce such estimates.\nData Sources. The surveyed papers used a wide range of data sources, but they fall into two broad categories: synthetic and empirical. We categorized data as empirical when it was collected from\n1When reporting survey results, we follow each percentage with a parenthesized number representing the raw count. The denominator for percentages is 91, except where otherwise noted.\na \u201creal world\u201d system, whether that was a randomized clinical trial, a global financial system, or user interaction with a website. The important distinction is that empirical data was collected from a process or a system that exists for some purpose beyond scientific research. Synthetic data includes anything else, including data generated from a randomly instantiated directed graphical model or from a simulation intended to reflect a real-world system. In our survey, we found many examples of both, and while synthetic data is used more frequently, both are still common. 81% (74) of papers surveyed used synthetic data, 67% (61) used empirical data, and 48% (44) used both.\nAlgorithms. The algorithm under evaluation is not part of the evaluation technique per se, but aspects of the algorithm strongly influence how evaluation can, and should, be performed. Algorithms fall into two broad categories, bivariate and multivariate, based on the number of variables they consider, although there are many variants.\nSome bivariate algorithms infer only the direction of effect (whether A causes B or B causes A). Others estimate the magnitude of effect between treatment and outcome, while adjusting for the effects of a number of covariates. Bivariate methods include Granger causality analysis [Granger, 1969], additive noise models [Peters et al., 2014], and analyses that use the potential outcomes framework [Rubin, 2005]. The most common variety of multivariate algorithm learns a directed acyclic graph (DAG). Multivariate algorithms are significantly more prevalent in the data, accounting for 60% (55/111) of papers surveyed. Bivariate algorithms account for 30% (34/111) of papers surveyed, split between those focused on orientation (10%), magnitude of effect (15%), or both (5%). The remaining papers in the survey fall in between, including those that aim to determine the joint effect of multiple treatment variables on a single outcome.\nEvaluation Measures. At the heart of any evaluation technique is a measure of performance. At a high level, evaluation measures fall into two categories: structural and distributional. Structural measures include all measures designed to assess whether the structure (including both existence of edges and edge orientation) learned by the algorithm matches the ground truth. Structural measures include structural Hamming distance (SHD), precision, recall, F1-score, true-positive rate, area under the ROC curve (AUROC), and structural intervention distance (SID) [Peters and Bu\u0308hlmann, 2015].\nDistributional measures capture how well the algorithm can estimate quantitative dependence. Such measures can be further subdivided into observational and interventional measures. Observational measures compare the learned distribution with an observational ground truth (i.e. probability queries which do not involve a do operator). This could be a measure of individual edge strengths in a directed graphical model or a measure of the error when predicting a given outcome variable. Interventional measures, on the other hand, compare the learned distribution to ground truth obtained through intervention. Common interventional measures include KL-divergence, total variation distance, and measures of average and conditional treatment effect.\nOf the types of evaluation measures, structural measures are the most common, being used in 55% (50) of papers surveyed. Distributional measures are slightly less common, being used in 46% (42) of papers. The vast majority of the distributional measures used, however, are observational rather than interventional; observational measures are used in 32% (29) of papers, while interventional measures are used in only 14% (13).\nThe choice of evaluation measure depends on both the data generating process and type of algorithm, which is reflected in our survey. When synthetic data is evaluated, structural measures are used 59% (44/74) of the time. However, when empirical data is evaluated, structural measures are used only 38% (23/61) of the time, since empirical data is less likely to have ground truth. This lack of ground truth sometimes prevents any significant evaluation for techniques using empirical data\u201426% (16/61) of empirical evaluations used only visual inspection of the results, with no ground truth. Table 2 summarizes the interaction between data source and evaluation measure in the survey."
    },
    {
      "heading": "2.2 Findings",
      "text": "The survey makes clear that the vast majority of papers that perform evaluation use either (1) synthetic data; or (2) empirical data combined with non-interventional measures (observational measures, structural measures, or visual inspection). Our proposed ideal evaluation (empirical data and interventional measures) is used in only 7% (6) of papers. This raises an obvious question: Are the most commonly used evaluation techniques sufficient for determining whether algorithms for learning causal models will work effectively in realistic scenarios? As we argue below, they are not."
    },
    {
      "heading": "3 The Case for Empirical Data",
      "text": "As already noted, nearly all causal modeling algorithms are ultimately designed for use outside of a laboratory, on real systems to infer useful causal knowledge about the world. Despite this, evaluation of such algorithms often uses synthetic rather than empirical data."
    },
    {
      "heading": "3.1 Limitations of Synthetic Data",
      "text": "Researchers have developed several approaches to generating synthetic data. The most common is to use a some form of directed graphical model. In some cases, the structure of the model is designed to match the causal structure of a realistic system, either by manually specifying the structure or by learning it from empirical data. Large-scale simulators designed for other reasons can also be used. In some cases, simulators can be complex enough to generate data that is effectively equivalent to empirical data, though such simulations vary in quality.\nSynthetic data is easy to collect, allows for straightforward comparison with ground truth, and facilitates systematic testing across a variety of data parameters. Its popularity is evident\u201484% (74) of surveyed papers used it in their evaluation, and 41% (30/74) of those used only synthetic data. However, using synthetic data for evaluation also has significant limitations. These include:\nUnquestioned assumptions\u2014Synthetic data tends to match the assumptions of the researcher running the study and any algorithms they have created. For example, a researcher developing an algorithm that outputs a DAG will be inclined to generate data from a DAG.\nUnknown influences\u2014Even the best data generators can only include the influences already known to researchers. Almost by definition, synthetic data generators cannot include any \u201cunknown unknowns\u201d that may influence the outputs of real-world systems. While latent variables can be added, they are still defined and created by the researcher, limiting the realism of the data.\nLack of standardization\u2014Synthetic data is typically generated differently by each researcher, and this lack of standardization impedes comparison between studies.\nResearcher degrees-of-freedom\u2014Synthetic data is typically designed and parameterized by the researchers who created the algorithm being evaluated, giving them an enormous range of choices. Such high \u201cresearcher degrees-of-freedom\u201d [Simmons et al., 2011] are a basic challenge to the validity of any study.\nThese factors significantly limit the external validity and realism of most synthetic data, making it insufficient as the sole source of data for evaluation. Synthetic data is not without value\u2014it can be a powerful way to assess features of an algorithm and test its performance under different conditions. However, it typically falls short in providing insights into how the algorithm will perform on data from a real-world system."
    },
    {
      "heading": "3.2 Benefits of Empirical Data",
      "text": "Empirical data is almost always more difficult to collect than simulated data, and information on the effects of interventions is typically also much more difficult to obtain. However, using empirical data has multiple benefits:\nRealistic complexity\u2014Empirical data typically has a distribution that is more complex than simulated data. That distribution is subject to realistic latent factors and measurement error. This creates a learning task that is often significantly harder than synthetic data, but also more closely matches the challenges of real-world settings.\nLower potential researcher bias\u2014Empirical data is typically not generated by the researcher who designed the algorithm being evaluated, and thus it is less subject to unintentional biases. In addition, individual data sets are often shared across the community, creating standardization and comparability across studies.\nReal-world demonstration\u2014The aim of research on algorithms for causal modeling is to have these algorithms used by others to infer causal models and reason about causal effects in real-world settings. Practitioners considering use of these methods may be legitimately skeptical about their effectiveness until they see successful demonstrations of accurate causal inference on real-world data.\nHowever, using empirical data poses challenges as well. Because it is generally not collected by the person using it, some features of the data may not be fully understood, hindering correct interpretation. Also, ground truth can be challenging to obtain, limiting evaluation to visual inspection or observational measures. This is unsatisfying at best and misleading at worst, since, when evaluating without ground truth, it can be easy to see meaning where none exists or to imagine explanations for many possible conflicting outputs. Despite these challenges, empirical data is still used frequently in practice; 67% (61) of surveyed papers use empirical data, and 28% (17/61) used only empirical data."
    },
    {
      "heading": "3.3 Sources of Empirical Data",
      "text": "Types of empirical data vary depending on the level of ground truth and the source of the ground truth. Purely observational data is the most readily available and is used most often. While this is rarely accompanied by full knowledge of the underlying structure, there are generally some dependencies that are known, either from common sense knowledge (such as temporal ordering) or from dependencies that have already been established by prior work. For a randomized controlled trial, the dependence between the measured treatment and outcome is generally taken as ground truth. The same is true for cases in which multiple potential outcomes can be recorded for each unit. This includes gene regulatory networks, flow cytometry analysis, and software systems, where essentially identical units can receive multiple treatments and thus produce multiple potential outcomes.\nBecause interventional measures and empirical data are used so infrequently, one might assume this is because such data sets are difficult to obtain. This is partially true\u2014there are significantly more observational data sets available than interventional data sets. However, a growing community is producing data sets that provide interventional effects. We describe some of them here.\nThe cause-effect pairs challenge [Mooij et al., 2016] provides data that is empirical and, while interventional effects are not available, the direction of causality is known. The 2016 Atlantic Causal Inference Conference Competition and subsequent competitions [Dorie et al., 2019, Hahn et al., 2019] created semi-synthetic data sets, producing synthetic treatment and outcome functions using covariates from a real-world system. A similar approach was used by Shimoni et al. [2018] for the IBM Causal Inference Benchmarking Framework. Flow cytometry data, measuring protein signaling pathways, is another common choice for interventional data [Sachs et al., 2005]. Dixit et al. [2016] provide data on gene expression, collected using their proposed Perturb-Seq technique to perform gene deletion interventions. There has also been work in partially randomized experiments, where a population is split into randomized and observational groups, creating parallel datasets for evaluation [Shadish et al., 2008]. Other sources of interventional and empirical data include results of advertising campaigns [Sun et al., 2015] and clinical studies [McDonald et al., 1992], as well as multiple challenges organized for machine learning conferences [Guyon et al., 2008, 2010]. Domain specific simulations are another useful source of data. While technically synthetic, a sufficiently sophisticated simulation falls on a spectrum between purely synthetic and purely empirical data. They are often highly complex, are created by someone other than the researcher, and are created for a purpose other than evaluation, making them ideal for evaluation. One popular simulation that is used for evaluation is the DREAM in silico data sets, since multiple combinations of single-gene interventions can be performed on identical networks [Schaffter et al., 2011].\nWe also introduce an additional source of empirical data where interventions are possible: large-scale software systems. These systems have many desirable properties for the purposes of empirical evaluation: (1) They are pre-existing systems created by people other than the researchers for a purpose other than evaluating algorithms for causal modeling; (2) They produce non-deterministic experimental results due to latent variables and natural stochasticity; (3) System parameters provide natural treatment variables; and (4) Each experiment is recoverable, allowing the same experiment to be performed multiple times with different combinations of interventions. Three such data sets are discussed in more detail in Section 5 and in the Supplementary Material.2"
    },
    {
      "heading": "3.4 How Different are the Results?",
      "text": "Readers may ask: In practice, what\u2019s the difference between using empirical data rather than synthetic data? If that difference is small, then the substantial extra work involved in evaluation with empirical data may not be worth the effort.\n2These data sets are available for download at http://kdl.cs.umass.edu/data.\nFigure 1: Comparison of TVD on empirical data and synthetic data derived from empirical data. (a) and (b): synthetic data with structure obtained from PC or GES. (c): TVD on empirical data.\nTo begin addressing this question, we conducted a series of experiments using the interventional data from the software systems mentioned above. Specifically, we used a common approach for generating somewhat realistic synthetic data. This approach uses an empirical data set to learn a causal model and then uses that model to generate synthetic data (and known ground truth) for model evaluation. While the final data set is synthetic, its structure may better approximate the empirical system, rather than being entirely defined by the researcher, lending it more credibility. We used this approach to generate synthetic data in the style of the three empirical data sets we generated from software systems. Since we now have both empirical and synthetic data, each with ground truth, we can use causal modeling algorithms to construct a model for both of these data sets and compare the conclusions we would draw from each.\nThe synthetic data used was created by first choosing an initial causal modeling algorithm to create a ground truth model from the empirical data. After learning a ground truth model with each of two algorithms that construct causal graphical models (PC and GES),3 we generated synthetic data using the resulting models. We then evaluated the same three algorithms on both the synthetic and empirical data. Figure 1 shows how mean TVD varies for different causal modeling algorithms and different data sets. The results shown are the mean TVD when evaluating PC, GES, and MMHC on two types of synthetic data sets (using the model as ground truth) and on the empirical data (using the known interventional effects). There is significant variability between the two methods of generating the synthetic ground truth model from the empirical data (PC and GES), both in the mean TVD and in the relative ordering of the algorithms. Comparing the synthetic and empirical results, some relative orderings of the algorithms are the same (e.g., network), but other orderings are significantly different (e.g., Postgres). These results suggest that algorithm performance cannot be expected to match between synthetic and empirical data, even when the synthetic data is created in a way that would be most expected to match aspects of the empirical data."
    },
    {
      "heading": "4 The Case for Interventional Measures",
      "text": "Many algorithms are currently evaluated based on their ability to learn causal structure. However, the actual desired task is almost never to model structure alone. In practice, estimating the magnitude of interventional effects is vitally important, and an algorithm that cannot distinguish between strong and weak effects is severely limited in scope. Despite this, the majority of current evaluations use observational or structural measures rather than measures of interventional effect."
    },
    {
      "heading": "4.1 Limitations of Observational Measures",
      "text": "Observational measures are widely used to evaluate algorithms for associational modeling, where the task of the algorithm is to discern statistical associations between two or more variables. In such applications, the primary focus is effectively modeling the magnitude and form of statistical dependence, rather than explicitly learning causal dependence. This highlights a severe and obvious limitation of observational measures:\n3We reach similar conclusions based on the results for MMHC, which are reported in the Supplementary Material.\nNon-causal\u2014Observational measures are, by definition, not causal. They measure the error of estimates of the outcome variable, but they do not measure that error under intervention. They provide a sense of how well an algorithm has learned statistical dependence, but not how well it has learned causal dependence. Despite this, observational measures are the only evaluation used in 23% (21/91) of papers surveyed."
    },
    {
      "heading": "4.2 Limitations of Structural Measures",
      "text": "Structural measures are easy to calculate, and they have a clear intuition. If an algorithm produces a causal structure and we know structural ground truth, it seems sensible to determine if the two structures match. This has led to the widespread adoption of structural measures: 55% (50) of surveyed papers used such measures, and 84% (42/50) of those used only structural measures. However, structural measures have several serious limitations:\nRequires known structure\u2014Calculating structural measures requires a full ground-truth graph structure, which is only rarely available for empirical data.\nConstrains research directions\u2014The prevalence of structural measures may constrain research to algorithms that can be evaluated with these measures. Algorithms that do not produce DAGs are less likely to be developed or favorably reviewed. Since structural measures can only be used by algorithms that produce a directed graphical model as output, they implicitly assume that directed graphical models are capable of accurately representing any causal process being modeled, an unlikely assumption.\nOblivious to magnitude and type of dependence\u2014Structural measures, by design, do not account for different magnitudes of dependence, so an error in an edge with a strong effect incurs the same penalty as an error in an edge with a very weak effect. In addition, structural measures are only able to measure which variables in a causal model change as the result of an intervention. In many cases, it is also necessary to determine how much or in what way a given target quantity will change with respect to an intervention.\nOblivious to likely treatments and outcomes\u2014In most cases, structural measures do not consider where an edge is located in the overall structure of the DAG, so an edge with many downstream effects is treated the same as a less central edge."
    },
    {
      "heading": "4.3 Benefits of Interventional Measures",
      "text": "In contrast to observational and structural measures, interventional measures have strong advantages:\nCorrespondence to actual use\u2014Interventional measures evaluate how well the model estimates interventional effects, which aligns more closely with the eventual use of nearly all causal models. For example, a directed acyclic graph is not the ultimate artifact of interest for most applications\u2014 DAGs are a representation that facilitates estimation of interventional effects [Spirtes et al., 2000, Pearl, 2009]. Thus, it seems natural to define an evaluation measure in terms of interventional effects rather than graphical structure.\nWeighting of different errors\u2014While most structural measures penalize each edge misorientation equally, interventional measures penalize misorientation errors proportionally to their effect on the estimation of interventional effect."
    },
    {
      "heading": "4.4 How Different are the Results?",
      "text": "Interventional measures are intended to capture something different than structural measures, but they are ultimately affected by the structure of the learned model, and we would expect structural errors to lead to interventional errors. Of course, interventional and structural measures are equal when structure and parameterizations are perfect, but they can differ significantly when the learned structure is only approximately correct (which is almost always the case). To assess the extent to which interventional measures capture different information than structural measures in such cases, we ran experiments using synthetic data. This allowed us to produce data where we could calculate both structural measures and interventional measures, since we have the full parameterized ground truth model to compare against.\nFigure 2: Structural and interventional measures compared on synthetic data with GES. For these experiments, we produced data from random DAG structures with conditional probability models drawn from a Dirichlet distribution. We generated 5000 instances, applied a causal modeling algorithm, and calculated various evaluation measures. Figure 2 shows the results for GES. SHD and SID are clearly strongly correlated, suggesting that both structural measures ultimately produce similar quality measures of the algorithm. However, SHD and TVD are only very weakly correlated, with many models scoring highly with one measure and poorly with the other. At least in this case, the interventional measure (TVD) appears to capture substantially different information than that of a structural measure (SHD). Results for PC and MMHC are reported in the Supplementary Material."
    },
    {
      "heading": "5 Example of an Evaluation",
      "text": "To further explain what we mean by empirical data and interventional measures, we describe one example of this type of evaluation, shown schematically in Figure 3. This example demonstrates one way that an evaluation with empirical data and interventional measures could be performed, though many other techniques are possible, depending on the algorithm, data source, evaluation measure, and the research question under consideration. In our example, we evaluate the PC algorithm [Spirtes et al., 2000], Greedy Equivalence Search (GES) [Chickering, 2003], and MMHC [Tsamardinos et al., 2006] by measuring total variation distance (an interventional measure defined later) on a data set produced by experimentation with a large-scale software system.\nThe most obvious way to evaluate how well an algorithm can learn causal models from real-world data is to compare the model\u2019s estimates to empirical data drawn from a system in which we can perform multiple interventions on the same units, giving us full interventional data in which we can assess every potential outcome for each unit. Large-scale software systems allow for this type of intervention because they let us run the same experiments multiple times under different conditions (e.g., different settings of key system parameters). An example of this is a Postgres database, where we can run the same queries with different settings of key configuration parameters. In this context, each query corresponds to a unit, a set of configuration parameters correspond to treatment, and variables such as runtime correspond to outcomes. Details about this data can be found in the Supplementary Material.\nMany algorithms for causal modeling are designed to run on observational data, in which only a single, non-randomized treatment assignment is observed for each unit. In the absence of an observational data set that matches our interventional data, we can create an observational-style data set by sub-sampling the full interventional data in a non-random manner. To do this, we select a single treatment assignment for each query. Selecting treatment at random is equivalent to a randomized controlled trial. In most observational contexts, however, treatment assignment would be based on covariates of the unit. For example, a database administrator might choose the configuration parameters based on features of each query. We use a similar process to create observational data by using a measured covariate of the query to probabilistically assign treatment.\nFigure 3: A diagram of one way to evaluate a causal modeling algorithm\nGiven such an observational data set, we can apply a causal modeling algorithm and learn a causal model. A fully parameterized model can produce an estimated interventional distribution P\u0302 by applying the do-calculus [Galles and Pearl, 1995]. Under this framework, causal quantities take the form of probability queries with do operators, for instance P (O|do(T = 1)). We can also estimate the actual interventional distribution P = P (O = o|do(T = t)) for any outcome o and treatment t, because we can measure the effects of both values of treatment for each query in our data set.\nWe then can use an interventional measure to compare the true interventional distribution P to the estimated distribution P\u0302 . One example of an interventional measure is total variation distance (TVD) [Lin, 1991], which measures the distance between two probability distributions. For discrete outcomes O, the quality of an estimated interventional distribution relative to a known distribution under TVD is straightforward to compute:\nTVP,P\u0302 ,T=t(O) = 1\n2\n\u2211\no\u2208\u2126(O)\n\u2223\u2223P (O = o|do(T = t))\u2212 P\u0302 (O = o|do(T = t)) \u2223\u2223,\nwhere \u2126(O) is the domain of O. This gives us a numerical measure of how well the estimated interventional estimates match the ground truth. A single TVD value is computed for each causal effect, which can then be aggregated for comparison. Results of this evaluation on the software data is shown in Figure 1c. For these datasets, we can conclude that GES has the best overall performance."
    },
    {
      "heading": "6 Conclusion",
      "text": "Evaluation is a key mechanism that determines how algorithms are viewed within the community, what research directions are pursued next, and whether our research has broader impacts outside the community. Our current evaluation techniques aim too low, and they fail to evaluate the full range of questions that our research goals imply.\nWe are not the first to point out the need for more robust evaluation techniques. Some of the datasets we discuss were created in response to recognition that better evaluation was necessary [Dorie et al., 2019, Shimoni et al., 2018, Mooij et al., 2016]. In addition, prior work has examined the importance of testing the generalizability of causal inferences drawn from observational data [Zhao et al., 2019, Keane and Wolpin, 2007] and comparing causal effects drawn from observational and experimental data [Cook et al., 2008, Eckles et al., 2016, Eckles and Bakshy, 2017, Gordon et al., 2019]. However, despite this, as our survey shows, empirical evaluation with interventional measures is rarely used by computer science researchers.\nWe acknowledge that, while the evaluation techniques we advocate are applicable to a wide range of algorithms, data sets may not be available for every task. The diverse tasks of causal modeling algorithms make it difficult to recommend a single data set and evaluation measure to evaluate every algorithm. However, the data sets and measures that are most commonly used are largely insufficient. The community would benefit if more data sets with interventional effects were created and made available for public use, allowing for a breadth of evaluation options.\nWe do not advocate abandoning synthetic data and structural measures. Both have many uses for evaluating algorithm performance and can be indispensable scientific tools. However, they are insufficient on their own. Instead, they should be viewed as a first step in evaluation. If we want causal modeling algorithms to be adopted outside our research community, we need demonstrations of their utility outside of a laboratory setting. If we do not evaluate on empirical data, we cannot be certain our algorithms will perform well on real data, and if we do not evaluate with interventional measures, we cannot be certain that the causal effects the algorithm infers will translate to actual, substantial causal effects. Expanding our routine evaluations will substantially improve the credibility and comparability of results, the external validity and trustworthiness of algorithms, and the efficiency with which we conduct our research."
    },
    {
      "heading": "Acknowledgments",
      "text": "This material is based upon work supported by the United States Air Force under Contract No, FA8750-17-C-0120. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the United States Air Force."
    },
    {
      "heading": "1 Additional Details on Software Data",
      "text": "We introduce a source of empirical data where interventions are possible: large-scale software systems. We performed experiments on three large computational systems: Postgres, the Java Development Kit, and HTTP processing. These systems have many desirable properties for the purposes of empirical evaluation: (1) They are pre-existing systems created by people other than the researchers for a purpose other than evaluating algorithms for causal discovery; (2) They produce non-deterministic experimental results due to latent variables and natural stochasticity; (3) System parameters provide natural treatment variables; and (4) Each experiment is recoverable, allowing the same experiment to be performed multiple times with different combinations of interventions.\nWithin each computational system, we measure three classes of variables: outcomes, treatments, and subject covariates. Here, outcomes are measurements of the result of a computational process, treatments correspond to system configurations and are selected such that they could plausibly induce changes in outcomes, and subject covariates logically exist prior to treatment and are invariant with respect to treatment. Using these variables, we can apply all combinations of treatments to all subjects, and we can use these results to estimate actual interventional distributions for the effects of each treatment variable on each outcome variable. We can also then sub-sample these experimental data sets in a manner which simulates observational bias to produce observational-style data sets, allowing us to evaluate an algorithm\u2019s performance on pseudo-observational data and evaluate it using actual interventional effects. These data sets will be made available after publication.\nWe had a number of goals in mind when gathering data from our real domains:\n\u2022 Causal Sufficiency: The algorithms we studied require that no pair of variables in the model are both caused by a latent variable. We can guarantee this is true for pairs of treatments and outcomes (since treatments have no parents in the original data set), but needed to employ domain knowledge to limit sources of causal sufficiency violations with regard to other pairs of variables.\n\u2022 Acyclicity: Each of the systems can be described by a \u201csingle-shot\u201d computational process which starts and finishes without the possibility for feedback.\n\u2022 Instance Independence: We took efforts to ensure that each execution of the computational process was independent of previous executions. In most cases, this required clearing caches and resetting other aspects of system state.\n\u2022 Plausible Dependence: We selected variables that we believed would be causally related.\nEach domain is characterized by three classes of variables: subject covariates, treatments, and outcomes. Under the factorial experiment design, outcomes were measured for every combination of subjects and treatments. This yields a data set with many records for the same subject, as in the example in Table 1. To permit greater opportunities for observational sampling, we performed multiple trials of each factorial experiment. Given the difficulty associated with modeling highly complicated outcomes such as runtime, we employed a normalization scheme for each data set, dividing outcome values by a \u201cbaseline\u201d value\u2014the median control-case outcome value. Thus, we ultimately recorded outcomes which represent a deviation from this baseline. In this regard, our experimental results resemble a within-subjects design ?, although without many of the pitfalls that plague experiments on humans, such as non-independence of outcome measurements. In the original data from each\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n91 0.\n05 38\n7v 2\n[ cs\n.A I]\n1 N\nov 2\n01 9\ndomain, subject covariates are either discrete, continuous, or binary; treatments are binary; and outcomes are continuous. We converted each of the variables to a discrete representation to make parametrization and inference more robust."
    },
    {
      "heading": "1.1 Java Development Kit",
      "text": "Our experiments on the Java Development Kit (version 1.7.0 60) used 2,500 Java projects obtained from GitHub as the subjects under study. We retrieved only projects which use the Maven build tool to facilitate automated compilation and execution. Additionally, we constrained our search to include only projects which had unit tests. This may introduce selection bias in our data collection processes, but this is acceptable. It is not important that our conclusions generalize to some population of computational systems, only that there are causal dependencies which hold on the sub-population under investigation. Of those, 473 compiled and ran without intervention. This group yielded a total of 7,568 subject-treatment combinations. For each combination, we compile and execute the unit tests of the Java project. In order to obtain full state recovery between each trial, any compiled project files were cleared between executions. Thirty-five CPU days were required to collect this data using several Amazon EC2 instances."
    },
    {
      "heading": "1.1.1 Treatments",
      "text": "\u2022 Aggressive Compiler Optimization: Disabling this option (enabled by default) prevents some compiler optimizations from running, potentially slowing down execution time but perhaps reducing compilation time. This option is disabled with the javac option -XX:+AggressiveOpts.\n\u2022 Emission of Debugging Symbols: Debugging symbols are used to provide a map through the compiled source code that can be used for interactive debugging and diagnostics. Inclusion of these symbols may require some time during the compilation phase, increase the size of the compiled program, and could possibly impact runtime. This corresponds to the -g flag of javac.\n\u2022 Garbage Collection Methodology: The Java Development Kit supports several garbage collection schemes. Two were considered: parallel and serial. These schemes are activated with the -XX:-UseParallelGC or -XX:-UseSerialGC arguments.\n\u2022 Code Obfuscation: Several third-party tools are capable of obfuscating compiled code, making reverse-engineering difficult. This process could also affect the size of the compiled project files. The yGuard1 tool was used for this purpose."
    },
    {
      "heading": "1.1.2 Outcomes",
      "text": "\u2022 Number of Bytecode Instructions: Before execution, Java code is compiled to an intermediate language referred to as bytecode. We measured the number of atomic instructions, or operations, in this compiled code to form this outcome using a custom-built bytecode analysis tool based on Javassist2.\n\u2022 Total Unit Test Time: Each project we gathered contains one or more unit tests. To capture the runtime of the full unit test workload, we computed the sum of runtimes of all unit tests for a given project.\n\u2022 Allocated Bytes: The Java Virtual Machine supports a profiling option (-agentlib:hprof=heap=sites) which can be used to track heap statistics throughout a program\u2019s execution. We utilized this feature to obtain the total number of bytes allocated during unit test execution.\n\u2022 Compiled Code Size: Java programs are often packaged in an format known as a JAR (Java ARchive). To characterize the size of the compiled code, we recorded the size in bytes of the associated JAR file.\n\u2022 Compilation Time: In order to execute unit tests, the entire project needs to be compiled. This outcome represents the time used to convert all source files to their bytecode equivalents."
    },
    {
      "heading": "1.1.3 Subject Covariates",
      "text": "All subject covariates were obtained using the JavaNCSS tool3.\n\u2022 # NCSS (non-comment source statements) in Project Source: This covariate is highly predictive of compiled code size. Conceivably, in observational settings, large projects could also be associated with more liberal use of advanced compilation settings and tools, such as a code obfuscator.\n\u2022 # NCSS, Functions, and Classes in Unit Test Source: These covariates are somewhat representative of the unit test workload. Projects with many lengthy unit tests may also have longer total unit test runtime.\n\u2022 # \u201cJavadoc\u201d comments in Unit Test Source: This covariate could be indicative of code quality. Well-commented code is perhaps more likely to be found in high-quality projects. This code may be more likely to be used in production environments, and thus could be less likely to be observed with debugging symbols. This feature is used in the treatment-biasing procedure for construction of observational data sets."
    },
    {
      "heading": "1.2 Postgres",
      "text": "Consistent with a data warehousing scenario, we employ a fixed database for our Postgres (version 9.2.2) experiments: a sample of the data from Stack Overflow, drawn from the Stack Exchange Data Explorer4. The data explorer also houses many user-generated queries. We collected 29,375 of the most popular queries to use as subjects for this study. Stack Exchange\u2019s data warehouse uses Microsoft SQL Server, which does not completely overlap with Postgres in supported features and syntax. Some queries use only ANSI-compliant syntax and run successfully on either SQL Server or Postgres. To obtain as large a set of subjects as possible, we employed a semantics-preserving\n1http://www.yworks.com/en/products yguard about.html 2http://www.csg.ci.i.u-tokyo.ac.jp/ chiba/javassist/ 3http://javancss.codehaus.org/ 4http://data.stackexchange.com/\nquery rewriting scheme to adapt queries into Postgres-compliant syntax wherever possible. This yielded a set of 11,252 user-generated queries which executed successfully within Postgres for a total of 90,016 subject-treatment combinations. In order to recover system state between trials, the shared memory setting (specifying how much main memory Postgres can use for caching) was set to 128 kilobytes, limiting caching significantly. Any queries which required more than 30 seconds to execute were marked as \u201cfailures\u201d in order to prevent long-running queries from holding up other queries, which typically required one second to execute. As with the JDK data set, this may induce sampling bias, but we are not aiming for our experimental findings to generalize to the broader population of database queries."
    },
    {
      "heading": "1.2.1 Treatments",
      "text": "\u2022 Indexing: A common administration task is to identify indices that can be used to accelerate lookup of commonly-referenced columns with a particular value or falling within a range. For our experiments, we employed two indexing settings: no indexing, and indexing on primary key/foreign key fields. Domain knowledge suggests that that the latter approach would dramatically reduce runtime of some queries. In all cases, the default B-tree index was employed.\n\u2022 Page Cost Estimates: In order to determine if an index should be used, the database employs estimates of the relative cost of sequentially accessing disk pages and randomly accessing disk pages. We utilized two extremes for this setting: one scheme in which random page access is estimated to be fast, relative to the sequential page access, and one scheme in which the opposite relation holds. The corresponding database settings we adjusted were random page cost and seq page cost.\n\u2022 Working Memory Allocation: The database engine can make use of fast random-access memory, if available, to store intermediate query results. The amount of working memory that is allocated to the system can be controlled with a configuration option. For our investigation, we employed a low-memory setting and a high-memory setting, with background knowledge suggesting that the latter would result in faster-executing queries. This treatment was instrumented with the work mem and temp buffers options."
    },
    {
      "heading": "1.2.2 Outcomes",
      "text": "\u2022 Blocks Read from Shared and Temporary Memory: These two outcomes identify the number of blocks, or memory regions, that were read during query execution. Shared memory is persistent (disk) and is accessed during normal table-retrieval procedures. Temporary memory is volatile (main memory) and is used for staging ordering or joining operations.\nServer\nMobile User Agent Proxy ServerCompression\n# HTML Tags ElapsedRaw Content Length Compressed\nContent Length\n# HTML Attributes"
    },
    {
      "heading": "1.2.3 Subject Covariates",
      "text": ""
    },
    {
      "heading": "1.3 Hypertext Transfer Protocol",
      "text": "For our experiment on HTTP & networking infrastructure, we used requests to specific web sites as subjects. We identified a number of target sites through a breadth-first web crawl initiated at dmoz.org. We ended the crawl after retrieving 5,472 sites. For 4,350 of those sites, we were able to issue successful web requests with all combinations treatments, yielding 34,800 subject-treatment combinations. We employed numerous techniques to ensure that content would not be cached, which could induce carryover across treatment regimes."
    },
    {
      "heading": "1.3.1 Treatments",
      "text": "\u2022 Use of a Mobile User Agent: Web browsers supply a user agent to identify themselves to the web servers that they request pages from. Some sites have different versions for mobile applications. We artificially adjusted the user agent from a standard user agent to a mobile user agent to explore this phenomena. This is accomplished with the HTTP User-Agent header.\n\u2022 Proxy Server: Web requests can be routed through a proxy, a server which issues web requests on behalf of a client. The additional time required to route the request to and from the proxy server can increase the elapsed time of the request. Our experiments were executed with Amazon EC2. Our \u201cclient\u201d computers were making web requests from the east cost of the United States, and a proxy server was set up on the west coast.\n\u2022 Compression: Applications can use the HTTP protocol to request that content be delivered with or without compression, possibly reducing the cross-network transmission time. In one compression configuration, the client requests identity compression, indicating that the content should be transmitted at face value. In another compression scheme, the client requests gzip, a common and effective scheme for HTTP content compression."
    },
    {
      "heading": "1.3.2 Outcomes",
      "text": "\u2022 # of HTML Attributes and Tags: These two outcomes describe the logical structure of the page. They may vary with respect to \u201cmobile user agent\u201d.\n\u2022 Elapsed Time: The time between issuance of the request and receipt of a response. This could be affected by network characteristics, which are determined in part by the time at which the request is issued and whether a proxy server is employed. Requests containing smaller payloads (influenced by compression) may also be faster to service.\n\u2022 Decompressed and Raw Content Length: Two outcomes representing the size of a web page before and after content decompression, if applicable."
    },
    {
      "heading": "1.3.3 Subject Covariates",
      "text": "Only one subject covariate was identified for the HTTP domain, the web server reported via the Server header. This variable was coarsened into a version with 7 levels: Apache/2, Other Apache, Microsoft-IIS, nginx, Other, and Unknown."
    },
    {
      "heading": "2 Identifying Consistent DAGs",
      "text": "To identify DAGs that can consistently estimate the all interventional distributions P (O|do(T )), we need to ensure that (1) the parent set of T is a valid adjustment set with respect to O, and (2) if T has a causal effect on O, there is a chain connecting T and O in the DAG model. The first condition is straightforward to satisfy since we know the only parent of any treatment to be the covariate used to introduce observational bias. The second condition requires identification of which pairs of treatments and outcomes are causally related. These d-connection properties were identified for each domain using the full interventional data set using the Friedman test for blocked difference in means, allowing for correction of subject variability ?. An edge was introduced between any causally related pair to satisfy condition (2). Then, ground truth interventional distributions P (O|do(T = t)) were produced by applying the do-Calculus model adjustment rules, and answering probability queries P (P |T = t) on the resulting model using belief propagation."
    },
    {
      "heading": "3 Pseudo-Observational Configurations",
      "text": "We can transform the factorial experiments on our real domains into pseudo-observational data by sub-sampling the experimental data in a way that is correlated with a \u201csubject covariate\u201d. This mirrors the process of treatment self-selection common to observational data. This transformation is outlined in Algorithm 1.\nInput: Interventional data set I , biasing strength \u03b2 \u2265 0, biasing covariate C Output: Observationally biased data set O, |O| = nd l\u2190 The number of distinct values of C foreach Subject e \u2208 I do\nLet Ce \u2208 {1..l} represent the C value of subject e Assign\u2190 {} foreach Treatment Tj do\nsej \u2190 { 1 if Ce \u00d7 j is even \u22121 if Ce \u00d7 j is odd p\u2190 logit\u22121(sej\u03b2) tj \u2190 Bernoulli(p) Assign\u2190 Assign \u222a {Tj = tj}\nend M \u2190 Record in I corresponding to (e,Assign) O \u2190 O \u222aM\nend Algorithm 1: Logistic Sampling of Observational Data"
    },
    {
      "heading": "4 Limitations of Empirical Data",
      "text": "In the paper, we discuss popular sources of empirical data that is suitable for evaluation. These data sets differ significantly in many ways, including level of realism and data quality, and they each have different benefits and limitations.\nThe cause-effect pairs challenge [?] provides observational data on pairs of variables where the direction of causality is known from domain knowledge. This data set is useful for evaluating bivariate orientation algorithms, but the lack of any additional measured covariates limits its utility for evaluating multivariate structure learning algorithms.\nThe 2016 Atlantic Causal Inference Conference Competition data [?] and the IBM Causal Inference Benchmarking Framework ? use covariates taken from a real-world data set, allowing for potentially complicated interactions between them. Treatment and outcome functions were then generated synthetically, using a variety of data generating processes to allow for the construction of many data sets with different features. This allows algorithms to be tested on many data sets, providing a more robust evaluation. However, the need to construct synthetic treatment and outcome functions limits the level of realism.\nThe software data we collected contains measurements of covariates, treatments, and outcomes from three real-world systems. While the treatment function is generated synthetically, the outcome function is not, lending the ground truth causal effects from treatment to outcome a high degree of realism. However, as with the above ACIC and IBM data sets, the treatment function still needs to be synthetically defined.\nThe flow cytometry data provided by ? contains measurements of protein signaling pathways, where multiple activating and inhibitory interventions were performed. However, the ground truth is not clearly obtainable and most analysis using this dataset relies on structural measures.\nPartially randomized experiments, where a population is split into randomized and an observational groups, are another useful source of empirical data [?]. The collection of randomized data drawn from the same base population as observational data creates a convenient ground truth for causal effect estimation. However, due the nature of these experiments, they require careful experimental design to make sure the populations are equvalient and the treatments are correctly assigned and measured.\nThe DREAM in silico data sets [?] are taken from a sophisticated simulation derived from multiple known gene regulatory network structures, which, while non-empirical, is intended to be complex enough to approximate empirical data. However, realism is limited due to the use of a simulator."
    },
    {
      "heading": "5 Additional Experiments",
      "text": "In the paper, we provided experiments that demonstrate that TVD and structural measures provide different information and that information is relevant for over and under specification. To expand on these results, we performed an additional experiment to evaluate if different types of measures would lead to different conclusions about the relative performance of causal modeling algorithms. Figure 4 shows results on synthetic data that demonstrate that TVD does, inf act, imply a very different ordering of the relative performance of different learning algorithms than that implied by SHD and SID. We began by constructing 30 random DAGs with 14 variables and E[N ] = 2. We generated parameters on those DAGs using each of the synthetic data techniques and sampled 5,000 data points from each DAG. Then, we applied PC, MMHC, and GES to the resulting data sets and measured the SID, SHD, and sum of pairwise total variations. As shown in Figure 4, some of the findings that would be reached with SID and SHD are not supported by a TV evaluation. The structural measures suggest that MMHC outperforms PC on the Dirichlet domain. However, the performance of the two algorithms is statistically indistinguishable as measured by TV. When measured with SID or SHD, GES does not outperform either MMHC or PC. However, GES is consistently the best performing algorithm in terms of interventional distribution accuracy.\nExperiments in the paper demonstrate that TVD can, at least in some cases, provide information that structural measures cannot. However, that does not mean that the additional information is\nuseful. To address this concern, we sought to measure how TVD responds to specific types of errors in learned structure. Specifically, we evaluate the effects of over-specification (extraneous edges) and under-specification (omitted edges) on model performance. We used our three empirical data sets drawn from large-scale computational systems (JDK, Postgres, and HTTP) to perform this analysis. From the original exhaustive experiments, we can identify which treatment-outcome pairs are causally related. We construct a partial DAG, consisting only of edges between treatment and outcome, by introducing an edge between each pair of causally related treatment and outcome. Then, a pseudo-observational data set can be constructed by sub-sampling treatment assignments according to a biasing covariate (details in Supplemental Materials). The resulting DAG model (illustrated for the JDK data set in Figure 1) consistently estimates distributions P (O|do(T = t)) for all treatment-outcome pairs.\nWe altered the consistent models of each data set to induce over-specification and underspecification. To quantify the effects of over-specification, we produced models in which one of the treatment variables had a directed edge into every outcome, regardless of the causal relationships in the true model. To quantify the effects of under-specification, we produced models in which one of the treatment variables had no outgoing edges. This process was repeated for each of our three domains and each treatment variable within that domain. For each model, a sum of pairwise total variations was computed as \u2211 T,O TVP,P\u0302 ,T=1(O), where P represents the reference distribu-\ntion given by the consistent model (as in Figure 1) and P\u0302 represents the distribution induced by the altered model. A comparison of TVD, SHD, and SID on these experiments is shown in Table 2.\nTwo properties are apparent. First, over-specification is penalized differently by different evaluation measures. For small data sets, such as the JDK domain, over-specified models have zero SID but significant TVD values due to loss of statistical efficiency. Second, penalizing over-specification and under-specification with equal cost, as in SHD, is inconsistent with interventional distribution quality. In these domains, model under-specification has 2-5x the distributional impact of underspecification as measured by total variation."
    },
    {
      "heading": "6 Additional Details on Presented Experiments",
      "text": "Figures 5 and 6 show the results of comparing synthetic and interventional measures on synthetic data for both MMHC and PC. (results for GES were presented in the paper) Interestingly, while the correlation between SID and SHD is relatively consistent for all three structure learning algorithms, the correlation between TVD and SHD varies substantially, from seemingly completely uncorrelated (GES) to very clearly correlated (PC). This suggests that, in some cases, structural measures can provide a decent proxy for interventional measures. However, it is unlikely that the researcher knows this to be the case ahead of time, and the comparative difference in TVD between the three algorithms suggests the value of using TVD when comparing multiple causal learning algorithms.\nWe also provide additional results for experiments discussed in the paper that created synthetic data sets by learning their structure from empirical data. While we reported results using GES and PC, here we show results for MMHC. Figure 7 shows the performance of three learning algorithms (GES, MMHC, and PC). MMHC was used to infer a causal model from empirical data, and that model was then used to generate the synthetic data. Compared with the results in the paper, the relative performance of different algorithms looks somewhat similar to the results using GES, though there are some differences (e.g., PC is clearly the worst on all data sets in Figure 7, while this is not the case for GES in Figure 1 in the paper).\nSample sizes for some of the software system data sets are small, so in Figure 7 and Figure 1 in the paper, we report results as distributions over 30 trials for each algorithm and data set."
    }
  ],
  "title": "The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data",
  "year": 2019
}
