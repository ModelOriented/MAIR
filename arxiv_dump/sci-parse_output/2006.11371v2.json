{"abstractText": "Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.", "authors": [{"affiliations": [], "name": "Arun Das"}], "id": "SP:da01c6f3b8c7c03e4c754789ff1bb00f79c3b1c3", "references": [{"authors": ["A.D. Torres", "H. Yan", "A.H. Aboutalebi", "A. Das", "L. Duan", "P. Rad"], "title": "Patient Facial Emotion Recognition and Sentiment Analysis Using Secure Cloud With Hardware Acceleration", "venue": "Comput. Intell. Multimed. Big Data Cloud with Eng. Appl., pp. 61\u201389, 2018.", "year": 2018}, {"authors": ["S.M. Lee", "J.B. Seo", "J. Yun", "Y.-H. Cho", "J. Vogel-Claussen", "M.L. Schiebler", "W.B. Gefter", "E.J. Van Beek", "J.M. Goo", "K.S. Lee"], "title": "Deep learning applications in chest radiography and computed tomography", "venue": "pp. 75\u201385, 2019.", "year": 2019}, {"authors": ["R. Chen", "L. Yang", "S. Goodison", "Y. Sun"], "title": "Deep-learning approach to identifying cancer subtypes using high-dimensional genomic data", "venue": "Bioinformatics, vol. 36, no. 5, pp. 1476\u20131483, 2020.", "year": 2020}, {"authors": ["R. Sayres", "A. Taly", "E. Rahimy", "K. Blumer", "D. Coz", "N. Hammel", "J. Krause", "A. Narayanaswamy", "Z. Rastegar", "D. Wu"], "title": "Using a deep learning algorithm and integrated gradients explanation to assist grading for diabetic retinopathy", "venue": "Ophthalmology, vol. 126, no. 4, pp. 552\u2013564, 2019.", "year": 2019}, {"authors": ["A. Das", "P. Rad", "K.K.R. Choo", "B. Nouhi", "J. Lish", "J. Martel"], "title": "Distributed machine learning cloud teleophthalmology IoT for predicting AMD disease progression", "venue": "Future Generation Computer Systems, vol. 93, pp. 486\u2013498, 2019.", "year": 2019}, {"authors": ["J. Son", "J.Y. Shin", "H.D. Kim", "K.-H. Jung", "K.H. Park", "S.J. Park"], "title": "Development and validation of deep learning models for screening multiple abnormal findings in retinal fundus images", "venue": "Ophthalmology, vol. 127, no. 1, pp. 85\u201394, 2020.", "year": 2020}, {"authors": ["N. Mohammadian Rad", "S.M. Kia", "C. Zarbo", "T. van Laarhoven", "G. Jurman", "P. Venuti", "E. Marchiori", "C. Furlanello"], "title": "Deep learning for automatic stereotypical motor movement detection using wearable sensors in autism spectrum disorders", "venue": "Signal Processing, vol. 144, pp. 180\u2013191, Mar 2018.", "year": 2018}, {"authors": ["A.S. Heinsfeld", "A.R. Franco", "R.C. Craddock", "A. Buchweitz", "F. Meneguzzi"], "title": "Identification of autism spectrum disorder using deep learning and the ABIDE dataset", "venue": "NeuroImage: Clinical, vol. 17, pp. 16\u201323, 2018.", "year": 2018}, {"authors": ["S.H. Silva", "A. Alaeddini", "P. Najafirad"], "title": "Temporal graph traversals using reinforcement learning with proximal policy optimization", "venue": "IEEE Access, vol. 8, pp. 63 910\u201363 922, 2020.", "year": 2020}, {"authors": ["C. You", "J. Lu", "D. Filev", "P. Tsiotras"], "title": "Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning", "venue": "Robotics and Autonomous Systems, vol. 114, pp. 1\u201318, Apr 2019.", "year": 2019}, {"authors": ["S. Grigorescu", "B. Trasnea", "T. Cocias", "G. Macesanu"], "title": "A survey of deep learning techniques for autonomous driving", "venue": "Journal of Field Robotics, vol. 37, no. 3, pp. 362\u2013386, 2020.", "year": 2020}, {"authors": ["D. Feng", "C. Haase-Schutz", "L. Rosenbaum", "H. Hertlein", "C. Glaser", "F. Timm", "W. Wiesbeck", "K. Dietmayer"], "title": "Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges", "venue": "IEEE Transactions on Intelligent Transportation Systems, pp. 1\u201320, 2020.", "year": 2020}, {"authors": ["A. Sahba", "A. Das", "P. Rad", "M. Jamshidi"], "title": "Image Graph Production by Dense Captioning", "venue": "2018 World Autom. Congr., vol. 2018-June. IEEE, Jun 2018, pp. 1\u20135.", "year": 2018}, {"authors": ["N. Bendre", "N. Ebadi", "J.J. Prevost", "P. Najafirad"], "title": "Human action performance using deep neuro-fuzzy recurrent attention model", "venue": "IEEE Access, vol. 8, pp. 57 749\u201357 761, 2020.", "year": 2020}, {"authors": ["A. Boles", "P. Rad"], "title": "Voice biometrics: Deep learning-based voiceprint authentication system", "venue": "2017 12th System of Systems Engineering Conference (SoSE). IEEE, 2017, pp. 1\u20136.", "year": 2017}, {"authors": ["S. Panwar", "A. Das", "M. Roopaei", "P. Rad"], "title": "A deep learning approach for mapping music genres", "venue": "2017 12th System of Systems Engineering Conference (SoSE). IEEE, 2017, pp. 1\u20135.", "year": 2017}, {"authors": ["G.D.L.T. Parra", "P. Rad", "K.-K.R. Choo", "N. Beebe"], "title": "Detecting internet of things attacks using distributed deep learning", "venue": "Journal of Network and Computer Applications, p. 102662, 2020.", "year": 2020}, {"authors": ["H. Chacon", "S. Silva", "P. Rad"], "title": "Deep learning poison data attack detection", "venue": "2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI). IEEE, 2019, pp. 971\u2013978.", "year": 2019}, {"authors": ["A. Kwasniewska", "M. Szankin", "M. Ozga", "J. Wolfe", "A. Das", "A. Zajac", "J. Ruminski", "P. Rad"], "title": "Deep Learning Optimization for Edge Devices: Analysis of Training Quantization Parameters", "venue": "IECON 2019 - 45th Annu. Conf. IEEE Ind. Electron. Soc., pp. 96\u2013101, Oct 2019.", "year": 2019}, {"authors": ["C. Zhang", "P. Patras", "H. Haddadi"], "title": "Deep learning in mobile and wireless networking: A survey", "venue": "IEEE Communications Surveys & Tutorials, vol. 21, no. 3, pp. 2224\u20132287, 2019.", "year": 2019}, {"authors": ["High Level Independent Group on Artificial Intelligence (AI HLEG"], "title": "Ethics Guidelines for Trustworthy AI", "venue": "Euorpean Comm., 2019. 22", "year": 2019}, {"authors": ["C. Cath", "S. Wachter", "B. Mittelstadt", "M. Taddeo", "L. Floridi"], "title": "Artificial intelligence and the good society: the us, eu, and uk approach", "venue": "Science and engineering ethics, vol. 24, no. 2, pp. 505\u2013528, 2018.", "year": 2018}, {"authors": ["K.H. Keskinbora"], "title": "Medical ethics considerations on artificial intelligence", "venue": "Journal of Clinical Neuroscience, vol. 64, pp. 277\u2013282, Jun 2019.", "year": 2019}, {"authors": ["A. Etzioni", "O. Etzioni"], "title": "Incorporating Ethics into Artificial Intelligence", "venue": "The Journal of Ethics, vol. 21, no. 4, pp. 403\u2013418, Dec 2017.", "year": 2017}, {"authors": ["N. Bostrom", "E. Yudkowsky"], "title": "The ethics of artificial intelligence", "venue": "The Cambridge Handbook of Artificial Intelligence, K. Frankish and W. M. Ramsey, Eds. Cambridge: Cambridge University Press, 2014, pp. 316\u2013334.", "year": 2014}, {"authors": ["B.C. Stahl", "D. Wright"], "title": "Ethics and privacy in ai and big data: Implementing responsible research and innovation", "venue": "IEEE Security & Privacy, vol. 16, no. 3, pp. 26\u201333, 2018.", "year": 2018}, {"authors": ["D.S. Weld", "G. Bansal"], "title": "The challenge of crafting intelligible intelligence", "venue": "Communications of the ACM, vol. 62, no. 6, pp. 70\u201379, May 2019.", "year": 2019}, {"authors": ["A. Lui", "G.W. Lamb"], "title": "Artificial intelligence and augmented intelligence collaboration: regaining trust and confidence in the financial sector", "venue": "Information & Communications Technology Law, vol. 27, no. 3, pp. 267\u2013283, Sep 2018.", "year": 2018}, {"authors": ["M. Hengstler", "E. Enkel", "S. Duelli"], "title": "Applied artificial intelligence and trustThe case of autonomous vehicles and medical assistance devices", "venue": "Technological Forecasting and Social Change, vol. 105, pp. 105\u2013120, Apr 2016.", "year": 2016}, {"authors": ["L. Chen", "A. Cruz", "S. Ramsey", "C.J. Dickson", "J.S. Duca", "V. Hornak", "D.R. Koes", "T. Kurtzman"], "title": "Hidden bias in the dud-e dataset leads to misleading performance of deep learning in structure-based virtual screening", "venue": "PloS one, vol. 14, no. 8, p. e0220113, 2019.", "year": 2019}, {"authors": ["R. Challen", "J. Denny", "M. Pitt", "L. Gompels", "T. Edwards", "K. Tsaneva- Atanasova"], "title": "Artificial intelligence, bias and clinical safety", "venue": "BMJ Quality & Safety, vol. 28, no. 3, pp. 231\u2013237, Mar 2019.", "year": 2019}, {"authors": ["F.H. Sinz", "X. Pitkow", "J. Reimer", "M. Bethge", "A.S. Tolias"], "title": "Engineering a Less Artificial Intelligence", "venue": "Neuron, vol. 103, no. 6, pp. 967\u2013979, Sep 2019.", "year": 2019}, {"authors": ["O. Osoba", "W. Welser"], "title": "An Intelligence in Our Image: The Risks of Bias and Errors in Artificial Intelligence", "venue": "RAND Corporation,", "year": 2017}, {"authors": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "title": "Adversarial Machine Learning at Scale", "venue": "5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, Nov 2016.", "year": 2017}, {"authors": ["I. Goodfellow", "J. Shlens", "C. Szegedy"], "title": "Explaining and harnessing adversarial examples", "venue": "3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, 2015.", "year": 2015}, {"authors": ["J. Su", "D.V. Vargas", "K. Sakurai"], "title": "One Pixel Attack for Fooling Deep Neural Networks", "venue": "IEEE Transactions on Evolutionary Computation, vol. 23, no. 5, pp. 828\u2013841, Oct 2019.", "year": 2019}, {"authors": ["S. Huang", "N. Papernot", "I. Goodfellow", "Y. Duan", "P. Abbeel"], "title": "Adversarial Attacks on Neural Network Policies", "venue": "5th International Conference on Learning Representations, ICLR 2017 - Workshop Track Proceedings, Feb 2017.", "year": 2017}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "pp. 1\u201338, 2019.", "year": 2019}, {"authors": ["K. Sokol", "P. Flach"], "title": "Explainability fact sheets: A framework for systematic assessment of explainable approaches", "venue": "FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 2020, pp. 56\u201367.", "year": 2020}, {"authors": ["C. Rudin"], "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "venue": "Nature Machine Intelligence, vol. 1, no. 5, pp. 206\u2013215, May 2019.", "year": 2019}, {"authors": ["D. Doran", "S. Schulz", "T.R. Besold"], "title": "What does explainable AI really mean? A new conceptualization of perspectives", "venue": "CEUR Workshop Proceedings, 2018.", "year": 2018}, {"authors": ["D. Castelvecchi"], "title": "Can we open the black box of ai?", "venue": "Nature News,", "year": 2016}, {"authors": ["A. Adadi", "M. Berrada"], "title": "Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)", "venue": "IEEE Access, vol. 6, pp. 52 138\u201352 160, 2018.", "year": 2018}, {"authors": ["F.K. Dosilovic", "M. Brcic", "N. Hlupic"], "title": "Explainable artificial intelligence: A survey", "venue": "2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2018 - Proceedings, 2018.", "year": 2018}, {"authors": ["S. Chakraborty", "R. Tomsett", "R. Raghavendra", "D. Harborne", "M. Alzantot", "F. Cerutti", "M. Srivastava", "A. Preece", "S. Julier", "R.M. Rao", "T.D. Kelley", "D. Braines", "M. Sensoy", "C.J. Willis", "P. Gurram"], "title": "Interpretability of deep learning models: A survey of results", "venue": "2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI). IEEE, Aug 2017, pp. 1\u20136.", "year": 2017}, {"authors": ["Z. Zhu", "E. Albadawy", "A. Saha", "J. Zhang", "M.R. Harowicz", "M.A. Mazurowski"], "title": "Deep learning for identifying radiogenomic associations in breast cancer", "venue": "Computers in biology and medicine, vol. 109, pp. 85\u201390, 2019.", "year": 2019}, {"authors": ["D. van Thiel", "W.F.F. van Raaij"], "title": "Artificial Intelligent Credit Risk Prediction: An Empirical Study of Analytical Artificial Intelligence Tools for Credit Risk Prediction in a Digital Era", "venue": "Journal of Accounting and Finance, vol. 19, no. 8, Dec 2019.", "year": 2019}, {"authors": ["J. Turiel", "T. Aste"], "title": "Peer-to-peer loan acceptance and default prediction with artificial intelligence", "venue": "Royal Society Open Science, vol. 7, no. 6, p. 191649, 2020.", "year": 1916}, {"authors": ["S. Lapuschkin", "S. W\u00e4ldchen", "A. Binder", "G. Montavon", "W. Samek", "K.-R. M\u00fcller"], "title": "Unmasking Clever Hans predictors and assessing what machines really learn", "venue": "Nature Communications, vol. 10, no. 1, p. 1096, Dec 2019.", "year": 2019}, {"authors": ["R. Agarwal", "N. Frosst", "X. Zhang", "R. Caruana", "G.E. Hinton"], "title": "Neural additive models: Interpretable machine learning with neural nets", "venue": "arXiv preprint arXiv:2004.13912, 2020.", "year": 2004}, {"authors": ["Y. Goyal", "U. Shalit", "B. Kim"], "title": "Explaining classifiers with causal concept effect (cace)", "venue": "arXiv preprint arXiv:1907.07165, 2019.", "year": 1907}, {"authors": ["A. Ghorbani", "J. Wexler", "J.Y. Zou", "B. Kim"], "title": "Towards automatic concept-based explanations", "venue": "Advances in Neural Information Processing Systems, 2019, pp. 9273\u20139282.", "year": 2019}, {"authors": ["M. Ibrahim", "M. Louie", "C. Modarres", "J. Paisley"], "title": "Global explanations of neural networks: Mapping the landscape of predictions", "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, 2019, pp. 279\u2013287.", "year": 2019}, {"authors": ["H. Li", "Y. Tian", "K. Mueller", "X. Chen"], "title": "Beyond saliency: Understanding convolutional neural networks from saliency prediction on layer-wise relevance propagation", "venue": "Image and Vision Computing, vol. 83-84, pp. 70\u201386, Mar 2019.", "year": 2019}, {"authors": ["C. Burns", "J. Thomason", "W. Tansey"], "title": "Interpreting Black Box Models via Hypothesis Testing", "venue": "arXiv preprint arXiv:1904.00045, Mar 2019.", "year": 1904}, {"authors": ["A. Chattopadhay", "A. Sarkar", "P. Howlader", "V.N. Balasubramanian"], "title": "Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks", "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, Mar 2018, pp. 839\u2013 847.", "year": 2018}, {"authors": ["V. Petsiuk", "A. Das", "K. Saenko"], "title": "RISE: Randomized Input Sampling for Explanation of Black-box Models", "venue": "British Machine Vision Conference 2018, BMVC 2018, Jun 2018.", "year": 2018}, {"authors": ["B. Kim", "M. Wattenberg", "J. Gilmer", "C. Cai", "J. Wexler", "F. Viegas", "R. Sayres"], "title": "Interpretability beyond feature attribution: Quantitative Testing with Concept Activation Vectors (TCAV)", "venue": "35th International Conference on Machine Learning, ICML 2018, 2018.", "year": 2018}, {"authors": ["M. Sundararajan", "A. Taly", "Q. Yan"], "title": "Axiomatic attribution for deep networks", "venue": "34th International Conference on Machine Learning, ICML 2017, 2017.", "year": 2017}, {"authors": ["M. Ancona", "E. Ceolini", "C. \u00d6ztireli", "M. Gross"], "title": "Towards better understanding of gradient-based attribution methods for deep neural networks", "venue": "6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings, 2018.", "year": 2018}, {"authors": ["G. Montavon", "S. Lapuschkin", "A. Binder", "W. Samek", "K.-R. M\u00fcller"], "title": "Explaining nonlinear classification decisions with deep taylor decomposition", "venue": "Pattern Recognition, vol. 65, pp. 211\u2013222, 2017.", "year": 2017}, {"authors": ["L.M. Zintgraf", "T.S. Cohen", "T. Adel", "M. Welling"], "title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "venue": "5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, Feb 2017.", "year": 2017}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra"], "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization", "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2017.", "year": 2017}, {"authors": ["S.M. Lundberg", "S.I. Lee"], "title": "A unified approach to interpreting model predictions", "venue": "Advances in Neural Information Processing Systems, 2017, pp. 4765\u20134774.", "year": 2017}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why Should I Trust You?", "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD \u201916. New York, New York, USA: ACM Press, 2016, pp. 1135\u20131144. 23", "year": 2016}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Learning Deep Features for Discriminative Localization", "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, Jun 2016, pp. 2921\u20132929.", "year": 2016}, {"authors": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "title": "Striving for simplicity: The all convolutional net", "venue": "3rd International Conference on Learning Representations, ICLR 2015 - Workshop Track Proceedings, 2015.", "year": 2015}, {"authors": ["B. Letham", "C. Rudin", "T.H. McCormick", "D. Madigan"], "title": "Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model", "venue": "The Annals of Applied Statistics, vol. 9, no. 3, pp. 1350\u20131371, Sep 2015.", "year": 2015}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.-R. M\u00fcller", "W. Samek"], "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation", "venue": "PLOS ONE, vol. 10, no. 7, p. e0130140, Jul 2015.", "year": 2015}, {"authors": ["R. Caruana", "Y. Lou", "J. Gehrke", "P. Koch", "M. Sturm", "N. Elhadad"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 1721\u20131730.", "year": 2015}, {"authors": ["M.D. Zeiler", "R. Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "Computer Vision \u2013 ECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds. Cham: Springer International Publishing, 2014, pp. 818\u2013833.", "year": 2014}, {"authors": ["B. Kim", "C. Rudin", "J. Shah"], "title": "The Bayesian case model: A generative approach for case-based reasoning and prototype classification", "venue": "Advances in Neural Information Processing Systems, 2014.", "year": 2014}, {"authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "venue": "2nd International Conference on Learning Representations, ICLR 2014 - Workshop Track Proceedings, Dec 2013.", "year": 2014}, {"authors": ["D. Erhan", "A. Courville", "Y. Bengio"], "title": "Understanding representations learned in deep architectures", "venue": "Department dInformatique et Recherche Operationnelle, University of Montreal, QC, Canada, Tech. Rep, vol. 1355, p. 1, 2010.", "year": 2010}, {"authors": ["L. Grosenick", "S. Greer", "B. Knutson"], "title": "Interpretable Classifiers for fMRI Improve Prediction of Purchases", "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 16, no. 6, pp. 539\u2013548, Dec 2008.", "year": 2008}, {"authors": ["V. Schetinin", "J.E. Fieldsend", "D. Partridge", "T.J. Coats", "W.J. Krzanowski", "R.M. Everson", "T.C. Bailey", "A. Hernandez"], "title": "Confident interpretation of Bayesian decision tree ensembles for clinical applications", "venue": "IEEE Transactions on Information Technology in Biomedicine, 2007.", "year": 2007}, {"authors": ["F. Rossi"], "title": "Building trust in artificial intelligence", "venue": "Journal of international affairs, vol. 72, no. 1, pp. 127\u2013134, 2018.", "year": 2018}, {"authors": ["T.C. King", "N. Aggarwal", "M. Taddeo", "L. Floridi"], "title": "Artificial Intelligence Crime: An Interdisciplinary Analysis of Foreseeable Threats and Solutions", "venue": "Science and Engineering Ethics, vol. 26, no. 1, pp. 89\u2013 120, Feb 2020.", "year": 2020}, {"authors": ["P. \u010cerka", "J. Grigien\u0117", "G. Sirbikyt\u0117"], "title": "Liability for damages caused by artificial intelligence", "venue": "Computer Law & Security Review, vol. 31, no. 3, pp. 376\u2013389, 2015.", "year": 2015}, {"authors": ["A.B. Arrieta", "N. D\u0131\u0301az-Rodr\u0131\u0301guez", "J. Del Ser", "A. Bennetot", "S. Tabik", "A. Barbado", "S. Garc\u0131\u0301a", "S. Gil-L\u00f3pez", "D. Molina", "R. Benjamins"], "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai", "venue": "Information Fusion, vol. 58, pp. 82\u2013115, 2020.", "year": 2020}, {"authors": ["J. Zou", "L. Schiebinger"], "title": "AI can be sexist and racist it\u2019s time to make it fair", "venue": "Nature, vol. 559, no. 7714, pp. 324\u2013326, Jul 2018.", "year": 2018}, {"authors": ["M. Du", "F. Yang", "N. Zou", "X. Hu"], "title": "Fairness in deep learning: A computational perspective", "venue": "arXiv preprint arXiv:1908.08843, 2019.", "year": 1908}, {"authors": ["S.-K. Yeom", "P. Seegerer", "S. Lapuschkin", "S. Wiedemann", "K.-R. M\u00fcller", "W. Samek"], "title": "Pruning by explaining: A novel criterion for deep neural network pruning", "venue": "arXiv preprint arXiv:1912.08881, 2019.", "year": 1912}, {"authors": ["S. Mishra", "B.L. Sturm", "S. Dixon"], "title": "Local interpretable modelagnostic explanations for music content analysis", "venue": "Proc. 18th Int. Soc. Music Inf. Retr. Conf. ISMIR 2017, 2017, pp. 537\u2013543.", "year": 2017}, {"authors": ["T. Peltola"], "title": "Local interpretable model-agnostic explanations of bayesian predictive models via kullback-leibler projections", "venue": "arXiv preprint arXiv:1810.02678, 2018.", "year": 1810}, {"authors": ["M. Rehman Zafar", "N. Mefraz Khan"], "title": "Dlime: A deterministic local interpretable model-agnostic explanations approach for computer-aided diagnosis systems", "venue": "arXiv preprint arXiv:1906.10263, 2019.", "year": 1906}, {"authors": ["S. Bramhall", "H. Horn", "M. Tieu", "N. Lohia"], "title": "Qlime-a quadratic local interpretable model-agnostic explanation approach", "venue": "SMU Data Science Review, vol. 3, no. 1, p. 4, 2020.", "year": 2020}, {"authors": ["S. Shi", "X. Zhang", "W. Fan"], "title": "A modified perturbed sampling method for local interpretable model-agnostic explanation", "venue": "arXiv preprint arXiv:2002.07434, 2020.", "year": 2002}, {"authors": ["C. Molnar"], "title": "Interpretable Machine Learning", "venue": "Lulu. com,", "year": 2020}, {"authors": ["L. Antwarg", "B. Shapira", "L. Rokach"], "title": "Explaining anomalies detected by autoencoders using shap", "venue": "arXiv preprint arXiv:1903.02407, 2019.", "year": 1903}, {"authors": ["M. Sundararajan", "A. Najmi"], "title": "The many shapley values for model explanation", "venue": "arXiv preprint arXiv:1908.08474, 2019.", "year": 1908}, {"authors": ["K. Aas", "M. Jullum", "A. L\u00f8land"], "title": "Explaining individual predictions when features are dependent: More accurate approximations to shapley values", "venue": "arXiv preprint arXiv:1903.10464, 2019.", "year": 1903}, {"authors": ["S.M. Lundberg", "G. Erion", "H. Chen", "A. DeGrave", "J.M. Prutkin", "B. Nair", "R. Katz", "J. Himmelfarb", "N. Bansal", "S.-I. Lee"], "title": "From local explanations to global understanding with explainable ai for trees", "venue": "Nature machine intelligence, vol. 2, no. 1, pp. 2522\u20135839, 2020.", "year": 2020}, {"authors": ["M. Vega Garc\u0131\u0301a", "J.L. Aznarte"], "title": "Shapley additive explanations for NO2 forecasting", "venue": "Ecol. Inform., vol. 56, p. 101039, Mar 2020.", "year": 2020}, {"authors": ["C.-K. Yeh", "B. Kim", "S.O. Arik", "C.-L. Li", "P. Ravikumar", "T. Pfister"], "title": "On concept-based explanations in deep neural networks", "venue": "arXiv preprint arXiv:1910.07969, 2019.", "year": 1910}, {"authors": ["A. Mahendran", "A. Vedaldi"], "title": "Salient Deconvolutional Networks", "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Springer, 2016, pp. 120\u2013135.", "year": 2016}, {"authors": ["P.-J. Kindermans", "K.T. Sch\u00fctt", "M. Alber", "K.-R. M\u00fcller", "D. Erhan", "B. Kim", "S. D\u00e4hne"], "title": "Learning how to explain neural networks: Patternnet and patternattribution", "venue": "6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings, Jan 2018.", "year": 2018}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ser. ICML17. JMLR.org, 2017, p. 31453153.", "year": 2017}, {"authors": ["G. Erion", "J.D. Janizek", "P. Sturmfels", "S. Lundberg", "S.-I. Lee"], "title": "Learning Explainable Models Using Attribution Priors", "venue": "arXiv preprint arXiv:1906.10670, Jun 2019.", "year": 1906}, {"authors": ["H. Yang", "C. Rudin", "M. Seltzer"], "title": "Scalable Bayesian rule lists", "venue": "34th International Conference on Machine Learning, ICML 2017, 2017.", "year": 2017}, {"authors": ["F. Doshi-Velez", "B.C. Wallace", "R. Adams"], "title": "Graph-sparse lda: A topic model with structured sparsity", "venue": "Proceedings of the Twenty- Ninth AAAI Conference on Artificial Intelligence, ser. AAAI15. AAAI Press, 2015, p. 25752581.", "year": 2015}, {"authors": ["F. Doshi-Velez", "B. Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608, 2017.", "year": 2017}, {"authors": ["R. Elshawi", "Y. Sherif", "M. Al-Mallah", "S. Sakr"], "title": "Interpretability in healthcare a comparative study of local machine learning interpretability techniques", "venue": "Proc. - IEEE Symp. Comput. Med. Syst., 2019.", "year": 2019}, {"authors": ["A. Holzinger", "A. Carrington", "H. M\u00fcller"], "title": "Measuring the Quality of Explanations: The System Causability Scale (SCS)", "venue": "KI - K\u00fcnstliche Intelligenz, pp. 193\u2013198, 2020.", "year": 2020}, {"authors": ["M. Yang", "B. Kim"], "title": "Benchmarking Attribution Methods with Relative Feature Importance", "venue": "CoRR, vol. abs/1907.09701, 2019.", "year": 1907}, {"authors": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "title": "Microsoft coco: Common objects in context", "venue": "European conference on computer vision. Springer, 2014, pp. 740\u2013755.", "year": 2014}, {"authors": ["B. Zhou", "A. Lapedriza", "A. Khosla", "A. Oliva", "A. Torralba"], "title": "Places: A 10 Million Image Database for Scene Recognition", "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 6, pp. 1452\u20131464, Jun 2018.", "year": 2018}, {"authors": ["D.A. Melis", "T. Jaakkola"], "title": "Towards robust interpretability with self-explaining neural networks", "venue": "Advances in Neural Information Processing Systems, 2018, pp. 7775\u20137784.", "year": 2018}, {"authors": ["R. Luss", "P.-Y. Chen", "A. Dhurandhar", "P. Sattigeri", "K. Shanmugam", "C.-C. Tu"], "title": "Generating contrastive explanations with monotonic attribute functions", "venue": "arXiv preprint arXiv:1905.12698, 2019.", "year": 1905}, {"authors": ["S. Mohseni", "E.D. Ragan"], "title": "A human-grounded evaluation benchmark for local explanations of machine learning", "venue": "arXiv preprint arXiv:1801.05075, 2018.", "year": 1801}, {"authors": ["J. Deng", "W. Dong", "R. Socher", "L. Li", "Kai Li", "Li Fei-Fei"], "title": "Imagenet: A large-scale hierarchical image database", "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248\u2013255.", "year": 2009}, {"authors": ["C.A. Stewart", "T.M. Cockerill", "I. Foster", "D. Hancock", "N. Merchant", "E. Skidmore", "D. Stanzione", "J. Taylor", "S. Tuecke", "G. Turner"], "title": "Jetstream: a self-provisioned, scalable science and engineering cloud environment", "venue": "Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure, 2015, pp. 1\u20138. 24", "year": 2015}, {"authors": ["C. Molnar", "G. Casalicchio", "B. Bischl"], "title": "iml: An r package for interpretable machine learning", "venue": "Journal of Open Source Software, vol. 3, no. 26, p. 786, 2018.", "year": 2018}, {"authors": ["A. Ghorbani", "A. Abid", "J. Zou"], "title": "Interpretation of neural networks is fragile", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, 2019, pp. 3681\u20133688.", "year": 2019}, {"authors": ["H.J. Weerts", "W. van Ipenburg", "M. Pechenizkiy"], "title": "A humangrounded evaluation of shap for alert processing", "venue": "arXiv preprint arXiv:1907.03324, 2019.", "year": 1907}, {"authors": ["S. Wang", "T. Zhou", "J. Bilmes"], "title": "Bias also matters: Bias attribution for deep neural network explanation", "venue": "International Conference on Machine Learning, 2019, pp. 6659\u20136667.", "year": 2019}, {"authors": ["P.-J. Kindermans", "S. Hooker", "J. Adebayo", "M. Alber", "K.T. Sch\u00fctt", "S. D\u00e4hne", "D. Erhan", "B. Kim"], "title": "The (un) reliability of saliency methods", "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer, 2019, pp. 267\u2013280.", "year": 2019}, {"authors": ["J. Adebayo", "J. Gilmer", "M. Muelly", "I. Goodfellow", "M. Hardt", "B. Kim"], "title": "Sanity checks for saliency maps", "venue": "Advances in Neural Information Processing Systems, 2018, pp. 9505\u20139515.", "year": 2018}], "sections": [{"text": "1 Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey Arun Das, Graduate Student Member, IEEE, and Paul Rad, Senior Member, IEEE\nAbstract\u2014Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.\nIndex Terms\u2014explainable ai, xai, interpretable deep learning, machine learning, computer vision, neural network.\nI. INTRODUCTION\nArtificial Intelligence (AI) based algorithms, especially using deep neural networks, are transforming the way we approach real-world tasks done by humans. Recent years have seen a surge in the use of Machine Learning (ML) algorithms in automating various facets of science, business, and social workflow. The surge is partly due to the uptick of research in a field of ML, called Deep Learning (DL), where thousands (even billions) of neuronal parameters are trained to generalize on carrying out a particular task. Successful use of DL algorithms in healthcare [1]\u2013[3], ophthalmology [4]\u2013[6], developmental disorders [7]\u2013[9], in autonomous robots and vehicles [10]\u2013[12], in image processing classification and detection [13], [14], in speech and audio processing [15], [16], cyber-security [17], [18], and many more indicate the reach of DL algorithms in our daily lives. Easier access to high-performance compute nodes using cloud computing ecosystems, high-throughput AI accelerators to enhance performance, and access to big-data\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\nA. Das is with the Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, TX, 78249 USA. e-mail: arun.das@utsa.edu.\nP. Rad is with the Department of Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, 78249 USA. e-mail: peyman.najafirad@utsa.edu.\nscale datasets and storage enables deep learning providers to research, test, and operate ML algorithms at scale in small edge devices [19], smartphones [20], and AI-based web-services using Application Programming Interfaces (APIs) for wider exposure to any applications.\nThe large number of parameters in Deep Neural Networks (DNNs) make them complex to understand and undeniably harder to interpret. Regardless of the cross-validation accuracy or other evaluation parameters which might indicate a good learning performance, deep learning (DL) models could inherently learn or fail to learn representations from the data which a human might consider important. Explaining the decisions made by DNNs require knowledge of the internal operations of DNNs, missing with non-AI-experts and end-users who are more focused on getting accurate solution. Hence, often the ability to interpret AI decisions are deemed secondary in the race to achieve state-of-the-art results or crossing human-level accuracy.\nRecent interest in XAI, even from governments especially with the European General Data Protection Regulation (GDPR) [21] regulation, shows the important realization of the ethics [22]\u2013[26], trust [27]\u2013[29], bias [30]\u2013[33] of AI, as well as the impact of adversarial examples [34]\u2013[37] in fooling classifier decisions. In [38], Miller et al. describes that curiosity is one of the primary reason why people ask for explanations to specific decisions. Another reason might be to facilitate better learning - to reiterate model design and generate better results. Each explanation should be consistent across similar data points and generate stable or similar explanation on the same data point over time [39]. Explanations should make the AI algorithm expressive to improve human understanding, confidence in decision making, and promote impartial and just decisions. Thus, in order to maintain transparency, trust, and fairness in the ML decision-making process, an explanation or an interpretable solution is required for ML systems.\nAn explanation is a way to verify the output decision made by an AI agent or algorithm. For a cancer detection model using microscopic images, an explanation might mean a map of input pixels which contribute to the model output. For a speech recognition model, an explanation might be the power spectrum information during a specific time which contributed more towards the current output decision. Explanations can be also based on the parameters or activations of the trained models explained either by using surrogates such as decision trees or by using gradients or other methods. In the context of reinforcement learning algorithms, an explanation might be given as to why an agent made a certain decision over another. However, the definitions of interpretable and explainable AI are often generic and might be misleading [40] and should\nar X\niv :2\n00 6.\n11 37\n1v 2\n[ cs\n.C V\n] 2\n3 Ju\nn 20\n20\n2 XAI\nScope: Where is the XAI method focusing on?\nIs it on a local instance or trying to understand the model as a whole?\nMethodology: What is the algorithmic approach?\nIs it focused on the input data instance or\nthe model parameters?\nUsage: How is the XAI method developed? Is it integrated to the model\nor can be applied to any model in general?\nLocal: Mainly focus on explanation of individual data instances. Generates one explanation map g per data x \u2208 X.\nGlobal: Tries to understand the model as a whole. Generally takes a group of data instances to generate one or more explanation maps.\nBackProb: Core algorithmic logic is dependent on gradients that are backpropagated from the output prediction layer back to the input layer.\nPerturbation: Core algorithmic logic is dependent on random or carefully chosen changes to features in the input data instance.\nIntrinsic: Explainability is baked into the neural network architecture itself and is generally not transferrable to other architectures.\nPost-Hoc: XAI algorithm is not dependent on the model architecture and can be applied to already trained neural networks.\nFig. 1. General categorization of the survey in terms of scope, methodology, and usage.\nintegrate some form of reasoning [41]. A collection of AI models, such as decision-trees and rulebased models, is inherently interpretable. However, there are affected by the drawbacks of Interpretability-versus-Accuracy trade-off compared to the Deep Learning models. This paper discusses the different approaches and perspectives of researchers to address the problem of the explainability of deep learning algorithms. Methods can be used effectively if the model parameters and architecture are already known. However, modern API-based AI services produce more challenges because of the relative \u2018black-box\u2019 [42] nature of the problem where the end-user has information only on the input provided to the deep learning model and not the model itself.\nIn this survey, we present a comprehensive overview of explainable and interpretable algorithms with a timeline of important events and research publications into three welldefined taxonomies as illustrated in Figure 1. Unlike many other surveys which only categorize and summarize the published research in a high-level, we provide additional mathematical overviews and algorithms of seminal works in the field of XAI. The algorithms presented in the survey are clustered into three well-defined categories which are described in detail in the following sections. Various evaluation techniques for XAI presented in literature are also discussed along with discussion on the limitations and future directions of these methods.\nOur contributions can be summarized as the following: 1) In order to systematically analyze explainable and inter-\npretable algorithms in deep learning, we taxonomize XAI to three well-defined categories to improve clarity and accessibility of the approaches. 2) We examine, summarize and classify the core mathematical model and algorithms of recent XAI research on the proposed taxonomy and discuss the timeline for seminal work. 3) We generate and compare the explanation maps for eight different XAI algorithms, outline the limitations of this approach, and discuss potential future directions to improve trust, transparency, and bias and fairness using\ndeep neural network explanations. Our survey is based on published research, from the year 2007 to 2020, from various search sources including Google Scholar, ACM Digital Library, IEEEXplore, ScienceDirect, Spinger, and preprints from arXiv. Keywords such as explainable artificial intelligence, XAI, explainable machine learning, explainable deep learning, interpretable machine learning were used as search parameters."}, {"heading": "II. TAXONOMIES AND ORGANIZATION", "text": "Prior published survey\u2019s on general explainability have classified XAI techniques based on scope and usage [43]. Key differences of this survey are classification based on methodology behind the XAI algorithms for deep learning, focus on mathematical summaries of the seminal papers, and evaluation strategies for XAI algorithms. We also mention popular open-source software implementations of various algorithms described in this survey. We summarize the taxonomies discussed in the survey in this section based on the illustration provided in Figure 1: \u2022 Scope: Scope of explanations can be either local or\nglobal. Some methods can be extended to both. Locally explainable methods are designed to express, in general, the individual feature attributions of a single instance of input data x from the data population X . For example, given a text document and a model to understand the sentiment of text, a locally explainable model might generate attribution scores for individual words in the text. Globally explainable models provide insight into the decision of the model as a whole - leading to an understanding about attributions for an array of input data. Local and global scope of explanations are described in detail in Section IV. \u2022 Methodology: Core algorithmic concept behind the explainable model can generally be categorized based on the methodology of implementation. In general, both local and global explainable algorithms can be categorized as either\n3\nbackpropagation-based or perturbation-based methods. In backpropagation-based methods, the explainable algorithm does one or more forward pass through the neural network and generates attributions during the backpropagation stage utilizing partial derivatives of the activations. Examples include saliency maps, saliency relevance maps, and class activation maps. Perturbation-based explainable algorithms focus on perturbing the feature set of a given input instance by either using occlusion, partially substituting features using filling operations or generative algorithms, masking, conditional sampling, etc. Here, generally, only forward pass is enough to generate the attribution representations without the need for backpropagating gradients. These methodology differences are described in Section V.\n\u2022 Usage: A well developed explainable method with a specific scope and methodology can be either embedded to the neural network model itself or applied as an external algorithm for explanation. Any explainable algorithm which is dependent on the model architecture fall into the model-intrinsic category. Most model-intrinsic algorithms are model-specific such that any change in the architecture will need significant changes in the method itself or minor changes of hyperparameters of the explainable algorithm. Generally, significant research interest is seen in developing model-agnostic post-hoc explanations, where the predictions of an already existing well-performing neural network model can be explained using ad-hoc explainable methods. Post-hoc methods are also widely applied in variety of input modalities such as images, text, tabular data, etc. These differences in the \u2019usage\u2019 of explainability methods are described in Section VI.\nIn Section VII, we discuss some of the evaluation strategies used to qualitatively or quantitatively evaluate the performance of XAI algorithms discussed in this survey. We present a list of desirable constraints applicable to XAI algorithms to improve its real-world performance as well as expressiveness in terms of transparency, trust, and bias understanding. These desirable qualities can be used as a guide to generate novel XAI algorithms which is favorable as well as expressive. Our study suggests that the evaluation methods are still immature and have an enormous potential for further research. We also provide a list of popular software packages that are open-sourced in GitHub platform. We chose the packages with considerable user support and implemented algorithms. All software platforms supports explaining either Scikit-Learn, Tensorflow, or PyTorch machine learning models. After describing the evaluation methods and software packages, we conclude our survey in Section VIII.\nIn our survey, all mathematical equations and algorithms described are based on a set of notations as described in Table I. The mathematical equations described in the survey might be different from their respective research publications as we have used similar notations to describe the same mathematical idea throughout the survey. This is done to aid the readers and have a common repository of notations. Also, a timeline of seminal research in the field is illustrated in Figure 6. The\ntimeline provides information such as the name of the XAI method, name of first author, and year of publication."}, {"heading": "III. DEFINITIONS AND PRELIMINARIES", "text": "Various prior publications debate the nuances in defining Explainability and Interpretability of neural networks [44], [45]. We support the general concept of explainable AI as a suite of techniques and algorithms designed to improve the trustworthiness and transparency of AI systems. Explanations are described as extra metadata information from the AI model that offers insight into a specific AI decision or the internal functionality of the AI model as a whole. Various Explainability approaches applied to Deep Neural Networks are presented in this literature survey. Figure 2 illustrates one such deep learning model which takes one input and generates one output prediction. Goal of explainable algorithms applied to deep neural networks are towards explaining these predictions using various methods summarized in this survey.\nGenerally, for an input x \u2208 Rd, a deep learning model function f(\u03b8) describes f : Rd \u2192 RC , where C is the number of output classes and \u03b8 the parameters of the model\n4 in a classification problem. Now, the model inference can be described as y\u0304 = f(\u03b8, x) where y\u0304 is the output prediction. We now define the key concepts explored in the survey, namely explainability of deep learning models. Further sections of the survey explain these definitions in finer detail.\nDefinition 1: Interpretability is a desirable quality or feature of an algorithm which provides enough expressive data to understand how the algorithm works.\nHere, interpretable domains could include images or text which are comprehensible by humans. Cambridge Dictionary defines: \u201cIf something is interpretable, it is possible to find its meaning or possible to find a particular meaning in it\u201d.\nDefinition 2: Interpretation is a simplified representation of a complex domain, such as outputs generated by a machine learning model, to meaningful concepts which are humanunderstandable and reasonable.\nOutput predictions of a simple rule-based model can be easily interpreted by traversing the rule-set. Similarly a small decision tree can be easily understood. Or the Deep Convolution Networks (CNN) model that can identify the parts of the input image that led to the decision.\nDefinition 3: An explanation is additional meta information, generated by an external algorithm or by the machine learning model itself, to describe the feature importance or relevance of an input instance towards a particular output classification.\nFor a deep learning model f with input x and output prediction of y\u0304 of class c, an explanation g can be generated, generally as an explanation map E, where E : Rd \u2192 Rd. Here, g is an object of same shape as the input which describes the feature importance or relevance of that particular dimension to the class output. For an image, the explanation map can be an equally sized pixel map whereas for text, it might be word-by-word influence scores.\nDefinition 4: For a deep learning model f , if the model parameters \u03b8 and the model architecture information are known, the model is considered a white-box.\nA white-box model improves model-debugging and promotes trust. However, knowing the model architecture and parameters alone won\u2019t make the model explainable.\nDefinition 5: A deep learning model f is considered a black-box if the model parameters and network architectures are hidden from the end-user.\nTypically, deep learning models served on web-based services or restricted business platforms are exposed using APIs which takes an input form the user and provides the model result as text, visual, or auditory presentation respective to the expected model output y\u0304."}, {"heading": "A. Why Is Research on XAI Important?", "text": "With the use of AI algorithms in healthcare [46], credit scoring [47], loan acceptance [48], and more, the need to explain an ML model result is important for ethical, judicial, as well as safety reasons. Even though there are different facets to why XAI is important, our study suggests that the most important concerns are three-fold: 1) trustability, 2) transparency, and 3) bias and fairness of AI algorithms. Current business models include interpretation as a step before serving\nFig. 3. Illustration from [35] showing an adversarial attack where an image class Panda is deliberately attacked to predict as a Gibbon with high confidence. Note that the attacked image is visually similar to the original image and humans are unable to understand any changes.\nFig. 4. Illustration from [49] showing how text in images can fool classifiers into believing that the text is a feature for a particular task.\nthe ML models on production systems, however are often limited to small tree-based models. With the use of highly nonlinear deep learning algorithms with millions of parameters in ML pipelines, XAI techniques must improve all three concerns mentioned above.\n1) Improves Transparency: XAI improves transparency and fairness by creating a human-understandable justification to the decisions and could find and deter adversarial examples [35] if used properly. Definition 6: A deep learning model is considered transparent if it is expressive enough to be humanunderstandable. Here, transparency can be a part of the algorithm itself or using external means such as model decomposition or simulations. Transparency is important to assess the quality of output predictions and to ward off adversaries. An adversarial example could hinder accurate decision making capabilities of a classifier by fooling the classifier into believing that a fake image is infact real. Figure 3 illustrates such an example where an image of a Panda is predicted as a Gibbon with high confidence after the original Panda image was tampered by adding some adversarial noise. Figure 4 illustrates a classifier learning to classify based on text data such as source tags or watermarks in advertisements in images. As we rely more on autonomous algorithms to aid our daily lives, quality of AI algorithms to mitigate attacks [34] and provide transparency in terms of model understanding, textual, or visual reports should be of prime importance. 2) Improves Trust: As a social animal, our social lives, decisions, and judgements are primarily based on the knowledge and available explanations to situations and\n5\nthe trust we generate. A scientific explanation or logical reasoning for a sub-optimal decision is better than a highly confident decision without any explanations. Definition 7: Trustability of deep learning models is a measure of confidence, as humans, as end-users, in the intended working of a given model in dynamic real-world environments. Thus, \u2018Why a particular decision was made\u2019 is of prime importance to improve the trust [77] of end-users including subject matter experts, developers, law-makers, and laypersons alike [78]\u2013[80]. Fundamental explanations to classifier prediction is ever so important to stakeholders and government agencies to build trustability as we transition to a connected AI-driven socio-economic environment. 3) Improves Model Bias Understanding and Fairness: XAI promotes fairness and helps mitigate biases introduced to the AI decision either from input datasets or poor neural network architecture. Definition 8: Bias in deep learning algorithms indicate the disproportionate weight, prejudice, favor, or inclination of the learnt model towards subsets of data due to both inherent biases in human data collection and deficiencies in the learning algorithm. Learning the model behavior using XAI techniques for different input data distributions could improve our understanding of the skewness and biases in the input data. This could generate a robust AI model [81]. Understanding the input space could help us invest in bias mitigation methods and promote fairer models. Definition 9: Fairness in deep learning is the quality of a learnt model in providing impartial and just decisions without favoring any populations in the input data\ndistribution. XAI techniques could be used as a way to improve the expressiveness and generate meaningful explanations\n6\nAs we discussed previously, the use of XAI could provide a software-engineering design on AI with a continuously evolving model based on prior parameters, explanations, issues, and improvements to overall design thereby reducing human bias. However, choosing the right methods for explanation should be done with care, while considering to bake-in interpretability to machine learning models [40]. We now proceed with detailed discussions as per the taxonomies."}, {"heading": "IV. SCOPE OF EXPLANATION", "text": ""}, {"heading": "A. Local Explanations", "text": "Consider a scenario where a doctor has to make a decision based on the results of a classifier output. The doctor needs careful understanding of the model predictions and concrete answers to the \u2018Why this decision?\u2019 question which requires an explanation of the local data point under scrutiny. This\nlevel of explaining individual decisions made by a classifier is categorized under locally explainable algorithms. Generally, locally explainable methods focus on a single input data instance to generate explanations by utilizing the different data features. Here, we are interested in generating g for explaining the decisions made by f for a single input instance x. A high-level diagram is illustrated in Figure 7.\nFounding research in local explanations used heatmaps, rulebased methods, Bayesian techniques, and feature importance matrices to understand feature correlations and importance towards output predictions. The output explanations were always positive real-valued matrices or vectors. Newer research in local explainable models improves the old methods by attribution maps, graph-based, and game-theory based models in which we get a feature-wise score of positive and negative correlations towards an output classification. Here, a positive attribution value means that the particular feature improves output class probability and a negative value means the feature decreased the output class probability. Mathematical equations described in this section and the rest of the survey follows notations tabulated in Table I.\n1) Activation Maximization: Interpreting a layer-wise feature importance of a Convolutional Neural Network (CNN) model is simpler in the first layer which generally learns the highlevel textures and edges. However, as we move deeper into the CNN, importance of specific layers towards a particular prediction is hard to summarize and visualize since parameters of subsequent layers are influenced by that of the previous layers. Hence, preliminary research tried to understand the neuronal activations to input instances as well as individual filters of specific layers.\nIn 2010, a locally explainable method called Activation Maximization was introduced by Erhan et al. [74], with focus on input patterns which maximize a given hidden unit activation. Here, the authors set an optimization problem of maximizing the activation of a unit. If \u03b8 is the parameters of the model, zi,j(\u03b8, x) is the activation of a particular unit i from layer j. By assuming fixed parameters \u03b8, an activation map can be found as:\nx\u2217 = arg max x s.t. \u2016x\u2016=\u03c1 zij(\u03b8, x) (1)\nAfter the optimization converges, we could either find an average of all local minima\u2019s to find an explanation map g or pick the one which maximizes the activations. Here, the\n7\ngoal is to minimize the activation maximization loss by finding larger filter activations correlated to specific input patterns. Thus, we could understand a layer-wise feature importance to an input instance. It was one of the first published research to express feature importance of deep learning models and was later improved by many researchers.\n2) Saliency Map Visualization: Saliency map generation in deep neural networks were first introduced by Simonyan et al. [73] as a way of computing the gradient of the output class category with respect to an input image. By visualizing the gradients, a fair summary of pixel importance can be achieved by studying the positive gradients which had more influence to the output. Authors introduced two techniques of visualization: 1) class model visualizations and 2) imagespecific class visualizations as illustrated in Figure 8.\nWe discuss class model visualization under the global explainable methods in Section IV-B. Image-specific class saliency visualization technique tries to find an approximate class score function Sc(I), where x is the input image with a label class c using first-order Taylor expansion:\nSc(I) \u2248 wTx+ b (2)\nwhere w is the derivative of the class score function Sc with respect to the input image x at a specific point in the image x0 such that:\nw = \u2202Sc\n\u2202x \u2223\u2223\u2223\u2223 x0\n(3)\nHere, with light image processing, we can visualize the saliency map with respect to the location of input pixels with positive gradients.\n3) Layer-wise Relevance BackPropagation (LRP): LRP technique introduced in 2015 by Bach et al. [69] is used to find relevance scores for individual features in the input data by decomposing the output predictions of the DNN. The relevance score for each atomic input is calculated by backpropagating the class scores of an output class node towards the input layer. The propagation follows a strict conservation property whereby a equal redistribution of relevance received by a neuron must be enforced. In CNNs, LRP backpropagates information regarding relevance of output class back to input layer, layer-by-layer. In Recurrent Neural Networks (RNNs), relevance is propagated to hidden states and memory cell. Zero relevance is assigned\nto gates of the RNN. If we consider a simple neural network with input instance x, a linear output y, and activation output z, the system can be described as:\nyj = \u2211 iwijxi + bj zj = f (yj) (4)\nIf we consider R(zj) as the relevance of activation output, the goal is to get Ri\u2190j , that is to distribute R(zj) to the corresponding input x:\nRi\u2190j = R (zj) xiwij\nyj + sign (yj) (5)\nFinal relevance score of individual input x is the summation of all relevance from zj for input xi:\nR (x) = \u2211 j Ri\u2190j (6)\nThe LRP method have been recently extended to learn the global explainability by using LRP explanation maps as an input to global attribution algorithms. We discuss some such models in section IV-B. Newer research has also shown the importance of using methods such as LRP for model specific operations such as network pruning [83]. Here, authors prune the least important weights or filters of a model by understanding the feature attributions of individual layer. This reduces the computation and storage cost of the AI models without significant drop in the model accuracy. This shows another aspect of using AI in understanding the model behavior and utilizing the new knowledge to improve model performance.\n4) Local Interpretable Model-Agnostic Explanations (LIME): In 2016, Ribeiro et al. introduced Local Interpretable ModelAgnostic Explanations (LIME) [65]. To derive a representation that is understandable by humans, LIME tries to find importance of contiguous superpixels (a patch of pixels) in a source image towards the output class. Hence, LIME finds a binary vector x\n\u2032 \u2208 {0, 1} to represent the presence or absence of a continuous path or \u2019superpixel\u2019 that provides the highest representation towards class output. This works on a patchlevel on a single data input. Hence, the method falls under local explanations. There is also a global explanation model based on LIME called SP-LIME described in the global explainable model sub section. Here, we focus on local explanations.\nAlgorithm 1 LIME algorithm for local explanations Input: classifier f , input sample x, number of superpixels n, number of features to pick m Output: explainable coefficients from the linear model\n1: y\u0304 \u2190 f.predict(x) 2: for i in n do 3: pi \u2190 Permute(x) . Randomly pick superpixels 4: obsi \u2190 f.predict(p) 5: disti \u2190 |y\u0304 \u2212 obsi| 6: end for 7: simscore\u2190 SimilarityScore(dist) 8: xpick \u2190 Pick(p, simscore,m) 9: L\u2190 LinearModel.fit(p,m, simscore)\n10: return L.weights\n8\nConsider g \u2208 G, the explanation as a model from a class of potentially interpretable models G. Here, g can be decision trees, linear models, or other models of varying interpretability. Let explanation complexity be measured by \u2126(g). If \u03c0x(z) is a proximity measure between two instances x and z around x, and L(f, g, \u03c0x) represents faithfulness of g in approximating f in locality defined by \u03c0x, then, explanation \u03be for the input data sample x is given by the LIME equation:\n\u03be(x) = arg min g\u2208G\nL(f, g, \u03c0x) + \u2126(g) (7)\nNow, in Equation 7, the goal of LIME optimization is to minimize the locality-aware loss L(f, g, \u03c0x) in a model agnostic way. Example visualization of LIME algorithm on a single instance is illustrated in Figure 9. Algorithm 1 shows the steps to explain the model for a single input sample and the overall procedure of LIME. Here, for the input instance we permute data by finding a superpixel of information (\u2018fake\u2019 data). Then, we calculate distance (similarity score) between permutations and original observations. Now, we know how different the class scores are for the original input and the new \u2018fake\u2019 data.\nWe can then make predictions on new \u2018fake\u2019 data using the complex model f . This depends on the amount of superpixels you choose from the original data. The most descriptive feature can be picked which improved prediction on the permuted data. If we fit a simple model, often times a locally weighted regression model, to the permuted data with m features and similarity scores as weights, we can use the feature weights, or coefficients, from the simple model to make explanations for the local behavior of the complex model. Recent years have seen many research improving and extending the LIME algorithm to a variety of new tasks. We summarize a few of them below: \u2022 In [84], Mishra et al. extended LIME algorithm to music\ncontent analysis by temporal segmentation, and frequency and time-frequency segmentation of input mel-spectogram. Their approach was called Sound-LIME (SLIME) and was applied to explain the predictions of a deep vocal detector. \u2022 In [85], Tomi Peltola described a KullbackLeibler divergence based LIME called KL-LIME to explain Bayesian predictive models. Similar to LIME, the explanations are\ngenerated using an interpretable model, whose parameters are found by minimizing the KL-divergence from the predictive model. Thus, local interpretable explanations are generated by projecting information from the predictive distribution to a simpler interpretable probabilistic explanation model. \u2022 In [86], Rehman et al. used agglomerative Hierarchical Clustering (HC) and K-Nearest Neighbor (KNN) algorithms to replace the random perturbation of the LIME algorithm. Here, authors use the HC method to group training data together as clusters and the KNN is used to find closest neighbors to a test instance. Once the KNN picks a cluster, that cluster is passed as the input data perturbation instead of a random perturbation as in LIME algorithm. Authors report that their approach generates model explanations which are more stable than traditional LIME algorithm. \u2022 In [87], Bramhall et al. adjusted the linear relations of LIME to consider non-linear relationships using a quadratic approximation framework called QuadraticLIME (QLIME). They achieve this by considering the linear approximations as tangentials steps within a complex function. Results on a global staffing company dataset suggests that the mean square loss (MSE) of LIME\u2019s linear relationship at local level improves while using QLIME. \u2022 In [88], Shi et al. introduced a replacement method to pick superpixels of information for image data using Modified Perturbed Sampling operation for LIME (MPS-LIME). Authors converted the traditional superpixel picking operation into a clique set construction problem by converting the superpixels to an undirected graph. The clique operation improves the runtime due to a considerable reduction in the number of perturbed samples in the MPS-LIME method. Authors compared their method with LIME using Mean Absolute Error (MAE) and Coefficient of determination R2 and reported better results in terms of understandability, fidelity, and efficiency.\n5) SHapley Additive exPlanations (SHAP): A game theoretically optimal solution using Shapley values for model explainability was proposed by Lundberg et al. [64]. SHAP explains predictions of an input x by computing individual feature contributions towards that output prediction. By formu-\n9 lating the data features as players in a coalition game, Shapley values can be computed to learn to distribute the payout fairly.\nIn SHAP method, a data feature can be individual categories in tabular data or superpixel groups in images similar to LIME. SHAP then deduce the problem as a set of linear function of functions where the explanation is a linear function of features [89]. If we consider g as the explanation model of an ML model f , z\u2032 \u2208 {0, 1}M as the coalition vector, M the maximum coalition size, and \u03c6j \u2208 R the feature attribution for feature j, g(z\u2032) is the sum of bias and individual feature contributions such that:\ng(z\u2032) = \u03c60 + M\u2211 j=1 \u03c6jz \u2032 j (8)\nLundberg et al. [64] further describes several variations to the baseline SHAP method such as KernelSHAP which reduces evaluations required for large inputs on any ML model, LinearSHAP which estimates SHAP values from a linear model\u2019s weight coefficients given independent input features, Low-Order SHAP which is efficient for small maximum coalition size M , and DeepSHAP which adapts DeepLIFT method [59] to leverage the compositional nature of deep neural networks to improve attributions. Since KernelSHAP is applicable to all machine learning algorithms, we describe it in Algorithm 2. The general idea of KernelSHAP is to carry out an additive feature attribution method by randomly sampling coalitions by removing features from the input data and linearizing the model influence using SHAP kernels.\nSHAP was also explored widely by the research community, was applied directly, and improved in many aspects. Use of SHAP in the medical domain to explain clinical decisionmaking and some of the recent works which have significant merits are summarized here: \u2022 In [90], Antwarg et al. extended SHAP method to explain\nautoencoders used to detect anomalies. Authors classify anomalies using the autoencoder by comparing the actual data instance with the reconstructed output. Since the final output is a reconstruction, authors suggests that the explanations should be based on the reconstruction error. SHAP values are found for top performing features and were divided into those contributing to and offsetting anomalies. \u2022 In [91], Sundararajan et al. express various disadvantages of SHAP method such as generating counterintuitive explanations for cases where certain features are not important. This \u2018uniqueness\u2019 property of attribution method is improved using Baseline Shapley (BShap) method. Authors\nAlgorithm 2 KernelSHAP Algorithm for Local Explanations Input: classifier f , input sample x Output: explainable coefficients from the linear model\n1: zk \u2190 SampleByRemovingFeature(x) 2: zk \u2190 hx(zk) . hx is a feature transformation to reshape to x 3: yk \u2190 f(zk) 4: Wx \u2190 SHAP (f, zk, yk) 5: LinearModel(Wx).fit() 6: Return LinearModel.coefficients()\nfurther extend the method using Integrated Gradients to the continuous domain. \u2022 In [92], Aas et al. explored the dependence between SHAP values by extending KernelSHAP method to handle dependent features. Authors also presented a method to cluster Shapley values corresponding to dependent features. A thorough comparison of the KernelSHAP method was carried out with four proposed methods to replace the conditional distributions of KernelSHAP method using empirical approach and either the Gaussian or the Gaussian copula approaches. \u2022 In [93], Lundberg et al. described an extension of SHAP method for trees under a framework called TreeExplainer to understand the global model structure using local explanations. Authors described an algorithm to compute local explanation for trees in polynomial time based on exact Shapley values. \u2022 In [94], VegaGarcia et al. describe a SHAP-based method to explain the predictions of time-series signals involving Long Short-Term Memory (LSTM) networks. Authors used DeepSHAP algorithm to explain individual instances in a test set based on the most important features from the training set. However, no changes in the SHAP method was done, and explanations were generated for each time step of each input instances."}, {"heading": "B. Global Explanations", "text": "AI model behavior for a suite of input data points could provide insights on the input features, patterns, and their output correlations thereby promoting transparency of model behavior. Various globally explainable methods deduce the complex deep models to linear counterparts which are easier to interpret. Rule-based and tree-based models such as decision trees are inherently globally interpretable. Output decision of individual branches of a tree can be traced back to the source. Similarly, linear models are often fully explainable given model parameters.\nGenerally, globally explainable methods work on an array of inputs to summarize the overall behavior of the blackbox model as illustrated in Figure 10. Here, the explanation gf describes the feature attributions of the model as a whole and not just for individual inputs. Thus, global explainability is important to understand the general behavior of the model f on large distributions of input and previously unseen data.\n10\n1) Global Surrogate Models: Global surrogate models could be used as a way to approximate the predictions of highly non-linear AI models with an interpretable linear model or a decision tree. Global explanations answers the \u2019How\u2019 in XAI, specifically \u201cHow generalized is my AI model?\u201d, \u201cHow do variations of my AI model perform?\u201d. A general use case of surrogate models in deep learning would be extraction of feature-rich layer embeddings for test inputs and training a linear classifier on the embeddings. The coefficients of the linear model could give insights to how the model behaves. In a high-level, SHAP and LIME can both be considered as surrogate models with different methodology to understand the local correlations than linear models. SpRAy technique we will see in Section IV-B5 also extract local features from a group of data to understand model behavior.\n2) Class Model Visualization: Activation maximization [74] introduced in Section IV-A1 can be also expanded as a global method using Class Model Visualization as described by Simonyan et al. [73]. Here, a given a trained ConvNet f and a class of interest c, the goal is to generate image visualizations I\u2032 which is representative of c. This is based on the scoring methods used to train f which maximizes the class probability score Sc(I) for c, such that:\nI \u2032\n= arg max I Sc(I)\u2212 \u03bb\u2016I\u201622 (9)\nThus, the generated images provides insight to what the blackbox model had learnt for a particular class in the dataset. Images generated using this technique is often called \u2018deep dream\u2019 due to the colorful artefacts generated in the visualizations corresponding to the output class under consideration. Figure 11 illustrates three numerically computed class appearance models learnt by a CNN model for goose, ostrich, and limousine classes respectively.\n3) LIME Algorithm for Global Explanations: LIME model [65] was extended with a submodular pick algorithm (SPLIME) to understand the global correlations of the model under study. This way, LIME provides a global understanding of the model from the individual data instances by providing a non redundant global decision boundary of the machine learning model. Generating global importance of individual features is done using a submodular pick algorithm (hence called SPLIME). Algorithm 3 describes the steps to generate a global explanation to the blackbox model f by learning individual feature importance of input samples x1, . . . , xn\u2208 X .\nAlgorithm 3 LIME Algorithm for Global Explanations Input: classifier f , input samples x1, . . . , xn\u2208 X Output: explanation matrix after submodular pick\n1: Define instances X and budget B 2: for x \u2208 X do 3: fLIME \u2190 LIME(f, x) 4: end for 5: Select B features from fLIME Submodular Pick: 6: M \u2190 GenerateMatrix(X,B) 7: Xmin \u2190 GreedyOptimization(M)\nIf B is the number of explanations to inspect called Budget, W , the explanation matrix, we start with explaining all instances x \u2208 X using LIME algorithm explained in Section IV-A4. In the domain of images, X represents individual input images and B represent the number of superpixels selected for the LIME algorithm. Then, we select B features from f which represents the image better. The submodular pick algorithm starts by generating a matrix of size X \u00d7 B and applying greedy optimization on the matrix such that it chooses minimum number of inputs min(X) which covers the most number of features max(F ). Here, SP-LIME works similar to a surrogate model by first extracting the independent explainability vectors using LIME operation. Hence, computational overhead, accuracy, and complexity depends partly on the amount out data used to understand the model globally.\n4) Concept Activation Vectors (CAVs): In [58], Kim et al. introduced Concept Activation Vectors (CAVs), a global explainability method to interpret the internal states of a neural network in human-friendly concept domain. Here, if we consider the machine learning model f(.) as a vector space Em spanned by basis vector em, we see that human understanding can be modelled as vector space Eh and implicit vectors eh which correspond to human-understandable concepts C. Hence, the explanation function of the model in a global sense, g, becomes g : Em \u2192 Eh.\nNow, human understandable concepts are generated from either input features of training data or user-provided data to simplify the lower-level features of the input domain. For example, a zebra can be deduced to positive concepts PC such as stripes as illustrated in Figure 12. A negative set of concepts, N , can be gathered, for example a set of random photos, to contrast the concepts for zebra. Layer activations for layer j of f , zj is calculated for both positive and negative concepts. The set of activations are trained using a binary classifier to distinguish between: {fj(x) : x \u2208 PC} and {fj(x) : x \u2208 N}.\nAuthors proposed a new method, Testing with CAVs (TCAV), which uses directional derivatives similar to gradient based methods to evaluate the sensitivity of class predictions of f to the changes in given inputs towards the direction of the concept C for a specific layer j. If h(j, k) is the logit of layer j for class k for a particular input, conceptual sensitivity of a class k to C can be computed as directional derivative SC,k,j(x) for a concept vector v j C \u2208 R m:\n11\nSC,k,j(x) = lim \u21920\nhj,k(zj(x) + v j C)\u2212 hj,k(zj(x))\n= \u2207hj,k(zj(x)) \u00b7 vjC , (10)\nA TCAV score can be calculated to find the influence of inputs towards C. If Xk denotes all inputs with label k, TCAV score is given by:\nTCAVC,k,j = |{x \u2208 Xk : SC,k,j(x) > 0}|\n|Xk| (11)\nTCAV unfortunately could generate meaningless CAVs if the input concepts are not picked properly. For example, input concepts generated randomly would inherently generate bad linear models for binary classification and thus TCAV score wouldn\u2019t be a good identifier for global explainability. Also, concepts with high correlations or shared objects in the data, such as cars and roads, could decrease the efficiency of TCAV method. Human bias in picking the concepts also is a considerable disadvantage of using concepts for explainability. The CAV method was further improved in numerous research papers which involved the primary author of CAV [58]: \u2022 In [52], Ghorbani et al. described a method called\nAutomatic Concept-based Explanations (ACE) to globally explain a trained classifier without human supervision unlike TCAV method. Here, authors carry out a multiresolution segmentation of instance to be explained. This generates multiple resolution segments from the same class. All segments are reshaped to similar input sizes and activations of each segment is found with respect to a specific chosen bottleneck layer. Clustering the activations and removing outliers reveals similarities within activations. TCAV scores of individual concepts provide an importance score of the same for particular classification. Authors carried out human subject experiments to evaluate their method and found inspiring results. One research question that arise is the importance of clusters in decision-making.\nAuthors showed that, by stitching the clustered concepts together as an image, a trained InceptionV3 deep neural network was capable of classifying the stitched image as the correct class category. This tends to show that the extracted concepts are suitable for decision-making within the deep learning model. \u2022 Work done by Goyal et al. [51] improved TCAV method by proposing a Causal Concept Effect (CaCE) model which looks at the causal effect of presence or absence of highlevel concepts towards deep learning model\u2019s prediction. Methods such as TCAVs can suffer from confounding of concepts which could happen if the training data instances have multiple classes in them, even with low correlation between the classes. Also, biases in dataset could influence concepts, as well as colors in the input data. CaCE can be computed exactly if we can change concepts of interest by intervening the counterfactual data generation. Authors call this Ground truth CaCE (GT-CaCE) and also elaborate a way to estimate CaCE using Variational Auto Encoders (VAEs) called VAE-CaCE. Experimental results on four datasets suggest improved clustering and performance of the CaCE method even when there are biases or correlations in the dataset. \u2022 In [95], Yeh et al. introduced ConceptSHAP to define an importance or \u201ccompleteness\u201d score for each discovered concept. Similar to ACE method mentioned earlier, one of the aims of ConceptSHAP is to have concepts consistently clustered to certain coherent spatial regions. However, ConceptSHAP finds the importance of each individual concepts with high completeness score from a set of m concept vectors Cs = {c1, c2, . . . , cm} by utilizing Shapley values for importance attribution.\n5) Spectral Relevance Analysis (SpRAy): SpRAy technique by Lapuschkin et al. [49] builds on top of the local instance based LRP explanations. In specific, authors described a spectral clustering algorithm on local explanations provided by LRP to understand the decision-making process of the model globally. By analyzing the spatial structure of frequently occurring attributions in LRP instances, SpRAy identifies\n12\nnormal and abnormal behavior of machine learning models. Algorithm 4 explains the SpRAy technique in detail. We start by finding local relevance map explanations to every individual data instances x \u2208 X using LRP method. The relevance maps are downsized to uniform shape and size to improve computation overhead and generate tractable solutions. Spectral cluster analysis (SC) is carried out on the LRP attribution relevance maps to cluster the local explanations in a highdimensional space. An eigenmap analysis is carried out to find relevant clusters by finding the eigengap (difference in two eigenvalues) of successive clusters. After completion, important clusters are returned to users. The clusters can be optionally visualized using t-Stochastic Neighbor Embedding (t-SNE) visualizations.\nAlgorithm 4 SpRAy Analysis Algorithm on LRP Attributions Input: classifier f , input samples x(1), . . . , x(n) Output: clustered input samples\n1: for x(i) \u2208 X do 2: fSpRAy \u2190 LRP (f, x(i)) 3: end for 4: Reshape fSpRAy 5: clusters\u2190 SC(fSpRAy) 6: clusters\u2217 \u2190 EigenMapAnalysis(clusters) 7: Return clusters\u2217 8: Optional: Visualize t-SNE(clusters\u2217)\n6) Global Attribution Mapping: When features have well defined semantics, we can treat attributions as weighted conjoined rankings [53] with each feature as a rank vector \u03c3. After finding local attributions, global attribution mapping finds a pair-wise rank distance matrix and cluster the attribution by minimizing cost function of cluster distances. This way, global attribution mapping can identify differences in explanations among subpopulations within the clusters which can trace the explanations to individual samples with tunable subpopulation granularity.\n7) Neural Additive Models (NAMs): In [50], Agarwal et al. introduced a novel method to train multiple deep neural networks in an additive fashion such that each neural network attend to a single input feature. Built as an extension to generalized additive models (GAM), NAM instead use deep learning based neural networks to learn non-linear patterns and feature jumping which traditional tree-based GAMs cannot learn. NAMs improved accurate GAMs introduced in [70] and are scalable during training to several GPUs.\nConsider a general GAM of the form:\ng(E[y]) = \u03b2+f1 (x1)+f2 (x2)+ \u00b7 \u00b7 \u00b7+fK (xK) (12)\nwhere fi is a univariate shape function with E[fi] = 0, x \u2208 x1, x2, . . . , xK is the input with K features, y is the target variable, and g(.) is a link function. NAMs can be generalized by parameterizing the functions fi with neural networks with several hidden layers and neurons in each layer. We can see individual neural networks applied to each features xi. The outputs of each fi is combined together using a summing operation before applying an activation. A high-level\ndiagram of NAM is provided in 13 taken from the source paper.\nAuthors proposed exp-centered (ExU) hidden units to overcome the failure of ReLU activated neural networks with standard initializations to fit jagged functions. NAMs should be able to learn jagged functions due to sharp changes in features in real-world datasets often encountered in GAMs. For ExU hidden units, the unit function can be calculated as h(x) = f(ew \u2217 (x \u2212 b)), where x, w, and b are the inputs, weights, and biases parameters. Authors used a weight initialization of training from a normal distribution N (x, 0.5) with x \u2208 [3, 4]. This globally explainable model provides average score of shape functions of individual neural networks to provide interpretable contributions of each features as positive and negative values. Negative values reduce the class probability while positive values improve the same.\nNAM is an interesting architecture because we can generate exact explanations of each feature space with respect of an output prediction. Newer research could open up venues to expand the ideas to CNNs and for other domains such as text."}, {"heading": "V. DIFFERENCES IN THE METHODOLOGY", "text": "Based on the core algorithmic approach followed in the XAI method, we can categorize XAI methods as the ones which focus on the changes or modifications input data and the ones which focus on the model architecture and parameters. These fundamental changes are categorized in our survey as perturbation-based and backpropagation-based respectively."}, {"heading": "A. Perturbation-Based", "text": "Explanations generated by iteratively probing a trained machine learning model with different variations of the inputs generally fall under perturbation based XAI techniques. These\n13\nperturbations can be on a feature level by replacing certain features by zero or random counterfactual instances, picking one or group of pixels (superpixels) for explanation, blurring, shifting, or masking operations, etc. As we discussed in the prior sections, LIME algorithm works on superpixels of information or features as illustrated in Figure 9. By iteratively providing input patches, visual explanations of individual superpixels are generated. SHAP has a similar method of probing feature correlations by removing features in a game theoretic framework. Intuitively, we see that methods trying to understand neuronal activities and the impact of individual features to a corresponding class output by any input perturbations mentioned above can be categorized as a group of method, which we here call perturbation-based XAI method. The methods described in this section are further summarized in Table III.\n1) DeConvolution nets for Convolution Visualizations: Zeiler et al. [71] visualized the neural activations of individual layers of a deep convolutional network by occluding different segments of the input image and generating visualizations using a deconvolution network (DeConvNet). DeConvNets are CNNs designed with filters and unpooling operations to render opposite results than a traditional CNN. Hence, instead of reducing the feature dimensions, a DeConvNet, as illustrated in Figure 14, is used to create an activation map which maps back to the input pixel space thereby creating a visualization of the neural (feature) activity. The individual activation maps could help understand what and how the internal layers of the deep model of interest is learning - allowing for a granular study of DNNs.\n2) Prediction Difference Analysis: A conditional sampling based multi-variate approached was used by Zintgraf et al. [62] to generate more targeted explanations on image classification CNNs. By assigning a relevance value to each input features with respect to the predicted class c, the authors summarize the positive and negative correlation of individual data features to a particular model decision. Given an input feature x, its feature relevance can be estimated by studying the changes in model output prediction for the inputs with different hidden features. Hence, if x\\i denotes the set of all input features except x, the task is to find the difference between p(c|x) and p(c|x\\i).\n3) Randomized Input Sampling for Explanation (RISE): The RISE method introduced by Petsiuk et al. [57] perturb an input image by multiplying it with randomized masks. The masked images are given as inputs and the saliency maps corresponding to individual images are captured. Weighted average of the masks according to the confident scores is used to find the final saliency map with a positive valued heatmap for individual predictions. Importance maps of the blackbox prediction is estimated using Monte Carlo sampling. A highlevel architecture is illustrated in Figure 15.\n4) Randomization and Feature Testing: The Interpretability Randomization Test (IRT) and the One-Shot Feature Test (OSFT) introduced by Burns et al. [55] focuses on discovering important features by replacing the features with uninformative counterfactuals. Modeling the feature replacement with a hypothesis testing framework, the authors illustrate an interesting way to examine contextual importance. Unfortunately, for deep learning algorithms, removing one or more features from the input isn\u2019t possible due to strict input dimensions for a pre-trained deep model. Zero-ing out values or filling in counterfactual values might lead to unsatisfactory performance due to correlation between features."}, {"heading": "B. BackPropagation- or Gradient-Based", "text": "Perturbation-based methods, as we saw in the previous section, focuses on variations in the input feature space to explain individual feature attributions of f towards the output class c. Gradient-based explainability methods, in contrast, utilize the backward pass of information flow in a neural network to understand neuronal influence and relevance of the\n14\ninput x towards the output. As we will see in the following subsections, majority of gradient-based methods focuses on either visualization of activations of individual neurons with high influence or overall feature attributions reshaped to the input dimensions. A natural advantage of gradient-based XAI methods are the generation of human understandable visual explanations.\n1) Saliency Maps: As mentioned in sub-section IV-A2, Simonyan et al. [73] introduced a gradient based method to generate saliency maps for convolutional nets. DeConvNet work by Zeiler et al. [71] mentioned previously as a perturbation method uses backpropagation for activation visualizations. DeConvNet work was impressive due to relative importance given to gradient value during backprop. With Rectified Linear Unit (ReLU) activation, a backprop on traditional CNNs would result in zero values for negative gradients. However, in DeConvNets, the gradient value is not clipped at zero. This allowed for accurate visualizations. Guided backpropagation methods [67], [96] are also another class of gradient based explanation which improved upon [73].\n2) Gradient class activation mapping (CAM): Most saliency methods use global average pooling layer for all pooling operations instead of maxpooling. Zhou et al. [66] modified global average pooling function with class activation mapping (CAM) to localize class-specific image regions on an input image with a single forward-pass. Grad-CAM [63] and GradCAM++ [56] improved the CAM operation for deeper CNNs and better visualizations.\nGradCAM is a class-discriminative attribution technique for localizing the neuronal activity of a CNN network. It allows class-specific query of an input image and also counterfactual explanations which highlights regions in the image which negatively contribute to a particular model output. GradCAM is successfully applied to explain classifiers in image classification, image segmentation, visual question answering (VQA), etc. Figure 16 illustrates a segmentation method utilizing GradCAM to improve the segmentation algorithm. Here, we see another example of using XAI explanations to improve performance of deep neural networks.\n3) Salient Relevance (SR) Maps: Li et al. [54] proposed Salient Relevance (SR) map which is a context aware salience map based on the LRP of input image. Hence, the first step is to find LRP relevance map for input image of interest with the same input dimensions. A context aware salience relevance map algorithm takes the LRP relevance maps and finds a saliency value for individual pixels. Here, a pixel is salient if a group of neighboring pixels are distinct and different from other pixel patches in the same and multiple scales. This is done to differentiate between background and foreground layers of the image.\nTo aid visualization, a canny-edge based detector is superimposed with the SR map to provide context to the explanation. We place SR in gradient based methods due to the use of LRP. Other relevance propagation methods based on Taylor decomposition [61] are also explored in literature, which are\n15\nslightly different in the methodology but have the same global idea.\nAlgorithm 5 describes the SR map generation in detail. Similar to SpRAy technique, we start with the LRP of the input instance. In contrast, we only find LRP attribution relevance score for a single input of interest x. Then a context aware saliency relevance (SR) map is generated by finding a dissimilarity measure based on the euclidean distance in color space and position. Multi-scale saliency at scales r, r\n2 , r 4 are found out and the immediate context of image x based on an attention function is added to generate the SR map.\nAlgorithm 5 Salient Relevance (SR) Algorithm Input: classifier f , input sample x, scale factor r Output: relevance map\n1: fLRP \u2190 LRP (x) 2: GenerateSRMap(x, fLRP) 3: S \u2190 MultiScaleSaliency(r, r\n2 , r 4 )\n4: SRMap\u2190 AttentionFunction(x, S) 5: Return SRMap\n4) Attribution Maps: In [60], Ancona et al. shows that the gradient method, where the gradient of output corresponding to input is multiplied by the input, is useful in generating an interpretable explanation to model outcomes. However, in [59], authors proposed Integrated Gradients (IG) and argue that most gradient based lack in certain \u2018axioms\u2019 which are desirable characteristics of any gradient based technique. Authors argue that methods such as DeepLift [98], Layer-wise relevance propagation (LRP) [69], Deconvolutional networks (DeConvNets) [71], and Guided back-propagation [67] have specific back-propagation logic that violates some axioms.\nFor each input data instance x, if we consider a baseline instance x\n\u2032 \u2208 Rn, the attributions of x on model f can be summarized by computing the integral of gradients at all points of a straight-line path from baseline x \u2032 to x. This method is called the Integrated Gradients such that:\nIGj(x, x\u2032) := (xj \u2212 x\u2032j)\u00d7 \u222b 1 \u03b1=0 \u2202F (x\u2032+\u03b1\u00d7(x\u2212x\u2032)) \u2202xj d\u03b1 (13)\nwhere j describes the dimension along which the gradient is calculated. During calculation in computers, the integral in equation 13 is efficiently approximated using summation instead. In many cases, baseline instance x \u2032\ni is chosen as a zero matrix or vector. For example, for image domain, the baseline image is chosen as a black image by default. For text classification, the baseline is a zero valued vector. However, choosing baselines arbitrarily could cause issues downstream. For example, a black baseline image could cause the attribution method to diminish the importance of black pixels in the source image.\nAttribution prior [99] concept tries to regularize the feature attributions during model training to encode domain knowledge. A new method, Expected Gradients (EG) was also introduced in the paper as a substitute feature attribution method instead of Integrated Gradients. Together, the attribution prior and EG methods encodes prior knowledge from the domain to aid training process leading to better model interpretability. Equation 14 shows how authors remove the influence of baseline images from integrated gradients by still following all the axioms of Integrated Gradient method. Here, D is the distribution of underlying data domain.\nEG(x) := \u222b x\u2032 (( xj \u2212 x\u2032j ) \u222b 1 \u03b1=0 \u03b4f(x\u2032+\u03b1\u00d7(x\u2212x\u2032)) \u03b4xj \u03b4\u03b1 ) .pD ( x\u2032 ) \u03b4x\u2032 (14)\nSince an integration over the whole training distribution is intractable, authors proposed to reformulate the integral as expectations such that:\nEG(x) := E x\u2032\u223cD,\u03b1\u223cU(0,1)\n[( xj \u2212 x\u2032j ) \u03b4f(x\u2032+\u03b1\u00d7(x\u2212x\u2032)) \u03b4xj ] (15)\n5) Desiderata of Gradient-based Methods: Gradient-based methods, as we saw, mainly use saliency maps, class activation maps, or other gradient maps for visualization of important features. Recent research have found numerous limitations in gradient-based methods. To improve gradient-based XAI techniques, Sundararajan et al. [59] describes four desirable qualities (axioms) that a gradient based method needs to follow:\n1) Sensitivity: If for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution [59]. For simple functions such as f(x) = 1\u2212ReLU(1\u2212 x), the function value saturates for x values greater than or equal to one. Hence, if we take simple gradients as an attribution method, sensitivity won\u2019t hold. 2) Implementation invariance: Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations. Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks [59]. Methods such as DeepLift and LRP break implementation invariance because they use discrete gradients, and chain rule doesn\u2019t old for discrete gradients in general. Generally, if the model fails to provide implementation invariance, the attributions are\n16\npotentially sensitive to unimportant features and aspects of the model definition. 3) Completeness: Attributions should add up to the difference between output of model function f for the input image x and another baseline image x \u2032 .\n\u03a3ni=1Gradientsi(x) = f(x)\u2212 f(x \u2032 ).\n4) Linearity: For a linearly composed neural network model f3 which is a linear combination of two neural network models f1 and f2 such that f3 = a\u00d7f1 +b\u00d7f2, then the attributions of the f3 is expected to be a weighted sum of attributions for f1 and f2 with weights a and b respectively.\nDespite human understandable explanations, gradient-based explanation maps have practical disadvantages and raises various concerns in mission-critical applications. We explain some of these concerns in later sections."}, {"heading": "VI. MODEL USAGE OR IMPLEMENTATION LEVEL", "text": ""}, {"heading": "A. Model Intrinsic", "text": "On a usage or implementation level, model intrinsic explainable methods have interpretable elements baked into them. These models are inherently interpretable either by following strict axioms, rule-based final decisions, granular explanations for decisions, etc. By definition, intrinsic methods of explanations are inherently model-specific. This means that\nthe explainer depends on the model architecture and cannot be re-used for other classifier architectures without designing the explanation algorithm specifically for the new architecture as illustrated in Figure 17.\n1) Trees and Rule-based Models: Shallow rule-based models such as decision trees and decision lists are inherently interpretable. Many explainable algorithms including LIME and SHAP uses linear or tree based models for their globally explainable extensions of the core algorithms. Letham et al. [68] introduced Bayesian Rule Lists (BRL) which is a generative model that yields a posterior distribution over possible decision lists to improve interpretability while keeping accuracy.\nThe rule list has an if, else, and elseif rules generalized as the IF-THEN rule antecedent and predictions. As we add more IF-THEN rules to the decision list, the model becomes more accurate and interpretable. However, support for explanations deteriorate with large number of conditions. One way to simplify the problem is to find the frequent rule patterns and learn a decision list from the distribution using Bayesian techniques.\nBy picking a sample rule list from the priori distribution and iteratively adding and editing the rules, BRL tries to optimize the rules such that the new rule distribution follows the posteriori distribution. Once optimized, new rules can be sampled from the posteriori distribution. Recent research have improved the scalability of BRL [100] by improving the theoretical bounds, computational reuse, and highly tuned language libraries.\n2) Generalized additive models (GAMs): Caruana et al. [70] introduced Generalized additive models (GAMs) with pairwise interactions (GA2Ms) to improve the accuracy while maintaining interpretability of GAMs. However, for certain models, GAMs require often millions of decision trees to provide accurate results using the additive algorithms. Also, depending on the model architecture, over-regularization reduces accuracy of GAM models which are fit using splines. Numerous methods have improved GAMs. Perhaps the most important work is the recent Neural Additive Models we discussed in subsection IV-B7.\n17\n3) Sparse LDA and Discriminant Analysis: A Bayesian non-parametric model, Graph-Sparse LDA, was introduced in [101] to find interpretable, predictive topic summaries to textual categories on datasets with hierarchical labeling. Grosenick et al. [75] introduced a method called Sparse Penalized Discriminant Analysis (SPDA) to improve the spatio-temporal interpretability and classification accuracy of learning algorithms on Functional Magnetic Resonance Imaging (FMRI) data.\nAs we see in published research, there are several restrictions to use model intrinsic architectures as it requires careful algorithm development and fine-tuning to the problem setting. The difficulty in using concepts from model intrinsic architectures and apply them in existing high-accuracy models to improve interpretability is a disadvantage of model-intrinsic methods. However, as long as a reasonable performance limit is set, model intrinsic architectures for XAI could help accelerate inherently interpretable models for future AI research."}, {"heading": "B. Post-Hoc", "text": "Explaining pre-trained classifier decisions require algorithms to look at AI models as black or white boxes. A black box means the XAI algorithm doesn\u2019t know the internal operations and model architectures. In white box XAI, algorithms have access to the model architecture and layer structures. Posthoc explanation methodology is extremely useful as existing accurate models can benefit from added interpretability. Most post-hoc XAI algorithms are hence model-agnostic such that the XAI algorithm will work on any network architectures as illustrated in Figure 18. This is one of the main advantages of post-hoc explainable algorithms. For example, an already trained well established neural network decision can be explained without sacrificing the accuracy of the trained model.\nDeconvolution network [71] could be used to generate posthoc explanations of layer-wise activations. Saliency maps [73] and most attribution based methods [59] are applied considering the network as a white or black box. LRP technique [69] discussed above is done after training the model completely. Shapley sampling methods [64] are also post-hoc and model agnostic. Activation maximization technique [74] is applicable to any network in which we can find gradients values to optimize activations."}, {"heading": "VII. EVALUATION METHODOLOGIES, ISSUES, AND FUTURE DIRECTIONS", "text": "So far, we focused on XAI algorithms and methods categorized under scope, methodology, and usage. The seminar works discussed in the survey is tabulated in Table V. A fundamental challenge in XAI research is to evaluate the several proposed algorithms on real-world settings. Our survey on evaluation techniques suggested that the field is still immature with primary focus on a human-in-the-loop evaluations. Quantitative general evaluation schemes are yet to be explored. However, we summarize here some of the methods which improve human understandability of explainability method results based on [38], [102], [103]. In general, each explanation should follow the below constraints to be usable by humans in a real-world setting:\n1) Identity or Invariance: Identical data instances must produce identical attributions or explanations. 2) Stability: Data instances belonging to the same class c must generate comparable explanations g. 3) Consistency: Data instances with change in all but one feature must generate explanations which magnifies the change. 4) Separability: Data instances from different populations must have dissimilar explanations. 5) Similarity: Data instances, regardless of class differences, closer to each other, should generate similar explanations. 6) Implementation Constraints: Time and compute requirement of the explainable algorithm should be minimal. 7) Bias Detection: Inherent bias in data instances should be detectable from the testing set. Similarity and separability measures help achieve this."}, {"heading": "A. Evaluation Schemes", "text": "Several evaluation schemes have been suggested by the research community in the recent years. We present here some of the evaluation techniques that are actively gaining traction from the research community: \u2022 System Causability Scale (SCS): As the explainability\nmethods are applied to human-facing AI systems which does automated analysis of data, evaluation of humanAI interfaces as a whole is also important. A System Causability Scale (SCS) was introduced in [104] to understand the requirements for explanations of a userfacing human-AI machine-interface, which are often domain specific. Authors described a medical scenario where the SCS tool was applied to Framingham Risk Tool (FRT) to understand the influence and importance of specific characteristics of the human-AI interface. \u2022 Benchmarking Attribution Methods (BAM): In a preprint publication, [105] introduced a framework called Benchmarking Attribution Methods (BAM) to evaluate the correctness of feature attributions and their relative importance. A BAM dataset and several models were introduced. Here, the BAM dataset is generated by copying pixel groups, called Common Features (CF), representing object categories from MSCOCO dataset [106] and pasting them to MiniPlaces dataset [107]. The hypothesis is that, if\n18\nTABLE V SUMMARY OF PUBLISHED RESEARCH IN EXPLAINABILITY AND INTERPRETABILITY OF DEEP LEARNING ALGORITHMS. * INDICATES THAT A PREPRINT VERSION WAS PUBLISHED AN YEAR PRIOR TO THE CONFERENCE OR JOURNAL VERSION.\nMethod Name Publication Year Scope Methodology Usage Agnostic or Specific Domain\nBayesian averaging over decision trees Schetinin et al. [76] 2007 GL OT IN MS TAB SPDA Grosenick et al. [75] 2008 GL OT IN MS TXT Activation Maximization Erhan et al. [74] 2010 LO BP PH MA IMG Gradient-based Saliency Maps Simonyan et al. [73] 2013 LO BP PH MA IMG Bayesian Case Model (BCM) Kim et al. [72] 2014 GL OT IN MS Any DeConvolutional Nets Zeiler et al. [71] 2013 LO BP PH MA IMG GAM Caruana et al. [70] 2015 GL OT IN MS TAB LRP Back et al. [69] 2015 Both BP PH MA IMG Guided Backprop Springenberg et al. [67] 2015 LO BP PH MA IMG Bayes Rule Lists Letham et al. [68] 2015 GL OT IN MS TAB CAM Zhou et al. [66] 2016 LO BP PH MA IMG LIME Ribeiro et al. [65] 2016 Both PER PH MA Any Shapley Sampling Lundberg et al. [64] 2017 Both PER PH MA Any Grad-CAM Selvaraju et al. [63] 2017* LO BP PH MA IMG Prediction Difference Analysis (PDA) Zintgraf et al. [62] 2017 LO PER PH MA IMG Deep Taylor Expansion Montavon et al. [61] 2017 LO OT PH MA IMG Deep Attribution Maps Ancona et al. [60] 2017 LO BP PH MA IMG Axiomatic Attributions Sundararajan et al. [59] 2017 LO BP PH MA IMG PatternNet and PatternAttribution Kindermans et al. [97] 2017 LO BP PH MA IMG Concept Activation Vectors Kim et al. [58] 2018 GL OT PH MA IMG RISE Petsiuk et al. [57] 2018 LO PER PH MA IMG Grad-CAM++ Chattopadhay et al. [56] 2018 LO BP PH MA IMG Randomization and Feature Testing Burns et al. [55] 2019 LO PER PH MA IMG Salient Relevance (SR) map Li et al. [54] 2019 LO BP PH MA IMG Spectral Relevance Analysis Lapuschkin et al. [49] 2019 GL BP PH MA IMG Global Attribution Mapping Ibrahim et al. [53] 2019 GL PER PH MA IMG Automatic Concept-based Explanations Ghorbani et al. [52] 2019 GL OT PH MA IMG CaCE Goyal et al. [51] 2019 GL OT PH MA IMG Neural Additive Models Agarwal et al. [50] 2020 GL OT IN MS IMG\nGlobal: GL, Local: LO, Others: OT, BackProp: BP, Perturbation: PER, Model-specific: MS, Model-agnostic: MA, Tabular: TAB, Image: IMG, Test: TXT, Any: Image, Text, or Tabular.\nwe have the same pixel group of information in the same spatial location of all of X , then the model should ignore it as a feature of relative importance. Hence, attribution methods focusing on pasted objects are simply not doing a good job at enhancing feature attributions of important features. Authors provided model contrast score (MCS) to compare relative feature importance between difference models, input dependence rate (IDR) to learn the dependence of CF on a single instance, and input independence rate (IIR) as a percentage score of images whose average feature attributions gr \u2208 R for region r with and without CF is less than a set threshold. \u2022 Faithfulness and Monotonicity: In [108], authors described a metric, named Faithfullness, to evaluate the correlation between importance scores of features to the performance effect of each feature towards a correct prediction. By incrementally removing important features and predicting on the edited data instance, we measures the effect of feature importance and later compare it against the interpreter\u2019s own prediction of relevance. In [109], authors introduce monotonic attribute functions and thus the Monotonicity metric which measures the importance or effect of individual data features on the performance of the model by incrementally adding each feature in the increasing order of importance to find model performance. The model performance is expected to increase as more important features are added.\n\u2022 Human-grounded Evaluation Benchmark: In [110], Moshseni et al. introduced a human-grounded evaluation benchmark to evaluate local explanations generated by an XAI algorithm. Authors created a subset of ImageNet dataset [111] and asked human annotators to manually annotate the images for the particular classes. A weighted explanation map was generated which summarized an average human representation of explanations. By comparing the explanations generated by locally explainable algorithms, authors presented a method to understand the precision of XAI explanations compared to human generated explanations. One fundamental flaw of this method could be added human bias in the explanations. However, human labels of individual data points from a large population could nullify the effect of inherent bias."}, {"heading": "B. Software Packages", "text": "OpenSource packages have greatly improved reproducible research and has been a real boon to recent research in deep learning and XAI alike. We mention here some XAI software packages available in GitHub. \u2022 Interpret by InterpretML can be used to explain black-\nbox models and currently supports explainable boosting, decision trees, decision rule list, linearlogistic regression, SHAP kernel explainer, SHAP tree explainer, LIME, morris sensitivity analysis, and partial dependence. Available at https://github.com/interpretml/interpret.\n19\n\u2022 IML package [113] is maintained by Christoph Molnar, author of [89]. The package covers feature importance, partial dependence plots, individual conditional expectation plots, accumulated local effects, tree surrogates, LIME, and SHAP. Available at https://github.com/christophM/iml. \u2022 DeepExplain package is maintained by Marco Ancona, author of [60]. The package supports various gradientbased techniques such as saliency maps, gradientinput, integrated gradients, DeepLIFT, LRP, etc. and perturbationbased methods such as occlusion, SHAP, etc. Available at https://github.com/marcoancona/DeepExplain. \u2022 DrWhy by ModelOriented is a package with several model agnostic and model specific XAI techniques including feature importance, ceteris paribus, partial dependency plots, conditional dependency, etc. Available at https://github.com/ModelOriented/DrWhy"}, {"heading": "C. A Case-study on Understanding Explanation Maps", "text": "In Figure 19, we illustrate the explanation maps generated using various gradient- and perturbation-based XAI techniques for four images from ImageNet [111] dataset to explain the decisions an InceptionV3 model pre-trained on ImageNet. Here, each row starts with an original image from ImageNet followed by explanation map generated by gradient algorithms such as 1) saliency maps, 2) gradient times input, 3) integrated gradients, 4) LRP, 5) DeepLIFT, and 6) GradCAM, and perturbation-based techniques such as 1) LIME and 2) SHAP.\nGradCAM generates a heatmap of values ranging from 0 to 1, where 0 means no influence and 1 means highest influence of individual pixels towards the model output decision. Similarly, SHAP method follows a scale for SHAP values. However, SHAP scale ranges from -0.3 to +0.3 indicating that negative values decrease output class probability and positive values increase the output class probability for the corresponding input. Here 0.3 is the largest SHAP value generated for the set of four images considered. Gradient visualizations of this figure are created using DeepExplain package while visualizations for Grad-CAM, LIME, and SHAP are created by their own individual implementations.\nOriginal image column of row (a) in Figure 19 indicates a correct prediction of an image of a Koala with 94.5% prediction accuracy, row (b) indicates a correct prediction of a sandbar image with 38.0% accuracy, row (c) indicates an incorrect prediction of a horse as an arabian camel with 17.4% accuracy, and row (d) indicates correct prediction of a leaf beetle with 95.5% percentage accuracy. We then compare the explanation maps, in different columns, generated by various XAI techniques as discussed above.\nFocusing on saliency maps, gradient times input, and integrated gradients in Figure 19, we can visually verify the improvements achieved by integrated gradients over the prior gradient-based methods. This is apparent in the images with lower class probabilities. For example, in row (b), we can verify that the integrated gradients generated high attributions around the sandy beach, plastic chairs, and a little bit of the blue\n20\nsky. As human evaluators, we can make sense of this output because human experience suggests that a sandbar involve a beach, hopefully on a sunny day with bright blue clouds. A stark difference is apparent in Grad-CAM visualizations where the class output generated a heatmap which is focused primarily on the plastic chair and sandy beach, without much emphasis on the clouds. Perturbation-based methods such as LIME and SHAP generated superpixels which maximized the class probability. Here, we see that LIME is focusing on primarily the chairs and the sky, whereas SHAP is focusing on the beach and the sky. We also note that SHAP values generated are very low, indicating lesser influence to the confidence score."}, {"heading": "D. Limitations of XAI Visualizations and Future Directions", "text": "The discussion above brings some important flaws of XAI visualizations and interpretability techniques - 1) the inability of human-attention to deduce XAI explanation maps for decisionmaking, and 2) unavailability of a quantitative measure of completeness and correctness of the explanation map. This suggests that the further use of visualization techniques for mission-critical applications must be reconsidered moving forward. Also, better ways of representing and presenting explanations should be considered. For example, in [115], Weerts et al. studied the impact of SHAP explanations in improving human performance for alert processing tasks. The authors presented a human-grounded study to evaluate whether certain decision-making scenarios can be improved by providing explanations to decisions. Results showed that additional SHAP explanations to class output probability did not improve the decision-making of individuals. Authors saw more interest in final class score for making decisions which could be catastrophic in mission-critical scenarios.\nSimilarly, in [110], Mohseni et al. presented a humangrounded evaluation benchmark and evaluated performance of LIME algorithm by comparing the explanation map generated by LIME to that of weighted explanation map of 10 human annotations. Results suggested that LIME creates some attributions irrelevant to human explanations which causes\nlow explanation precision compared to weighted explanation map generated by human annotators. This sheds light to the importance of understanding the mode of explanations as application-grounded, human-grounded, and functionallygrounded explanations [102] to improve explanation maps by meta information generated by humans, adding more constraints to explanations, or introducing formal definitions of explanations to the optimization problem.\nSeveral other flaws of explanation map visualization are explained by researchers in recent publications. In [114], Ghorbani et al. showed that small perturbations on the input instance generate large changes in the output interpretations that popular XAI methods generate. These adversarial examples, thus threw off the interpretable saliency maps generated by popular methods such as DeepLIFT and Integrated Gradients. This is illustrated in Figure 20. Additionally, in [116], Wang et al. showed that bias term which is often ignored could have high correlations towards attributions.\nIn [117], Kindermans et al. explained that explanations of networks are easily manipulable by simple transformations. Authors note that expressiveness of Integrated Gradients [59] and Deep Taylor Decomposition [61] highly depend on the reference point, for example a baseline image x \u2032 , and suggest that the reference point should be a hyperparameter instead of being determined a priori. Authors mentioned that most gradient-based methods attribute incorrectly to constant vector transformations and that input invariances should be a prerequisite for reliable attributions.\nIn [118], Adebayo et al. suggested that gradient-based methods are inherently dependent on the model and data generating process. Authors proposed two randomization tests for gradient methods namely model parameter randomization test and data randomization test. Model parameter randomization test compared the output of saliency method for a trained model versus the same model with random weights. Data randomization test applied the same saliency method for an input instance and the same instance with a set of invariances. Authors found that Gradients and GradCAM passed the sanity\n21\nchecks while Guided Backprop and Guided GradCAM methods failed the tests suggesting that these methods will generate some explanations even without proper training.\nNewer methods proposed in literature such as explaining with Concepts [51], [52], [58] which we discussed in subsection IV-B4 could be viewed as a new class of meta-explanations which improve both perturbation- and gradient-based XAI methods. By exploring explanations as concepts, one could have additional meta information on the factors which contributed to individual class predictions along with traditional explanation by locally explainable algorithms.\nIn [119], Zhou et al. introduced Interpretable Basis Decomposition as a way of decomposing individual explanation based on different objects or scenes in the input instance. By decomposing the decision to several individual concept explanations, IBD could help evaluate importance of each concepts towards a particular decision.\nIn [97], Kindermans et al. suggested improvements to gradient-based methods and proposed PatternNet and PatternAttribution which can estimate the component of the data that caused network activations. Here, PatternNet is similar to finding gradients but is instead done using a layer-wise backprojection of the estimated signal (data feature) to the input space. PatternAttribution improves upon LRP to provide a neuron-wise attribution of input signal to the corresponding output class."}, {"heading": "VIII. CONCLUSION", "text": "Blindly trusting the results of a highly predictive classifier is, by today\u2019s standard, inadvisable, due to the strong influence of data bias, trustability, and adversarial examples in machine learning. In this survey, we explored why XAI is important, several facets of XAI, and categorized them in respect of their scope, methodology, usage, and nature towards explaining deep neural network algorithms. A summary of the seminal algorithms explained in the survey are tabulated in Table V.\nOur findings showed that considerable research in XAI is focused in model-agnostic post-hoc explainability algorithms due to their easier integration and wider reach. Additionally, there is a large interest in additive and local surrogate models using superpixels of information to evaluate the input feature attributions. Researchers are uncovering limitations of explanation maps visualizations and we see a shift from local perturbation- and gradient-based models due to their shortcomings in adversarial attacks and input invariances. A new trend in using concepts as explanations are gaining traction. However, evaluating these methods are still a challenge and pose an open question in XAI research.\nCurrent research landscape in XAI evaluation illustrates that the field of XAI is still evolving and that XAI methods should be developed and chosen with care. User studies have shown that typical explanation maps alone might not aid in decision making. Human bias in interpreting visual explanations could hinder proper use of XAI in mission-critical applications. Recent developments in human-grounded evaluations shows promising improvements to the XAI evaluation landscape."}], "title": "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey", "year": 2020}