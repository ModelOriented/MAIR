{
  "abstractText": "Despite the growing popularity of modern machine learning techniques (e.g. Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs) An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.",
  "authors": [
    {
      "affiliations": [],
      "name": "Daniel L. Marino"
    },
    {
      "affiliations": [],
      "name": "Chathurika S. Wickramasinghe"
    },
    {
      "affiliations": [],
      "name": "Milos Manic"
    }
  ],
  "id": "SP:36c4cfa5a506ffd7888bd3afe7ca35f52014da6a",
  "references": [
    {
      "authors": [
        "D. Kwon",
        "H. Kim",
        "J. Kim",
        "S.C. Suh",
        "I. Kim",
        "K.J. Kim"
      ],
      "title": "A survey of deep learning-based network anomaly detection",
      "venue": "Cluster Computing, Sep 2017. [Online]. Available: https://doi.org/10.1007/ s10586-017-1117-8",
      "year": 2017
    },
    {
      "authors": [
        "B.S. Sridhar",
        "A. Hahn",
        "M. Govindarasu"
      ],
      "title": "Cyber Physical System Security for the Electric Power Grid",
      "venue": "vol. 100, no. 1, 2012.",
      "year": 2012
    },
    {
      "authors": [
        "R. Raj",
        "I. Lee",
        "J. Stankovic"
      ],
      "title": "Cyber-Physical Systems : The Next Computing Revolution",
      "venue": "pp. 731\u2013736, 2010.",
      "year": 2010
    },
    {
      "authors": [
        "W.L.W. Lee",
        "S.J. Stolfo",
        "K.W. Mok"
      ],
      "title": "A data mining framework for building intrusion detection models",
      "venue": "Proceedings of the 1999 IEEE Symposium on Security and Privacy Cat No99CB36344, vol. 00, pp. 120\u2013132, 1999. [Online]. Available: http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=766909",
      "year": 1999
    },
    {
      "authors": [
        "A. Buczak",
        "E. Guven"
      ],
      "title": "A survey of data mining and machine learning methods for cyber security intrusion detection",
      "venue": "IEEE Communications Surveys & Tutorials, vol. PP, no. 99, p. 1, 2015.",
      "year": 2015
    },
    {
      "authors": [
        "K. Amarasinghe",
        "M. Manic"
      ],
      "title": "Toward explainable deep neural network based anomaly detection",
      "venue": "2018 11th International Conference on Human System Interactions (HSI), July 2018.",
      "year": 2018
    },
    {
      "authors": [
        "W. Samek",
        "T. Wiegand",
        "K. M\u00fcller"
      ],
      "title": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models",
      "venue": "CoRR, vol. abs/1708.08296, 2017. [Online]. Available: http://arxiv.org/abs/1708.08296",
      "year": 2017
    },
    {
      "authors": [
        "D. Gunning"
      ],
      "title": "Explainable artificial intelligence (xai)",
      "venue": "Defense Advanced Research Projects Agency (DARPA), nd Web, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "Parliament",
        "C. of the European Union"
      ],
      "title": "General data protection regulation",
      "venue": "2016.",
      "year": 2016
    },
    {
      "authors": [
        "B. Goodman",
        "S.R. Flaxman"
      ],
      "title": "European union regulations on algorithmic decision-making and a",
      "venue": "AI Magazine, vol. 38, no. 3, pp. 50\u201357, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "D. Lowd",
        "C. Meek"
      ],
      "title": "Adversarial learning",
      "venue": "Proceeding of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining - KDD \u201905, p. 641, 2005. [Online]. Available: http://portal.acm.org/citation.cfm?doid=1081870.1081950",
      "year": 2005
    },
    {
      "authors": [
        "M. Barreno",
        "B. Nelson",
        "A.D. Joseph",
        "J.D. Tygar"
      ],
      "title": "The security of machine learning",
      "venue": "Machine Learning, vol. 81, no. 2, pp. 121\u2013148, 2010.",
      "year": 2010
    },
    {
      "authors": [
        "D. Wagner",
        "P. Soto"
      ],
      "title": "Mimicry Attacks on Host-Based Intrusion Detection Systems",
      "venue": "Proceedings of the 9th ACM conference on Computer and communications security, pp. 255\u2013264, 2002. [Online]. Available: http://portal.acm.org/citation.cfm?doid=586110.586145",
      "year": 2002
    },
    {
      "authors": [
        "J. Li",
        "W. Monroe",
        "T. Shi",
        "A. Ritter",
        "D. Jurafsky"
      ],
      "title": "Adversarial learning for neural dialogue generation",
      "venue": "CoRR, vol. abs/1701.06547, 2017. [Online]. Available: http://arxiv.org/abs/1701.06547",
      "year": 2017
    },
    {
      "authors": [
        "M. Barreno",
        "B. Nelson",
        "R. Sears",
        "A.D. Joseph",
        "J.D. Tygar"
      ],
      "title": "Can machine learning be secure?",
      "venue": "Proceedings of the 2006 ACM Symposium on Information, computer and communications security. ACM,",
      "year": 2006
    },
    {
      "authors": [
        "C. Frederickson",
        "M. Moore",
        "G. Dawson",
        "R. Polikar"
      ],
      "title": "Attack strength vs. detectability dilemma in adversarial machine learning",
      "venue": "arXiv preprint arXiv:1802.07295, 2018.",
      "year": 1802
    },
    {
      "authors": [
        "I.J. Goodfellow",
        "J. Shlens",
        "C. Szegedy"
      ],
      "title": "Explaining and harnessing adversarial examples (2014)",
      "venue": "arXiv preprint arXiv:1412.6572.",
      "year": 1412
    },
    {
      "authors": [
        "A. Kurakin",
        "I. Goodfellow",
        "S. Bengio"
      ],
      "title": "Adversarial examples in the physical world",
      "venue": "arXiv preprint arXiv:1607.02533, 2016.",
      "year": 2016
    },
    {
      "authors": [
        "N. Papernot",
        "P. McDaniel",
        "I. Goodfellow",
        "S. Jha",
        "Z.B. Celik",
        "A. Swami"
      ],
      "title": "Practical black-box attacks against machine learning",
      "venue": "Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM, 2017, pp. 506\u2013519.",
      "year": 2017
    },
    {
      "authors": [
        "M. Tavallaee",
        "E. Bagheri",
        "W. Lu",
        "A.A. Ghorbani"
      ],
      "title": "A detailed analysis of the kdd cup 99 data set",
      "venue": "Computational Intelligence for Security and Defense Applications, 2009. CISDA 2009. IEEE Symposium on. IEEE, 2009, pp. 1\u20136.",
      "year": 2009
    },
    {
      "authors": [
        "L. van der Maaten",
        "G. Hinton"
      ],
      "title": "Visualizing data using t-SNE",
      "venue": "Journal of machine learning research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.",
      "year": 2008
    }
  ],
  "sections": [
    {
      "text": "Index Terms\u2014Adversarial Machine Learning, Adversarial samples, Explainable AI, cyber-security,\nAccepted version of the paper appearing in the proceedings of the 44th Annual Conference of the IEEE Industrial Electronics Society, IECON 2018.\nI. INTRODUCTION\nThe increasing incorporation of Cyber-based methodologies for monitoring and control of physical systems has made critical infrastructure vulnerable to various cyber-attacks such as: interception, removal or replacement of information, penetration of unauthorized users and viruses [1] [2] [3] [4]. Intrusion detection systems (IDSs) are an essential tool to detect such malicious attacks [5].\nExtendibility and adaptability are essential requirements for an IDS [6]. Every day, new strains of Cyber-attacks are created with the objective of deceiving these systems. As a result, machine learning and data-driven intrusion detection systems are increasingly being used in cyber-security applications [6]. Machine learning techniques such as deep neural networks have been successfully used in IDSs [6] [7]. However, these techniques often work as a black-box model for the user. [8] [7].\nWith machine learning being increasingly applied in the operation of critical systems, understanding the reason behind\nthe decisions made by a model has become a common requirement to the point that governments are starting to include it into legislation [10] [11]. Recent developments in the machine learning community have been focused in the development of methods which are more interpretable for the users. Explainable AI (Figure 1) [8] makes use of visualizations and natural language descriptions to explain the reasoning behind the decisions made by the machine learning model.\nIt is crucial that the inner workings of data-driven models are transparent for the engineers designing IDSs. Decisions presented by explainable models can be easily interpreted by a human, simplifying the process of knowledge discovery. Explainable approaches help on diagnosing, debugging, and understanding the decisions made by the model, ultimately increasing the trust on the data-driven IDS.\nIn the case of data-driven IDS, when the model is presented with new attacks where data is not available, the model might mis-classify an attack as normal, leading to a breach in the system. Understanding the reasons behind the misclassification of particular samples is the first step for debugging and diagnosing the system. Providing clear explanations for the cause of misclassification is essential to decide which steps to follow in order to prevent future attacks.\nIn this paper, we present an explainable AI interface for diagnosing data-driven IDSs. We present a methodology to explain incorrect classifications made by the model following an adversarial approach.\nAlthough adversarial machine learning is usually used to\nar X\niv :1\n81 1.\n11 70\n5v 1\n[ cs\n.L G\n] 2\n8 N\nov 2\n01 8\ndeceive the classifier, in this paper we use it to generate explanations by finding the minimum modifications required in order to correctly classify the misclassified samples. The difference between the original and modified samples provide information of the relevant features responsible for the misclassification. We show the explanations provide satisfactory insights behind the reasoning made by the data-driven models, being congruent with the expert knowledge of the task.\nThe rest of the paper is organized as follows: Section II presents an overview of adversarial machine learning; Section III describes the presented methodology for explainable IDS systems. Section IV describes the experimental results carried out using the NSL-KDD99 intrusion detection benchmark dataset. Section V concludes the paper."
    },
    {
      "heading": "II. ADVERSARIAL MACHINE LEARNING",
      "text": "Adversarial machine learning has been extensively used in cyber-security research to find vulnerabilities in data-driven models [12] [13] [14] [15] [16] [17]. Recently, there has been an increasing interest on adversarial samples given the susceptibility of Deep Learning models to these type of attacks [18] [19].\nAdversarial samples are samples crafted in order to change the output of a model by making small modifications into a reference (usually real) sample [18]. These samples are used to detect blind spots on ML algorithms.\nAdversarial samples are crafted from an attacker perspective to evade detection, confuse the classifier [18], degrade performance [20] and/or gain information about the model or the dataset used to train the model [17]. Adversarial samples are also useful from a defender point of view given that they can be used to perform vulnerability assessment [16], study the robustness against noise, improve generalization and debug the machine learning model [20].\nIn general, the problem of crafting an adversarial sample is\nstated as follows [17]:\nmax x\u0302\nL(x\u0302)\u2212 \u2126(x\u0302) (1)\ns.t. x\u0302 \u2208 \u03c6(x)\nwhere: \u2022 L(x) measures the impact of the adversarial sample in\nthe model, \u2022 \u2126(x) measures the capability of the defender to detect\nthe adversarial sample. \u2022 x\u0302 \u2208 \u03c6(x) ensures the crafted sample x\u0302 is inside the\ndomain of valid inputs. It also represents the capabilities of the adversary.\nThe objective of Eq. (1) can be interpreted as crafting an attack point x\u0302 that maximizes the impact of the attack, while minimizing the chances of the attack to be detected. The definition of L(x) will depend on the intent of the attack and the available information about the model under attack. For example: 1) L(x) can be a function that measures the difference between a target class y\u0302 and the output from the model; 2) \u2126(x) measures the discrepancy between the reference x0 and the modified sample x\u0302\nIn this paper, instead of deceiving the classifier, we use Equation 1 to find the minimum number of modifications needed to correctly classify a misclassified sample. The methodology is described in detail in section III."
    },
    {
      "heading": "III. EXPLAINING MISCLASSIFICATIONS USING ADVERSARIAL MACHINE LEARNING",
      "text": "In this paper we are interested in generating explanations for incorrect estimations made by a trained classifier. Figure 2 presents an overview of the presented explainable interface. The presented methodology modifies a set of misclassified samples until they are correctly classified. The modifications are made following an adversarial approach: finding the minimum modifications required to change the output of the model. The difference between the modified samples and the original real samples is used to explain the output from the\nclassifier, illustrating the most relevant features that lead to the misclassification.\nIn the following sections we explain in detail each component of the presented explainable interface."
    },
    {
      "heading": "A. Data-driven classifier",
      "text": "The classifier p (y = k|x, w) estimates the probability of a given sample x to belong to class k. The classifier is parameterized by a set of parameters w that are learned from data. Learning is performed using standard supervised learning. Given a training dataset D = { (x(i),y(i)) }M i\nof M samples, the parameters w of the model are obtained by minimizing the cross-entropy:\nw\u2217 = arg min w M\u2211 i H ( y(i), p ( y|x(i), w )) where y(i) is a one-hot encoding representation of the class:\nyi = { 1 if x(i) \u2208 class i 0 otherwise\nDepending on the complexity of the model p (y|x, w) and the dataset D, the model may misclassify some of the samples. We are interested on provide explanations for these incorrect estimations."
    },
    {
      "heading": "B. Modifying misclassified samples",
      "text": "In this paper, instead of deceiving the classifier, we make use of adversarial machine learning to understand why some of the samples are being mis-classified. The idea of this approach is that adversarial machine learning can help us to understand the decision boundaries of the learned model.\nThe objective is to find the minimum modifications needed in order to change the output of the classifier for a real sample x0. This is achieved by finding an adversarial sample x\u0302 that is classified as y\u0302 while minimizing the distance between the real sample (x0) and the modified sample x\u0302:\nmin x\u0302\n(x\u0302\u2212 x0)T Q (x\u0302\u2212 x0) (2)\ns.t argmaxk p (y = k|x\u0302, w) = y\u0302 (3) xmin x\u0302 xmax\nwhere Q is a symmetric positive definite matrix, that allows the user to specify a weight in the quadratic difference metric.\nThe program in Equation 2 can serve multiple purposes depending on how x0 and y\u0302 are specified. For the purpose of explaining incorrect classifications, in this paper, x0 represents the real misclassified samples that serve as a reference for the modified samples x\u0302. Furthermore, the value of y\u0302 is set to the correct class of x0. In this way, the program (2) finds a modified sample x\u0302 that is as close as possible to the real misclassified sample x0, while being correctly classified by the model as y\u0302.\nWe constrain the sample x\u0302 to be inside the bounds (xmin,xmax). These bounds are extracted from the maximum\nand minimum values found in the training dataset. This constraint ensures that the adversarial example is inside the domain of the data distribution.\nThe class y\u0302 of the adversarial sample is specified by the user. Note that y 6= y\u0302, i.e. the class of the real sample is different from the class of the adversarial sample.\nThe optimization problem in Eq. 2 provides a clear objective to solve. However, the problem as stated in Eq. 2 is not straightforward to solve using available deep-learning optimization frameworks. In order to simplify the implementation, we modify the way the constraints are satisfied by moving the constraint in Eq. 3 into the objective function:\nmin x\u0302\nH(y\u0302, p (y|x\u0302, w))\u03b1I(x\u0302,y\u0302) (4)\n+ (x\u0302\u2212 x0)T Q (x\u0302\u2212 x0) s.t xmin x\u0302 xmax\nwhere: \u2022 (x0,y) is a reference sample from the dataset \u2022 x\u0302 is the modified version of x0 that makes the estimation\nof the class change from y to the target y\u0302 \u2022 H(y\u0302, p (y|x\u0302, w)) is the cross-entropy between the esti-\nmated adversarial sample class p (y|x\u0302, w) and the target class y\u0302 \u2022 I(x\u0302,y\u0302) is an indicator function that specifies whether the adversarial sample x\u0302 is being classified as y\u0302\nI(x\u0302,y\u0302) = { 0 if argmaxk p (y = k|x\u0302, w) = y\u0302 1 otherwise\nThis function provides a mechanism to stop the modifications once the sample x\u0302 is classified as y\u0302. We assume this function is not continuous, hence, the gradients with respect the inputs are not required. \u2022 \u03b1 is a scale factor that can be used to weight the contribution of the cross-entropy H to the objective loss.\nThe problem stated in Eq. 4 can be seen as an instance of the adversarial problem stated in Eq. 1, where the crossentropy H represents the effectiveness (L) of the modifications while the quadratic difference represents the discrepancy (\u2126) between x0 and x\u0302."
    },
    {
      "heading": "C. Explaining incorrect estimations",
      "text": "We used the adversarial approach stated in Eq. 4, to generate an explanation for incorrect classification. Using a set of x0 misclassified samples as reference, we use Eq. 4 to find the minimum modifications needed to correctly classify the samples. We use as target y\u0302 the real class of the samples.\nThe explanations are generated by visualizing the difference (x0 \u2212 x\u0302) between the misclassified samples x0 and the modified samples x\u0302. This difference shows the deviation of the real features x0 from what the model considers as the target class y\u0302.\nThe explanations can be generated for individual samples x0 or for a set of misclassified samples. We used the average\ndeviation (x0 \u2212 x\u0302) to present the explanations for a set of misclassified samples."
    },
    {
      "heading": "IV. EXPERIMENTS AND RESULTS",
      "text": ""
    },
    {
      "heading": "A. Dataset",
      "text": "For experimental evaluation, we used the NSL-KDD intrusion detection dataset [21]. The NSL-KDD dataset is a revised version of the KDD99 dataset [22], a widely used benchmark dataset used for intrusion detection algorithms. The NSLKDD dataset removes redundant and duplicate records found in the KDD99 dataset, alleviating some of the problems of the KDD99 dataset mentioned in [21].\nThe NSL-KDD consists of a series of aggregated records extracted from a packet analyzer log. Besides normal communications, the dataset contains records of attacks that fall in four main categories: DOS, R2L, U2R and proving. For our experiments, we only considered normal, DOS and probe classes.\nThe dataset consists of 124926 training samples and 16557 testing samples. We used the same split provided by the authors of the dataset. To alleviate the effects of the unbalanced distribution of the samples over the classes, we trained the models extracting mini-batches with an equal ratio of samples from each class. Samples were extracted with replacement. A detailed description of the dataset features can be found in [23].\nThe dataset samples were normalized in order to make the quadratic distance metric in Equation 4 invariant to the features scales. We used the mean and standard deviation of the training dataset for normalization:\nx(i) \u2190 x (i) \u2212MEAN ({x|x \u2208 D})\nSTD ({x|x \u2208 D})"
    },
    {
      "heading": "B. Classifiers",
      "text": "One of the advantages of the methodology presented in this paper is that it works for any classifier which has a defined\ngradient \u2207xH of the cross-entropy loss with respect to the inputs.\nFor the experimental section, we used the following datadriven models: 1) a linear classifier, and 2) a Multi Layer Perceptron (MLP) classifier with ReLU activation function. We used weight decay (L2 norm) and early-stopping regularization for training both models.\nTable I shows the accuracy achieved with the linear and the MLP classifiers. We can see that the MLP classifier provides higher accuracy in the training and testing datasets."
    },
    {
      "heading": "C. Modified misclassified samples",
      "text": "We used t-SNE [24] to visualize the misclassified samples (x0) and the modified/corrected samples (x\u0302) found using Equation 4. t-SNE is a dimensionality reduction technique commonly used to visualize high-dimensional datasets.\nFigure 3 shows the visualization of the misclassified samples x0 and the corresponding modified samples x\u0302 using tSNE. This figure shows the effectiveness of the presented methodology to find the minimum modifications needed to correct the output of the classifier. No visual difference in the visualization can be observed between the real and the modified samples. The modified samples x\u0302 are close enough to the real samples x0 that the modified samples occlude the real samples in Figure 3b."
    },
    {
      "heading": "D. Explaining incorrect estimations",
      "text": "Figure 4 shows the generated explanation for Normal samples being incorrectly classified as DOS. Figure 4a shows the\nexplanations for the Linear model while Figure 4b shows the explanations for the MLP model.\nWe observed that the explanations for both models provide similar qualitative explanations. Figures 4a and 4b can be naturally interpreted as follows: Normal samples were mis-classified as DOS because:\n\u2022 high number of connections to the same host (count) and to the same destination address (dst host count) \u2022 low connection duration (duration) \u2022 low number of operations performed as root in the\nconnection (num root) \u2022 low percentage of samples have successfully logged in\n(logged in, is guest login) \u2022 high percentage of connections originated from the same\nsource port (dst host same src port rate) \u2022 low percentage of connections directed to different ser-\nvices (diff srv rate) \u2022 low number of connections were directed to the same\ndestination port (dst host srv count) Figures 4a and 4b that a high number of connections with low duration and low login success rate is responsible for misclassifying Normal samples as DOS. These attributes are clearly suspicious and match what a human expert would consider as typical behavior of a DOS attack, providing a satisfactory explanation for the incorrect classification.\nA more detailed view of the difference between the duration of real (x0) and modified (x\u0302) samples is presented in Figure 5. This figure shows that all misclassified Normal samples had connections with a duration of zero seconds, which the classifier considers suspicious for a Normal behavior.\nThe graphs also provide a natural way to extract knowledge and understand the concepts learned by the model. For example, figures 4a and 4b show the classifier considers suspicious when there is a low percentage of connections directed to different services (low diff srv rate).\nA parallel analysis can be performed to other misclassified samples. Figure 6 provides explanations for DOS attacks misclassified as Normal connections for Linear and MLP models. Overall, Figures 6a and 6b, show that the samples are\nmisclassified as Normal because they have: (a) lower error rate during the connection (b) higher login success. These features are usually expected to belong to normal connections, which successfully explains the reason for the misclassification.\nThe explanations shown in Figures 4 and 6 do not take categorical features into account. In order to include categorical features into the analysis, we perform a round operation to the inputs of the indicator I(x\u0302,y\u0302). 1 This ensures the objective function in Equation 4 takes into consideration the effects of the rounding operation.\nFigure 7 shows the explanations generated when considering categorical features. Figure 7a shows the deviation of continuous features, providing the same information as the explanations from Figures 4 and 6. Figure 7b shows the comparison between the histograms of misclassified samples and modified samples. Figure 7b shows that protocol type was not modified, suggesting that this feature is not relevant in order to explain the misclassification. On the other hand, the service feature was modified in almost all samples. The figure shows that most of the Normal samples misclassified as DOS used a private service. Changing the service value helps the classifier to correctly estimate the class of the samples. The explanation shows that the model considers communication with private service as suspicious."
    },
    {
      "heading": "V. CONCLUSION",
      "text": "In this paper, we presented an approach for generating explanations for the incorrect classification of a set of samples. The methodology was tested using an Intrusion Detection benchmark dataset.\nThe methodology uses an adversarial approach to find the minimum modifications needed in order to correctly classify the misclassified samples. The modifications are used to find and visualize the relevant features responsible for the misclassification. Experiments were performed using Linear\n1Given that we do not use the gradient of I(x\u0302,y\u0302) during the optimization process, we are allowed to include discontinuous operations like the rounding function\nand Multilayer perceptron classifiers. The explanations were presented using intuitive plots that can be easily interpreted by the user.\nThe proposed methodology provided insightful and satisfactory explanations for the misclassification of samples, with results that match expert knowledge. The relevant features found by the presented approach showed that misclassification often occurred on samples with conflicting characteristics between classes. For example, normal connections with low duration and low login success are misclassified as attacks, while attack connections with low error rate and higher login success are misclassified as normal."
    },
    {
      "heading": "VI. DISCUSSION",
      "text": "An advantage of the presented approach is that it can be used for any differentiable model and any classification task. No modifications of the model are required. The presented approach only requires the gradients of the model crossentropy w.r.t. the inputs. Non-continuous functions can also be incorporated into the approach, for example rounding operations for integer and categorical features.\nThe presented adversarial approach can be extended to perform other analysis of the model. For example, instead of finding modifications for correct the predicted class, the modifications can be used to deceive the classifier, which can be used for vulnerability assessment. Future research will be conducted to incorporate the explanations to improve the accuracy of the model without having to include new data into the training procedure."
    }
  ],
  "title": "An Adversarial Approach for Explainable AI in Intrusion Detection Systems",
  "year": 2018
}
