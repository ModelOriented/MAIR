{"abstractText": "Explainability has been an important goal since the early days of Artificial Intelligence. Several approaches for producing explanations have been developed. However, many of these approaches were tightly coupled with the capabilities of the artificial intelligence systems at the time. With the proliferation of AI-enabled systems in sometimes critical settings, there is a need for them to be explainable to end-users and decision-makers. We present a historical overview of explainable artificial intelligence systems, with a focus on knowledge-enabled systems, spanning the expert systems, cognitive assistants, semantic applications, and machine learning domains. Additionally, borrowing from the strengths of past approaches and identifying gaps needed to make explanations userand contextfocused, we propose new definitions for explanations and explainable knowledge-", "authors": [{"affiliations": [], "name": "Shruthi Chari"}, {"affiliations": [], "name": "aDaniel M. Gruen"}, {"affiliations": [], "name": "Oshani Seneviratne"}, {"affiliations": [], "name": "Deborah L. McGuinness"}], "id": "SP:7fdd812a66d4cad5192f2f012f7e9499a1177a47", "references": [{"authors": ["D.A. Norman"], "title": "The psychology of everyday things", "venue": "Basic books,", "year": 1988}, {"authors": ["J. Hollan", "E. Hutchins", "D. Kirsh"], "title": "Distributed cognition: toward a new foundation for humancomputer interaction research", "venue": "ACM Transactions on Computer-Human Interaction (TOCHI), vol. 7, no. 2, pp. 174\u2013196, 2000.", "year": 2000}, {"authors": ["E. Hutchins"], "title": "The technology of team navigation, Intellectual teamwork: social and technological foundations of cooperative work, L", "venue": "1990.", "year": 1990}, {"authors": ["W. Swartout", "C. Paris", "J. Moore"], "title": "Explanations in knowledge systems: Design for explainable expert systems", "venue": "IEEE Expert, vol. 6, no. 3, pp. 58\u201364, 1991.", "year": 1991}, {"authors": ["E.H. Shortliffe"], "title": "MYCIN: a rule-based computer program for advising physicians regarding antimicrobial therapy selection.", "venue": "Dept. of Computer Sci.,", "year": 1974}, {"authors": ["W.J. Clancey", "R. Letsinger"], "title": "NEOMYCIN: Reconfiguring a rule-based expert system for application to teaching", "venue": "Dept. of Computer Sci., Stanford University Stanford,", "year": 1982}, {"authors": ["D.L. McGuinness", "A. Glass", "M. Wolverton", "P.P. Da Silva"], "title": "Explaining Task Processing in Cognitive Assistants That Learn.", "venue": "AAAI Spring Symp.: Interaction Challenges for Intelligent Assistants,", "year": 2007}, {"authors": ["O. Seneviratne", "S.M. Rashid", "S. Chari", "J.P. McCusker", "K.P. Bennett", "J.A. Hendler", "D.L. McGuinness"], "title": "Knowledge Integration for Disease Characterization: A Breast Cancer Example", "venue": "Int. Semantic Web Conf. San Francisco, USA: Springer, 2018, pp. 223\u2013238.", "year": 2018}, {"authors": ["D. Gunning"], "title": "Explainable artificial intelligence (XAI)", "venue": "Defense Advanced Research Projects Agency (DARPA), nd Web, vol. 2, 2017.", "year": 2017}, {"authors": ["J. Hendler", "E. Miller"], "title": "Integrating applications on the semantic web", "venue": "J. of the Institute of Electrical Engineers of Japan, vol. Vol 122(10), pp. 676\u2013680, 2002.", "year": 2002}, {"authors": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "title": "Deep learning", "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015. 20We have used abbreviations in our references as per the IEEE reference guide https:// ieeeauthorcenter.ieee.org/wp-content/uploads/IEEE-Reference-Guide.pdf S. Chari, D. Gruen, O. Seneviratne, D. McGuinness /", "year": 2015}, {"authors": ["J.S. Dhaliwal", "I. Benbasat"], "title": "The use and effects of knowledge-based system explanations: theoretical foundations and a framework for empirical evaluation", "venue": "Information systems research, vol. 7, no. 3, pp. 342\u2013362, 1996.", "year": 1996}, {"authors": ["E. Shortliffe"], "title": "Computer-based medical consultations: MYCIN", "year": 2012}, {"authors": ["Y. Lou", "R. Caruana", "J. Gehrke"], "title": "Intelligible models for classification and regression", "venue": "Proc. of the 18th ACM SIGKDD Int. Conf. on Knowledge discovery and data mining. ACM, 2012, pp. 150\u2013158.", "year": 2012}, {"authors": ["A. Holzinger", "C. Biemann", "C.S. Pattichis", "D.B. Kell"], "title": "What do we need to build explainable AI systems for the medical domain?", "venue": "arXiv preprint arXiv:1712.09923,", "year": 2017}, {"authors": ["D.L. McGuinness", "F. Van Harmelen"], "title": "OWL web ontology language overview", "venue": "W3C recommendation, vol. 10, no. 10, p. 2004, 2004.", "year": 2004}, {"authors": ["T. Lebo", "S. Sahoo", "D. McGuinness", "K. Belhajjame", "J. Cheney", "D. Corsar", "D. Garijo", "S. Soiland-Reyes", "S. Zednik", "J. Zhao"], "title": "PROV-O: The prov ontology", "venue": "W3C recommendation, 2013.", "year": 2013}, {"authors": ["P. Groth", "A. Gibson", "J. Velterop"], "title": "The anatomy of a nanopublication", "venue": "Information Services & Use, vol. 30, no. 1-2, pp. 51\u201356, 2010.", "year": 2010}, {"authors": ["B. Mittelstadt", "C. Russell", "S. Wachter"], "title": "Explaining explanations in AI", "venue": "Proc. of the Conf. on fairness, accountability, and transparency. ACM, 2019, pp. 279\u2013288.", "year": 2019}, {"authors": ["O. Biran", "C. Cotton"], "title": "Explanation and justification in machine learning: A survey", "venue": "IJCAI-17 workshop on explainable AI (XAI), vol. 8, 2017, p. 1.", "year": 2017}, {"authors": ["V. Arya", "R.K. Bellamy", "P.-Y. Chen", "A. Dhurandhar", "M. Hind", "S.C. Hoffman", "S. Houde", "Q.V. Liao", "R. Luss", "A. Mojsilovi\u0107"], "title": "One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques", "venue": "arXiv preprint arXiv:1909.03012, 2019.", "year": 1909}, {"authors": ["W.R. Swartout", "J.D. Moore"], "title": "Explanation in second generation expert systems", "venue": "Second generation expert systems. Springer, 1993, pp. 543\u2013585.", "year": 1993}, {"authors": ["F. Doshi-Velez", "M. Kortz", "R. Budish", "C. Bavitz", "S. Gershman", "D. O\u2019Brien", "S. Schieber", "J. Waldo", "D. Weinberger", "A. Wood"], "title": "Accountability of AI under the law: The role of explanation", "venue": "arXiv preprint arXiv:1711.01134, 2017.", "year": 2017}, {"authors": ["L.H. Gilpin", "D. Bau", "B.Z. Yuan", "A. Bajwa", "M. Specter", "L. Kagal"], "title": "Explaining explanations: An approach to evaluating interpretability of machine learning", "venue": "arXiv preprint arXiv:1806.00069, 2018.", "year": 1806}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "Artificial Intelligence, vol. 267, pp. 1\u201338, 2019.", "year": 2019}, {"authors": ["B.Y. Lim", "A.K. Dey", "D. Avrahami"], "title": "Why and why not explanations improve the intelligibility of context-aware intelligent systems", "venue": "Proc. of the SIGCHI Conf. on Human Factors in Computing Systems. ACM, 2009, pp. 2119\u20132128.", "year": 2009}, {"authors": ["M. Ribera", "\u00c0. Lapedriza"], "title": "Can we do better explanations? A proposal of user-centered explainable AI.", "venue": "IUI Workshops,", "year": 2019}, {"authors": ["R. Maimone", "M. Guerini", "M. Dragoni", "T. Bailoni", "C. Eccher"], "title": "PerKApp: A general purpose persuasion architecture for healthy lifestyles", "venue": "J. of biomedical informatics, vol. 82, pp. 70\u201387, 2018.", "year": 2018}, {"authors": ["A. Glass", "D.L. McGuinness", "M. Wolverton"], "title": "Toward establishing trust in adaptive agents", "venue": "Proc. of the 13th Int. Conf. on Intelligent user interfaces. ACM, 2008, pp. 227\u2013236.", "year": 2008}, {"authors": ["E. Rich"], "title": "Stereotypes and user modeling", "venue": "User models in dialog systems. Springer, 1989, pp. 35\u201351.", "year": 1989}, {"authors": ["K. Sugiyama", "K. Hatano", "M. Yoshikawa"], "title": "Adaptive web search based on user profile constructed without any effort from users", "venue": "Proc. of the 13th Int. Conf. on World Wide Web. ACM, 2004, pp. 675\u2013684.", "year": 2004}, {"authors": ["F. Doshi-Velez", "B. Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608, 2017.", "year": 2017}, {"authors": ["L. Kagal"], "title": "Accountability In RDF (AIR) Web Rule Language", "venue": "2009. [Online]. Available: http://dig.csail.mit.edu/2009/AIR/", "year": 2009}, {"authors": ["D.L. McGuinness", "P.P. Da Silva"], "title": "Explaining answers from the semantic web: The inference web approach", "venue": "Web Semantics: Sci., Services and Agents on the World Wide Web, vol. 1, no. 4, pp. 397\u2013413, 2004.", "year": 2004}, {"authors": ["F. Lecue"], "title": "On the role of knowledge graphs in explainable AI", "venue": "Semantic Web J. (Forthcoming), 2019.", "year": 2019}, {"authors": ["R. Hasan", "F. Gandon"], "title": "Explanation in the Semantic Web: a survey of the state of the art", "venue": "Research. Rep., 2012.", "year": 2012}, {"authors": ["A. Borgida", "R.J. Brachman", "D.L. McGuinness", "L.A. Resnick"], "title": "CLASSIC: A structural data model for objects", "venue": "ACM Sigmod record, vol. 18, no. 2. ACM, 1989, pp. 58\u201367. S. Chari, D. Gruen, O. Seneviratne, D. McGuinness /", "year": 1989}, {"authors": ["G. Tecuci", "M. Boicu", "C. Ayers", "D. Cammons"], "title": "Personal cognitive assistants for military intelligence analysis: Mixed-initiative learning, tutoring, and problem solving", "venue": "First Int. Conf. on Intelligence Analysis. Citeseer, 2005, pp. 2\u20136.", "year": 2005}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "Proc. of the 22nd ACM SIGKDD Int. Conf. on knowledge discovery and data mining. ACM, 2016, pp. 1135\u20131144.", "year": 2016}, {"authors": ["J.P. McCusker", "M. Dumontier", "R. Yan", "S. He", "J.S. Dordick", "D.L. McGuinness"], "title": "Finding melanoma drugs through a probabilistic knowledge graph", "venue": "PeerJ Computer Sci., vol. 3, p. e106, 2017.", "year": 2017}, {"authors": ["D. Gunning", "D.W. Aha"], "title": "DARPA\u2019s Explainable Artificial Intelligence Program", "venue": "AI Magazine, vol. 40, no. 2, pp. 44\u201358, 2019.", "year": 2019}, {"authors": ["P.F. Patel-Schneider", "D.L. McGuinness", "R.J. Brachman", "L.A. Resnick"], "title": "The CLASSIC knowledge representation system: Guiding principles and implementation rationale", "venue": "ACM SIGART Bulletin, vol. 2, no. 3, pp. 108\u2013113, 1991.", "year": 1991}, {"authors": ["J.P. McCusker", "S.M. Rashid", "Z. Liang", "Y. Liu", "K. Chastain", "P. Pinheiro", "J.A. Stingone", "D.L. McGuinness"], "title": "Broad, Interdisciplinary Science In Tela: An Exposure and Child Health Ontology", "venue": "Proc. of the 2017 ACM on Web Sci. Conf. ACM, 2017, pp. 349\u2013357.", "year": 2017}, {"authors": ["I. Nunes", "D. Jannach"], "title": "A systematic review and taxonomy of explanations in decision support and recommender systems", "venue": "User Modeling and User-Adapted Interaction, vol. 27, no. 3-5, pp. 393\u2013444, 2017.", "year": 2017}, {"authors": ["D.L. McGuinness", "A. Borgida"], "title": "Explaining subsumption in description logics", "venue": "IJCAI (1), 1995, pp. 816\u2013821.", "year": 1995}, {"authors": ["D.L. McGuinness"], "title": "Explaining reasoning in description logics", "venue": "Ph.D. dissertation, Rutgers University, 1996.", "year": 1996}, {"authors": ["R.J. Brachman", "J.G. Schmolze"], "title": "An overview of the KL-ONE knowledge representation system", "venue": "Readings in artificial intelligence and databases. Elsevier, 1989, pp. 207\u2013230.", "year": 1989}, {"authors": ["T. Berners-Lee", "D. Dimitroyannis", "A.J. Mallinckrodt", "S. McKay"], "title": "World Wide Web", "venue": "Computers in Physics, vol. 8, no. 3, pp. 298\u2013299, 1994.", "year": 1994}, {"authors": ["T. Berners-Lee", "J. Hendler", "O. Lassila"], "title": "The semantic web", "venue": "Scientific american, vol. 284, no. 5, pp. 28\u201337, 2001.", "year": 2001}, {"authors": ["A. Hogan", "A. Harth", "A. Polleres"], "title": "Scalable authoritative OWL reasoning for the web", "venue": "Int. J. on Semantic Web and Information Systems (IJSWIS), vol. 5, no. 2, 2009.", "year": 2009}, {"authors": ["T. Kuhn", "P.E. Barbano", "M.L. Nagy", "M. Krauthammer"], "title": "Broadening the scope of nanopublications", "venue": "Extended Semantic Web Conf. Springer, 2013.", "year": 2013}, {"authors": ["J. Valdez", "M. Kim", "M. Rueschman", "V. Socrates", "S. Redline", "S.S. Sahoo"], "title": "ProvCaRe semantic provenance knowledgebase: evaluating scientific reproducibility of research studies", "venue": "AMIA Annual Symp. Proc., vol. 2017. American Medical Informatics Association, 2017, p. 1705.", "year": 2017}, {"authors": ["N.N. Agu", "N. Keshan", "S. Chari", "O. Seneviratne", "J.P. McCusker", "A. Das", "D.L. McGuinness"], "title": "G- PROV: Provenance Management for Clinical Practice Guidelines", "venue": "Proc. of the Semantic Web solutions for large-scale biomedical data analytics Workshop. CEUR, 2019, p. to appear.", "year": 2019}, {"authors": ["S. Hertling", "H. Paulheim"], "title": "WebIsALOD: providing hypernymy relations extracted from the web as linked open data", "venue": "Int. Semantic Web Conf. Springer, 2017, pp. 111\u2013119.", "year": 2017}, {"authors": ["A. Gyrard", "M. Gaur", "S. Shekarpour", "K. Thirunarayan", "A. Sheth"], "title": "Personalized Health Knowledge Graph", "venue": "Int. Semantic Web Conf. (ISWC) 2018 Contextualized Knowledge Graph Workshop, 2018.", "year": 2018}, {"authors": ["M. Horridge", "B. Parsia", "U. Sattler"], "title": "Laconic and precise justifications in OWL", "venue": "Int. semantic web Conf. Springer, 2008, pp. 323\u2013338.", "year": 2008}, {"authors": ["J. d. Kleer", "J. Doyle", "G.L. Steele Jr", "G.J. Sussman"], "title": "AMORD explicit control of reasoning", "venue": "ACM SIGPLAN Notices, vol. 12, no. 8, pp. 116\u2013125, 1977.", "year": 1977}, {"authors": ["T. Berners-Lee", "D. Connolly", "L. Kagal", "Y. Scharf", "J. Hendler"], "title": "N3logic: A logical framework for the world wide web", "venue": "Theory and Practice of Logic Programming, vol. 8, no. 3, pp. 249\u2013269, 2008.", "year": 2008}, {"authors": ["A.E. Giuliano", "J.L. Connolly", "S.B. Edge", "E.A. Mittendorf", "H.S. Rugo", "L.J. Solin", "D.L. Weaver", "D.J. Winchester", "G.N. Hortobagyi"], "title": "Breast cancer\u2014major changes in the American Joint Committee on Cancer eighth edition cancer staging manual", "venue": "CA: A Cancer J. for Clinicians, vol. 67, no. 4, pp. 290\u2013303, 2017.", "year": 2017}, {"authors": ["D.C. Engelbart"], "title": "Toward augmenting the human intellect and boosting our collective IQ", "venue": "Communica- S. Chari, D. Gruen, O. Seneviratne, D. McGuinness / tions of the ACM, vol. 38, no. 8, pp. 30\u201333, 1995.", "year": 1995}, {"authors": ["M.R. Ebling"], "title": "Can cognitive assistants disappear?", "venue": "IEEE Pervasive Computing,", "year": 2016}, {"authors": ["R.G. Farrell", "J. Lenchner", "J.O. Kephjart", "A.M. Webb", "M.J. Muller", "T.D. Erikson", "D.O. Melville", "R.K. Bellamy", "D.M. Gruen", "J.H. Connell"], "title": "Symbiotic cognitive computing", "venue": "AI Magazine, vol. 37, no. 3, pp. 81\u201393, 2016.", "year": 2016}, {"authors": ["K. Conley", "J. Carpenter"], "title": "Towel: Towards an Intelligent To-Do List.\u201d in AAAI Spring Symp.: Interaction Challenges for Intelligent Assistants", "year": 2007}, {"authors": ["F. Baader", "I. Horrocks", "U. Sattler"], "title": "Description logics", "venue": "Handbook on ontologies. Springer, 2004, pp. 3\u201328.", "year": 2004}, {"authors": ["P.P. Da Silva", "D.L. McGuinness", "R. Fikes"], "title": "A proof markup language for semantic web services", "venue": "Information Systems, vol. 31, no. 4-5, pp. 381\u2013395, 2006.", "year": 2006}, {"authors": ["K. Vanlehn"], "title": "The behavior of tutoring systems", "venue": "Int. J.of artificial intelligence in education, vol. 16, no. 3, pp. 227\u2013265, 2006.", "year": 2006}, {"authors": ["V. Aleven", "B.M. Mclaren", "J. Sewall", "K.R. Koedinger"], "title": "A new paradigm for intelligent tutoring systems: Example-tracing tutors", "venue": "Int. J. of Artificial Intelligence in Education, vol. 19, no. 2, pp. 105\u2013 154, 2009.", "year": 2009}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability", "venue": "Queue, vol. 16, no. 3, pp. 31\u201357, 2018.", "year": 2018}, {"authors": ["D. Bau", "B. Zhou", "A. Khosla", "A. Oliva", "A. Torralba"], "title": "Network dissection: Quantifying interpretability of deep visual representations", "venue": "Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, 2017, pp. 6541\u20136549.", "year": 2017}, {"authors": ["S. Wachter", "B. Mittelstadt", "C. Russell"], "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GPDR", "venue": "Harv. JL & Tech., vol. 31, p. 841, 2017.", "year": 2017}, {"authors": ["J. van der Waa", "M. Robeer", "J. van Diggelen", "M. Brinkhuis", "M. Neerincx"], "title": "Contrastive explanations with local foil trees", "venue": "arXiv preprint arXiv:1806.07470, 2018.", "year": 1806}, {"authors": ["B. Zhou", "D. Bau", "A. Oliva", "A. Torralba"], "title": "Interpreting deep visual representations via network dissection", "venue": "IEEE transactions on pattern analysis and machine intelligence, 2018.", "year": 2018}, {"authors": ["M. Harradon", "J. Druce", "B. Ruttenberg"], "title": "Causal learning and explanation of deep neural networks via autoencoded activations", "venue": "arXiv preprint arXiv:1802.00541, 2018.", "year": 1802}, {"authors": ["J. Pearl"], "title": "Theoretical impediments to machine learning", "venue": "2017.", "year": 2017}, {"authors": ["S.C.-H. Yang", "P. Shafto"], "title": "Explainable Artificial Intelligence via Bayesian Teaching", "venue": "NIPS 2017 workshop on Teaching Machines, Robots, and Humans, 2017.", "year": 2017}, {"authors": ["R.K. Bellamy", "K. Dey", "M. Hind", "S.C. Hoffman", "S. Houde", "K. Kannan", "P. Lohia", "J. Martino", "S. Mehta", "A. Mojsilovic"], "title": "AI fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias", "venue": "arXiv preprint arXiv:1810.01943, 2018.", "year": 1810}, {"authors": ["J.R. Zilke", "E.L. Menc\u0131\u0301a", "F. Janssen"], "title": "DeepRED\u2013Rule extraction from deep neural networks", "venue": "Int. Conf. on Discovery Sci. Springer, 2016, pp. 457\u2013473.", "year": 2016}], "sections": [{"text": "Keywords. KG4XAI, Explainable Knowledge-Enabled Systems, Historical Evolution"}, {"heading": "1. Introduction", "text": "The growing incorporation of Artificial Intelligence (AI) capabilities in systems across industries and consumer applications, including those that have significant, even lifeor-death implications, has led to an increased demand for explainability. To accept and appropriately apply insights from AI systems, users often require an understanding of how the system arrived at its results.\nSuch an understanding can include having a model of how the underlying AI system operates, how it was constructed, and how the data used to develop and train it matches the situations in which it was used. It can include information about the specific features of the current situation that contributed to the system\u2019s determination. It can also include descriptions of the underlying rationales and reasoning paths the system used to arrive at a conclusion, which in turn can be based on observed statistical regularities, models of underlying mechanisms and causal relationships, and temporal patterns. We draw a distinction, between transparency,1 by which we mean general information about a system\u2019s operation, capabilities, underlying training data, and fairness, and explainability, by which we mean the ability of a system to provide information describing and justifying how a specific result was determined along with the overall context. We build on\n1In this chapter we use quotes for terms that we introduce or for direct quotations from publications, and we use italics to either emphasize terminology from papers or highlight important terms.\nar X\niv :2\n00 3.\n07 52\n0v 1\n[ cs\n.A I]\n1 7\nthis notion of explainability and present desired properties for explanations and redefine explanations supporting a user\u2019s perspective in Section 2.\nBy their very nature, explanations are user focused; explanations are needed because they provide information that would otherwise be absent that helps a user trust, apply, and maximally benefit from the AI system\u2019s operation. Thus, the need for explanation, and the types of explanations required, are contextual, depending on users, their roles, their prior knowledge, and the situation. For example, a physician recommending a nonstandard treatment regimen might want to understand what aspect of the current patient\u2019s condition led to an unexpected result, and how the reasoning behind it aligns with scientific knowledge about biological and pharmacological mechanisms. A patient-facing explanation for the same result may need to include more basic information on the condition and what is unique about the patient\u2019s situation. An explanation aimed at a hospital administrator or insurance coordinator may need to include information about potential biases that could lead to a lack of fairness in the recommendation.\nExplanations can have deeper value beyond the \u201cgating\u201d role they play in helping users determine which results should be trusted and applied. Explanations provided in the above example could contribute to the mental model the physician is constructing of the patient, and of diseases and biological mechanisms in general, that could be valuable in future treatment decisions they make for that patient and others. Explanations also contribute to the model users are creating of the system itself, by exposing the kinds of information and processing mechanisms the system utilizes. Norman famously described how users construct mental models of systems with which they interact across gulfs of execution and interpretation [1]. With AI systems, explanations can help users simultaneously construct models of the system with which they are interacting, and of the underlying domain and situation in which the system is being used.\nThe importance of explainability is particularly salient with collaborative AI systems meant to work in tandem with human users to augment rather than supplant their skills and capabilities. A \u201cDistributed Cognition\u201d approach [2] is informative here, in which cognition is seen to take place not within the head of any one individual, but rather through the exchange and transformation of representations across multiple actors and artifacts [3]. The ability for a system to provide explanations, and respond to queries that reference other information relevant to the situation, expands the range of ways in which the system and human actors can interact."}, {"heading": "1.1. Historical Evolution", "text": "Explainability has been a major goal since the early days of AI. In this chapter, we focus on the broad class of knowledge-enabled systems, instead of simply knowledge-based systems. We include rule-based systems as well as hybrid AI systems that may include a wide range of reasoning components including potentially inductive or abductive reasoning as well as the more traditional deductive reasoning. As such, we include historical explanation work (e.g., [4,5,6]) and also explanation work aimed more at evolving hybrid AI systems (e.g., [7,8,9]). The survey includes the domains of expert systems, cognitive assistants, Semantic Web [10], and, more recently, explanations that work with black-box models, i.e., deep learning models [11]. With this background, we will now present a historical perspective on the evolution of explainable AI.\nMany early AI systems took a rule-based, expert system approach. Expert systems (e.g., [5,4]) were inherently explainable in that they used a set of rules to come to con-\nclusions, so explanations could be generated that provided a detailed or abstracted collection of the rule executions as an explanation of a conclusion. During the expert system era, much work focused on explaining these systems and their decisions to the end-user. Explanations were broadly intended to address the Why, What, and How aspects of an AI system that produces a result. Dhaliwal et al. [12] provide an overview of these explanation types and state that the Why explanations were populated with the justification for a conclusion, the How explanations contained a trace of the mechanistic functioning of the system, and, the What explanations exposed the system\u2019s decision variables involved in the conclusion. Explanations produced by these systems were mainly focused on introducing the rationale behind a system\u2019s decision and the way the system works. Additionally, while trace-based explanations produced by expert systems captured the why and how aspects, they typically did not account for the context of a user when they generated explanations. There were a wide range of expert systems early on. For example, MYCIN [13] was an early expert system that supported medical diagnosis using a rule-based inference engine and included a trace-based explanation component.\nToday, with the availability of vast volumes of data, deep learning algorithms are being widely used. However, these models are largely uninterpretable, and a significant focus of explainable AI research (e.g., DARPA XAI Report 2017 [9]) is focused on explaining the underlying mechanisms of these black-box models. In our opinion, gaining transparency into the black boxes can be useful, and it may decrease the \u201cunintelligibility aspect\u201d [14]. However, it is not enough to provide personalized, tailored, and trustworthy [15] explanations to consumers of AI models. Additionally, machine learning (ML) models often output a score or probability as predictions. While the number may be useful to understand some level of confidence, a single number lacks context, and thus is often inadequate without additional information. Semantic Web representation and reasoning work is well suited to help here. Standards for representing terminology, e.g., RDF and OWL [16], as well as representing provenance (e.g., PROV-O [17] and nanopublications [18]), have emerged and can be used to encode information along with its provenance and a system\u2019s reasoning provenance, and this may be used to augment explanations. The inclusion of provenance into the underlying representation and thus potentially into the explanations partially addresses the What and Why aspects of the reasoning behind presenting explanations to consumers.\nRecently, researchers have acknowledged an increasing need to include explainability modules into AI systems. As a consequence, several survey papers [19,20,15] have highlighted past noteworthy efforts in explainable AI.2 These survey papers emphasize the fact that different situations, users, and contexts demand different kinds of explanation [21]. Different AI systems are geared toward addressing an explanation type (or rarely, a combination), e.g., expert systems typically provide trace-based explanations, deep learning models can be leveraged to offer contrasting explanations, etc. We believe that the next-generation AI systems need to go beyond the Why, What, How aspects and produce explanations that additionally prioritize issues related to the setting, users\u2019 understanding, and contexts. At a minimum, these AI systems need to include a provenance component to support trust and provide users with tools that can access a reasoning trace\n2We choose to use the explainable AI phrasing for explainability efforts in AI as XAI has come to be associated with the DARPA explainable AI (XAI) program. Our focus is broader than the program\u2019s focus on explaining and interpreting black box ML methods from a cognitive perspective.\nto further explore or serve as a means of understanding. In Section 3, we will review a few of these past approaches mentioned in this section."}, {"heading": "1.2. Shift and Current Focus of Explainable AI", "text": "The evolution of AI systems has been heavily influenced by the availability of resources, computing power and data. As previously stated, AI has moved from primarily using rule-based expert systems to using ML methods, and, sometimes, hybrid methods. With these changes, there has been a shift in the focus of explainability, due to the new challenges of the interpretability of complex ML models. The initial explanation focus on working from system traces to provide a notion of what was done has expanded to including a focus on including a notion of interpretability of an underlying ML model. This interpretable ML work may be a first generation of explanation of ML, but as we will expand on below, more is needed. Additionally, in the Semantic Web [10], and more generally, in knowledge representation-based applications, the focus has expanded from traditional What explanations to include explanations addressing information attribution and provenance aspects. The motivations for those expansions include improving the trustworthiness of information being represented in knowledge graphs (KGs), and further, to provide more context for users as they are deciding how to use the information in analysis applications. Further, AI models are now being employed in user-facing settings where there is a need for personalized conclusions. Hence, there is a need to rethink explanations produced by AI systems from a user perspective and include components to educate users, align with their cognitive model, help them trust the system, provide relevant information, and tailor suggestions to a user\u2019s contexts [19,20]. Borrowing strengths from explanations provided by past approaches, we will attempt to present, synthesize, and refine a definition of explanation and explainable knowledge-enabled systems with an acknowledgment of desired explanation properties that fit today\u2019s settings."}, {"heading": "2. Terminology", "text": "Several researchers have proposed comprehensive definitions of explanations [22,23,7, 24] and have presented explanation components that they deem necessary to satisfy either their work or the domains where they hope the explanations will be useful. However, with a shift of focus in AI we feel the need to revisit the work on defining explanation as we consider what is desirable in next-generation \u201cexplainable knowledge-enabled systems.\u201d In this section, we list desirable properties (Section 2.1) for both explanations and explainable knowledge-enabled systems that generate these explanations, and use these properties as a basis to provide definitions."}, {"heading": "2.1. Desirable Properties", "text": ""}, {"heading": "2.1.1. Explanations", "text": "As a part of our list of desirable properties for explanations, we present properties, such as, improving user appeal, and achieving user understandability, that have been explored as explanation components in the past, and that will be useful in designing explanations suitable for the end-user. In addition, we propose including a higher priority on the in-\nclusion of features, such as, provenance and adapting to user\u2019s context that will have a renewed focus in making explanations user-centric and in mitigating the unintelligibility aspect of current ML methods. We are aligning with others who have called for a greater user focus in explanations [25,26,27].\n\u2022 Be understandable: Borrowing from desired properties of explanations stated by Swartout and Moore [22], we highlight that for explanations to be understandable by the user, the explanations should use terminology familiar to the user. If terminology is potentially unfamiliar, then we also suggest that capabilities be included for obtaining definitions of terms, thereby educating users. Understandability has the potential to be significantly increased if the AI system incorporates user feedback and a model of user context. \u2022 Include provenance: Provenance is a property of explanations that has either been absent in some past descriptions of explanations [22], or has not had the emphasis that it deserves now. As systems expand to include more diverse content the need for capturing provenance increases. Explanations need to include provenance that includes information about the domain knowledge utilized by the system, along with the methods used to obtain that knowledge. We borrow from the \u201ccounterfactual faithfulness\u201d idea proposed by [23], and argue that, as part of the provenance components, explanations need to carry causal information about the conclusion, if present in domain literature or supported by expert knowledge. \u2022 Appeal to user: Paraphrasing Swartout and Moore [22], we note that explanations need to be rich, coherent, and appeal to the user.We propose that explanations need to expose facts that the user finds resourceful and sufficient for further exploration. A resourceful explanation contains enough granular content and evidence to appeal to the user\u2019s mental cognition and current needs. A sufficient explanation contains content that the user requires to carry out their tasks. A subtlety in generating explanations that appeal to the user would be to tailor the explanation length to the user\u2019s needs and preferences, i.e., to avoid lengthy explanations with content that might not be useful to the user or that they already understand. Further, we acknowledge that the resourcefulness and sufficiency aspects of explanations might be hard to measure in real-time. However, we suggest that explainable knowledge-enabled systems should be designed after an analysis of user requirements and utilize techniques to employ dynamic and static evaluation strategies to help realize these goals. More specifically, dynamic strategies could involve interactive mechanisms, such as the delivery of persuasive messages used by Maimone et al. [28], and static evaluation strategies could include user surveys conducted to evaluate the effectiveness of the systems, such as the one by Glass et al. [29]. \u2022 Adapt to users\u2019 context: Besides being user-centric, explanations need to be tailored to the user\u2019s current scenario and context. Explainable AI systems not only need to leverage information about the user (as may have been captured in a user profile [30,31]), but they also need to identify the user\u2019s intent and adapt the explanation form to connect to the user\u2019s mental model and align with the user\u2019s intent. For example, an explanation may include a contrastive hypothesis that relates to the user\u2019s intent or statistical evidence to provide more support to enhance a user\u2019s belief. In a later chapter, \u201cDirections for Explainable Knowledge-enabled Systems,\u201d in this book, we present different explanation types and their various focii that would allow AI systems to generate diverse explanations.\nOverall, explanations should serve beyond their original aim to teach [4], and provide trustworthy, transparent, unambiguous accounts of automated tasks to end-users."}, {"heading": "2.1.2. Explainable Knowledge-Enabled Systems", "text": "While many have attempted to define explanations (e.g., [22,32]), additional efforts have attempted to improve the generation of explanations (e.g., [7,29,33]) and tackle various aspects of explainability (e.g., [34,19]). To begin to address the need of building explainable, knowledge-enabled AI systems, we present a list of desirable properties from the synthesis of our literature review of past explanation work. Our review primarily spans knowledge representation in expert systems [22], provenance and reasoning efforts in the Semantic Web [18], user task-processing workflows in cognitive assistants [7,35], and efforts to reduce unintelligibility in the ML domain [9,24,21]. Additionally, we analyzed explanation requirements from current literature, answering an increased need for usercomprehensibility [36], accountability [32] and user-focus [19]. In our literature review in Section 3 we will highlight approaches that exhibit these properties.\n\u2022 Modularity: A modular design, such as, the one proposed by Swartout and Moore [4], is desirable, as it would allow systems to adapt models and functioning to users\u2019 requirements and scenarios. This property would also allow for the AI system to include explanation facilities that tap into various modules to expose information requested by and conducive to the user\u2019s needs. \u2022 Interpretability: Borrowing from Mittlestadt et al. [19] and Hasan and Gandon [37], we believe that the interpretability of explainable knowledge-enabled systems enables them to be transparent, lending to the ability to provide trace-based accounts of their working. Additionally, we utilize Gilpin et al\u2019s definition of interpretability as a \u201cscience of comprehending what a model did.\u201d However, if the models used in the system are not interpretable, we propose that they should consider including proxy methods to be interpretable, for example, utilizing linear proxy models proposed by Gilpin et al. [24] that serve as a simplified proxy of the full model. \u2022 Support provenance: Paraphrasing from the explanation requirements suggested by Hasan and Gandon [37], we agree that explainable knowledge-enabled systems should store the provenance of the information that their models rely on beyond just metadata. We believe that the inclusion of provenance aids AI systems in generating resourceful and sufficient explanations for users, providing them with resources for further exploration. \u2022 Adapt to user\u2019s needs: We propose that AI systems need to be adaptive and interactive, adapting their functioning and explanation generation capabilities to suit the user\u2019s requests and contexts. To this end, and to provide tailored explanations, Ribera and Lapedriza [27] have identified user categories (domain experts, AI researchers, and lay users) and presented their contrasting demands from an AI system. Further, the ability to be adaptive would be enhanced by a modular design, as suggested earlier, and would aid the system in generating explanations in various forms to suit the user\u2019s understanding and their needs. \u2022 Include explanation facilities: Inspired by McGuinness et al.\u2019s cognitive assistants explanation frameworks [7,35], we propose that the design of the explanation facilities should be addressed early and in detail in the design phase, to ensure that\nthe AI system is capable of supporting the requirements of the explanation facilities within its design. Explanation facilities could constitute a wide-range of userfacing interfaces, such as, dialogue systems, visualizations, and feedback systems that the user interacts with and provides feedback to the AI system about the explanations generated or a need for further clarifications. Hence, since explanation facilities would require additional information, such as, provenance, and would need the system to incorporate feedback and adapt to context, we recommend that their design be coupled with the AI system design.\n\u2022 Include/Access a knowledge store: We recommend that explainable knowledgeenabled systems store the domain knowledge they rely on, the user\u2019s mental model they appeal to, and the explanation components they are generating. Additionally, we relax the inclusion of knowledge in that an AI system might provide access to a knowledge store - as the system may host it, or it may use some other system\u2019s hosting and contribute to and access that store. By knowledge store, we refer to data storage mechanisms (KGs or semantic representations are preferred) that can store knowledge of various forms spanning categories such as background knowledge, domain knowledge, etc.\n\u2022 Support compliance and obligation checks: In addition to hosting/accessing knowledge stores, we recommend that explainable knowledge-enabled systems store an encoding set of expert knowledge in their field of application. These encodings should be sufficient to determine if the system complies with the standards and practices in that field. Additionally, we also recommend that explainable knowledge-enabled systems attempt to adhere to standards for the proposed explainable AI models, such as [24,21]. Furthermore, we suggest that compliance and obligation checks be evaluated on the system post-construction."}, {"heading": "2.2. Definitions", "text": "Having identified desirable properties for explanations and explainable knowledgeenabled systems, will now provide a set of definitions leveraging our review of the explanation literature and our analysis of the current AI landscape. Our goal is to reflect the needs of explainable AI in current times and provide a summary of the desirable properties to achieve better explainability."}, {"heading": "2.2.1. Explanation", "text": "We define an explanation in the computational world as, \u201can account of the system, its workings, the implicit and explicit knowledge used in its reasoning processes and the specific decision, that is sensitive to the end-user\u2019s understanding, context, and current needs.\u201d"}, {"heading": "2.2.2. Explainable Knowledge-Enabled Systems", "text": "We define \u201cexplainable knowledge-enabled systems\u201d to be, \u201cAI systems that include a representation of the domain knowledge in the field of application, have mechanisms to incorporate the users\u2019 context, are interpretable, and host explanation facilities that generate user-comprehensible, context-aware, and provenance-enabled explanations of the mechanistic functioning of the AI system and the knowledge used.\u201d"}, {"heading": "3. Approaches", "text": "We present past approaches that have addressed various aspects of explainability related to trust, transparency, provenance and interpretability. To the extent possible, we group publications by technical domain: knowledge-based systems, Semantic Web applications, cognitive assistants, and ML systems, in an attempt to show the progression of methods within those domains. In Section 3.1, we consider work from the 1970s-1990s that sought to utilize the trace explainability strengths of rule-based systems to explain the process used to arrive at decisions. In Section 3.2, we review provenance and explanation modeling efforts and posit them as contributors to the development of trustworthy and explainable semantic applications. In Section 3.3, we focus on efforts to explain task-based workflows in personal assistants and intelligent tutoring settings. We end with a review of papers that improve the interpretability and trust aspects of ML methods in Section 3.4. While each of these vast domains has large volumes of published literature, we restrict ourselves to seminal work on explainability in the domain or publications that have introduced novel techniques to tackle different aspects of explainability. As a conclusion of each domain subsection, we provide a brief summary of the methods utilized to address explainability and describe any lessons applicable for the development of future explainable AI methods.\nTable 1 contains an evaluation of the foundational AI systems, reviewed against the criteria we defined for explainable knowledge-enabled systems (Section 2.1). The\nchronological order allows us to view trends in explainability over the years and also helps expose shifts in the areas of focus and strengths of the class of approaches. We observe that explanations were well-explored as a topic of interest in the AI community from the early 1990s - mid-2000s. We note that, even within the expert systems era, the AI architecture evolved from simply generating trace-based accounts of decisions to including modular explanation facilities ([38,4,6]) that sometimes could produce provenance-enabled ([6]), adaptive and user-customizable ([43]) explanations. Additionally, observe that, among other classes of approaches in our review, explanations have been best established in cognitive assistants, which also have the most direct impact on human decision-making capabilities. However, we notice that, with more recent systems in the Semantic Web and ML domains, there has been a shift in explainability from building explanation facilities to minimally ensuring that AI models are interpretable ([40] and support provenance [44] for further tracing. Further, most AI systems in our review ([5,6,38,4,35,7,39,44]) satisfy the \u2018Compliance Checks\u2019 criteria by leveraging logical rule-based or other deductive reasoners to check or enforce compliance. Also, systems such as the Disciple-LTA and Common Ground Learning and Explanation (COGLE) deployed in critical settings of military and aviation, respectively, have features to allow both expert and lay users to provide feedback about the system\u2019s explanations and outputs. Hence, indicating that system supported features are partially driven by the domain of application. Finally, while our evaluation was conducted on a carefully selected set of approaches, our findings on explainability trends are in-line with a larger, systematic review conducted by Nunes and Jannach [45], who noted that explainability was best explored in the expert systems and cognitive assistants domains."}, {"heading": "3.1. Knowledge-based systems", "text": "The 80s decade saw the rise of knowledge-based and expert systems, that were designed to assist humans where human resources were limited [12]. Expert systems and knowledge-based systems both contained an encoding of knowledge. More specifically, in the case of expert systems, the knowledge encoded was that of expert\u2019s knowledge, typically in the form of rules. In our review, we will not make distinctions between these two classes of systems and will focus on identifying the explainability components of these systems. From an implementation perspective, both of these systems required the engineering and encoding of multiple rules to support inference. This reliance on rules made these systems inherently explainable, as one could trace back the rules to identify the factors that lead to a conclusion. Subsequently, researchers have introduced different types of explanations [6,5], and approaches to improve explanation generation [38], and to introduce more granular content into explanations generated by these systems [4]."}, {"heading": "3.1.1. Early Expert Systems: MYCIN and NEOMYCIN", "text": "The MYCIN [5,13] paper was one of the first to introduce computer-based explanations, and, is regarded as a foundational and seminal work. The goal of the MYCIN system was to identify highly probable carriers of infectious diseases, and suggest treatments for the diseases. The system provided explanations by exposing the inference trace that lead to a decision. The system was able to trace back and expose the reasoning, that served as justifications of decisions. In particular, MYCIN provided Why and How explanations [12]. The Why explanations included facts and task-based information to address a user\u2019s\nqueries. The How explanations explained the manner and trace in which the system generated the conclusion.\nTo enhance the Why and How explanations, a descendant of MYCIN - NEOMYCIN [6] produced strategic explanations comprised of meta-knowledge and the problemsolving strategies to adapt the MYCIN system to a teaching setting. NEOMYCIN built on MYCIN\u2019s inability to explain beyond the expert knowledge known to the system and added a component that leveraged explicit encodings of problem-solving strategies used to generate the medical knowledge for use in its explanations. To this end, the NEOMYCIN system used a meta-strategy to decide what portion of the rules to invoke from data sources, including an etiological taxonomy, disease knowledge, and causal associations. The metastrategy contained rules that a human would use to undertake tasks such as building hypothesis, pursuing them, identifying problems, etc. In essence, the NEOMYCIN system attempted to mimic human decision-solving, where one would eliminate a hypothesis based on the search space, and not by merely navigating the knowledge (\u201cbridge concepts\u201d [6]) that the system already holds. Further, NEOMYCIN introduced the idea of separating knowledge to make the system more accessible, which was further adopted by Moore and Swartout in their Explainable Expert System [4] effort, discussed later in this section. While the strategic explanations generated by NEOMYCIN are desirable, they might be onerous for user consumption due to a surplus of details."}, {"heading": "3.1.2. Explainable Description Logics: CLASSIC", "text": "McGuinness and Borgida took an approach to explanation where each of the inferences that the underlying logical reasoning system could execute had a declarative explanation description and those individual explanation components could be used to build simple, complex, abstracted, or otherwise customized explanations [46]. Additionally, every expert rule that a knowledge-based system builder encoded in the system included a structured component that could be used to explain when that rule was used. These explanation \u201cbreadcrumbs\u201d could then be used to assemble explanations when a user\u2019s actions triggered the execution of a rule.\nThe authors implemented their approach in the CLASSIC knowledge representation system, a description-logic-based language that provided a framework \u201cto define structured concepts and make assertions about individuals in a knowledge base\u201d [43]. The complete set of foundational inference rules that could be explained for the underlying description logic reasoner was also available for reuse in other systems [47]. This style of encoding axioms for every inference that a system could execute was also leveraged in the axiomatic semantics for other predecessors to today\u2019s description logic-based recommended language for encoding ontologies on the web: OWL [16]. The axioms for RDF, RDFS, and DAML+OIL were described in W3C Note3 and then were used in a number of different reasoners to provide trace-based explanation capabilities."}, {"heading": "3.1.3. Explainable Expert System", "text": "Moore and Swartout coined the term \u2018Explainable Expert Systems\u2019 (EES) in their widely-cited work [4]. The EES framework that aimed to provide explanations and was tested in a Program Enhancement Advisor setting. The explanations generated by the\n3DAML+OIL axioms note link: https://www.w3.org/TR/daml+oil-axioms\nEES system borrowed from and had components of various knowledge sources including domain, problem-solving and system terminology. Further, the design of their EES system supported the generation of the various components of the explanations and were made of knowledge bases, a program writer, an explanation generator, an interpreter, and an execution trace. The EES system used a planning algorithm, wherein goals are reformulated if no viable match is found in the domain knowledge. The reformulation of goals was achieved by the representation of the domain knowledge into a concept hierarchy, via a language, such as, KL-ONE [48]. The EES framework was interactive in nature, and goals were reformulated based on user dialogue with the system. Additionally, users\u2019 queries were used as a cue to interleave domain and problem-solving knowledge traces into their explanations."}, {"heading": "3.1.4. Summary", "text": "The explainable knowledge-based systems that we discussed introduced several types of explanations, including Why, How, and Strategic explanations (described earlier in Section 1.1). However, their reliance on encoding a large rule base makes them difficult to scale and extremely human-intensive to maintain. Today, we see the semi-automatic generation of rules and knowledge-base population via natural language processing and ontology-enabled extraction techniques. Many learnings from knowledge-based systems have been reused and expanded in the Semantic Web, as will be illustrated in Section 3.2."}, {"heading": "3.2. Semantic Web", "text": "The creation of the World Wide Web (WWW) [49] made it possible to create content online and make existing content available online in digital formats. In their seminal paper, Berners-Lee, Hendler, and Lassila [50] state that the Semantic Web was intended to unify content being published online through tagging content with unique identifiers, or Uniform Resource Identifiers (URIs), representing the content utilizing well formed definitions from taxonomies and ontologies, and borrowing from the knowledge representation world to utilize structuring mechanisms for data. While these properties are desirable and necessary to enable data sharing and achieve a semantic understanding of digital content, they are not sufficient to make the content explainable to a broad range of users. However, the Semantic Web community has tackled the provenance aspect and trace-based aspects of explainability and developed several provisions to both include provenance in the semantic representation [18,17] and to supporting reasoning mechanisms, [51] to generate traces. As a direct consequence of the Semantic Web, the textual content is more accessible in knowledge graphs (KGs) via semantic representations [52]. Additionally, KG provisions have made it possible to provide justifications and provenance to suggestions. In this section, we will review some provenance encoding efforts and explainable semantic applications."}, {"heading": "3.2.1. Provenance modeling efforts", "text": "There have been two somewhat recent foundational provenance efforts that paved the way for provenance-aware applications, namely the World Wide Web Consortium\u2019s work on a recommended standard for provenance on the web (PROV) with its associated en-\ncoding as an ontology PROV-O [17] and nanopublications [18]. Nanopublications provide a structure to associate triple statements with their provenance. In general, provenance is essential as it encodes information that can be used to explore where information came from and this information can be used to build trust in applications when they use this information to expose the evidence behind their recommendations."}, {"heading": "3.2.2. Nanopublications", "text": "Nanopublications were conceived to help disambiguate and represent the context for scientific statements that were extracted from textual corpora and made available as triples. The authors identified that contextual information present in a document was imperative to understand a statement in relation to the full document. Hence, they designed nanopublications that provided a mechanism to associate metadata or annotations with statements. The schema of nanopublications has evolved over the years. In its current state,4 nanopublications are composed of three named graph components, Assertions, Publication Information, and Provenance. The Publication Information graph stores metadata information about the creation of the content, or how it came to be, such as, the date of creation, author, etc. The Provenance graph contains metadata, such as, citation information. The assertion graph contains one or more subject-predicate-object statements with domain content.\nKuhn et al. [53] have proposed an Atomic, Independent, Declarative, and Absolute (AIDA) framework to encode atomic and indisputable assertion statements. They describe a metananopublication world in which nanopublications can be created from other nanopublications via different channels, for example, from authors creating content from scientific results, and from data mining algorithms generating nanopublications from existing unstructured data sources. Essentially through the metananopublications concept, the authors highlight that provenance can be interleaved and chained, to reflect the real world where multiple entities depend on each other at various levels of granularities."}, {"heading": "3.2.3. The Provenance Ontology (PROV-O)", "text": "The PROV-O ontology [17] provides a formal mechanism to support comprehensive modeling of the provenance of digital objects. In their ontology, they support three primary forms of provenance contributors, agent-centered, object-centered, and processcentered forms. In PROV-O 5, provenance is modeled via three simple class types, i.e., \u2018entities\u20196 which are generated by activities, and \u2018entities\u2019 and \u2018activities\u2019 that are \u2018associated with\u2019 and \u2018attributed to\u2019 agents, respectively. In the W3C note, the editors showcase the adequacy of the PROV-O ontology in modeling a use case where a blogger is exploring the provenance chain of a newspaper article while finding out who compiled the chart included in the article. The use case also illustrates that provenance needs to be modeled comprehensively to ensure that users have a complete understanding of the information they are viewing.\nThere have been ontology alignment efforts on the PROV-O ontology to enhance usability and increase interoperability. These efforts include alignment of PROV-O with standard ontologies, such as, the TIME ontology, Semantic Sensor Network Ontology\n4Nanopublication Guidelines: http://nanopub.org/guidelines/working_draft/ 5PROV-O ontology W3C note: https://www.w3.org/TR/prov-o/ 6Classes and properties are referred to by their label, and are enclosed within single quotes\n(SSN), and the Basic Formal Ontology (BFO). The PROV-O ontology has also served as a foundational ontology for several other provenance ontologies (e.g., Provenance for Clinical and Healthcare Research (ProvCare [54]) and Guideline Provenance Ontology (G-Prov [55])) that support provenance modeling in specific use cases with different levels of granularity."}, {"heading": "3.2.4. Provenance and Related Semantic Knowledge Graphs", "text": "The Semantic Web community also allows for different alternatives of representing that information based on granularity and content needs such as named graphs, 7 reification, 8 etc., and there exist cross-domain open source KGs that host somewhat comparably rich provenance (e.g., Wikidata, 9 WebIsALOD [56]). Additionally, while we believe that provenance modeling is crucial to provide high-quality, trustworthy information to consumers, we acknowledge that it is not sufficient to capture user context or to personalize results. Recently, there has been an emergence of KGs that encode contextual and personal information [57,28], lending to the personalizing of semantic applications that are enabled by these KGs. Gyrard et al. [57] described the components of a personalized healthcare knowledge graph (PHKG) that are needed to monitor user health to help users combat chronic diseases, such as, asthma and obesity. In a similar effort, Maimone et al. developed Perkapp [28], a persuasive system that monitors people\u2019s lifestyles and persuades them to make healthier choices and stay on track. Their persuasive, knowledgebased system architecture contains a set of expert-generated rules and outputs persuasive context-aware messages to users based on their adherence to the rules."}, {"heading": "3.2.5. Reasoning Efforts", "text": "We now present a selective overview of the reasoning efforts. We briefly introduced RDFS reasoning efforts in Section 3.1. RDFS reasoning results in justifications or tracebased accounts of Why a conclusion was made by the system, based on which rule fired. However, these justifications can be overwhelming for human consumption. To address this, Horridge et al. [58] proposed laconic and precise justifications that do not 1. conceal detail, 2. expose axioms that are relevant to the justification, and 3. are atomic, in that multiple fine-grained cores can be highlighted. Besides the laconic justification effort, there have been other efforts to improve explainability of justifications and we discuss one such effort, the AIR (Accountability in RDF or AMORD10 [59] in RDF) language."}, {"heading": "3.2.6. Explanations for Automated Policy Reasoning", "text": "AIR language that had a broader focus on modeling explanations serving to explain inference traces from policy reasoning. AIR is a Semantic Web-based rule language focused on generating and tracking explanation for inferences and actions [33]. The Massachusetts Institute of Technology (MIT) Decentralized Information Group developed the AIR language, as an extension to N3Logic [60] to support accountable privacy protection in Web-based information systems conforming to Linked Data principles. Accountability and privacy protection are enabled through auditable trace-based explana-\n7Named Graphs: https://www.w3.org/2009/07/NamedGraph.html 8Reification: https://www.w3.org/TR/rdf-primer/ 9Wikidata: https://www.wikidata.org/wiki/Wikidata:Main_Page 10AMORD (A Miracle Of Rare Device) is an explanation system developed for MIT scheme in the 1970\u2019s\ntions. AIR supports Linked Rules, which can be combined and reused like Linked Data. Additionally, AIR explanations can be used for further reasoning.\nAIR provides two independent ontologies. One ontology allows the specification of AIR rules,11 and the other one allows describing justifications.12 The reasoning steps of the AIR reasoner are considered as events and modeled as subclasses of air:Event. air:Rule represents rules, and it is defined as a subclass of air:Operation. The ontology also provides properties to enable representing variable mappings in the performed operations. AIR provides a means to write explicit explanations using the assertion property associated with rules. This property is composed of two components, air:Statement, which is the set of triples being asserted, and air:Justification, which is the explicit justification that needs to be associated with the statement. Example policy reasoning with explanations using AIR: Parts of the Massachusetts Disability Discrimination Law were translated into a computer interpretable policy using AIR. A user\u2019s phone records requesting some service and subsequently getting denied based on his disability recorded in the phone logs were captured in RDF. Once the AIR reasoner is invoked with the policy file, and the phone log in RDF, a user can visualize the annotated transaction log that contains the reasoning output. Figure 1 contains a partial proof tree with natural language assertions."}, {"heading": "3.2.7. Semantic applications", "text": "Aside from the various representation mechanisms described earlier that support provenance encoding and personalized content, there have been many semantic applications\n11AIR rules ontology: http://dig.csail.mit.edu/TAMI/2007/amord/air 12AIR justifications ontology: http://dig.csail.mit.edu/2009/AIR/airjustification.n3 13Image available at: http://dig.csail.mit.edu/TAMI/2008/JustificationUI/howto.html\n(e. g., [8,41]) enabled by these representations that are explainable. We briefly describe two of these efforts.\nIn our automatic breast cancer characterization effort [8], we developed a visual interface to assist physicians in their diagnosis process by providing justifications of the treatment rules that resulted in a stage change of a patient between changing guideline editions. We considered the 7th and 8th edition of the American Joint Committee on Cancer (AJCC) cancer staging guidelines [61]. Our system reasoned using a knowledge base of encoded cancer staging rules, and inferred the stage of the patient based on their metastasis parameters and biomarkers. Our system could automatically determine the staging, explain how the stage was derived, and explain any restaging that happened. In another effort, McCusker et al. developed a framework [41] that encoded semantic connections between drugs, proteins, and diseases and allowed users to look for potential repurposing of drugs. A novel aspect of this system was that the interface allowed the user to explore why a drug may be used to target a particular disease, thus having a potential causal explanation as opposed to many other drug repurposing efforts that focused only on correlations. The system also included weights on all of the links in the graph so that users could get a sense of how strongly the evidence supports a relationship.\nThe semantic applications we reviewed primarily utilize scientific evidence to present factual content, discover new content, and automate human-intensive tasks. In Section 3.3, we review explanation modeling frameworks, such as, the Inference Web [35], which also have semantic representations but are used in more typical cognitive assistant settings."}, {"heading": "3.2.8. Summary", "text": "The Semantic Web efforts we described address various components of explainability. Although, even these interpretable systems, powered by KGs and ontologies, do not entirely address all aspects of explainability that we detail in Section 2. However, we believe that semantic representations for explainability can evolve from the existing semantic representations for provenance, accountability and context. Hence, we believe that the strengths of the Semantic Web, coupled with ML methods, will be a significant contributor to hybrid explainable AI systems."}, {"heading": "3.3. Cognitive Assistants", "text": "Cognitive Assistants are systems that are used to \u201caugment human intelligence\u201d [62] and aid humans in decision-making and problem-solving. These assistants have grown from their former role of professional assistants, educating users in a particular domain, to being widely accessible as personal assistants, aiding users in their everyday tasks. These assistants function in a tight coupling with the user and, hence, their design, knowledge bases, and interactions are driven by users\u2019 cognitive capabilities and needs. Further, these assistants play various roles from fostering positive behavior change, to training people with the necessary problem-solving skills in a domain, to providing tailored information based on an understanding of user context [63]. As the proliferation of general purpose, conversational cognitive assistants grows, it will become increasingly important that they include a representation of the user\u2019s goals, and \u201ctheory-of-mind\u201d elements that support effective communication and collaboration. [64]"}, {"heading": "3.3.1. DARPA PAL program", "text": "An ambitious and multi-university program, the Defense Advanced Research Projects Agency (DARPA) program, Personal Assistant that Learns (PAL),14 gave rise to the Cognitive Assistant that Learns and Organizes (CALO) system. CALO was a large effort including over 20 collaboration organizations aimed at building a cognitive agent that can assist in a wide range of day-to-day office-related tasks, including sending out emails, memos, maintaining a to-do list [65], etc. Henceforth, several projects leveraged the CALO work, the most famous is Apple\u2019s personal assistant Siri. In our review, we will cover some of the seminal explainable cognitive assistants [35,7] and user studies [29] that resulted from or were refined within the CALO project, that are explainable in their own right.\nInference Web was one of the early modular explanation frameworks, and it built upon the strengths from the Semantic Web [10], Description Logics [66], and expert systems communities, to generate explanations for distributed, web-based systems that were interacting with users. The framework provided explanations that contained the provenance of the information (both implicit and explicit), and the proof for inference traces to novice users and agents alike. Additionally, the framework could abstract explanations to suit users\u2019 understanding and to avoid lengthy proofs that would overwhelm the users (similar to the breadcrumbs features provided by the CLASSIC system (Section 3.1.2)). Besides the ability to abstract explanations, the framework was also capable of providing explanations in different formats and even had a built-in explanation dialogue that would display questions and answers. Users could then interact with the answers and pose follow-up questions. The framework achieved its explanation capabilities via a modular architecture consisting of an IWBase, a data repository of the metainformation about the information used by the framework; an IWAbstractor, abstractor component that converted lengthy Proof Markup Language (PML) [67] proofs to PML explanations; an IWExplainer, an explanation dialogue component that would generate explanations for users; and an IWBrowser, a browser for displaying the explanations. While the Inference Web framework did not include a context-specific component, it provided some context modeling options and was capable of providing a wide range of customized explanation capabilities that included direct support for encoding trust and user models.\nMcGuinness et al. [7] expanded on their earlier Inference Web [35] framework, and developed an Integrated Cognitive Explanation Environment (ICEE) that generated explanations for task reasoning. ICEE served as an explanation component on the CALO system, in which multiple reasoning techniques, including task processing, numerous learning components, along with statistical and deductive methods, all worked together. Since CALO served as a cognitive assistant in the workplace, the tasks involved processing workplace automation activities, such as requesting quotes from different sources (e.g., GetQuotes was one of the sub-tasks [7]). Additionally, the reasoning techniques used in CALO used multiple knowledge sources to generate conclusions that needed to be explained. The ICEE explanation architecture (shown in Figure 2) consisted of several components critical to generating explanations: an explanation dispatcher that interpreted a user\u2019s explanation request and invoked different reasoning components based on the type of explanation request, a task manager explainer that further invoked task manager wrappers to gather task execution information, a task state database that maintained\n14PAL: https://www.darpa.mil/about-us/timeline/personalized-assistant-that-learns\nthe execution traces and states of the tasks, and a justification generator that created explanations from the task execution processing information.\nThe authors conducted a user study aimed at understanding the types of questions that users wanted answered. These explanation request types included questions about the motivation of a task, status, execution history, forward-looking execution plans, task ordering, or explicit questions about time [7]. The classifications of these explanation requests into different request types helped invoke appropriate explanation strategies. Additionally, the system hosted introspective predicates were used to identify the types of information to be included in explanations based on the request\u2019s intent. Broadly, the introspective predicates were grouped into basic procedure information, metadata about task definitions; execution information, details about the task execution; and, projection information, information about future task processing. The ICEE framework provides an example of many of the components needed in explainable hybrid AI systems and demonstrated how they can be used to provide user-customized explanations.\nAnother noteworthy effort from CALO was Glass, et al. [29]\u2019s user study that assessed the trust and understandability aspects of adaptive systems. They used the CALO system as an adaptive system use case in their study. Their findings grouped users\u2019 concerns into eight themes: 1). High-level usability of complex prototypes, 2). being ignored, 3). context-sensitive questions, 4). granularity of feedback, 5). transparency, 6). provenance, 7). managing expectations, and 8). autonomy and verification. While there were some system-related concerns that could be addressed via system improvements (high-level usability, verification), there were also other concerns, such as, provenance, the granularity of feedback, the transparency targeting the users\u2019 perception of trust in the agent. They found that the trust level of most users in the system increased significantly with the inclusion of provenance and context-sensitive aspects. Therefore, this study concluded that users who work with cognitive agents would like an interactive dialogue and personalized experience and would prefer provenance information to understand the working of these complex systems, to some degree. The themes identified in this paper remain desired features for our complex, hybrid systems of today that use both statistical ML and reasoning techniques."}, {"heading": "3.3.2. Intelligent tutors", "text": "Intelligent tutoring is a sub-domain of cognitive assistants, where adaptive task-oriented systems are utilized for training humans in a particular domain. Hence, intelligent tutors need to appeal to the human cognition and understand and evolve their learning capabilities and grasp of the domain. In a seminal work, VanLeHan [68] noted that there are two loops to human tutoring, an inner and outer loop. He noted that the inner loop worked in tandem with the human, helping them at each step, assessing their competence, and updating the student model, while the outer loop identified a new task to execute based on the student\u2019s assessment. Enhancements have been proposed to VanLeHan\u2019s inner and outer loop proposition, one of which is a behavior graph [69] that kept track of the possible problem-solving strategies that students can adopt. The edges in a behavior graph represented the different ways in which students could solve problems, and the nodes represented the acceptable states. In general, intelligent tutors host an inherent, domainspecific knowledge component that is used to undertake tasks.\nA use case on explainable, intelligent tutors was explored in a military setting by a Disciple-LTA [39] system. They used an iterative problem-solving approach in intelligence tasks to assist analysts. These tasks were broken down into executable steps to which evidence could be associated to find solutions (also termed as \u201ctask-reduction\u201d). The solutions were then combined at the task level, or \u201csolution-composition,\u201d to produce conclusions. A sample conclusion from this system was \u201cThere is strong evidence that Location-A is a training base for terrorist operations.\u201d [39] The Disciple-LTA architecture consisted of different reasoning agents: learners, tutors, and problem-solvers, all of which read from and wrote into the knowledge base of an ontology and its rules."}, {"heading": "3.3.3. Summary", "text": "The cognitive assistant literature is vast and continues to grow with the emergence of personal assistants, such as, Apple\u2019s Siri, Amazon\u2019s Alexa, etc. In our review, we have covered explanation facilities in DARPA\u2019s CALO project [35,7,29], and have also briefly discussed Intelligent Tutors [68,69,39]. While the focus of explanations in the CALO cognitive assistant was on explaining task-based workflows, the underlying system contained a set of hybrid deductive reasoners coupled with numerous learning components, and thus is representative of today\u2019s hybrid learning systems. User requirements were utilized to design explanation strategies and determine the execution of the next task, dictated by user feedback. Cognitive assistants have begun to focus on the end-user, and are supporting facilities to account for user perspective, to some extent, unlike expert systems (Section 3.1) that focused primarily on generating explanations of inference traces."}, {"heading": "3.4. Explainability in Machine Learning (ML)", "text": "ML algorithms have been rapidly advancing, proliferating in various domains, even highprecision domains, such as, healthcare and finance. However, these algorithms, are typically more opaque than previous expert systems (Section 3.1), semantic applications (Section 3.2), and cognitive assistants (Section 3.3). Hence, the ML domain faces large challenges in addressing the trustworthiness, transparency, and intelligibility15 of their\n15Our definition of intelligibility is very similar to the description proposed by Lipton [70] and Lou et al. [14], in that intelligible models are interpretable wherein the contribution of model features to a decision can be deciphered.\nmodels. Additionally, even within the ML domain, there has been a shift from the dependence on simpler linear algorithms that were less complex, to non-linear, \u201cblack-box\u201d models, such as, deep learning [11]. While ML algorithms are often achieving high accuracy, they are typically unable to explain why they arrived at a classification or score (view the tradeoff in Figure 3). However, there have been techniques to circumvent these issues, such as, providing confidence scores for the results of models to induce trust (posthoc interpretations [21,19]), attaching semantic information to results [71], presenting contrastive or counterfactual explanations to provide intuition for the model\u2019s functioning [72,73], etc. Formally, the interpretability techniques for ML models can be grouped into two categories [19], one class aimed at post-hoc interpretations that contain explanations about the results to provide perspective on the model\u2019s functioning, and the other aimed at improving transparency to offer an intuition for the model\u2019s functioning. We want to clarify that although ML models might not be considered traditional knowledgeenabled system candidates, we have included them in our review due to the emergence of hybrid systems composed of ML models and semantic methods. We believe that a review of explainability approaches in the ML domain will be fruitful for introducing explanation components into these hybrid systems."}, {"heading": "3.4.1. DARPA XAI Program", "text": "DARPA\u2019s Explainable AI (XAI) program16 focuses on building explainable models that achieve high accuracy and on methods to enable human users to trust and understand these models. We will discuss selected XAI efforts mentioned in the DARPA XAI reports [9,42] that have a knowledge explainability component to them.\n16DARPA XAI program website: https://www.darpa.mil/program/ explainable-artificial-intelligence\nBau et. al. [71], have developed a network dissection technique to align the intermediate layer results of convolutional neural networks (CNN) with semantic concepts. They make two contributions, a network dissection technique to identify what the network is learning at each step by comparing it to semantic concepts, and the construction of disentangled representation to align encodings between the network\u2019s output and a semantic concept. The disentangled representations were designed to provide a notion of the \u201chuman perception of what it means for a concept to be mixed up\u201d [71]. Further, the authors also assembled a new dataset, the Broadly and Densely Labeled Dataset (Broden) [74] of objects, that contained low-level compositions of objects used as semantic concepts. This work addresses the deep explanation component of Figure 3, wherein feature modifications are being made to make deep learning algorithms interpretable. Similarly, as part of the same program, a team of Charles River Analytica (CRA) researchers developed a technique to learn the causal nature of CNN activations [75]. In this work, Harradon et al. [75] construct a causal graph in-line with Judea Pearl\u2019s do-calculus [76] method. They ground the network activations in a P(O,P,C) graph, where C represents concepts of network representations that humans can identify, P is the input, and O is the output. However, unlike the network dissection paper [71], the causal graph is learned via an unsupervised autoencoder method. Hence, it might be challenging to trust the causal graphs that are learned.\nSince the XAI program by DARPA is an ongoing initiative, some of the work mentioned in the slideware17 remains unpublished. However, we briefly summarize some of these unpublished methods that we believe are relevant to our explainability review. In the Common Ground Learning and Explanation (COGLE) project,18 being led by PARC, a system is being built to explain to humans the workings of an autonomous Unmanned Aircraft System (UAS) testbed. The COGLE system explains the workings of the UAS reinforcement learning decision-making algorithm to users, conveys an understanding of the system\u2019s future behavior, and uses a common ground vocabulary to present these explanations. The common ground vocabulary is generated by including both human understandable and machine-understandable terminology, hence, hoping to ensure a dialogue between ML algorithms and humans. The common ground idea corroborates a requirement put forth by Doshi-Velez et al., [23] that \u201cto build AI systems that can provide explanation in terms of human-interpretable terms, we must both list those terms and allow the AI system access to examples to learn them.\u201d In another effort, researchers at Rutgers have proposed a technique to choose optimal examples to explain a model\u2019s decision via Bayesian Teaching [77]. The explanation by the Bayesian Teaching method is an explanation by examples technique, wherein model-agnostic probabilistic methods are used to identify the most probable data points that lead to a conclusion. The hypothesis that Yang et al. [77] present is that the data is most representative of the algorithm\u2019s conclusions, and humans tend to understand more intuitively through examples.\nIn summary, the DARPA XAI program (of which the report is a by-product [9]) is largely focused on improving explainability of deep learning models through local interpretation methods, or \u201cknowing the reasons for specific decisions\u201d [32] and posthoc interpretations. These focus points, to some extent, address the trustworthiness and intelligibility aspects of the explainability of ML models.\n17DARPA explainable AI slideware: https://www.darpa.mil/attachments/ explainableAIProgramUpdate.pdf\n18COGLE: https://www.parc.com/blog/explainable-ai-an-overview-of-parcs-cogle-project-with-darpa/"}, {"heading": "3.4.2. Taxonomies in explainable ML", "text": "Besides the DARPA XAI program, there have been other recent efforts in the ML domain to support the explainability of ML models. A team of researchers from IBM Research have built the AI Fairness 360 [78] and AI Explainability 360 [21] toolkits to identify bias in datasets and ML algorithms, and to describe the explainability of ML models, respectively. In their AI fairness 360 toolkit, Bellamy et al. [78] define metrics to identify bias in three stages of the dataset, the algorithm, and the predictions of the algorithm, in their goal to improve fairness in the entire ML workflow. While, in Section 1, we noted that we do not account for fairness in explainability, we acknowledge that the exposition of the fairness of the algorithm and data could increase trust in the model. Furthermore, in the AI explainability 360 effort, Arya et al. [21] designed a taxonomy resource to provide a structure of the explanation space to benefit algorithm designers who are looking to include necessary components in explanations. More specifically, their taxonomy (Figure 4) helps in identifying methods to introduce local (explanations of portions of the model that lead to a conclusion), global (an explanation of the entire model), and post-hoc interpretations (explaining the results) of models via careful inclusion of features and mechanisms during the design of ML models. However, their taxonomy focused more on model interpretability and features of the model, rather than on intended use of the model by users.\nResearchers at MIT conducted a literature review of published explainable AI papers and cataloged the explanation methods used by ML algorithms into a taxonomy [24]. Their taxonomy grouped papers into three categories, methods that emulate the processing of the data, explanations of representations (such as, the network dissection technique [71]), and explanation-producing networks. Their hope was for future methods to use the taxonomy as a reference to build explainable models. Additionally, they note that certain ML methods, such as, decision trees are more interpretable than black-box\nmodels and hence are being utilized as proxies [79] to explain the conclusions of these black-box models. Similarly, Gilpin et al. [24] proposed the Local Interpretable Modelagnostic Explanations (LIME) framework [40], that can be utilized to generate linear models on perturbations of the black-box model input to get a sense of the functioning of the black-box models."}, {"heading": "3.4.3. Summary", "text": "From a review of the ML domain, we can infer that the explainability techniques being developed are mainly tackling challenges of model interpretability and generating post-hoc interpretations of the model\u2019s conclusions or input data. While these two broad categories might seem insufficient, the breadth of innovative approaches [71,77,21,72] being developed are promising and can help in building interpretable, and hybrid models, aided by explainable models (e.g., KGs, causal methods). In summary, what makes the models that we describe in this section candidates for explainable knowledge-enabled systems is that they utilize knowledge to provide an intuition for the functioning of unintelligible models [71], or to build a vocabulary (COGLE: 19) to explain conclusions/inputs/workings of the algorithms. Additionally, prior knowledge of the requirements of explanations are being encoded as taxonomies [21,24] to serve as checks for future explainable models, and knowledge of existing linear models [79,40] are being leveraged to enhance the explanation capabilities of ML models. Orthogonally, the interpretability research in the ML domain is helping researchers understand that humans prefer richer, social, contrasting, and selective explanations [19]."}, {"heading": "4. Conclusion", "text": "We presented foundational approaches to explainable, knowledge-enabled systems, and identified themes for explainability within these approaches. We presented our definition of \u201cexplainable knowledge-enabled systems\u201d to cover a broad range of past and present AI systems including expert systems, Semantic Web, cognitive assistants, and ML domains. Additionally, we believe that, with the increasing focus on explainable AI, we are at the cusp of a new era of AI where explainability plays a pivotal role in the adoption of AI systems. We provided synthesized, refined definitions of knowledge-enabled systems from a user perspective and included properties that are desirable for when a system needs to generate provenance-aware, personalized, and context-aware explanations.\nWe reminded our readers that different AI domains and varying methodologies are differently suited for various aspects of explanations. The next-generation hybrid AI systems would benefit from these identified strengths, utilizing a potentially, carefully chosen combination of these techniques to provide more complete, satisfying explanations. For instance, we identified that trace-based explanation facilities are well-explored in expert systems, provenance encoding in the Semantic Web domain is capable of representing different granularities of evidence, the modular, task-based explanation facilities of cognitive asssistants can generate atomic explanation components, and that interpretability efforts in the ML domain are giving rise to taxonomical checks for explainable AI models that can be adapted to other AI fields. However, we noted that these AI systems\n19COGLE: https://www.parc.com/blog/explainable-ai-an-overview-of-parcs-cogle-project-with-darpa/\ndo not fully account for aspects such as user context and causality and are only capable of generating explanations belonging to a restricted set of explanation types. To address these issues, we present directions for research and describe different explanation types in a later chapter, \u201cDirections for Explainable Knowledge-enabled Systems,\u201d that might play a key role in furthering explainable AI.\nIn conclusion 20, we believe that with the increased adoption of AI systems, there is an increased need for systems to be interpretable, adaptive, interactive, and, most importantly, able to generate explanations that not only provide an overview of the AI system, but serve as a means to educate users and help in their future explorations. To address these lofty goals of explainability, we believe that we need to learn from strengths of past foundational approaches and adapt/expand on them in the user-centric needs of the current AI landscape to build hybrid AI systems that are interpretable, knowledge-enabled, adaptive, and context and provenance-aware."}, {"heading": "5. Acknowledgments", "text": "This work is partially supported by IBM Research AI through the AI Horizons Network. We thank our colleagues from IBM Research, Amar Das, Morgan Foreman and ChingHua Chen, and from RPI, James P. McCusker, and Rebecca Cowan, who greatly assisted the research and document preparation."}], "title": "Foundations of Explainable Knowledge-Enabled Systems", "year": 2020}