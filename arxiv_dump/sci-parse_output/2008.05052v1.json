{"abstractText": "Shapley value is a concept from game theory. Recently, it has been used for explaining complex models produced by machine learning techniques. Although the mathematical definition of Shapley value is straight-forward, the implication of using it as a model interpretation tool is yet to be described. In the current paper, we analyzed Shapley value in the Bayesian network framework. We established the relationship between Shapley value and conditional independence, a key concept in both predictive and causal modeling. Our results indicate that, eliminating a variable with high Shapley value from a model do not necessarily impair predictive performance, whereas eliminating a variable with low Shapley value from a model could impair performance. Therefore, using Shapley value for feature selection do not result in the most parsimonious and predictively optimal model in the general case. More importantly, Shapley value of a variable do not reflect their causal relationship with the target of interest.", "authors": [{"affiliations": [], "name": "Sisi Ma"}, {"affiliations": [], "name": "Roshan Tourani"}], "id": "SP:403c85c5b40d5842f8136f9e0b7e8e02d7db0934", "references": [{"authors": ["Constantin F Aliferis", "Ioannis Tsamardinos", "Alexander Statnikov"], "title": "Hiton: a novel markov blanket algorithm for optimal variable selection", "venue": "In AMIA Annual Symposium Proceedings,", "year": 2003}, {"authors": ["Nitzan Shalom Artzi", "Smadar Shilo", "Eran Hadar", "Hagai Rossman", "Shiri Barbash-Hazan", "Avi Ben-Haroush", "Ran D Balicer", "Becca Feldman", "Arnon Wiznitzer", "Eran Segal"], "title": "Prediction of gestational diabetes based on nationwide electronic health records", "venue": "Nature Medicine,", "year": 2020}, {"authors": ["Riccardo Bellazzi", "Blaz Zupan"], "title": "Predictive data mining in clinical medicine: current issues and guidelines", "venue": "International journal of medical informatics,", "year": 2008}, {"authors": ["Shay Cohen", "Gideon Dror", "Eytan Ruppin"], "title": "Feature selection via coalitional game theory", "venue": "Neural Computation,", "year": 2007}, {"authors": ["Piotr Dworzynski", "Martin Aasbrenn", "Klaus Rostgaard", "Mads Melbye", "Thomas Alexander Gerds", "Henrik Hjalgrim", "Tune H Pers"], "title": "Nationwide prediction of type 2 diabetes comorbidities", "venue": "Scientific reports,", "year": 2020}, {"authors": ["JB Heaton", "NG Polson", "Jan Hendrik Witte"], "title": "Deep learning for finance: deep portfolios", "venue": "Applied Stochastic Models in Business and Industry,", "year": 2017}, {"authors": ["Ron Kohavi", "George H John"], "title": "Wrappers for feature subset selection", "venue": "Artificial intelligence,", "year": 1997}, {"authors": ["Zachary C Lipton"], "title": "The mythos of model", "venue": "interpretability. Queue,", "year": 2018}, {"authors": ["Scott M Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in neural information processing systems,", "year": 2017}, {"authors": ["Lara Mikenina", "H-J Zimmermann"], "title": "Improved feature selection and classification by the 2-additive fuzzy measure", "venue": "Fuzzy sets and systems,", "year": 1999}, {"authors": ["Richard E Neapolitan"], "title": "Learning bayesian networks, volume", "year": 2004}, {"authors": ["Art B Owen", "Cl\u00e9mentine Prieur"], "title": "On shapley value for measuring importance of dependent inputs", "venue": "SIAM/ASA Journal on Uncertainty Quantification,", "year": 2017}, {"authors": ["Lloyd S Shapley"], "title": "A value for n-person games", "venue": "Contributions to the Theory of Games,", "year": 1953}, {"authors": ["Navin Sharma", "Pranshu Sharma", "David Irwin", "Prashant Shenoy"], "title": "Predicting solar generation from weather forecasts using machine learning", "venue": "IEEE international conference on smart grid communications (SmartGridComm),", "year": 2011}, {"authors": ["F Anthony"], "title": "Shorrocks. Decomposition procedures for distributional analysis: a unified framework based on the shapley value", "venue": "Technical report, mimeo, University of Essex,", "year": 1999}, {"authors": ["Peter Spirtes", "Clark N Glymour", "Richard Scheines", "David Heckerman"], "title": "Causation, prediction, and search", "venue": "MIT press,", "year": 2000}, {"authors": ["Alexander Statnikov", "Nikita I Lytkin", "Jan Lemeire", "Constantin F Aliferis"], "title": "Algorithms for discovery of multiple markov boundaries", "venue": "Journal of Machine Learning Research,", "year": 2013}, {"authors": ["Erik \u0160trumbelj", "Igor Kononenko"], "title": "Explaining prediction models and individual predictions with feature contributions", "venue": "Knowledge and information systems,", "year": 2014}, {"authors": ["Xin Sun", "Yanheng Liu", "Jin Li", "Jianqi Zhu", "Xuejie Liu", "Huiling Chen"], "title": "Using cooperative game theory to optimize the feature selection", "venue": "problem. Neurocomputing,", "year": 2012}, {"authors": ["Ioannis Tsamardinos", "Constantin F Aliferis"], "title": "Towards principled feature selection: relevancy, filters and wrappers", "venue": "In AISTATS,", "year": 2003}, {"authors": ["SHI Xingjian", "Zhourong Chen", "Hao Wang", "Dit-Yan Yeung", "Wai-Kin Wong", "Wangchun Woo"], "title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "venue": "In Advances in neural information processing systems,", "year": 2015}, {"authors": ["Mohammad Zaeri-Amirani", "Fatemeh Afghah", "Sajad Mousavi"], "title": "A feature selection method based on shapley value to false alarm reduction in icus a genetic-algorithm approach", "venue": "In 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),", "year": 2018}], "sections": [{"text": "ar X\niv :2\n00 8.\n05 05\n2v 1\n[ cs\n.L G\nKeywords: Causal Bayesian Networks, Predictive Models, Shapley Value, Model Explanation, Model Interpretability"}, {"heading": "1. Introduction", "text": "With the increased availability of data and the rapid advancement in predictive modeling, predictive models are playing important roles in many domains (Bellazzi and Zupan, 2008; Heaton et al., 2017; Xingjian et al., 2015; Sharma et al., 2011). Many predictive modeling methods, such as deep learning, produce highly complex models containing thousands of predictor variables. Although these models can be highly accurate, they are also very difficult to interpret. Various methods have been developed for model interpretation (Lipton, 2018). Shapley value based model interpretation methods have gained a lot of popularity recently, since they have solid theoretical foundation and has been demonstrated to produce interpretations that matches human intuitions (S\u030ctrumbelj and Kononenko, 2014; Lundberg and Lee, 2017). Also, since Shapley value of a variable takes into account the variables\u2019 individual as well as combined contribution for predicting a target of interest, it\nc\u00a92020 Sisi Ma and Roshan Tourani.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v/.html.\nis also widely adopted as a metric for feature importance (often considered a component of model interpretation as well.). Many recent studies reports Shapley value for variables in reported predictive models (AlAref et al., 2020; Dworzynski et al., 2020; Artzi et al., 2020). And, many variants of Shapley value feature importance based feature selection methods were proposed (Zaeri-Amirani et al., 2018; Cohen et al., 2007; Mikenina and Zimmermann, 1999; Sun et al., 2012).\nAlthough the definition of Shapley value is relatively straight-forward, the implication of using it as a model explanation tool is yet to be described. Possibly because the goal of model explanation and interpretation can be hard to define (Lipton, 2018). Here, we focus on the relationship between Shapley value and a variable\u2019s importance for the predictive task. We also examine whether the Shapley value is indicative of a variable\u2019s causal relationship with the target of interest (i.e. causal interpretation). We seek answers to the questions of the following nature: If a variable have a larger Shapley value compared to other variables, does it mean removing it would result in a more significant performance loss? What are the consequences of using Shapley value as a heuristic for feature selection? Does a variable\u2019s Shapley value indicate causal relationship with respect to the target?\nThe reminder of the paper is organized as the following. In section 2, we introduce relevant concepts for Shapley value and predictive modeling. In section 3, we consider predictive modeling as a coalitional game and examine the characteristics of Shapley value of each variable. We assume that the predictive models are built on data generated from a faithful causal Bayesian network. We attempt to establish correspondence between Shapley value of variables and the structural properties of the Bayesian network. We illustrate the implications of using Shapley value for attributing variable importance. We also demonstrate that, in general, there is no relationship between Shapley value and causality. The key results and practical implications for using Shapley value for model interpretation is summarized in section 4."}, {"heading": "2. Notations and Definitions", "text": "In this section, we provide a minimal set of definitions of essential concepts and analytical tools used in subsequent sections of the paper. Other relevant definitions and theorems regarding predictive modeling, variable importance, and (causal) Bayesian networks are included in Appendix A through D. Unless specifically mentioned, we use uppercase letters to denote a variable (e.g. X, Y ) and its corresponding vertex in the causal Bayesian network underlying the data generation processes of these variables. We use bold uppercase letters to denote a set of variables or vertices (e.g. Z). We use lowercase letters to denote instantiations or values of the corresponding uppercase variables (e.g. x is a instantiation of X).\nFor all the discussion below, we use the following common notations. V = {V1, V2, . . . , Vp} denotes all measured variables. T denotes the target of interest with respect to a predictive model. p(V, T ) denotes the joint distribution over of all variables.\nThe Shapley value is a solution concept in game theory (Shapley, 1953). The Shapley value defines the division of total payoff generated by all players to individual players according to their contribution.\nDefinition 1 Coalitional game. A coalitional game is a tuple \u3008N, v\u3009 where N = {1, 2, . . . , n} is a finite set of n players, and v : 2N \u2192 IR is a characteristic function such that v(\u2205) = 0.\nv is a function defined over subsets of N that describes the value of each subset of N. The goal is to come up with a solution to distribute the total amount of payoff from all players, i.e. v(N), to each player. We use \u03c6i(v) to denote the distributed payoff to player i, according to the value function v. The Shapley value is a distribution solution \u03c6 that have unique properties.\nDefinition 2 Shapley value. For a game \u3008N, v\u3009, Shapley value for a player i is defined as:\n\u03c6i(v) = \u2211\nS\u2286N\u2212{i}\n(|N| \u2212 |S| \u2212 1)!|S|!\n|N|! [v(S \u222a {i}) \u2212 v(S)] .\nBriefly, the Shapley value of a variable i is the weighted sum of the contribution of i in each subset ofN. The contribution of i with respect to a subset S is computed by v(S\u222a{i})\u2212v(S).\nThe Shapley value is considered a uniquely fair way for distributing the total payoff v(N) into (\u03c61(v), \u03c62(v), . . . , \u03c6n(v)) for the n players, since it satisfies the following characteristics (in fact, Shapley value is the only solution to distribute v(N) that satisfies all the characteristics below):\n\u2022 Efficiency: \u2211\ni\u2208N \u03c6i(v) = v(N).\n\u2022 Symmetry: If for two players i and j, \u2200S \u2286 N\u2212 {i, j}, v(S \u222a {i}) = v(S \u222a {j}), then \u03c6i(v) = \u03c6j(v).\n\u2022 Dummy: If \u2200S \u2286 N\u2212 {i}, v(S \u222a {i}) = v(S), then \u03c6i(v) = 0.\n\u2022 Additivity: For any pair of games \u3008N, v\u3009, \u3008N, w\u3009: \u03c6(v + w) = \u03c6(v) + \u03c6(w), where \u2200S, (v + w)(S) = v(S) + w(S).\nSince predictive modeling could be viewed as a coalitional game, Shapley value have been considered as a metric for variable importance and model explanation.\nDefinition 3 Predictive Modeling as a coalition game. Let f(V) be a predictive model for a target of interest T . f(\u00b7) could be view as a coalition game \u3008V,m\u3009, where each variable Vi \u2208 V is a player in the coalition game, and m : 2\n|N | \u2192 IR is a function that maps a subset of variables S to a real number representing the contribution of S for predicting T .\nThere are a variety of choices for m, depending on nature of the task. One popular task that Shapley value has been applied to is model explanation of individual observations (S\u030ctrumbelj and Kononenko, 2014; Lundberg and Lee, 2017). The goal of this task is to explain why a predictive model made a specific prediction for the outcome given the observed predictor values from an observation. In this case, m is commonly defined as the deviation of the predictive outcome from a null model (which would make prediction based solely on the distribution of the outcome). Another task that Shapley value has been applied to is the attribution of variable importance (Owen and Prieur, 2017; Shorrocks, 1999).\nThe goal of this task is to assign a quantitative value to each variable to indicate their importance for the predictive performance of the model over a collection of observations. In this case, m can be based on any metric for predictive performance, such as the area under the receiver operating curve (AUC), accuracy, sensitivity, specificity, or the mean absolute deviation of the predictive outcome from a null model. The last metric is generated by the most popular Shapley value model explanation package SHAP (Lundberg, 2018), and therefore is commonly reported in application focused literature. Similar to mentioned above, to define m, the predictive performance metric of choice need to be subtracted by the predictive performance of a null model, to ensure the efficiency and dummy characteristics are met.\nIn this study, we focus our discussion on using Shapley value for evaluating variable importance and its implications. Our results can be extended to model explanation of individual observations, which would be the topic of a subsequent paper."}, {"heading": "3. Shapley Value, Network Structure, Causation, and Prediction", "text": "In this Section, we discuss the relationship between Shapley value and variable importance in a predictive model. We also describe the relationship between a variable\u2019s Shapley value with its (causal) structural property with respect to the target of interest characterized by the (causal) Bayesian network over V \u222a {T}.\nWe restrict our discussion to faithful (causal) Bayesian networks. We denote \u3008V\u222aT,G, p\u3009 a (causal) Bayesian network faithful to distribution p over variable set V\u222a T . And f(V) is a predictive model for T as a coalition game \u3008V,m\u3009, where m is maximized for each S \u2286 V only when p(T |S) is estimated accurately."}, {"heading": "3.1 Shapley Value Summands and Conditional Independence", "text": "We first introduce theorems relating the summands of Shapley value of a variable to its structural properties in the (causal) Bayesian network. This is achieved by establishing the connection between conditional independence relations in a faithful Bayesian network, which directly corresponds to the structural properties of the network, with the summands of Shapley value of a variable.\nBriefly, a Shapley value summand for variable i given a subset S with respect to predicting T is m(S \u222a {i}) \u2212m(S); it captures the additional contribution of i to T given S. This bears conceptual similarity to the conditional independence between i and T given S (Definition 11). If we constrain function m to be maximized for each S \u2286 V only when p(T |S) is estimated accurately (similar to the treatment for the performance metric in Theorem 17), then, we have: (1) the conditional independence relationship p(T |i,S) = p(T |S) corresponds to m(S \u222a {i}) \u2212m(S) = 0; And, (2) the conditional dependence relationship p(T |i,S) 6= p(T |S) corresponds to m(S \u222a {i})\u2212m(S) > 0.\nTheorem 4 \u2200S \u2286 V \u2212 {X}, m(S \u222a X) \u2212 m(S) > 0 , iff X \u2208 PC(T ). (i.e. All Shapley value summands of parents and children of T are none-zero.)\nProof We prove \u2200S \u2286 V \u2212 {X}, m(S \u222a X) \u2212m(S) > 0 \u21d0 X \u2208 PC(T ). And note that each step of the proof is reversible, so we omit the proof for the sufficiency.\nSince X \u2208 PC(T ), there is an edge between X and T . From Theorem 22, we know that, X 6\u22a5 T |S, \u2200S \u2286 V \u2212 {X}. Therefore, according to Definition 11, \u2200S, p(T |X,S) 6= p(T |S) where p(S) > 0, i.e. X gives more information regarding T in addition to any S. And since we constrain m to be functions that are maximized for each S \u2286 V only when p(T |S) is estimated accurately, \u2200S \u2286 V \u2212 {X}, m(S \u222a {X}) \u2212m(S) > 0.\nCorollary 5 Not all Markov boundary members (strongly relevant variables) of T have all non-zero Shapley value summands.\nProof Let X be a parent of the children of T , and X 6\u2208 PC(T ). X is a Markov boundary member of T but it has at least one Shapley value summands being zero due to Theorem 4.\nTheorem 6 \u2200X that is not connected to T by any path, all summands of its Shapley value m(S \u222a {X})) \u2212m(S) are zero, \u2200S \u2286 V \u2212 {X}.\nProof Given Theorem 27, X is an irrelevant variable. Given Theorem 22, \u2200S \u2286 V\u2212{X}, X \u22a5 T |S, i.e. \u2200S, p(T |X,S) = p(T |S) where p(S) > 0. Therefore, \u2200S \u2282 V \u2212 {X},m(S \u222a \u2212{X}) \u2212m(S) = 0.\nTheorem 6 is related to the dummy characteristics of the Shapley value. The theorems in this section illustrate that given faithfulness and specific m functions, the Shapley value summands contain rich information about the structural properties of a variable with respect to a target in a (causal) Bayesian network. This connection between Shapley value summands and the structural property of the Bayesian network can be exploited in two ways. Firstly, examining the distribution of the Shapley summands values can give insight into the structural property of a variable. For example, even one summand of Shapley value being zero indicates that the variable is not directly connected to (or is not a direct cause or direct effect in a causal Bayesian network) of the target. Secondly, computing Shapley value exactly is often computationally intensive, since all 2|V| summands associated with 2|V| models need to be computed. Therefore, the Shapley value is often computed by sampling from all possible subsets S (S\u030ctrumbelj and Kononenko, 2014). Given the structure of a faithful Bayesian network, one might be able to sample S more efficiently and reduce the computation needed to achieve a good approximation of the Shapley value."}, {"heading": "3.2 Shapley Value of Variables in a Faithful Bayesian Network", "text": "When using the Shapley value for model explanation or feature importance, the summands of a variable are not examined individually. But rather, the resulted sum, i.e. Shapley value, for individual variables are compared. The theorem below describe the relationship between the Shapley value of a variable and the variable\u2019s structural property in a faithful (causal) Bayesian network. Specifically, when variable Vj d-separates Vi from T , Vj \u2019s Shapley value is larger than that of Vi.\nTheorem 7 If Vi \u22a5 T |Vj and Vj 6\u22a5 T |Vi in a faithful (causal) Bayesian network \u3008V \u222a T,G, p\u3009, then the Shapley value of Vj is larger than the Shapley value of Vi, i.e. phij > phii.\nProof First we separate the summation in Shapley value of Vi to two components, \u03c6i|j\u0302 , the sum over sets S where Vj /\u2208 S, and \u03c6i|j, the sum over sets S where Vj \u2208 S,\n\u03c6i = \u2211\nS\u2286V\u2212{Vi}\n(|N| \u2212 |S| \u2212 1)!|S|!\n|N|! [m(S \u222a {Vi})\u2212m(S)] = \u03c6i|j\u0302 + \u03c6i|j ,\n\u03c6i|j\u0302 = \u2211\nS\u2286V\u2212{Vi},Vj /\u2208S\n(|N| \u2212 |S| \u2212 1)!|S|!\n|N|! [m(S \u222a {Vi})\u2212m(S)]\n\u03c6i|j = \u2211\nS\u2286V\u2212{Vi},Vj\u2208S\n(|N| \u2212 |S| \u2212 1)!|S|!\n|N|! [m(S \u222a {Vi})\u2212m(S)]\nSimilarly, for variable Vj, we write \u03c6j = \u03c6j |\u0302i+\u03c6j|i where \u03c6j |\u0302i only sums over S where Vi /\u2208 S and \u03c6j|i only sums over S where Vi \u2208 S.\nNow we rewrite all the sums with S \u2286 V \u2212 {Vi, Vj},\n\u03c6i = \u2211\nS\u2286V\u2212{Vi,Vj}\n{\n(|N| \u2212 |S| \u2212 1)!|S|!\n|N|! [m(S \u222a {Vi})\u2212m(S)] +\n+ (|N| \u2212 |S| \u2212 2)!(|S| + 1)!\n|N|! [m(S \u222a {Vi, Vj})\u2212m(S \u222a {Vj})]\n}\n,\n\u03c6j = \u2211\nS\u2286V\u2212{Vi,Vj}\n{\n(|N| \u2212 |S| \u2212 1)!|S|!\n|N|! [m(S \u222a {Vj})\u2212m(S)] +\n+ (|N| \u2212 |S| \u2212 2)!(|S| + 1)!\n|N|! [m(S \u222a {Vi, Vj})\u2212m(S \u222a {Vi})]\n}\n.\nNow writing \u03c6i\u2212\u03c6j we can easily cancel out the m(S) and m(S\u222a{Vi, Vj}) terms, and write \u03c6i \u2212 \u03c6j as,\n\u03c6i \u2212 \u03c6j = \u2211\nS\u2286V\u2212{Vi,Vj}\n(\n(|N| \u2212 |S| \u2212 1)!|S|!\n|N|! +\n(|N| \u2212 |S| \u2212 2)!(|S| + 1)!\n|N|!\n)\n\u00d7 [m(S \u222a {Vi})\u2212m(S \u222a {Vj})] ,\nSince Vi \u22a5 T |Vj, i.e. p(T |S, Vi, Vj) = p(T |S, Vj), we have m(S\u222a{Vi, Vj})\u2212m(S\u222a{Vj}) = 0. And Vj 6\u22a5 T |Vi, i.e. p(T |S, Vi, Vj) 6= p(T |S, Vi), we have: m(S\u222a{Vi, Vj})\u2212m(S\u222a{Vi}) > 0. Therefore, m(S \u222a {Vi})\u2212m(S \u222a {Vj}) < 0, and thus, \u03c6j > \u03c6i.\nTheorem 7 state that, in a faithful (causal) Bayesian network, if Vj renders Vi conditional independent of T , i.e. Vj contains all information in Vi regarding T , the Shapley value of Vj is larger than that of Vi. It is worth noting that, the converse is true only under special conditions such as when the graph is a chain (V1 \u2192 V2 \u2192 ... \u2192 Vi \u2192 ... \u2192 Vj \u2192 ... \u2192 T ). In general, the magnitude of the Shapley value of two variables do not entail their\nconditional independence relationship. It is easy to consider an example where Vi and Vj are both parents of the target T , one of them could have a larger Shapley value, but none of them is conditionally independent of T given the other. The fact that one can not infer conditional independency from Shapley value in the general case is related to our discussion in Section 3.1. Specifically, the conditional independence relationship corresponds to the summands of Shapley value. Summing the summands together results in information loss with respect to conditional independency."}, {"heading": "3.3 Shapley Value of Variables: Implications for Prediction and Causation", "text": "In this section, we introduce two theorems that have important implications for using Shapley value for feature importance and model explanation. We focus on the Shapley values of the Markov boundary members of T , since these variables are of critical importance for both prediction and causation. Predictively, the Markov boundary of T is the most parsimonious set of variables that contains all the information regarding T and thus the optimal predictor set of T (Theorem 17). Causally, the faithful causal Bayesian networks, the Markov Boundary of T constitutes of the direct causes, direct effects, and direct cause of direct effects of T (Theorem 25).\nTheorem 8 Markov boundary members of T can have smaller Shapley value compared to non-Markov boundary members.\nProof We proof the theorem by an example: Let G be a faithful Bayesian network containing 5 vertices, where A \u2192 T , B \u2192 T , C \u2192 T , A \u2192 S, B \u2192 S, C \u2192 S. The data generating process is the following:\nA \u223c N (0, 22)\nB \u223c N (0, 22)\nC \u223c N (0, 22)\nT = A+B +C +N (0, 22)\nS = A+B + C +N (0, 22)\nWe use a linear regression model and the ordinary R2 as the m(\u00b7) function. We have:\nm(\u2205) = 0\nm({A}) = m({B}) = m({C}) = 1\n4\nm({S}) = 9\n16\nm({A,B}) = m({B,C}) = m({A,C}) = 1\n2\nm({A,S}) = m({B,S}) = m({C,S}) = 7\n12\nm({A,B, S}) = m({A,C, S}) = m({B,C, S}) = 5\n8\nm({A,B,C}) = m({A,B,C, S}) = 3\n4\nApplying Shapley value formular, we have:\n\u03c6A = \u03c6B = \u03c6C = 95/576 = 0.1649 . . .\n\u03c6S = 49/192 = 0.2552 . . .\nThe Markov Boundary members A, B, and C all have smaller Shapley value compared to non-Markov boundary member S.\nTheorem 9 The sum of the Shapley values of all Markov boundary members of T can be smaller than the Shapley value of a non-Markov boundary member.\nProof We prove the theorem by an example: Let G be a faithful Bayesian network containing 3 vertices, where C \u2192 A, C \u2192 B, A\u2192 T , B \u2192 T . The data generating process is the following:\nP (C = 1) = P (C = 2) = P (C = 3) = P (C = 4)\nP (A = 1|C = 1) = 0.05;P (B = 1|C = 1) = 0.05\nP (A = 1|C = 2) = 0.05;P (B = 1|C = 2) = 0.95\nP (A = 1|C = 3) = 0.95;P (B = 1|C = 3) = 0.05\nP (A = 1|C = 4) = 0.95;P (B = 1|C = 4) = 0.95\nP (T = 1|A = 0, B = 0) = 0.9\nP (T = 1|A = 0, B = 1) = 0.05\nP (T = 1|A = 1, B = 0) = 0.15\nP (T = 1|A = 1, B = 1) = 0.9\nWe compute the Shapley value for the best possible model (the model that results in irreducible error), such classifier will predict T = 1 if P (T = 1|S = s) > 0.5, and predict T = 0 otherwise. We use the m function: \u2211\ns P (S = s)max(P (T = 1|S = s), P (T = 0|S = s)).\nWe have:\nm(\u2205) = 0.5;m({A}) = m({B}) = 0.0525;m({C}) = 0.8235\nm({A,B}) = 0.9;m({A,C}) = m({B,C}) = 0.8597;\nm({A,B,C}) = 0.9;\nand the Shapley values are:\n\u03c6A = \u03c6B = 0.0903 . . . ;\u03c6C = 0.2194 . . .\nWe have (\u03c6A + \u03c6B) < \u03c6C , i.e. the sum of the Shapley values of all Markov boundary members of T can be smaller than that of a non-Markov boundary member.\nTheroem 8 and 9 have several implications for using Shapley values in predictive modeling. With respect to feature selection, a non-zero Shapley value does not indicate a variable is non-redundant. The Shapley value of a non-Markov boundary member C in the proof for Theorem 9, is redundant for predicting T with the presence of A and B, but its Shapley value is non-zero and larger than the Markov boundary members. Similarly, a smaller Shapley value does not indicate a variable is redundant. Several recent publications uses Shapley value for feature selection (Zaeri-Amirani et al., 2018; Cohen et al., 2007; Mikenina and Zimmermann, 1999; Sun et al., 2012). Typical methods either select a set of variables of a fixed size with the highest Shapley value, or employ recursive feature elimination using Shapley value as the method for ranking variables. Only selecting the top ranked several variables could potentially miss a Markov boundary member and results in suboptimal performance. Whereas, the recursive feature elimination by Shapley value could result in optimal feature set with redundant features, since any feature that have higher Shapley value than the Markov boundary member with the lowest Shapley value would be kept. With respect to model interpretation, the magnitude of Shapley value of variables do not necessarily correspond to causality. In the example from Theorem 8, S is neither a cause nor an effect of T , but it has larger Shapley value compared to all other causes of T . Moreover, the magnitude of Shapley value of variables also do not necessarily correspond to local causality: In the example from Theorem 9, C is an indirect cause T , but it has larger Shapley value compared to the sum of all direct causes of T 1."}, {"heading": "4. Discussion and Conclusion", "text": "In this study, we established the theoretical background for examining the Shapley value and relating it to faithful (causal) Bayesian network. This approach revealed several implications for using Shaplely value for variable importance and model explanation. We summarize the key points from this study:\n\u2022 The summands of Shapley value correspond to conditional independency under specific definition of m.\n\u2022 Using Shapley value for feature selection do not guarantee obtaining the minimal optimal feature set.\n\u2022 Magnitude of Shapley value of variables do not necessarily correspond to causality.\n\u2022 Variables that are in the local causal neighborhood of T do not necessarily have larger Shapley value compared to other variables.\nDespite the theoretical importance of Theorem 8 and Theorem 9, they are possibility results proven by examples. We intend to explore the following questions in the future:\n1. In general, the relationships learned by supervised learning predictive learning algorithms are not guar-\nanteed to reflect causal relationships. Therefore, there is no reason to believe the explanation of these models via Shapley values or other methods should reflect causality.\nWhat are the general conditions under which the disassociation between Shapley value and causality emerges? How prevalent is the disassociation between Shapley value and causality in datasets collected from different domains? How does the performance of Shapley value based feature selection methods compare to other types of feature selection methods?"}, {"heading": "Appendix A: Predictive Models and Variable Importance", "text": "First, we introduce a set of concepts that are related to predictive modeling. We focus our discussion on the importance of variables in the predictive model, since one of the utility of Shapley value is to access variable importance.\nDefinition 10 Optimal variable set (Tsamardinos and Aliferis, 2003). Given a data set D sampled from p(V, T ), a learning algorithm L, and a performance metric M, variable set Vopt is an optimal variable set of T if applying L on Vopt maximizes the performance metric M for predicting T .\nTo put it plainly, the optimal variable set Vopt for predicting T is a subset of variables that produces a model that maximizes the predictive performance T . This definition is intuitive but not very useful, since it neither provides the mathematical characteristics of Vopt nor a way to identify Vopt. Therefore, next we introduce the Markov Blanket theory and related concepts, since it describes the statistical characteristics of the optimal variable set and provides the theoretical foundation for deriving the optimal variable set.\nWe first define conditional independence, a key concept underlying variable importance.\nDefinition 11 Conditional Independence (Pearl, 2009). Let V be a set of variables and p(V) be the joint distribution over V, \u2200X,Y,Z \u2282 V, X and Y are conditionally independent given Z, if p(X|Y,Z) = p(X|Z), where p(Z) > 0.\nIn other words, variables inY do not provide additional information regardingX when Z is available. And therefore including variables inY in a model for predicting variables inX is redundant, when Z is part of the model. We use the symbol\u22a5 to represent independence and symbol | to represent conditioning. The statement \u201cX and Y are conditionally independent given Z\u201d is expressed as X \u22a5 Y|Z.\nTo define a variable\u2019s relevance with respect to predicting a target variable T , Kohavi and John (Kohavi et al., 1997) first introduced the strong, weak, and irrelevant variables.\nDefinition 12 Strong relevance. Let Si = V\u2212Vi, the set of all features except Vi. A feature Vi is strongly relevant to T iff there exists some vi, t, and si for which p(Vi = vi,Si = si) > 0, such that p(T = t|Vi = vi,Si = si) 6= p(T = t|Si = si).\nDefinition 13 Weak relevance. A feature Vi is weakly relevant to T iff it is not strongly relevant, and \u2203S\u2032i \u2282 Si and some vi, t, and s \u2032 i for which p(Vi = vi,S \u2032 i = s \u2032 i) > 0, such that p(T = t|Vi = vi,S \u2032 i = s \u2032 i) 6= p(T = t|Si = s \u2032 i).\nDefinition 14 Irrelevance. A feature is relevant if it is either weakly relevant or strongly relevant; otherwise, it is irrelevant.\nThe strongly relevant variables contain distinct information about T given all other variables, omitting them from the model will result in suboptimal models for T . Weakly relevant variables do not contain additional information about T given all other variables. But they contain additional information about T given a subset of other variables. They\nare redundant when all the strongly relevant variables are part of the model for T .The irrelevant variables do not contain any information about T .\nThe Kohavi and John definitions for variable relevance focus on the distinct information content regarding T in individual variable with respect to other variables, whereas Markov boundary and Markov blanket considers the joint information in a set of variables with respect to the target of interest.\nDefinition 15 Markov blanket (Pearl, 2009; Aliferis et al., 2003). A Markov blanket M of the response variable T in the joint probability distribution p(V, T ) is a set of variables conditioned on which all other variables are independent of T , that is, \u2200Y \u2208 V \u2212M, T \u22a5 Y |M.\nDefinition 16 Markov boundary (Pearl, 2009; Aliferis et al., 2003). If no proper subset of Markov blanket M of T satisfies the definition of Markov blanket of T , then M is a Markov boundary (MB) of T .\nThe correspondence between Markov blanket and optimal feature set of T was established in the theorem below.\nTheorem 17 If M is a performance metric that is maximized only when p(T |V) is estimated accurately, and L is a learning algorithm that can approximate any conditional probability distribution, then variable set M is a Markov blanket of T if and only if it is an optimal feature set of T . Variable set M is a Markov boundary of T if and only if it is a minimal optimal feature set of T (Tsamardinos and Aliferis, 2003; Statnikov et al., 2013)."}, {"heading": "Appendix B: Faithful Bayesian Networks", "text": "We next introduce a set of concepts related to Bayesian network, since we will be examining Shapley value for predictive models built from data generated from faithful Bayesian networks.\nDefinition 18 Bayesian network (Neapolitan et al., 2004). Let V be a set of variables and p be a joint probability distribution over V. Let G be a directed acyclic graph (DAG) such that all vertices of G correspond one-to-one to members of V. \u2200X \u2208 V, X is conditionally independent of all non-descendants of X, given the parents of X (i.e. Markov condition holds). The triplet \u3008V, G, p\u3009 defines a Bayesian network\nDefinition 19 Faithfulness (Spirtes et al., 2000). If all and only the conditional independence relations that are true in the joint distribution p are entailed by the Markov condition applied to a DAG G, then p and G are faithful to one another.\nGiven faithfulness, it is possible to establish the relationship between the structural property of the Bayesian network and the statistical properties of its joint distribution. The structural property of a Bayesian network can be described with the help of d-separation and d-connection:\nDefinition 20 d-separation and d-connection (Spirtes et al., 2000). A collider on a path p is a vertex with two incoming edges that belong to p. A path between X and Y given a conditioning set Z is open, if (i) every collider of p is in Z or has a descendant in Z, and (ii) no other nodes on p are in Z. If a path is not open, then it is blocked. Two variables X and Y are d-separated given a conditioning set Z in Bayesian network iff every path between X , Y is blocked. if 6 \u2203Z \u2282 V that d-separates X and Y , X and Y are d-connected.\nIn a faithful Bayesian Network, d-connection and d-separation (structural properties of the network) have a one-to-one correspondence to all conditional dependence and independence relations (statistical properties of the joint distribution), as stated in the Theorem 21:\nTheorem 21 Two variables X and Y are d-separated given a conditioning set Z in a faithful Bayesian network iff X \u22a5 Y |Z. It follows, that if they are d-connected, they are conditionally dependent given Z (Spirtes et al., 2000).\nBelow we list other two theorems that can be derived from Theorem 21. They are particularly useful when it comes to the local neighbourhood of a target variable of interest T .\nTheorem 22 In a faithful BN \u3008V, G, p\u3009, there is an edge between the pair of nodes X,Y \u2208 V, iff X 6\u22a5 Y |S, \u2200S \u2286 V \u2212 {X,Y } (Spirtes et al., 2000).\nTheorem 23 In a faithful BN \u3008V, G, p\u3009, let PC(Vi) denote the set containing all parents of children of Vi. If for a triple of nodes X,T, Y in G, X \u2208 PC(Y ), Y \u2208 PC(T ), and X 6\u2208 PC(T ), then X \u2192 Y \u2190 T , iff X 6\u22a5 T |S \u222a Y , \u2200S \u2286 V\u2212 {X,T} (Spirtes et al., 2000)."}, {"heading": "Appendix C: Faithful Bayesian Networks and Variable Importance in Predictive Models", "text": "Also, since the notion of strongly, weakly and irrelevant variables are based on conditional independence, we can also infer their network structural properties given faithfulness (Tsamardinos and Aliferis, 2003; Aliferis et al., 2003).\nTheorem 24 In a faithful Bayesian network, a variable X \u2208 V is strongly relevant if and only if X \u2208MB(T )..\nTheorem 25 In a faithful Bayesian network \u3008V, G, p\u3009, the unique Markov boundary MB(T ) correspond to the parents, children, and parents of the children of T .\nTheorem 26 Let \u3008V\u222aT,G, p\u3009 be a faithful Bayesian network. A variable Vi \u2208 V is weakly relevant, iff it is not strongly relevant and there is an undirected path from Vi to T .\nTheorem 27 Let \u3008V \u222a T,G, p\u3009 be a faithful Bayesian network. A variable Vi \u2208 V is irrelevant, iff there is no path from Vi to T ."}, {"heading": "Appendix D: Faithful Causal Bayesian Networks", "text": "Next, we introduce causal Bayesian network. A causal Bayesian network is a Bayesian network with causally relevant edge semantics. In a causal Bayesian network, the parents of a variable X are the direct causes of X, the children of X are direct effects of X, the non-parents ancestors of X are indirect causes of X, the non-children descendants of X are indirect effects of X. The causal edge semantics and causal Bayesian network can be defined as the following:\nDefinition 28 Causation (Pearl, 2009). Let do(X = xi) denote a manipulation, where the value of X is set to xi. If \u2203xi, xj , such that p(Y |do(X = xi)) 6= p(Y |do(X = xj)), then X is a cause of Y .\nDefinition 29 Causal Bayesian Network (Pearl, 2009; Spirtes et al., 2000). A causal Bayesian network \u3008V, G, p\u3009 is the Bayesian network \u3008V, G, p\u3009 with the additional semantics that if there is an edge X \u2192 Y in G, then X directly causes Y , \u2200X,Y \u2208 V. (ref citation Spirtes causality book)\nAll the theoretical results introduced in the sections above for faithful Bayesian network applies to faithful causal Bayesian network. The main difference is, in a causal Bayesian network, structural properties can be interpreted causally. For example, Theorem 25 can be rewritten for the causal Bayesian network as below.\nTheorem 30 Let \u3008V, G, p\u3009 be a faithful causal Bayesian network. The unique Markov boundary MB(T ) corresponds to the direct causes, direct effects, and the direct causes of the direct effects of T .\nFor brevity, we do not restate the causal correspondence for all theorems regarding faithful Bayesian network."}], "title": "Predictive and Causal Implications of using Shapley Value for Model Interpretation", "year": 2020}