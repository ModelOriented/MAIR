{"abstractText": "The lack of interpretability and transparency are preventing economists from using advanced tools like neural networks in their empirical work. In this paper, we propose a new class of interpretable neural network models that can achieve both high prediction accuracy and interpretability in regression problems with time series cross-sectional data. Our model can essentially be written as a simple function of a limited number of interpretable features. In particular, we incorporate a class of interpretable functions named persistent change filters as part of the neural network. We apply this model to predicting individual\u2019s monthly employment status using high-dimensional administrative data in China. We achieve an accuracy of 94.5% on the out-of-sample test set, which is comparable to the most accurate conventional machine learning methods. Furthermore, the interpretability of the model allows us to understand the mechanism that underlies the ability for predicting employment status using administrative data: an individual\u2019s employment status is closely related to whether she pays different types of insurances. Our work is a useful step towards overcoming the \u201cblack box\u201d problem of neural networks, and provide a promising new tool for economists to study administrative and proprietary big data.", "authors": [{"affiliations": [], "name": "Yucheng Yang"}, {"affiliations": [], "name": "Zhong Zheng"}, {"affiliations": [], "name": "Ranran Wang"}, {"affiliations": [], "name": "Lingzhou Xue"}, {"affiliations": [], "name": "Linfeng Zhang"}, {"affiliations": [], "name": "Guang Zeng"}], "id": "SP:08211e482cf51c43a5dc913c0974962a41ca55f6", "references": [{"authors": ["Joshua D Angrist", "J\u00f6rn-Steffen Pischke"], "title": "Mostly harmless econometrics: An empiricist\u2019s companion", "venue": "Princeton university press,", "year": 2008}, {"authors": ["Susan Athey"], "title": "The impact of machine learning on economics. In The economics of artificial intelligence: An agenda, pages 507\u2013547", "year": 2018}, {"authors": ["Alessandro Casini", "Pierre Perron"], "title": "Structural breaks in time series", "venue": "arXiv preprint arXiv:1805.03807,", "year": 2018}, {"authors": ["Victor Chernozhukov", "Denis Chetverikov", "Mert Demirer", "Esther Duflo", "Christian Hansen", "Whitney Newey", "James Robins"], "title": "Double/debiased machine learning for treatment and structural parameters, 2018", "year": 2018}, {"authors": ["Raj Chetty", "Nathaniel Hendren", "Patrick Kline", "Emmanuel Saez"], "title": "Where is the land of opportunity? the geography of intergenerational mobility in the united states", "venue": "The Quarterly Journal of Economics,", "year": 2014}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["Liran Einav", "Jonathan Levin"], "title": "Economics in the age of big data", "venue": "Science, 346(6210),", "year": 2014}, {"authors": ["Shuaizhang Feng", "Yingyao Hu", "Robert Moffitt"], "title": "Long run trends in unemployment and labor force participation in urban china", "venue": "Journal of Comparative Economics,", "year": 2017}, {"authors": ["John Giles", "Park Albert", "Juwei Zhang"], "title": "What is china\u2019s true unemployment rate", "venue": "China Economic Review,", "year": 2005}, {"authors": ["Diederik P Kingma", "Jimmy Ba"], "title": "Adam: A method for stochastic optimization", "venue": "arXiv preprint arXiv:1412.6980,", "year": 2014}, {"authors": ["Jon Kleinberg", "Himabindu Lakkaraju", "Jure Leskovec", "Jens Ludwig", "Sendhil Mullainathan"], "title": "Human decisions and machine predictions", "venue": "The quarterly journal of economics,", "year": 2018}, {"authors": ["Tao Lei"], "title": "Interpretable neural models for natural language processing", "venue": "PhD thesis, Massachusetts Institute of Technology,", "year": 2017}, {"authors": ["Atif Mian", "Kamalesh Rao", "Amir Sufi"], "title": "Household balance sheets, consumption, and the economic slump", "venue": "The Quarterly Journal of Economics,", "year": 2013}, {"authors": ["Atif Mian", "Amir Sufi"], "title": "The consequences of mortgage credit expansion: Evidence from the us mortgage default crisis", "venue": "The Quarterly Journal of Economics,", "year": 2009}, {"authors": ["Atif Mian", "Amir Sufi"], "title": "House of debt: How they (and you) caused the Great Recession, and how we can prevent it from happening", "year": 2015}, {"authors": ["Sendhil Mullainathan", "Jann Spiess"], "title": "Machine learning: an applied econometric approach", "venue": "Journal of Economic Perspectives,", "year": 2017}, {"authors": ["W James Murdoch", "Chandan Singh", "Karl Kumbier", "Reza Abbasi-Asl", "Bin Yu"], "title": "Definitions, methods, and applications in interpretable machine learning", "venue": "Proceedings of the National Academy of Sciences,", "year": 2019}, {"authors": ["Emmanuel Saez", "Gabriel Zucman"], "title": "Wealth inequality in the united states since 1913: Evidence from capitalized income tax data", "venue": "The Quarterly Journal of Economics,", "year": 2016}, {"authors": ["Robert Tibshirani"], "title": "Regression shrinkage and selection via the lasso", "venue": "Journal of the Royal Statistical Society: Series B (Methodological),", "year": 1996}, {"authors": ["Jeffrey M Wooldridge"], "title": "Introductory econometrics: A modern approach", "venue": "Nelson Education,", "year": 2016}, {"authors": ["Yongxin Yang", "Irene Garcia Morillo", "Timothy M Hospedales"], "title": "Deep neural decision trees", "venue": "arXiv preprint arXiv:1806.06988,", "year": 2018}, {"authors": ["Ming Yuan", "Yi Lin"], "title": "Model selection and estimation in regression with grouped variables", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2006}], "sections": [{"text": "The lack of interpretability and transparency are preventing economists from using advanced tools like neural networks in their empirical work. In this paper, we propose a new class of interpretable neural network models that can achieve both high prediction accuracy and interpretability in regression problems with time series cross-sectional data. Our model can essentially be written as a simple function of a limited number of interpretable features. In particular, we incorporate a class of interpretable functions named persistent change filters as part of the neural network. We apply this model to predicting individual\u2019s monthly employment status using high-dimensional administrative data in China. We achieve an accuracy of 94.5% on the out-of-sample test set, which is comparable to the most accurate conventional machine learning methods. Furthermore, the interpretability of the model allows us to understand the mechanism that underlies the ability for predicting employment status using administrative data: an individual\u2019s employment status is closely related to whether she pays different types of insurances. Our work is a useful step towards overcoming the \u201cblack box\u201d problem of neural networks, and provide a promising new tool for economists to study administrative and proprietary big data."}, {"heading": "1 Introduction", "text": "Traditionally, economists have relied on interpretable models like linear regressions and logistic regressions, which provide clear insights on the causal or statistical relationships in small datasets [1, 20]. Recently, the use of administrative and proprietary big data has led to some exciting work in empirical economics [14, 13, 5, 15, 18]. Though such datasets enjoy the richness of variables and large sample size, advanced tools like neural networks have not been adopted widely in their analysis as was originally expected [7]. Methodologically, there have been some attempts to bring machine learning tools to economic analysis. However, most of those successful applications are limited to relatively simple models like Lasso, ridge regressions or decision trees [16, 4, 11]. Economists are still nervous about using more advanced tools like neural networks, since they mostly deliver outcomes from a complicated black box without transparency or interpretability [2], even though they are more accurate than conventional models.\nTo address this dilemma, we propose a new class of interpretable neural network models to study panel data2. Our model allows us to take advantage of the high accuracy of neural networks, while\n\u2217Yang and Zheng contributed equally to this paper. We thank Bin Dong, Guanhua Huang, Dake Li, Chris Sims, Ranran Wang, Lingzhou Xue, Linfeng Zhang, Guang Zeng and audience in the BIBDR Economics and Big Data Workshop for helpful comments.\n2Panel data are \u201cdata with a cross section of units with repeated observations on them over time\u201d. It is also called time series cross-sectional data. In this paper, we will use these two names interchangeably.\nPreprint. Under review.\nar X\niv :2\n01 0.\n05 31\n1v 2\n[ ec\non .E\nM ]\n2 7\nO ct\nremaining interpretable like linear or logistic regressions. To this end, we design a modified version of neural networks, which only consists of limited compositions of interpretable and differentiable operators on a limited number of variables. A key feature of our model is that we can incorporate interpretable function forms with unknown parameters as part of the neural network. In this paper, we incorporate a class of persistent change filters as part of the network, which turns out particularly helpful in time series cross-sectional data analysis. Besides constructing the interpretable features from function compositions, we also require the final model to take only a small number of effective features, by incorporating a penalty term in the loss function. The comparison between our model and conventional neural network model is illustrated in Figure 1, and we will present the detailed model setup in Section 2.1.\nThe model we propose in this paper has three main virtues. First, it is easy to interpret. As is shown in the right panel in Figure 1, our model can essentially be written as a simple function of a limited number of interpretable features (the red circles), and the interpretable features arise from limited number of \u201cneural network\u201d type of function compositions. While training the model, we can generate the most informative interpretable features automatically, without doing feature engineering work by hand. Second, it has the nice architecture similar to conventional neural networks, which makes it powerful to fit the data patterns and can be trained efficiently using stochastic gradient descent (SGD) algorithms. We will illustrate the prediction power of this model in our application in Section 3. Third, our model is quite robust and stable subject to data missing problems, which is a critical feature of interpretable models.\nAs an application, we use the interpretable neural network models to predict individual\u2019s monthly employment status using high-dimensional administrative data in China. We achieve an accuracy of 94.5% on the test set, which is comparable to the best-performed conventional machine learning methods. In addition, we clearly understand how the model predict an individual\u2019s employment status with her payment records of different types of insurances. Both the accuracy and the interpretability are robust subject to data missing problems.\nThis paper contributes to the literature with the following distinct features. First, we expand the machine learning toolbox for economists [16, 4, 11] by introducing a modified version of neural networks that achieve high accuracy without sacrificing interpretability and transparency. Second, we contribute to the large literature on interpretable machine learning. Previous work in computer vision and natural language processing [6, 23, 12] mostly focus on interpreting how hidden units of neural networks represent local features of images and texts. In this paper, we propose an interpretable model to study time series cross-sectional data, to help people understand the mechanisms behind the collective human behavior. In the terminology of the overview paper on interpretable machine learning by [17], our model delivers \u201cmodel-based interpretability\u201d, which is different from work that delivers \u201cpost hoc interpretability\u201d by approximating outcomes from complicated models with simple functions. Last but not least, the application of our model to predicting individual employment status with China\u2019s administrative data also contribute to the large literature on China\u2019s unemployment rate estimation in the absence of reliable official statistics [9, 8]. Our work stands out in this literature as\na novel application of machine learning methods on administrative big data, rather than traditional accounting procedure on household survey data.\nThe remaining of the paper is organized as follows. In Section 2, we present the architecture and training algorithms of our model. In particular, we encode a class of interpretable functions named persistent change filters. In Section 3 we apply the model to an administrative dataset to predict individual\u2019s employment status in China. Finally we conclude with discussions on the future work."}, {"heading": "2 Interpretable Neural Networks for Panel Data", "text": ""}, {"heading": "2.1 Mathematical Formulation", "text": "Consider a model yit = g(xit, \u03b8), where yit is outcome for observation i at time t and xit is high dimension inputs. We hope to construct an intepretable function g(\u00b7, \u03b8), which is a composition of several operators as g = g(M) \u25e6 g(M\u22121) \u00b7 \u00b7 \u00b7 \u25e6 g(1). Here we choose M = 4. {g(m)} are basic components defined as below with undetermined parameters to be trained in the neural network.\n1. g(1): Decision-tree-like splitting Sp(x) = Sigmoid(px+ q). For x \u2208 R1, a sigmoid function can function as a decision-tree-like splitting operator like a differentiable indicator function [21]. For example, if p is close to 1 and q is a large negative number, the output can be close to 0 and 1 to discriminate whether x is smaller or larger than a threshold. Here p and q are unknown parameters. 2. g(2): Dimension reduction Re(x). For x \u2208 Rm, define Re(x) : Rm \u2192 Rn(n m) as a certain dimension reduction function with unknown parameters. For example, Re(x) could be linear combination with sparse inputs, and the linear coefficients are unknown parameters. 3. g(3): Other interpretable transforms Trans(x). For x \u2208 Rn, we can propose other transforms to generate interpretable features and incorporate them into the model. In the next section, we introduce a general class of interpretable transforms named persistent change filter, which turns out helpful in regressions with panel data. Trans(x) also include unknown parameters. 4. g(4): simple linear or logistic function as the final layer. The effective inputs of the final layer are required to be sparse, by adding a penalty term in the loss function.\nOur model can essentially be written as a simple function g(4) of a limited number of interpretable features (outcomes of g(3)), which makes it easy to interpret. Its structure as the composition of differentiable functions makes it easily trainable as conventional neural networks.\n2.2 A General Class of Features for Time Series Data: Persistent Change Filter\nFor a particular time series for some certain observation (the i subscript is omitted here for simplicity), x\u03c4 \u2208 [0, 1], \u03c4 = 1, 2...t, we define the persistent change filter D = pt \u2212 qt, where:\np1 = x1, p\u03c4+1 = x\u03c4+1 + kx\u03c4+1p\u03c4 + (1\u2212 k)p\u03c4 q1 = 1\u2212 x1, q\u03c4+1 = (1\u2212 x\u03c4+1) + k(1\u2212 x\u03c4+1)q\u03c4 + (1\u2212 k)q\u03c4\nHere k \u2208 [0, 1] is a smoothing parameter. Then we can map the original time series {xt}, t = 1, 2, ..., T to a persistent change filter time series D({x\u03c4}t\u03c4=1), t = 1, 2, ..., T . The definition of the persistent change filter D is motivated by identifying a persistent change in time series data. We illustrate this idea first with two examples and formalize with a proposition.\nExample I For a binary time series xt \u2208 {0, 1}, t = 1, 2...T = 1000. xt begins with 0 for some periods, and persistently switch to 1 for the last t0 periods: xt = 1[t > T \u2212 t0]. We plot the persistent change filter on the entire time series D({x\u03c4}T\u03c4=1) against the number of periods t0 between the transition and the terminal period in the left panel of Figure 2. We find that no matter what smoothing variable k we choose, the persistent change filter fits perfectly with t0 on the 45\u25e6 line. This implies that the persistent change filter captures the duration of a persistent change in such a binary time series 0, ..., 0, 1, 1, ..., 1.\nExample II In real world, such perfect binary data may not exist. Consider time series xt, where everything is the same as Example I, except that a random 5% of xt are replaced by abnormal data as\n(a) Persistent Change Filter on Data without Missing\n(b) Persistent Change Filter on Data with Missing Problems\nFigure 2: Persistent Change Filter Captures the Duration of a Persistent Change\nxt = 0.9 when t > T \u2212 t0. Then we plot the persistent change filter D({xt}) against t0 for different choices of the smoothing parameter k in the right panel of Figure 2. For the the persistent change filter without any smoothing (i.e. k = 1), the plot deviates from the 45\u25e6 line significantly. When k decreases, the plot get closer to the 45\u25e6 line. So an optimal choice of k would help capture the duration of a persistent change in such a time series with potential data missing. To note, k would be left as an unknown parameter that our model would learn from the data.\nWe formalize the examples above with the following proposition:\nPROPOSITION 2.1. When k \u2192 1, pT uniformly converges to D(0) defined as below:\nD(0)(xT , xT\u22121...x1) = T\u2211 i=1 i\u220f j=1 xT\u2212j+1.\nWhat does D(0) means for the time series? Consider the binary time series xt \u2208 {0, 1}, t = 1, 2...T , D(0) is the lasting time periods for recent xi\u2019s to be 1. In other words,\nD(0)(x) = m \u21d0\u21d2 xT = xT\u22121... = xT\u2212m+1 = 1, xT\u2212m = 0\nThus D(0) gives information on the lasting time since the most recent period when the binary time series persistently switch to 1. For continuous xt in [0, 1], D(0) is differentiable at any point, which means it could be part of a trainable model. However, D(0) only captures the jumps from low values to high values, so we extend D(0) to D so that it can capture both persistent jumps and drops in the time series3. This is the qT term in the formulation. As is discussed above, we also add the trainable smoothing parameter k to adjust for missing or abnormal data problems. For a more elaborate discussion and the origins of the persistent change filter, please refer to Supplement D."}, {"heading": "2.3 Composition and Training", "text": "With the components above, we can give a full architecture of g. Suppose X = (X1, ...Xm) is an m-dimension vector as the original input, we have\nX(1) = g(1)(X, \u03b8(1)) = (Sp(X1),Sp(X2)...Sp(Xm))\n3Some might find the idea of the persistent change filter seems related to the structural break literature in time series analysis [3]. However, the structural break detection methods are not applicable for feature constructions, since they mostly focus on detecting break points with formal statistical tests, while we hope to get an explicit function transform as part of the interpretable model.\nX(2) = g(2)(X(1), \u03b8(2)) = Re(X(1))\nX(3) = g(3)(X(2), \u03b8(3)) = Trans(X(2))\ny = g(4)(X(3), \u03b8(4))\nThus we have g = g(4) \u25e6 g(3) \u25e6 g(2) \u25e6 g(1). The full architecture of interpretable neural network function g is in the right panel of Figure 1. It is easy to interpret since it can be written as a simple function of a small number of interpretable features (red circles), which arise from limited number of \u201cneural network\u201d type of function compositions.\nFinally, we add Lasso [19] or group Lasso [22] penalty to the loss function to require sparse input:\nL(\u03b8, \u03c6) = EX,Y Loss(g(x, \u03b8), y) + penalty(g(4)) (1)\nSince all components are differentiable in loss function (1), the model can be trained like a conventional neural network with Adam [10]."}, {"heading": "3 Application to Administrative Data", "text": ""}, {"heading": "3.1 Employment Status Prediction with Administrative Data", "text": "In this section, we apply the interpretable neural network model to predict individual\u2019s monthly employment status using high-dimensional administrative data as input. The administrative data comes from a four-million city in China4, and includes basic demographic information (age, family relations, gender, education, etc.), as well as individual level monthly payments to six different kinds of social insurances and the Housing Provident Fund (HPF)5. With all these features, together with employment/unemployment labels on part of the sample (about 400,000 individuals are labelled every month), we construct an interpretable neural network model to predict employment status of all the individuals in the population each month. With the prediction results, we can calculate unemployment rate on the whole population as an important economic indicator for policy makers. This application is novel and important given the unreliability of China\u2019s official unemployment statistics [8].\nFor individual i in calendar month t, her employment status is denoted as yit \u2208 {0, 1}, where 1 means employment and 0 means unemployment. Her payment amounts for different insurances, among other individual features are denoted as xijt where j = 1, 2...m correspond to different features. To predict yit, we stack features of the individual in the past t0 period as model inputs, denote as x\u0303it = (xij\u03c4 ) where j = 1, 2...m, \u03c4 = t, t \u2212 1, ..., t \u2212 t0 + 1. We assume the model is invariant to time t, which means P (yit = 1|x\u0303it) = f(x\u0303it), \u2200i, t. For standard machine learning, f(\u00b7) is usually complicated and requires feature engineering. In this paper, we will follow Section 2 to set up a more interpretable model to predict yit with high accuracy."}, {"heading": "3.2 Interpretable Neural Network Model Setup", "text": "Following Section 2, we write the model as P (yit = 1|x\u0303it) = g(x\u0303it, \u03b8) where g = g(4) \u25e6 g(3) \u25e6 g(2) \u25e6 g(1). In this example, the interpretable transform g(3) is the persistent change filter. Suppose the original input variables of g is a mt0 \u00d7 1 vector:\nx\u0303it = (xij\u03c4 ) where j = 1, 2 . . .m, \u03c4 = t, t\u2212 1, . . . , t\u2212 t0 + 1. m = 7 is the number of insurances to pay and t0 = 6 denotes the lagged periods we consider in model inputs. We define the detailed form for g = g(4) \u25e6 g(3) \u25e6 g(2) \u25e6 g(1) as below:\n4For confidentiality, we would not release any city-specific information including the city name in this paper. 5In China, there are five major types of social insurances: endowment insurance, basic medical insurance, unemployment insurance, employment injury insurance, and maternity insurance. For urban residents, basic medical insurance consists of \u201cbasic medical insurance for working urban residents\u201d and \u201cbasic medical insurance for non-working urban residents\u201d. Together with the Housing Provident Fund (HPF), which we take as one type of insurance from now on, our administrative data include detailed individual level payments to seven types of insurances in total. To note, those who pay for employment injury insurance or unemployment insurance are not necessarily employed or unemployed, and vice versa.\n1. Decision-tree-like splitting g(1) : Rmt0 \u2192 Rmt0 . x\u0303 (1) it = (x (1) ij\u03c4 ) = g (1)(x\u0303it, \u03b8 (1)) = (Sp(xi1t),Sp(xi2t)...Sp(xim,t\u2212t0+1)) with j = 1, 2...m, \u03c4 =\nt, t \u2212 1, . . . , t \u2212 t0 + 1. Here the R1 \u2192 R1 splitting function Sp(x) = Sigmoid(px + q). The trainable parameters \u03b8(1) = {p, q} are shared for different dimensions.\n2. Dimension reduction g(2) : Rmt0 \u2192 Rt0 . x\u0303 (2) it = (x (2) i\u03c4 ) t\u2212t0+1 \u03c4=t = g (2)(x\u0303 (1) it , \u03b8 (2)) = (Re(x(1)it ),Re(x (1) i,t\u22121)...Re(x (1) i,t\u2212t0+1)) This step is\ndimension reduction for the m-dimensional variables x(1)i\u03c4 for a given \u03c4 . Here the Rm \u2192 R1 function Re(Z) = Sigmoid(wTZ + b). The trainable parameters are \u03b8(2) = {w, b}. In our example, w will be interpreted as the contribution of each kind of insurance or HPF payment to the variable where we are going to calculate the persistent change filter.\n3. Interpretable transform: persistent change filter g(3) : Rt0 \u2192 R1. x\u0303 (3) it = g (3)(x\u0303 (2) it , \u03b8 (3)) = D(x\u0303(2)it ) = D((x (2) i\u03c4 ) t\u2212t0+1 \u03c4=t ) where D(\u00b7) is the persistent change filter\ndefined in Section 2.2. Thus x\u0303(3)it is an interpretable feature. \u03b8 (3) = {k} is trainable.\n4. Logistic regression g(4) : R1 \u2192 R1. For the final component of g, we use a simple form of logistic regression: P (yit) = g(4)(x\u0303\n(3) it , \u03b8 (4)) = Sigmoid(ux\u0303(3)it + v) where trainable parameters are \u03b8 (4) = {u, v}. As g(3)\ngives a one-dimensional output, we omit the penalty on g(4).\nWith all components above, we can get the interpretable neural network model as g = g(4) \u25e6 g(3) \u25e6 g(2) \u25e6 g(1). The loss function is E(\u2212 logP (yit|x\u0303it)), where E means taking average above all the data observations (x\u0303it, yit). \u03b8 = {p, q, w, b, k, u, v} is the set of all the trainable parameters in the model. The final value for these parameters would deliver the concrete form of the full model g."}, {"heading": "3.3 Dataset and Model Results", "text": "With the administrative data in Section 3.1, we use monthly payment data of various social insurances and the HPF from July 2016 to December 2017 to construct x\u0303it. To construct a balanced data sample, we randomly select 20,000 employed observations and 20,000 unemployed observations to get a balanced sample. Both positive and negative samples are evenly divided into two parts to get the training set and the test set.\nFor performance evaluation, We will consider other interpretable models (logistic regressions) as well as more complicated models (random forests6). These models require handcrafted features as model inputs. Based on the descriptive statistics and domain knowledge (see the details in Supplement A) on this problem, we construct the following features:\n1. \u201cInsurance count\u201d (IC). The number of types of insurances the individual pays in period t. 2. \u201cNaive persistent change\u201d (NPC). For each kind of insurance, we construct the \u201cnaive persistent\nchange\u201d D(0)(xt, xt\u22121, ..., xt\u2212t0+1) following the definition in Proposition 2.1, where x\u03c4 is a binary variable on whether the individual does not pay7 for a specific insurance in period \u03c4 . Compared to the persistent change filter measure, these \u201cnaive\u201d measures do not incorporate any trainable parameters, and come directly from feature engineering. 3. \u201cPayment change\u201d (PC). We construct another \u201cnaive persistent change\u201d variable taking the same expression based on the following binary time series x\u03c4 : whether the number of types of insurances the individual has paid is larger than 2. The feature with threshold 2 is constructed with insights from the descriptive statistics (see Supplement A).\nWith these features in hand, we compare the performance of the following 11 models:\n6We omit neural networks with different hyperparameters here because their accuracy is worse than the random forests in the test set.\n7As is discussed in Proposition 2.1, D(0) only works for increasing time series ..., 0, 0,..., 0, 1,..., 1. So here we define x\u03c4 as whether the individual does NOT pay, rather than pay a specific insurance in period \u03c4 . The persistent change measure that captures both jumps and drops is derived in the D.\n1. The interpretable neural network (IntNN) model (1) we propose in this paper. No handcrafted feature is needed in this model. 2. Logistic regression (Logistic) with (2) 7 \u201cnaive persistent changes\u201d, \u201cpayment change\u201d and insurance count; (3) 7 \u201cnaive persistent changes\u201d; (4) \u201cpayment change\u201d and insurance count; (5) \u201cpayment change\u201d; (6) insurance count. 3. Random forests (RF) with (7) 7 \u201cnaive persistent changes\u201d, \u201cpayment change\u201d and insurance count; (8) 7 \u201cnaive persistent changes\u201d; (9) \u201cpayment change\u201d and insurance count; (10) \u201cpayment change\u201d; (11) insurance count\nThe results of model performances are in Table 1.\nTable 1 shows that the model we propose (first row) achieves the highest accuracy. Models based on random forest algorithm and feature engineering enjoys higher accuracy than logistic regressions, but are not transparent for interpretation. Traditional interpretable models like logistic regressions perform much worse in terms of accuracy."}, {"heading": "3.4 Model Interpretation", "text": "The interpretable neural network models deliver a clear mechanism behind the model outcome. Now we will look into the model estimation results, and interpret the model we obtain. We look into the model parameters for each component of g = g(4) \u25e6 g(3) \u25e6 g(2) \u25e6 g(1)\n1. Decision-tree-like splitting g(1): For \u2200j, \u03c4 , x(1)ij\u03c4 = Sigmoid(pxij\u03c4 + q). The estimates are p = 2.60, q = \u221241.46. As is discussed in Section 2.1, with such p and q, g(1) would transform small payment values to 0, while keep large payment values close to 1. So, it approximates an indicator function. 2. Linear combination g(2): For \u2200\u03c4 , x(2)i\u03c4 = Sigmoid(wT (x (1) ij\u03c4 ) m j=1 + b).\nThe estimates of the m-dimensional vectors w = (\u22121.73, 1.84,\u22124, 0.64, 0.62,\u22120.66, 4.19)T , and each element corresponds to endowment insurance, urban working medical insurance, unemployment insurance, employment injury insurance, maternity insurance, urban non-working medical insurance, and the Housing Provident Fund (HPF) respectively. The intercept b = 3.67. In this layer, we build up a 1-dimension variable x(2)i\u03c4 from the linear combination of the 7- dimensional payment records after the decision-tree-like splitting. The output is positively correlated with payments of urban working medical insurance, employment injury insurance, maternity insurance, and the Housing Provident Fund (HPF), while negatively correlated with payments of endowment insurance, unemployment insurance, urban non-working medical insurance.8\n3. Persistent Change Filter g(3): x\u0303(3)it = D((x (2) i\u03c4 ) t\u2212t0+1 \u03c4=t , k).\nThe estimated smoothing parameter is k = 0.9999999. As we show in Proposition 2.1, k \u2192 1 means no smoothing for data missing or abnormal data pattern is imposed for the persistent change filter. Thus this is a persistent change filter directly on x(2)i\u03c4 , and larger value corresponds to a persistent jump of x(2)i\u03c4 , while smaller value corresponds to a persistent drop of x (2) i\u03c4 .\n4. Logistic regression g(4): P (yit = 1) = Sigmoid(ux\u0303 (3) it + v)\nThe estimates are u = 1.06, v = \u22122.18. This is a differentiable version of linear transform with coefficient close to 1, so it is similar to identity.\n8We also compare w with parameters from logistic regression models, and find they share qualitatively the same interpretation while our model obtains higher accuracy. See Supplement B for more details.\nTo summarize, our interpretable neural network model predicts individual\u2019s employment status simply with the persistent change filter of a composite variable from a linear combination of whether an individual pay each type of insurance. The weights of the linear combination imply the relative importance of each insurance when predicting the employment status. From the model, we learn that when an individual that used to consistently pay urban working medical insurance, employment injury insurance, maternity insurance, or the Housing Provident Fund (HPF), but suddenly drop out from those insurance programs, has a larger chance to become unemployed. Similarly, for individuals that are beginning to get enrolled in endowment insurance, unemployment insurance, urban non-working medical insurance, they have a larger chance to get unemployed."}, {"heading": "3.5 Robustness of the Model", "text": "As is discussed in [17], an interpretable model should be robust to data missing or abnormal data problem. We randomly set 10% of all the payment records to 0, and train the same 11 models as above and our model still perform the best in term of accuracy. Also, the parameter estimates deliver qualitatively equivalent interpretations. The value of parameter k in the persistent change filter becomes smaller to balance missing. Please refer to Supplement C for more details."}, {"heading": "4 Conclusion", "text": "The lack of interpretability and transparency are preventing economists from using neural networks in their empirical work. In this paper, we propose a new class of interpretable neural network models that can achieve both high prediction accuracy and interpretability in regression problems with time series cross-sectional data. Our model could essentially be written as a simple function of a limited number of interpretable features, which arise from limited number of \u201cneural network\u201d type of function compositions. In particular, we incorporate a class of interpretable functions named persistent change filters as part of the neural network. The model is easy to interpret by design, while is powerful enough to fit the data well due to similar architecture as conventional neural networks.\nAs an application, we use the interpretable neural network models to predict individual\u2019s monthly employment status using high-dimensional administrative data in China. We achieve a high accuracy as 94.5% on the out-of-sample test set, which is comparable to the best-performed conventional machine learning methods. Furthermore, we interpret the model to understand the mechanisms behind. We find that it predicts individual\u2019s employment status simply with the persistent change filter of a composite variable from a linear combination of whether an individual pay each type of insurance. From the model, we learn that when an individual that used to consistently pay urban working medical insurance, employment injury insurance, maternity insurance, or the Housing Provident Fund (HPF), but suddenly drop out from those insurance programs, has a larger chance to become unemployed. We compare our model with logistic regression models, and find the model interpretations are qualitatively the same, while our model delivers much higher prediction accuracy.\nThis paper propose a promising method to overcome the \u201cblack box\u201d problem of neural networks, and contribute to the machine learning toolbox of economists by introducing a modified version of neural networks that is both accurate and easy to interpret. With the massive use of administrative and proprietary big data in economics and policy research, lots of research could be done with this new tool."}, {"heading": "A Descriptive Statistics", "text": "Table 2 presents some descriptive statistics on insurance payment behaviors of both the employed and the unemployed sample. The upper panel is the number of employed and unemployed individuals\nthat pay each type of the insurances in the most recent month in the sample. From this panel we can see that the unemployed individuals tend to pay endowment insurance, unemployment insurance and urban working medical insurance, while the employed individuals pay all the insurances except for urban non-working medical insurance, since they mostly prefer urban working medical insurance. The lower panel reports the number of types of insurances that paid by each employed and unemployed individual in the most recent month, we find that the employed individuals tend to pay more kinds of insurances. With the elbow method, we find 2 is a good threshold to discriminate the employed and the unemployed if we hope to do some traditional feature engineering, since most of the unemployed sample pay less or equal to 2 types of insurances.\nB Interpretation Comparison: Our Model vs Logistic Regression\nWe compare our model with the logistic regression model with the \u201cnaive persistent changes\u201d of seven kinds of insurances as model inputs. The definition of \u201cnaive persistent change\u201d is in Section 3.3. The linear combination weights in g(2) of our interpretable neural network model and the logistic coefficients of each variable are in Table 3.\nFrom Table 3, we find the interpretations of our model and the logistic regression model are qualitatively the same. As is discussed in the definition of \u201cnaive persistent change\u201d in Section 3.3, larger naive persistent change means the individual switch from paying to not paying a specific type of insurance. Thus negative coefficients in the logistic regression imply a positive relationship between jumps in payments and the probability getting employed, which share the same interpretation as positive coefficients in our model. Here we find the signs of all the coefficients in the interpretable neural\nnetworks are exactly opposite to each other, which means they share exactly the same interpretation qualitatively."}, {"heading": "C Robustness of the Model", "text": "C.1 Model Accuracy with Data Missing\nThe model we propose is robust to data missing or abnormal data problems. To check the robustness, we randomly set 10% of all the payment records to 0 as data missing. Then we rerun the same 11 models as in Section 3.3. The model accuracy results are in Table 4.\nFrom Table 4, we can see that all the results are similar subject to the 10% data missing. Though data missing makes all the models worse off, our interpretable model remain the best performance among all the competitors. In the next section, we will see the model parameters are also robust, which delivers robust model interpretation for the problem.\nC.2 Model Interpretation with Data Missing\nIn Section 3.4, we interpret our model clearly through the model structure. Besides transparency of the model structure, robustness is another important feature of model interpretability. In this section, we illustrate the robustness of our model with model parameters we estimate with 10% missing data in Section C.\n1. Decision-tree-type splitting g(1): For \u2200j, \u03c4 , x(1)ij\u03c4 = Sigmoid(pxij\u03c4 + q). The estimates are p = 2.76, q = \u221243.96, which are both close to those in the baseline model. 2. Dimension reduction g(2): For \u2200\u03c4 , x(2)i\u03c4 = Sigmoid(wT (x (1) ij\u03c4 ) m j=1 + b). The estimates of the m-\ndimensional vectors w = (\u22120.76, 1.57,\u22122.12, 0.55, 0.5,\u22120.8, 3.39)T . The intercept b = 2.87. All the estimates are close to those in the baseline model.\n3. Persistent Change Filter g(3): x\u0303(3)it = D((x (2) i\u03c4 ) t\u2212t0+1 \u03c4=t , k). The estimated smoothing parameter\nis k = 0.89, different from the baseline estimates 0.999999. This is totally sensible: due to data missing, we need to impose some smoothing to get a reasonable persistent change filter time series that predict the outcomes well. This also confirms the rationale of the persistent change filter definition, as is detailed discussed in Supplement D. 4. Logistic regression g(4): P (yit) = Sigmoid(ux\u0303 (3) it + v). The estimates are u = 1.82, v = \u22125.96,\nwhich are both close to those in the baseline model.\nTo summarize, our model is quite robust subject to data missing problems, and deliver essentially the same interpretations of the model outcomes.\nD Motivation and Variants of Persistent Change Filter\nThe persistent change filter is a monotone reduction designed for univariate time-series input x = (x1, x2...xT ), xt \u2208 [0, 1] and mainly captures the persistent jumps or drops in it.\nD.1 Persistent change D(0) for Binary Inputs\nConsider the binary time series xt \u2208 {0, 1}, t = 1, 2...T , persistent change is defined as the lasting time periods for recent xi\u2019s to be 1. In other words,D(0)(x) = m \u21d0\u21d2 xT = xT\u22121... = xT\u2212m+1 = 1, xT\u2212m = 0. This variable gives information on the moment the input turns into 1 and the lasting time.\nD.2 Continuous Persistent change D(1) for Continuous Inputs\nWhen we have a continuous input xt \u2208 [0, 1], t = 1, 2..., T , we find a compatible expression for persistent change which reduces to the original version if we restrict the input to be binary.\nD(1) = T\u2211 t=1 t\u220f j=1 xT\u2212j+1\nIn an iterative way, we can define a series pt, t = 1, 2..., T and p1 = x1, pt+1 = xt+1 + xt+1pt, then we have the continuous persistent change measure D(1) = pT . To understand the iterative definition, we see when xt+1 gets to 0, it will almost clean up the historical accumulation pt. When xt+1 remains to be 1, it will keep the historical accumulation pt and pt+1 accumulates by adding xt+1.\nD.3 Persistent change D(2): Symmetrical to Jumps and Drops\nThe measures D(0) and D(1) can only capture time series change from small values (like 0) to big values (like 1). To treat small and large values equally, we define a new measure D(2) on continuous value time series xt \u2208 [0, 1], t = 1, 2..., T :\np1 = x1, pt+1 = xt+1 + xt+1pt; q1 = 1\u2212 x1, qt+1 = (1\u2212 xt+1) + (1\u2212 xt+1)qt\nD(2) = pT \u2212 qT In this definition, both jumps and drops in xt are addressed.\nD.4 Smooth Persistent change D(3): Adding a trainable smooth variable k\nTo weaken the cleaning-up effect against abnormal time series change due to missing data or data error problems, we define a smoothing variable k \u2208 [0, 1], and define a new measure D(3) on continuous value time series xt \u2208 [0, 1], t = 1, 2..., T :\nps1 = x1, p s t+1 = xt+1 + kxt+1pt + (1\u2212 k)pt\nqs1 = 1\u2212 x1, qst+1 = (1\u2212 xt+1) + k(1\u2212 xt+1)qst + (1\u2212 k)qst\nD(3) = psT \u2212 qsT Compared to D(2), the multiplication term xt+1pt is replaced by kxt+1pt + (1 \u2212 k)pt. In other words, larger k means larger cleaning-up effect, while the accumulation effect is unaffected. When k = 1 we have D(3) = D(2). We leave k trainable to let the data finds the best value. Finally, we have the notion of persistent change filter D = D(3).\nD.5 Why Do We Call It Persistent Change Filter?\nConsider four pairs of inputs:\nx (1) 1 = (0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), x (2) 1 = (1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\nx (1) 2 = (0, 0, 0, 1, 1, 1, 1, 1, 1, 1,0, 1, 1, 1, 1), x (2) 2 = (1, 1, 1, 0, 0, 0, 0, 0, 0, 0,1, 0, 0, 0, 0);\nx (1) 3 = (0.1, 0.1, 0.1, 0.9, 0.9, 0.9, ..., 0.9, 0.9), x (2) 3 = (0.9, 0.9, 0.9, 0.1, 0.1, 0.1, ..., 0.1, 0.1);\nx (1) 4 = (0.1, 0.1, 0.1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,0.1, 0.9, 0.9, 0.9, 0.9),\nx (2) 4 = (0.9, 0.9, 0.9, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,0.9, 0.1, 0.1, 0.1, 0.1);\nand the persistent change measures we defined above are in Table 5.\nFrom Table 5, we have the following intuitive findings as below:\n1. As is shown in the first two rows, D(1) is equivalent to D(0) for binary inputs, while it can also handle continuous inputs as the last two rows. 2. D(2) captures the persistent change idea, but is vulnerable to abnormal data points, as is in the second and forth rows. 3. D(3) can still capture the persistent change idea even upon abnormal data points. Here the smoothing parameter k is selected arbitrarily, while it could be trained to an optimal value when we hope to use D(3) as input features in regression models."}], "year": 2020}