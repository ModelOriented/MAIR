{"abstractText": "Interpretability of machine learning algorithms is an urgent need. Numerous methods appeared in recent years, but do their explanations make sense? In this paper, we present a thorough theoretical analysis of one of these methods, LIME, in the case of tabular data. We prove that in the large sample limit, the interpretable coefficients provided by Tabular LIME can be computed in an explicit way as a function of the algorithm parameters and some expectation computations related to the black-box model. When the function to explain has some nice algebraic structure (linear, multiplicative, or sparsely depending on a subset of the coordinates), our analysis provides interesting insights into the explanations provided by LIME. These can be applied to a range of machine learning models including Gaussian kernels or CART random forests. As an example, for linear functions we show that LIME has the desirable property to provide explanations that are proportional to the coefficients of the function to explain and to ignore coordinates that are not used by the function to explain. For partition-based regressors, on the other side, we show that LIME produces undesired artifacts that may provide misleading explanations.", "authors": [{"affiliations": [], "name": "Damien Garreau"}, {"affiliations": [], "name": "Ulrike von Luxburg"}], "id": "SP:84b9841bbfae4be6da1e7d4f5a9a6be9cace0bbf", "references": [{"authors": ["A. Adadi", "M. Berrada"], "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)", "venue": "IEEE Access,", "year": 2018}, {"authors": ["D. Alvarez-Melis", "T.S. Jaakkola"], "title": "On the robustness of interpretability methods. Preprint, available at arxiv", "venue": "org/ abs/ 1806", "year": 2018}, {"authors": ["G. Biau", "E. Scornet"], "title": "A random forest guided", "venue": "tour. Test,", "year": 2016}, {"authors": ["S. Boucheron", "G. Lugosi", "P. Massart"], "title": "Concentration inequalities: A nonasymptotic theory of independence", "venue": "Oxford university press,", "year": 2013}, {"authors": ["L. Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "title": "Classification and regression", "year": 1984}, {"authors": ["T.B. Brown", "B. Mann", "N. Ryder", "M. Subbiah", "J. Kaplan", "P. Dhariwal", "A. Neelakantan", "P. Shyam", "G. Sastry", "A. Askell"], "title": "Language models are few-shot learners. Preprint, available at arxiv", "venue": "org/ abs/", "year": 2005}, {"authors": ["R. Caruana", "Y. Lou", "J. Gehrke", "P. Koch", "M. Sturm", "N. Elhadad"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data mining,", "year": 2015}, {"authors": ["P. Cortez", "A. Cerdeira", "F. Almeida", "T. Matos", "J. Reis"], "title": "Modeling wine preferences by data mining from physicochemical properties", "venue": "Decision Support Systems,", "year": 1998}, {"authors": ["D. Garreau", "U. von Luxburg"], "title": "Explaining the explainer: A first theoretical analysis of lime", "venue": "In Proceedings of the 33rd International Conference on Artificial Intelligence and Statistics (AISTATS),", "year": 2020}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "F. Giannotti", "D. Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "ACM computing surveys (CSUR),", "year": 2018}, {"authors": ["D. Harrison Jr.", "D.L. Rubinfeld"], "title": "Hedonic housing prices and the demand for clean air", "venue": "Journal of Environmental Economics and Management,", "year": 1978}, {"authors": ["T. Hastie", "R. Tibshirani", "R. Friedman"], "title": "The Elements of Statistical Learning", "year": 2001}, {"authors": ["T.J. Hastie", "R.J. Tibshirani"], "title": "Generalized additive models, volume 43", "venue": "CRC press,", "year": 1990}, {"authors": ["A.E. Hoerl", "R.W. Kennard"], "title": "Ridge regression: Biased estimation for nonorthogonal problems", "year": 1970}, {"authors": ["K.S. Jones"], "title": "A statistical interpretation of term specificity and its application in retrieval", "venue": "Journal of Documentation,", "year": 1972}, {"authors": ["M.S. Kovalev", "L.V. Utkin", "E.M. Kasimov"], "title": "SurvLIME: a method for explaining machine learning survival models. Preprint, available at arxiv", "venue": "org/ abs/", "year": 2003}, {"authors": ["S.M. Lundberg", "S.-I. Lee"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["S. Mishra", "B.L. Sturm", "S. Dixon"], "title": "Local interpretable model-agnostic explanations for music content analysis", "venue": "In ISMIR,", "year": 2017}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should I trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data mining,", "year": 2016}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,", "year": 2018}, {"authors": ["B. Schoelkopf", "A.J. Smola"], "title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "year": 2001}, {"authors": ["L.S. Shapley"], "title": "A value for n-person games", "venue": "Contributions to the Theory of Games,", "year": 1953}, {"authors": ["S. Shi", "Y. Du", "W. Fan"], "title": "An extension of lime with improvement of interpretability and fidelity. Preprint, available at arxiv", "venue": "org/ abs/", "year": 2004}, {"authors": ["C.J. Stone"], "title": "Additive regression and other nonparametric models", "venue": "The Annals of Statistics,", "year": 1985}, {"authors": ["J.A. Tropp"], "title": "User-friendly tail bounds for sums of random matrices", "venue": "Foundations of computational mathematics,", "year": 2012}, {"authors": ["R. Turner"], "title": "A model explanation system", "venue": "In IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP),", "year": 2016}, {"authors": ["L.V. Utkin", "M.S. Kovalev", "E.M. Kasimov"], "title": "SurvLIME-Inf: a simplified modification of SurvLIME for explanation of machine learning survival models. Preprint, available at arxiv", "venue": "org/ abs/", "year": 2005}, {"authors": ["R. Vershynin"], "title": "High-dimensional probability: An introduction with applications in data science, volume 47", "year": 2018}, {"authors": ["S. Wachter", "B. Mittelstadt", "L. Floridi"], "title": "Why a right to explanation of automated decision-making does not exist in the general data protection regulation", "venue": "International Data Privacy Law,", "year": 2017}], "sections": [{"text": "Keywords: machine learning, interpretability, explainable AI, statistical learning theory"}, {"heading": "1. Introduction", "text": "In recent years, many methods aiming to provide interpretability of machine learning algorithms have appeared. These methods give explanations for virtually all machine learning models currently in use, including the most complex. However, it is still unclear how these methods operate in the absence of proper theoretical treatment.\nInterpretability is important for several reasons. First, most of the recent advances in machine learning have been achieved by models whose increasing complexity seems to know no limit. The latest and maybe most shocking example of this trend is the recent disclosure of GPT-3 (Brown et al., 2020), a 175 billions parameters language model. Though in most cases the model itself may be inaccessible to us, it is interesting to note that sometime the architecture and the parameters are known. One could very well read the code and sometime even check the value of each individual coefficient. Even then, it is today very challenging for a human to understand how a particular prediction is made. Maybe for this reason recent models are more and more frequently referred to as black boxes.\nSecond, while some users mainly care about performance (that is, accuracy), some specific applications demand interpretability of the algorithms involved in the decision-\nc\u00a92020 Damien Garreau and Ulrike von Luxburg.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.\nar X\niv :2\n00 8.\n11 09\n2v 2\n[ st\nat .M\nL ]\n1 8\nSe p\nmaking process. This is particularly true in healthcare. The main worry of practitioners is that the model that they are training learns a rule yielding good accuracy on the train and test sets, but making little common sense and leading to dramatic decisions when deployed in the wild. For instance, Caruana et al. (2015) describe a model trained to predict probability of death from pneumonia. This model ends up assigning less risk to patients if they also have asthma. Of course, from a medical point of view, this is nonsense, and deploying the model in a real-life situation would lead asthmatic patients to receive less prompt treatment and therefore increase their risk of ending up in critical condition. One can surmise that the model learned that asthma was predictive of a lower risk because these patients, a contrario, received the quickest treatment. In this hypothetical example, interpretability of the model would help us not releasing the flawed model. For instance, one could investigate a few cases with an interpretability algorithm, and realize that asthma is associated with a decrease in the risk. Such sanity check is not possible \u201cas is\u201d with most recent machine learning models, due to their complexity. We refer to Turner (2016) for other interesting examples.\nThe current spread of machine learning in all aspects of our life make the previous example not so hypothetical anymore. Thus there is an urgent need for interpretability. It is interesting to note that this need is recognized by the lawmakers, at least in the European Union, where the European Parliament adopted in 2018 the General Data Protection Regulation (GDPR). Part of the GDPR is a clause on automated decision-making, stating to some extent a right for all individuals to obtain \u201cmeaningful explanations of the logic involved.\u201d Even if there is an ongoing debate on whether this disposition is legally binding (Wachter et al., 2017), for the first time, we find written in law the idea that decision-making algorithms cannot stay black boxes.\nThe main question underlying our work is the following:"}, {"heading": "Do these explanations make sense, in particular when the black-box model is simple?", "text": "By simple, we mean a model that a human can already explain to some degree (e.g., a linear model or a shallow decision tree). If the answer to the previous question is affirmative, we would like to formally prove it, in order to get certifiable interpretability under reasonable assumptions. If, on the contrary, the answer is negative for some models, we think that this raises concern for the widespread use of such interpretability methods. Indeed, if a particular method fails to explain how a simple linear model predicted a value for a given example, how can we trust this same method to explain how a deep convolutional neural network predicted the class of an object in an image? We believe that there is a need for theoretical guarantees for interpretability. There should be some minimal proof of correctness for any interpretability method in order to trust the results thereof. For instance, showing that one recovers the important coefficients when the model to explain is linear, or that the algorithm is provably ignoring coordinates that are not used by the model to explain. This paper attempts to answer these questions in the case of a method called Local Interpretable Model-agnostic Explanations (LIME, Ribeiro et al., 2016). We will see that the answer to both of these questions is affirmative when LIME is used for tabular data with default settings.\nWithout giving too much details on the inner working of LIME (which we will do in Section 2), we want to briefly summarize how it operates. Essentially, given a black-box model f , in order to explain an example \u03be \u2208 Rd, LIME\n(1) creates perturbed examples xi;\n(2) gets the prediction of the black box model f at this examples;\n(3) weight the predictions with respect to the proximity between xi and \u03be;\n(4) trains a weighted interpretable model on these new examples.\nThe output of LIME is then the top coefficients of the interpretable model (if it is linear). What makes LIME really interesting and so popular is the use of interpretable features, namely discretized features. For instance, if the third feature is real-valued, instead of saying that \u201cfeature number 3 is important for the prediction f(\u03be),\u201d LIME indicates to the user that \u201cfeature 3 being between 1.5 and 7.8 is important.\u201d More precisely, LIME outputs a linear surrogate model, whose coefficients (the interpretable coefficients) are given as explanation to the user. These coefficients are our primary center of interest.\nContributions. In this paper, we restrict ourselves to tabular data (that is, data lying in Rd). We show the following:\n\u2013 Explicit expression for LIME\u2019s surrogate model. We show that when the surrogate model is obtained by ordinary least-squares and the number of perturbed samples is large, the interpretable coefficients obtained by LIME are close to a vector \u03b2f whose expression is explicit. This statement is true with high probability with respect to the sampling of perturbed examples.\n\u2013 Role of the hyperparameters. Leveraging the explicit expression of \u03b2f , we show several interesting properties of LIME. In particular, \u03b2f is linear in f , stable with respect to small perturbations of f , and only depends on the bins into which \u03be belongs. We also obtain the behavior of \u03b2f for small and large bandwidth (the main free parameter of the method).\n\u2013 Linear model. When f has some simple algebraic structure, we show how to compute \u03b2f in closed-form. In particular, when f is linear, we recover the main result of Garreau and von Luxburg (2020), but this time for the default weights, arbitrary bins, and arbitrary input parameters. We end up with essentially the same conclusion: the explanations provided by LIME are proportional to the coefficients of f along each coordinate.\n\u2013 Revealing artifacts. In this paper, we reach beyond the linear case, showing explicit results for general additive f and multiplicative f . The first case includes general linear models, whereas the latter encompass, for instance, indicator functions with rectangular domain and the Gaussian kernel. By linearity, we obtain a closed-form statement for partition-based regressor, for instance CART trees. In this last case, we leverage the theory to explain artifacts observed when explaining a CART tree with LIME.\nThe main difficulty in our analysis comes from the non-linear nature of the interpretable features (defined as indicator functions) and the complicated overall machinery of LIME. In contrast with our previous work (Garreau and von Luxburg, 2020), we managed to keep the analysis very close to the default implementation (found at https://github.com/ marcotcr/lime as of August 2020), the only visible price to pay being additional notation. As we will see, these become quite manageable in the simple cases that we consider. All our theoretical claim are validated experimentally and the code of all the experiments of the paper can be found at https://github.com/dgarreau/tabular_lime_theory.\nRelated work. LIME is a posthoc, local interpretability method. In other words, it provides explanations (i) \u201cafter the fact\u201d (the model is already trained), and (ii) for a specific example. We refer to the exhaustive review papers by Guidotti et al. (2018) (especially Section 7.2), and Adadi and Berrada (2018) for an overview of such methods. It seems that LIME has quickly become one of the most widely-used posthoc interpretability method. But besides the practical interest, LIME has also generated consequent academic attention, with many variations on the method being proposed in the last three years. For instance, the same set of authors later proposed Anchor-LIME (Ribeiro et al., 2018), also based on the production of perturbed examples, but producing simpler \u201cif-then\u201d rules as explanations. Further specializations of LIME were proposed, in specific settings, namely time series analysis (Mishra et al., 2017), and survival model analysis (Kovalev et al., 2020; Utkin et al., 2020). More recently, some attempts have been made to refine further the method in order to improve both the interpretability and the fidelity (the accuracy of the surrogate model) (LEDSNA, Shi et al., 2020).\nHowever, few papers looked into the theoretical analysis of the method. A notable exception is Lundberg and Lee (2017), which considers a variation on LIME called SHAP (SHapley Additive exPlanations). In this paper, three desirable properties for an additive feature interpretability method are proposed: (i) local accuracy (matching the predictions of the global model locally), (ii) missingness (missing features get a zero weight), and (iii) consistency. It is shown that the only possibility to satisfy the three properties is given by LIME with a specific choice of weights and a different sampling scheme. However, these weights coming from game theory (Shapley, 1953) are hard to compute. It is nevertheless a beautiful result, and their Corollary 1 is very close in spirit to our Corollary 18 below\u2014 though for another algorithm.\nThe closest work to the present paper is our previous work Garreau and von Luxburg (2020), which considers a modification of the LIME algorithm for tabular data. Namely, the interpretable components are chosen in a very specific way (the quantiles are those of a Gaussian distribution), and the parameters of the algorithm match the mean of this Gaussian. Moreover, the weights of the perturbed examples are not the weights used in the default implementation of LIME. In this setting, we showed in our previous work that the values of the interpretable coefficients stabilize towards some values which are attained in closed-form when the model to interpret is linear.\nOur analysis in the current paper goes far beyond the one of the previous conference paper. In particular, we consider (i) arbitrary discretization, (ii) general weights including the weights chosen by the default implementation, and (iii) non-linear models, including radial basis kernel function and indicator functions with rectangular support. Hence, our\nanalysis does not concern a considerably simplified version, but the LIME algorithm in its default implementation, and we can apply our more general results to much larger range of algorithms including kernel algorithms and random forests, for example. In the Appendix we explain how the present paper can recover our earlier analysis as a special case.\nSummary of the paper. In Section 2, we introduce LIME in the context of tabular data. From now on, we will refer to this version of LIME as Tabular LIME. Section 3 contains our main result, as well as a short discussion and an outline of the proof. The implications of the result for models having a nice algebraic structure are discussed further in Section 4. All the proofs and additional results are collected in Appendix."}, {"heading": "2. Tabular LIME", "text": "In this section we present Tabular LIME and fix the notation of the paper while doing so. Originally proposed for text and image data, the public implementation of LIME also has a version for tabular data. It is on this version that we focus in the present paper.\nWe will assume that all the features are continuous. Indeed, when there are some discrete features, the sampling scheme of Tabular LIME changes slightly and so would our analysis. Note that the default implementation discretizes the continuous features: this is the road we will follow."}, {"heading": "2.1 Quick overview of the algorithm", "text": "Regression. The core procedure of LIME is designed for regression: LIME aims to explain a real-valued function f : Rd \u2192 R at a fixed example \u03be \u2208 Rd. If one desires to apply LIME for classification, then one must use LIME for regression with f the likelihood\nof being in a given class, that is,\nf(x) = P (y(x) = c|\u03b61, . . . , \u03b6m) ,\nwhere \u03b61, . . . , \u03b6m \u2208 Rd is a training set. In this paper, we study Tabular LIME in the context of regression, with the undertone that it is possible to readily transpose our results for classification.\nAlgorithm 1 GetSummaryStatistics: Getting summary statistics from training data Require: Train set X = {\u03b61, . . . , \u03b6m}, number of bins p 1: for j = 1 to d do 2: for b = 0 to p do 3: qj,b \u2190 Quantile(\u03b61,j , . . . , \u03b6m,j ; b/p) . split each dimension into p bins 4: for b = 1 to p do 5: S \u2190 {\u03b6i,j s.t. qj,b\u22121 < \u03b6i,j \u2264 qj,b} . get the training data falling into bin b 6: \u00b5j,b \u2190 Mean(S) . compute the empirical mean 7: \u03c32j,b \u2190 Var(S) . compute the empirical variance 8: return qj,b, \u00b5j,b, and \u03c3 2 j,b for 1 \u2264 j \u2264 d and 1 \u2264 b \u2264 p\nGeneral overview. Let us now detail how Tabular LIME operates, a prerequisite to the analysis we aim to conduct. We begin with a high-level description of the algorithm. In the next section, we detail each step and fix additional notation.\n\u2013 Step 1: binning. First, Tabular LIME creates interpretable features by splitting each feature\u2019s input space in a fixed number of bins p. The idea is to have ranges, not only features, as outputs of the algorithm. As in Figure 1, we prefer to know that a low value of a parameter is important for the prediction, not only that the parameter itself is important. We explain the bin creation in more details in the next section and we refer to Algorithm 1 for additional details.\n\u2013 Step 2: sampling. Second, Tabular LIME samples perturbed examples x1, . . . , xn \u2208 Rd. For each new example, Tabular LIME samples a bin uniformly at random on each axis and then samples according to a truncated Gaussian whose parameters are given as input to the algorithm. When no training data is provided, one can also directly provide to Tabular LIME the coordinates of the bins and the mean and variance parameters for the sampling. The intuition is to try and mimic the distribution of the data, even though this data may not be available (remember that LIME aims to explain a black-box model f). We describe the sampling procedure in more details in the next section and we refer to Algorithm 2 for a synthetic view of the sampling procedure.\n\u2013 Step 3: surrogate model. Finally, a surrogate model is trained on the interpretable features, weighted by some positive weights \u03c0i depending on the distance between the xis and \u03be. The final product is a visualization of the most important coefficients if no feature selection mode is selected by the user. Algorithm 3 summarizes Tabular LIME, while Figure 1 presents a typical output.\nAlgorithm 2 Sample: Sampling a perturbed example Require: Bin boundaries qj,p, mean parameters \u00b5j,p, variance parameters \u03c3 2 j,p, bin indices\nof the example to explain, b?\n1: for j = 1 to d do 2: bj \u2190 SampleUniform({1, . . . , p}) . sample bin index 3: (q`, qu)\u2190 (qj,bj\u22121, qj,bj ) . get the bin boundaries 4: xj \u2190 SampleTruncGaussian(q`, qu, \u00b5j,b, \u03c32j,b) . sample a truncated Gaussian 5: zi,j \u2190 1bj=b?j . mark one if same box\n6: return x, z\nIn order to analyze Tabular LIME, we need to be more precise with regards to the operation of the algorithm. We now proceed to give more details about the sampling procedure (Section 2.2) and the training of the surrogate model (Section 2.3).\nAlgorithm 3 TabularLIME: Tabular LIME for regression, default implementation\nRequire: Black-box model f , number of new samples n, example to explain \u03be, positive bandwidth \u03bd, number of bins p, bin boundaries qj,b, means \u00b5j,b, variances \u03c3 2 j,b\n1: b? \u2190 BinIDs(\u03be, q) . get the bin indices of \u03be 2: for i = 1 to n do 3: xi, zi \u2190Sample(q, \u00b5, \u03c3, b?) 4: \u03c0i \u2190 exp ( \u2212\u20161\u2212zi\u20162\n2\u03bd2\n) . compute the weight\n5: \u03b2\u0302 \u2208 arg min\u03b2\u2208Rd+1 \u2211n\ni=1 \u03c0i(f(xi)\u2212 \u03b2>zi)2 + \u2126(\u03b2) . compute the surrogate model 6: return \u03b2\u0302"}, {"heading": "2.2 Sampling perturbed examples", "text": "In this section, we explain exactly how the sampling of perturbed examples introduced in Algorithm 2 operates.\nDefining the bins. The first step in Tabular LIME is to create interpretable features along each dimension j \u2208 {1, . . . , d}. This is achieved by dividing each dimension into p \u2265 2 bins. The intuition for this is added interpretability: instead of knowing that a coordinate is important for the prediction, Tabular LIME gives a range of values for the coordinate that is important for the prediction. We emphasize that p is constant across all dimensions 1 \u2264 j \u2264 d. This is the default behavior of Tabular LIME, although it can happen that pj < p if there are not enough data along an axis. We will assume throughout the paper that there are always enough data points if a training set is used, even though the current analysis can be extended to pj varying across dimensions. Note that if p = 1, there are no bins: zij = 1 for any i, j and the surrogate model is just the empirical mean of the f(xi)s.\nThe boundaries of the bins are an input of Tabular LIME (Algorithm 3). For each feature j \u2208 {1, . . . , d}, we denote these boundaries by\nqj,0 < qj,1 < \u00b7 \u00b7 \u00b7 < qj,p .\nThe products of these bins form a partition of S := \u220fd j=1[qj,0, qj,p]. In addition to the bins boundaries, Tabular LIME takes as input some mean and variance parameters for each bin along each dimension. We denotes these by (\u00b5j,b, \u03c3 2 j,b) \u2208 R \u00d7 R+. As we will see in the next paragraph, these parameters (bin boundaries, means, and standard deviation) are computed from a training set if provided.\nAlgorithm 4 BinID: Getting the bin indices of the instance to explain\nRequire: Example to explain \u03be, bin boundaries qj,b 1: for j = 1 to d do 2: for b = 1 to p do 3: if qj,b\u22121 < \u03bej < qj,b then . if \u03bej belongs to bin b 4: b?j = b . then b ? j = b\n5: break 6: return b?\nTraining data. In the default operation mode of Tabular LIME, a training set X = {\u03b61, . . . , \u03b6m} is given as an input to Tabular LIME. Note that this training set is not necessarily the training set used to train the black-box model f . In that case, the boundaries of the bins are obtained by considering the quantiles of X across each dimension. Intuitively, along each dimension j, we split the real line in p bins such that a proportion 1/p of the data falls into each bin along this axis. More formally, the boundaries are obtained by taking the quantiles of level b/p for b \u2208 {0, 1, . . . , p}. That is, the qj,b are such that\n\u22001 \u2264 j \u2264, \u22001 \u2264 b \u2264 p, 1 m m\u2211 i=1 1\u03b6i,j\u2208[qj,b\u22121,qj,b] \u2248 1 p .\nIn particular, for each 1 \u2264 j \u2264 d, qj,0 = min1\u2264i\u2264m \u03b6i,j and qj,p = max1\u2264i\u2264m \u03b6i,j . When a training set X is used, \u00b5j,b (resp. \u03c3j,b) corresponds to the mean (resp. the standard deviation) of the training data on the bin b along dimension j. We refer to Figure 2 for a visual depiction of this process.\nAssuming that the example to explain \u03be belongs to S, then each \u03bej falls into a bin b?j along coordinate j. The (straightforward) computation of b? is given by Algorithm 4. We will see that b? is an important quantity: the d-dimensional bin containing \u03be plays a special role in our results.\nWe note that Garreau and von Luxburg (2020) consider the case where \u00b5j,b = \u00b5j and \u03c3j,b = \u03c3j for any b as simplifying assumptions. In addition, the bins boundaries qj,b were chosen to be the exact Gaussian quantiles. We do not make such assumptions here.\nSampling scheme. The next step is the sampling of n perturbed examples x1, . . . , xn \u2208 Rd. First, along each dimension, Tabular LIME samples the bin indices of the perturbed samples. We write this sample as a matrix B \u2208 Rn\u00d7d, where bij corresponds to the bin index of example i along dimension j. In the current implementation of Tabular LIME, the bi,js are i.i.d. distributed uniformly on {1, . . . , p}.\nThe bin indices bi,js are subsequently used in two ways. On one hand, Tabular LIME creates the binary features based on these bin indices. Formally, we define zi,j = 1bi,j=b?j ,\nTabular LIME sampling scheme\nthat is, we mark one if bi,j is the same bin as the one \u03bej (the jth coordinate of the example \u03be) falls into. We collect these binary features in a matrix Z \u2208 Rn\u00d7(d+1) defined as\nZ :=  1 z11 z12 . . . z1d 1 z21 z22 . . . z2d ... ... ... ...\n1 zn1 zn2 . . . znd  . On the other hand, the bin indices are used to sample the new examples x1, . . . , xn. Let 1 \u2264 i \u2264 n, the new sample xi is sampled independently dimension by dimension in the following way: xi,j is distributed according to a truncated Gaussian random variable with parameters qj,bij\u22121, qj,bi,j , \u00b5j,bi,j , and \u03c3 2 j,bi,j\n. More precisely, xi,j conditionally to {bi,j = b} has a density given by\n\u03c1j,b(t) := 1\n\u03c3j,b \u221a 2\u03c0 \u00b7\nexp\n( \u2212(t\u2212\u00b5jb)2\n2\u03c32j,b ) \u03a6(rj,b)\u2212 \u03a6(`j,b) 1t\u2208[qj,b\u22121,qj,b] , (1)\nwhere we set `j,b := qj,b\u22121\u2212\u00b5j,b\n\u03c3j,b and rj,b := qj,b\u2212\u00b5j,b \u03c3j,b , and \u03a6 is the cumulative distribution\nfunction of a standard Gaussian random variable. We denote by TN(\u00b5, \u03c32, `, r) the law of a truncated Gaussian random variable with mean parameter \u00b5, scale parameter \u03c3, left and right boundaries ` and r. Note that the means (resp. standard deviations) of these truncated random variables are generally different from the input means (resp. standard deviations). We denote by \u00b5\u0303j,b (resp. \u03c3\u0303j,b) the mean (resp. standard deviation) of a TN(\u00b5j,b, \u03c3 2 j,b, qj,b\u22121, qj,b) random variable. We refer to Figure 2 for an illustration.\nIt is important to understand that the sampling of the new examples does not depend on \u03be, but rather on the bin indices of \u03be. Therefore, any two given instances to explain will lead to the same sampling scheme provided that they fall into the same d-dimensional bin."}, {"heading": "2.3 Surrogate model", "text": "We now focus on the training of the surrogate model. As announced in Algorithm 3, the new samples receive positive weights given by\n\u03c0i := exp\n( \u2212\u20161\u2212 zi\u20162\n2\u03bd2\n) = exp \u22121 2\u03bd2 d\u2211 j=1 1bi,j 6=b?j  , (2) where \u2016\u00b7\u2016 is the Euclidean norm, and \u03bd is a positive bandwidth parameter. Intuitively, this weighting scheme counts in how many coordinates the bin of the perturbed sample differs from the bin of the example to explain and then applies an exponential scaling. If all the bins are the same (the perturbed sample falls into the same hyperrectangle as \u03be), then the weight is 1. On the other hand, if the perturbed sample is \u201cfar away\u201d from \u03be, \u03c0i can be quite small (relatively to \u03bd). Here, far away means that xi does not fall into the same bins as \u03be: it can be the case that xi is close in Euclidean distance to \u03be. In the\ndefault implementation of Tabular LIME, the bandwidth parameter \u03bd is fixed to \u221a\n0.75d, but we do not make this assumption and work with arbitrary positive \u03bd. Our main result is actually true for more general weights, see Appendix A for their definition. In particular, this generalization includes the weights studied in Garreau and von Luxburg (2020) as a special case.\nThe local surrogate model of LIME is then obtained by optimizing a regularized, weighted square loss\n\u03b2\u0302n \u2208 arg min \u03b2\u2208Rd+1 { n\u2211 i=1 \u03c0i(f(xi)\u2212 \u03b2>zi)2 + \u2126(\u03b2) } , (3)\nwhere zi is the ith row of Z. The coefficients of the surrogate model, collected in \u03b2\u0302n, are the central output of Tabular LIME and will be our main focus of interest. We often refer to the coordinates of \u03b2\u0302n as the interpretable coefficients.\nIn the default implementation, \u2126(\u03b2) = \u03bb \u2016\u03b2\u20162, where \u2016\u00b7\u2016 is the Euclidean norm and \u03bb is a positive regularization parameter. That is, Tabular LIME is using ridge regression (Hoerl and Kennard, 1970) to obtain the surrogate model. But the default choice of hyperparameters makes the surrogate models obtained by ridge in this setting indistinguishable from one obtained by ordinary least-squares. More precisely, Tabular LIME is using the scikit-learn default implementation of ridge, which has a penalty constant equal to one (that is, \u03bb = 1). For a large choice of bandwidth, the weights \u03c0i are close to 1 and the \u2016y \u2212 ZW\u03b2\u20162 term in Eq. (3) is O (n) and dominates unless the penalty constant is at least of the same order. But the default n is equal to 5000 and we investigate the limit in large sample size (meaning that n is always of order 103 in our experiments), thus there is virtually no difference between taking ordinary least-squares and ridge. Therefore, even though our analysis is true only for ordinary least-squares (\u03bb = 0), we recover the results of the default implementation for all reasonable bandwidth choices. As a consequence, unless otherwise mentioned, all the experiments in the paper are done with the default regularization (\u03bb = 1). For smaller bandwidth parameters, the weights are closer to 0 and the regularization begins to play a non-negligible role. We demonstrate the effects of regularization in Figure 3.\nFeature selection. In the final step of Tabular LIME, the user is presented with a visualization of the largest coefficients of the surrogate model. Note that some feature selection mode can be used before this last step. That is, the final output of Tabular LIME is not the given of all coefficients of the surrogate model. However, by default, when the dimension is strictly greater than 6, no feature selection is used and the user is presented with the top 5 interpretable coefficients, as in Figure 1. Therefore we do not consider feature selection in the present work. We will report all the interpretable coefficients since there is randomness in the ranking of the coefficients due to the randomness of the sampling. Note also that because of this randomness in the construction of \u03b2\u0302 via the sampling of the perturbed examples x1, . . . , xn, we will always report the result of several runs of Tabular LIME on any given example (usually 100), see Figure 4.\nLet us summarize the implementation choices for Tabular LIME that we consider in our analysis. The d features are considered to be continuous and are discretized (the choice discretize continuous=True is default) along p bins. The default choice is p = 4 (discretizer=\u2018quartile\u2019 is default), we will sometimes use another value for p. The bin\nboundaries qj,b as well as the location and scale parameter for the sampling \u00b5j,b and \u03c3j,b are arbitrary (computed from the appropriate dataset unless otherwise mentioned). We consider default weights given by Eq. (10), with prescribed bandwidth \u03bd (kernel width=nu is not default) and the surrogate model is obtained by ridge regression with regularization parameter \u03bb = 1. As to feature selection, we do not consider any (feature selection=\u2018none\u2019). The free parameters of the method are the number of bins on each dimension p and the bandwidth \u03bd: we will focus mostly on them."}, {"heading": "3. Main result: explicit expression for \u03b2\u0302n in the large sample size", "text": "In this section, we state our main result. Recall that \u03b2\u0302n is the random vector containing the coefficients of the surrogate linear model given by Tabular LIME for a given model f and example \u03be. In a nutshell, when the number of new samples n is large, \u03b2\u0302n concentrates around a vector \u03b2, for which we provide an explicit expression. This explicit expression depends on f and \u03be (via b?), as well as the parameter of Tabular LIME introduced in the previous section (the bandwidth \u03bd, the number of bins p, as well as the bins boundaries q and the mean and standard deviation on each bin \u00b5 and \u03c3). Since \u03be and the algorithm parameters are fixed, we only emphasize the dependency in f (which will vary from section to section) and we write \u03b2f . We state this result in Section 3.1 and present some immediate consequences in Section 3.2. We then present a brief outline of the proof in Section 3.3."}, {"heading": "3.1 Explicit expression for \u03b2\u0302n in the large sample size", "text": "In order to make our result precise, in particular to define the vector \u03b2f , we need to introduce further notation.\nRecall that p is the fixed number of bins along each dimension and \u03bd > 0 is the bandwidth parameter for the weights are the main free parameters of Tabular LIME. A key normalization constant in our computations is given by\nc := 1\np +\n( 1\u2212 1\np\n) e \u22121 2\u03bd2 . (4)\nWe will also denote by x \u2208 R a random variable that has the same law as the xis (recall that they are i.i.d. random variables by construction). To this x is associated a vector of bin indices b \u2208 {1, . . . , p}d, binary features z \u2208 {0, 1}d and a weight \u03c0 \u2208 R+ in the same way xi was associated to bi, zi, and \u03c0i. All expectations in the following are taken with respect to the random variable x.\nWe are now armed with enough notation to state our main result.\nTheorem 1 (Explicit expression for \u03b2\u0302n in the large sample size). Assume that Tabular LIME operates with the default weights (Eq. (2)) and fits the surrogate model with ordinary least squares (\u2126 = 0 in Eq. (3)). Set \u03b5 > 0 and \u03b7 \u2208 [0, 1]. Suppose that the function to explain f is bounded by a positive constant M on S = \u220f j [qj,0, qj,p]. For any example \u03be \u2208 S for which we want to create an explanation, define \u03b2f \u2208 Rd+1 such that\n\u03b2f0 := c \u2212d (1 + d pc\u2212 1 ) E [\u03c0f(x)]\u2212 pc pc\u2212 1 d\u2211 j=1 E [\u03c0zjf(x)]  , (5)\nand, for any 1 \u2264 j \u2264 d,\n\u03b2fj := c \u2212d ( \u2212pc pc\u2212 1 E [\u03c0f(x)] + p2c2 pc\u2212 1 E [\u03c0zjf(x)] ) . (6)\nThen, for every\nn \u2265 max 4608Md 2(d+ p)2 e 1 \u03bd2 log 8d\u03b7 \u03b52c2d , 10368d3(p+ d)4M2 e 2 \u03bd2 log 8d\u03b7 \u03b52c4d  , we have P(\u2016\u03b2\u0302n \u2212 \u03b2f\u2016 \u2265 \u03b5) \u2264 \u03b7.\nStrictly speaking, Tabular LIME only provides the interpretable coefficients \u03b2fj for 1 \u2264 j \u2264 d. We nonetheless provide the intercept \u03b2f0 for completeness\u2019 sake. Intuitively, Theorem 1 states that:\n\u2013 if the number of perturbed samples is large enough, the explanations provided by Tabular LIME for any f at a given example \u03be stabilize around a fixed value, \u03b2f ;\n\u2013 this \u03b2f has an explicit expression, simple enough that we can hope to use it in order to answer the question we asked in the introduction.\nIn particular, for reasonably large n, we can focus on \u03b2f in order to gain insight on the explanations provided by Tabular LIME with default settings. This will be our agenda in Section 4, where we will assume specific structures for f . For the time being, let us try to understand what Theorem 1 implies for Tabular LIME.\nLet us assume for a moment that c = 1 and that \u03c0 = 1 almost surely (we will see in Section 3.2.3 why this is justified in certain cases). Notice that, by definition of zj , E [zjf(x)] = 1pE [ f(x) \u2223\u2223\u2223bj = b?j]. Thus, for any 1 \u2264 j \u2264 d, the interpretable coefficients take the simple expression\n\u03b2fj = p p\u2212 1 ( E [ f(x) \u2223\u2223bj = b?j]\u2212 E [f(x)]) . (7) Up to constants, \u03b2fj = E [ f(x)\n\u2223\u2223\u2223bj = b?j]. In other words, \u03b2fj is the expected value of the model conditioned to xj falling into the same bin as \u03bej . Intuitively, \u03b2 f j is high (resp. low) if f takes larger (resp. smaller) than average values on the d-dimensional bins above \u03bej . The general picture is of course more complicated, since we ignored completely the role of \u03bd and p in this discussion.\nIt is remarkable that Theorem 1 analyzes Tabular LIME as is, the only difference with the default implementation being \u2126 = 0. Moreover, Theorem 1 is true under pretty mild assumptions. Essentially, we only require f to be bounded on the bins. If these bins are computed from a finite training set X , then S is compact and we are essentially requiring that f be well-defined on S, which is virtually always the case for most machine learning models. An important assumption that can be easily overlooked is that \u03be should lie in S = \u220f j [qj,0, qj,p] for the theorem to hold. In particular, Theorem 1 does not say anything\nabout examples to explain that do not belong to the bins provided to the algorithm. We leave the analysis of Tabular LIME explanations for \u03be /\u2208 S to future work.\nThe strongest limitation of Theorem 1 is the poor concentration for small bandwidths. Indeed, for small \u03bd (say \u03bd < 1), according to Theorem 1, one has to take n polynomial in d in order to witness the convergence. This usually means that it is quite hard to use Theorem 1 in practice for \u03bd < 1 as soon as the dimension is greater than 10. Moreover, in that case, the effect of regularization start to come into play (see the discussion in Section 2.3). However, the default bandwidth is \u221a 0.75d 1 as soon as d \u2248 10, and Theorem 1 is quite satisfactory to study LIME in its normal use."}, {"heading": "3.2 Direct consequences: properties of LIME that can be deduced from the explicit expression", "text": "We now present some additional consequences of Theorem 1, which are true without assuming anything on f other than boundedness."}, {"heading": "3.2.1 Linearity of explanations", "text": "We first notice that the vector \u03b2f depends linearly on f . Indeed, a careful reading of Eqs. (5) and (6) reveals that \u03b2f depends on f only through the expectations E [\u03c0f(x)] and E [\u03c0zjf(x)]. Since \u03c0 and z do not depend on f , by linearity of the expectation, we see that\n\u03b2f+g = \u03b2f + \u03b2g .\nThis fundamental remark has two consequences.\nFirst, we will soon specialize Theorem 1 to more explicit models f , in order to answer to our main question. Quite a number of models can be written in an additive form (think of a generalized additive model, a kernel regressor, or a random forest). Linearity will allow us to focus on the building bricks of these models (a kernel function or an indicator function). We give a simple example of this property in Figure 5.\nSecond, let us assume that our knowledge of f is imperfect. More precisely, let us split the function to explain in two parts: (i) the part coming from the black-box model f by itself, and (ii) the part coming from small perturbations such as numerical errors or measurement noise. Linearity allows us to focus on the perturbation part separately, and prove the following:\nProposition 2 (Stability of the explanations given by Tabular LIME). Suppose that \u03be \u2208 S. Consider f and g two functions that are bounded on S. Then, under the assumptions of Theorem 1,\n\u2016\u03b2f \u2212 \u03b2g\u2016 \u2264 \u221a d(9d+ 4p2) e 1 2\u03bd2\np\u2212 1 \u2016f \u2212 g\u2016\u221e .\nAs expected, small perturbations of the function to explain do not perturb the explanations too much, which is a desirable property. Proposition 2 is proven in Appendix E."}, {"heading": "3.2.2 Explanations only depend on the bin indices of \u03be", "text": "The interpretable coefficients \u03b2fj depend only on the bin indices b ? j of the example \u03be. Indeed, Eqs. (5) and (6) reveal that only the sampling of x depends on the actual coordinates of \u03be. But if we recall Section 2.2, this sampling only depends on the bin indices of \u03be. Therefore, Tabular LIME provides the same explanation for any two instances falling in the same bins along each dimension, up to some noise coming from the sampling procedure. See Figure 6 for an illustration of this phenomenon. In a sense, this behavior gives a certain stability to the explanations provided by Tabular LIME: if two examples to explain \u03be and \u03be\u2032 are very close, they are likely to have the same bin indices, and therefore the same \u03b2f . On the other hand, if \u03be and \u03be\u2032 are close but do not have the same bin indices, the explanations are likely to be quite different. This could be an explanation for the instability of explanations observed by Alvarez-Melis and Jaakkola (2018).\nIn particular, in the general case, the value of the surrogate model at \u03be cannot be the same as f(\u03be). The local accuracy property of Lundberg and Lee (2017) is thus not satisfied in the general case by Tabular LIME."}, {"heading": "3.2.3 Dependency on the bandwidth parameter \u03bd", "text": "Suppose that the bandwidth is large, that is, \u03bd \u2192 +\u221e. In that case, it is clear from their definitions that c\u2192 1 and \u03c0i \u2192 1 almost surely. By dominated convergence, Eq. (5) and (6) imply that\n\u03b2f0 \u2212\u2192 ( 1 + d\np\u2212 1\n) E [f(x)]\u2212 p\np\u2212 1 d\u2211 j=1 E [zjf(x)] , (8)\nand, for any 1 \u2264 j \u2264 d,\n\u03b2fj \u2212\u2192 \u2212p p\u2212 1 E [f(x)] + p2 p\u2212 1 E [zjf(x)] . (9)\nWe show this convergence phenomenon in Figure 7. In cases where the bandwidth choice of the default implementation \u03bd = \u221a 0.75d is large, this approximation is well-satisfied. In\nfact, the bandwidth parameter then becomes redundant: it is equivalent to give weight 1 to every perturbed sample.\nOn the other hand, when \u03bd \u2192 0, it is straightforward to show that \u03b2fj \u2192 0 for any 1 \u2264 j \u2264 d. In between these two extremes, the behavior of the interpretable coefficients can be pretty unpredictable. As demonstrated in Figure 7, the interpretable coefficients can even cancel for positive values of \u03bd. This is a worrying phenomenon: for some values of the bandwidth \u03bd, the explanation provided by Tabular LIME is negative, while it becomes positive for other choices. In the first case, the trusting user of Tabular LIME would grant a positive influence for the parameter, in contrast to a negative influence in the second case. This is not only a theoretical worry: if the values of these interpretable coefficients are large enough, they may be ranked amongst the top five usually displayed to the user using the default settings.1"}, {"heading": "3.3 Outline of the proof of Theorem 1", "text": "Before turning to specializations of Theorem 1 to specific classes of models, we provide a short outline of the proof (the complete proof is provided in appendix).\nSince we restrict our analysis to \u2126 = 0, Eq. (3) becomes a simple weighted least-squares problem. Let us collect the weights \u03c0i in a diagonal matrix W \u2208 Rn\u00d7n given by\nW :=  \u03c01 0 . . . 0 0 \u03c02 . . . ... ... . . . . . . 0\n0 \u00b7 \u00b7 \u00b7 0 \u03c0n  . 1. We are not the first to point out the critical role of \u03bd, see for instance https://christophm.github.io/\ninterpretable-ml-book/lime.html\nwhen \u03bd \u2192 +\u221e. We also note that some values of the bandwidth can cancel the interpretable coefficient (here around \u03bd = 1). The vertical red line marks the default choice for \u03bd, given by \u221a 0.75d.\nWe also define y \u2208 Rn coordinate-wise by yi := f(xi). Then the solution of Eq. (3) is given in closed-form by\n\u03b2\u0302n = ( Z>WZ )\u22121 Z>Wy .\nLet us set \u03a3\u0302n := 1 nZ >WZ and \u0393\u0302n := 1 nZ >Wy and notice that \u03b2\u0302n = \u03a3\u0302 \u22121 n \u0393\u0302n. Elementary computations show that both \u03a3\u0302n and \u0393\u0302n can be written as empirical averages of expressions depending on the perturbed samples xi. Since the sampling of the xis is i.i.d., the weak law of large numbers guarantees that\n\u03a3\u0302n P\u2212\u2192 \u03a3 and \u0393\u0302n P\u2212\u2192 \u0393f ,\nwhere we defined \u03a3 := E[\u03a3\u0302n] and \u0393f := E[\u0393\u0302n]. These quantities are given by Proposition 10 and Eq. (24). Since \u03a3 is invertible in closed-form for a large class of weights (Proposition 11), we can set \u03b2f := \u03a3\u22121\u0393f and proceed to show that \u03b2\u0302n is close from \u03b2\nf in probability, whose expression is given by Corollary 16. A prerequisite to the concentration of \u03b2\u0302n are the concentration of \u03a3\u0302n (Proposition 22) and the concentration of \u0393\u0302n (Proposition 24). Together with the control of \u2225\u2225\u03a3\u22121\u2225\u2225 op\n(Proposition 12), a binding lemma (Lemma 27) allows us to put everything together in Appendix I."}, {"heading": "4. Special cases of Theorem 1", "text": "We now specialize Theorem 1 in three situations where f has some additional structure, allowing us to go further in the computation of \u03b2f . Namely, we will assume that f is an additive function over the coordinates (Section 4.1), multiplicative along the coordinates (Section 4.2), and finally a function depending only on a subset of the coordinates (Section 4.3). Our goal is to show how to use Theorem 1 in specific cases and how to gain insights into the explanations provided by Tabular LIME in these cases.\nBefore being able to complete this program, we need to introduce some additional notation. In order to use Theorem 1 for models with additional structure, we will need to make the expectations computations more explicit. Recall that we set x the random variable corresponding to the sampling of the perturbed examples (see Section 2.2). For any \u03c8 : R\u2192 R, we now introduce the notation\n\u22001 \u2264 j \u2264 d, \u22001 \u2264 b \u2264 p, e\u03c8j,b := E [ e \u2212(1\u2212zj) 2 2\u03bd2 \u03c8(xj) \u2223\u2223\u2223\u2223bj = b] . (10) When \u03c8 = 1, we just write ej,b instead of e 1 j,b, and when \u03c8 = id, we write e \u00d7 j,b.\nEven though this can look cumbersome, this notation is quite helpful. The reason is quite simple: whenever we need to compute an expectation with respect to x, we will use the law of total expectation with respect to the events {bj = b} for b \u2208 {1, . . . , p}. The idea is to \u201ccut\u201d the expectation depending on which bins x falls into on each dimension.\nFor any 1 \u2264 j \u2264 d, we also define the constants\nc\u03c8j := 1\np p\u2211 b=1 e\u03c8j,b ,\nthe average value of the e\u03c8j,b coefficients over all the bins for a given feature. Finally, we set C := \u220fd j=1 cj . They can be seen as a generalization of c ad C, the normalization constants encountered in Theorem 1.\nFirst computations. In the default implementation of Tabular LIME, if b = b?j , then zj = 1, and 0 otherwise. Thus the computation of ej,b is straightforward in this case:\nej,b =\n{ 1 if b = b?j\ne \u22121 2\u03bd2 otherwise.\nThe expression of cj := c 1 j is also quite simple. In particular, cj does not depend on j and we find\n\u22001 \u2264 j \u2264 d, cj = c := 1\np +\n( 1\u2212 1\np\n) e \u22121 2\u03bd2 ,\nrecovering the expression of c given in Section 3. As a consequence, note that C = cd. We now have all the required tools to specialize Theorem 1 in practical cases."}, {"heading": "4.1 General additive models, including linear functions", "text": "In this section, we consider functions that can be written as a sum of functions where each function depends only on one coordinate. Namely, we make the following assumption on f :\nAssumption 1 (Additive function). We say that f : Rd \u2192 R is additive if there exist arbitrary functions f1, . . . , fd : Rd \u2192 R such that, for any x \u2208 Rd,\nf(x) = d\u2211 j=1 fj(xj) .\nGeneral additive models generalize linear models, where fj : x 7\u2192 fj \u00b7x for any 1 \u2264 j \u2264 d. They were popularized by Stone (1985) and Hastie and Tibshirani (1990). We refer to Chapter 9 in Hastie et al. (2001) for an introduction to general additive models, and in particular Section 9.1.1 regarding the training thereof. If f is a general additive model, we can specialize Theorem 1, and examine the explanation provided by Tabular LIME in this case. Rather than giving the concentration result (which remains unchanged), we focus directly on \u03b2f .\nProposition 3 (Computation of \u03b2f for an additive function). Assume that f satisfies Assumption 1 and that the assumptions of Theorem 1 are satisfied (in particular, each fj is bounded on [qj,0, qj,p]). Set \u03be \u2208 S. Then Theorem 1 holds and \u03b2f \u2208 Rd+1 is given by\n\u03b2f0 = 1\npc\u2212 1 d\u2211 k=1 \u2211 b6=b?k efkk,b ,\nand, for any 1 \u2264 j \u2264 d,\n\u03b2fj = pc\npc\u2212 1 ( e fj j,b?j \u2212 c fj j c ) .\nThe proof of Proposition 3 is a direct consequence of Theorem 1 and Proposition 17. Proposition 3 has several interesting consequences.\nSplitting the coordinates. A careful reading of the expression of \u03b2f in Proposition 3 reveals that the j-th interpretable coefficient \u03b2fj depends only on fj , the part of the function depending on the j-th coordinate. In other words, Tabular LIME splits the explanations coordinate by coordinate for general additive models. This property is desirable in our opinion. Indeed, since our model f depends on the j-th coordinate only through the function fj , then fj alone should be involved on the part of the explanation which is concerned by j.\nIgnoring unused coordinates. Suppose for a moment that f is additive but does not depend on coordinate j at all. That is, fj(x) = \u03ba for any x, where \u03ba is a constant. Then, by linearity of the conditional expectation, e fj j,b = \u03baej,b for any b. By definition of the normalization constant c fj j , we have c fj j = \u03bac. Therefore\ne fj j,b?j \u2212 c fj j c = 0 ,\nand we deduce immediately that \u03b2fj = 0. In other words, for an additive f , Tabular LIME provably ignores unused coordinates. This is also a property that one could reasonably expect from any interpretability algorithm. We show that this property also holds for more general weights in the Appendix (see Proposition 17)."}, {"heading": "4.1.1 Linear functions", "text": "We can be even more precise in the case of linear functions. In this case, the functions fj are defined, with a slight abuse of notation, as fj(x) = fj \u00b7 x with fj \u2208 R for any 1 \u2264 j \u2264 d. Recall that, for any 1 \u2264 j \u2264 d and 1 \u2264 b \u2264 p, we defined \u00b5\u0303j,b as the mean of the random variable TN(\u00b5j,b, \u03c3j,b, qj,b\u22121, qj,b).\nCorollary 4 (Computation of \u03b2f , linear f). Assume that f is linear. Let us set \u03be \u2208 S and define\n\u02dc\u0303\u00b5j := \u00b5\u0303j,b?j +\n\u2211 b 6=b?j e \u22121 2\u03bd2 \u00b5\u0303j,b\n1 + (p\u2212 1) e \u22121 2\u03bd2\n, (11)\nthe weighted average of the \u00b5\u0303j,b across dimension j. Then Theorem 1 holds and \u03b2 f \u2208 Rd+1 is given by\n\u03b2f0 = f( \u02dc\u0303\u00b5)\u2212 1\npc\u2212 1 d\u2211 j=1 (\u00b5\u0303j,b?j \u2212 \u02dc\u0303\u00b5j)fj ,\nand, for any 1 \u2264 j \u2264 d,\n\u03b2fj = fj\np\u2212 1 p\u2211 b=1 (\u00b5\u0303j,b?j \u2212 \u00b5\u0303j,b) . (12)\nThe proof of Corollary 4 is a direct application of Proposition 3. We now present this proof, in order to show how to use Theorem 1 in specific cases.\nProof. Reading Proposition 3, we need to compute e fj j,b for all 1 \u2264 j \u2264 d and 1 \u2264 b \u2264 p. By linearity, all that is required is the computation of e\u00d7j,b, which is equal to e \u00d7 j,b = \u00b5\u0303j,b if b = b?j and e \u22121 2\u03bd2 \u00b5\u0303j,b otherwise by definition of \u00b5\u0303. We deduce that c \u00d7 j = \u00b5\u0303j,b?j + \u2211 b6=b?j \u00b5\u0303j,b, and \u02dc\u0303\u00b5j = c fj j c by definition of \u02dc\u0303\u00b5 (Eq. (11)). Following Proposition 3, we obtain\n\u03b2f0 = f( \u02dc\u0303\u00b5)\u2212 1\npc\u2212 1 d\u2211 j=1 (\u00b5\u0303j,b?j \u2212 \u02dc\u0303\u00b5j)fj ,\nand, for any 1 \u2264 j \u2264 d, \u03b2fj = pc\npc\u2212 1 (\u00b5\u0303j,b?j \u2212 \u02dc\u0303\u00b5j)fj .\nWe simplify further this last display using\n\u02dc\u0303\u00b5j \u2212 \u00b5\u0303j,b?j = \u00b5\u0303j,b?j +\n\u2211 b 6=b?j e \u22121 2\u03bd2 \u00b5\u0303j,b\n1 + (p\u2212 1) e \u22121 2\u03bd2\n\u2212 \u00b5\u0303j,b?j (definition of \u02dc\u0303\u00b5)\n=\n\u2211 b 6=b?j e \u22121 2\u03bd2 (\u00b5\u0303j,b \u2212 \u00b5\u0303j,b?j )\n1 + (p\u2212 1) e \u22121 2\u03bd2\n.\nThe result follows after some algebra.\nIntuitively, Corollary 4 tells us that \u03b2fj is high if fj is high, but with a weight depending on the discretization scheme. This weight is the difference between the mean of xj on the bin containing \u03bej and the average value of the means of xj on all the other bins. This corresponds to the intuition given after Theorem 1. We demonstrate in Figure 8 the accuracy of our theoretical predictions.\nAs in Garreau and von Luxburg (2020), we recover a linear dependency in the fjs: for a linear model, the interpretable coefficient along dimension j is proportional to the coefficient of the linear model. This is also, in our opinion, a nice property of any interpretable algorithms: since a linear model is already interpretable to some extent, the interpretable version thereof should coincide up to constants.\nHowever, as noted in Garreau and von Luxburg (2020), the proportionality coefficient can be zero in some cases, leading to cancellation of the jth interpretable component in the explanation whatever the value of fj may be. In particular, a wrong choice of p can achieve this cancellation. We demonstrate this phenomenon in Figure 9. This cancellation is not a good property. One would expect the choice of hyperparameters to not be so brittle with respect to a change in p. The bins created from a training set are not that different if the value of p changes from one unit, and therefore the explanations should not differ entirely.\nLet us now explain what is happening in Figure 9. We obtained the expression of \u03b2fj in Eq. (12). If 1p\u22121 \u2211p b=1(\u00b5\u0303j,b?j \u2212 \u00b5\u0303j,b) = 0, then \u03b2 f j = 0, whatever the value of fj may be. We can rewrite this condition as\n\u00b5\u0303j,b?j = 1 p\u2212 1 \u2211 b 6=b?j \u00b5\u0303j,b .\nIntuitively, along dimension j, if the means on the boxes balance the mean on the special box, then \u03b2fj vanishes. In the experiment depicted in Figure 9, we considered a uniformly distributed training set on [\u221210, 10]. Thus the means \u00b5j,b are evenly distributed across [\u2212 \u2212 10, 10], as well as the modified means \u00b5\u0303j,b. Thus, if we consider a \u03be in a central position, the following happens:\n\u2013 if p is even, then \u00b5\u0303j,b?j and 1 p\u22121 \u2211 b 6=b?j \u00b5\u0303j,b are far away (top panel of Figure 9);\n\u2013 if p is odd, then \u00b5\u0303j,b?j is in a central position and approximately equal to 1 p\u22121 \u2211 b6=b?j \u00b5\u0303j,b.\nThe corresponding coefficient vanishes (bottom panel of Figure 9).\nIt is interesting to note, however, that it is not possible to cancel out the interpretable coefficient by a clever choice of bandwidth in Eq.(12), contrarily to what is done in Garreau and von Luxburg (2020). In fact, the magnitude of the explanations does not depend on the bandwidth at all. This seems due to the specific setting considered in Garreau and von Luxburg (2020), especially choosing \u00b5j,b and \u03c3j,b not depending on b."}, {"heading": "4.2 Multiplicative models, including indicator functions and Gaussian kernels", "text": "In this section, we turn to the study of functions f that can be written as a product of functions where each term depends only on one coordinate. Namely, we now make the following assumption on f :\nAssumption 2 (Multiplicative function). We say that f is multiplicative if there exist functions f1, . . . , fd : R\u2192 R such that\n\u2200x \u2208 Rd, f(x) = d\u220f j=1 fj(xj) .\nIn this case, as promised, we also can be more explicit in the statement of Theorem 1. In the previous section, we only report the value of \u03b2f , since the concentration result remains unchanged.\nProposition 5 (Computation of \u03b2f for a multiplicative function). Assume that f satisfies Assumption 2 and suppose that the assumptions of Theorem 1 hold (in particular, for each 1 \u2264 j \u2264 d, fj is bounded on [qj,0, qj,p]). Set \u03be \u2208 S. Then Theorem 1 holds and \u03b2f \u2208 Rd+1 is given by\n\u03b2f0 =\n\u220fd k=1 c fk k\nC\n1 + d\u2211 j=1 1 pc\u2212 1 ( 1\u2212 efjj,b?j \u00b7 c c fj j ) , and, for any 1 \u2264 j \u2264 d,\n\u03b2fj =\n\u220fd k=1 c fk k\nC \u00b7 pc pc\u2212 1 ( e fj j,b?j \u00b7 c c fj j \u2212 1 ) .\nProposition 5 is a consequence of Lemma 19 in the Appendix (which is true for more general weights).\nAs in the additive case, we can see that (i) fj only comes into play for the value of \u03b2 f j , up to a multiplicative constant common to all interpretable coefficients; (ii) unused coordinates are ignored in the explanation. Let us give a bit more details regarding the second point and let us assume that for a certain index j, the function f does not depend on the jth coordinate. In other words, fj(x) = \u03ba for any x \u2208 R where \u03ba is a constant. Then, by definition, e\nfj j,b?j = \u03baej,b?j and c fj j = \u03bacj . It follows from Proposition 5 that \u03b2 f j = 0. For a\nmultiplicative f , Tabular LIME provably ignores unused coordinates. This is also true for more general weights (see Lemma 19 in the Appendix).\nWe now give two fundamental examples in which multiplicative functions are the building block of the model: tree regressors when with splits parrallel to the axis and kernel methods using the Gaussian kernel."}, {"heading": "4.2.1 Indicator functions and partition-based models", "text": "Partition-based models split the feature space and then fit a simple model on each element of the partition. Simply put, once trained, such models outputs predictions according to\nf(x) = \u2211 A\u2208P f(A)1x\u2208A , (13)\nwhere we set f(A) the value of f on A. In particular, these partition-based rules include treebased methods (see Section 9.2 in Hastie et al. (2001) for an introduction). Random trees are the building bricks of very popular regression algorithms such as random forests (Breiman, 2001), which are considered as one of the most successful general-purpose algorithms in modern-times (Biau and Scornet, 2016). In this section, we want to answer the following question: Do the explanations provided by Tabular LIME make sense when f is a partitionbased model? We will focus on, CART trees (Breiman et al., 1984), but most of the discussion remains true if the elements of P are rectangles.\nWithout loss of generality, we will assume from now on that A is included into the bins of Tabular LIME onto each dimension. Indeed, instead of P, one can always consider the partition formed by the intersection between P and the LIME grid. Formally, for any 1 \u2264 j \u2264 d, if we call Aj the projection of A in the j-th dimension, then there exist 1 \u2264 b \u2264 p such that\nAj \u2286 [qj,b\u22121, qj,b] =: Aj .\nThus for any A \u2208 P, there exists a rectangle A such that the Aj are the bins. By linearity, if we want to understand how Tabular LIME produces interpretable coefficients for f , we only need to understand how Tabular LIME produces interpretable coefficients for the function 1A : x 7\u2192 1x\u2208A for any A \u2208 P. The explanation for f will simply be the (weighted) sum of the explanations.\nWe now make the fundamental observation that 1A is a multiplicative function whenever A is a rectangles with faces parallel to the axes. Indeed, in this case we can write A = \u220fd j=1Aj , and therefore 1x\u2208A = \u220fd j=1 1xj\u2208Aj . Hence we can apply Proposition 5 to the function 1A. Before giving our result for indicator functions, we need to introduce two important concepts.\nDefinition 1 (Relative importance). Let A \u2286 Rd be a rectangle such that A is included in a d-dimensional bin A given to LIME. In other words, we assume that A = \u220fd j=1[sj , tj ] and that, for each 1 \u2264 j \u2264 d, there exist a b such that [sj , tj ] \u2286 [qj,b\u22121, qj,b]. We define the relative importance of A by\nI(A) := d\u220f j=1\n\u03a6 ( tj\u2212\u00b5j,b \u03c3j,b ) \u2212 \u03a6 ( sj\u2212\u00b5j,b \u03c3j,b ) \u03a6 ( qj,b\u2212\u00b5j,b \u03c3j,b ) \u2212 \u03a6 ( qj,b\u22121\u2212\u00b5j,b \u03c3j,b\n) . Intuitively, I(A) is the mass given to A by the truncated Gaussian measure normalized by the volume of A. One can check that a first order approximation of I(A) is given by Vol(A)/Vol(A) by setting \u03a6(x) \u2248 12 + 1\u221a 2\u03c0 x. If A is very small with respect to A or is located in a low density region of A, then I(A) = 0. On the contrary, if A is large with respect to A or is located in a high density region of A, then I(A) = 1.\nWe now present a notion of distance between \u03be and A.\nDefinition 2 (Bin distance). Let A \u2286 Rd and A be a rectangle of the LIME grid such that A \u2286 A. Let \u03be be a point in S. We define the bin distance between \u03be and A by\nd (\u03be, A) := #{k s.t. \u03bek /\u2208 Ak} .\nTo put it simply, d (\u03be, A) counts in how many directions the projection of \u03be does not fall into the projection of A. If \u03be is in the same d-dimensional bin as A, then d (\u03be, A) = 0. On the contrary, if \u03be is not aligned with A in any given direction, then d (\u03be, A) = d. Note that d (\u03be, A) can be high without the Euclidean distance between \u03be and A being high.\nWe are now able to give the expression of \u03b2A which is short for \u03b21A .\nProposition 6 (Computation of \u03b2f for an indicator function). Let A be a rectangle with faces parallel to the axes such that A \u2286 A, where A is a d-dimensional bin given as input to Tabular LIME. Then Theorem 1 holds for 1A and for any 1 \u2264 j \u2264 d, if \u03bej \u2208 Aj,\n\u03b2Aj = I(A) exp\n( \u2212d(\u03be,A)\n2\u03bd2 ) pd\u22121cd\u22121 ,\nand\n\u03b2Aj = \u2212I(A) exp\n( \u2212d(\u03be,A)+1\n2\u03bd2 ) (p\u2212 1)pd\u22121cd\u22121\notherwise. The intercept is given by\n\u03b2A0 = I(A) exp\n( \u2212d(\u03be,A)\n2\u03bd2 ) pdcd \u2212 1 pc d\u2211 j=1 \u03b2Aj .\nAs a direct consequence of Proposition 6, we can see that the interpretable coefficients are small when \u03be (the example to explain) is far away from A (the support of the function to explain). This is a desirable property: we want the explanations to be zero since the function is completely flat in this case. Note however that there is a notable exception: whenever"}, {"heading": "Explanations for an indicator function", "text": "arrow pointing north-east means that both \u03b2A1 and \u03b2 A 2 take large positive values at \u03be, the base-point of the arrow. We see that the interpretable coefficients are small when far away from the hyper-rectangle A. But artifacts appear one the bins that are aligned with A along the axes.\n\u03bej is aligned with A along coordinate j, then \u03b2 A j can be large even though \u03be is far away from A. We refer to Figure 10 for an illustration of this phenomenon. We believe that this is not a desirable behavior and that the explanations should be close to zero in all directions when \u03be is far away from A (relatively to \u03bd). We will see that these artifacts will be added up when considering the explanation for the total partition, possibly creating wrong explanations.\nAs a second consequence of Proposition 6, we obtain the expression of \u03b2f for a partitionbased model.\nCorollary 7 (Computation of \u03b2f for partition-based model). Let \u03be \u2208 S. Let f be a partition-based model defined as in Eq. (13). Assume that each A \u2208 P is a rectangle with faces parallel to the axes and is included in a d-dimensional bin of Tabular LIME. Then, for any 1 \u2264 j \u2264 d,\n\u03b2fj = 1\npd\u22121cd\u22121 \u2211 A\u2208P \u03bej\u2208Aj f(A)I(A) e \u2212d(\u03be,A) 2\u03bd2 \u2212 1 (p\u2212 1)pd\u22121cd\u22121 \u2211 A\u2208P \u03bej /\u2208Aj f(A)I(A) e \u2212d(\u03be,A)+1 2\u03bd2 . (14)\nEssentially, the explanation for \u03be is the difference between two weighted averages: the average value of f \u201cabove\u201d \u03bej to the left and the average value of f everywhere else to the\nright. The weights of each term are the product of the relative importance of the rectangle and an exponential decay driven by the bin distance between \u03be and A. This is maybe more visible when \u03bd is large. In that case, Eq. (14) becomes\n\u03b2fj = 1\npd\u22121 \u2211 A\u2208P \u03bej\u2208Aj f(A)I(A)\u2212 1 (p\u2212 1)pd\u22121 \u2211 A\u2208P \u03bej /\u2208Aj f(A)I(A) .\nIn the last display, pd\u22121 counts the number of d-dimensional bins that are above \u03bej , while (p \u2212 1)pd\u22121 counts all the others bins. We demonstrate the accuracy of our theoretical predictions in Figure 11 in the particular case of a CART tree.\nPerhaps the most striking feature of Eq. (14) is the positive influence of rectangles that are aligned with \u03be. Indeed, when d (\u03be, A) is small with respect to \u03bd2, then the f(A)I(A) e\u2212d(\u03be,A)/(2\u03bd\n2) terms have a huge influence in Eq. (14). Even though these rectangles can be far away from \u03be: d (\u03be, A) only counts how many directions do not align \u03be and A. To put it plainly, Tabular LIME can give positive influence to j whereas the function to explain is totally flat in the vicinity of \u03be if there is a bump in f aligned with \u03be in this direction. We demonstrate this effect in Figure 12. We do not think that this is a desired behavior. In high dimension, such artifacts are however less common since it is harder to be aligned.\nWe want to conclude this section by noticing that Corollary 7 also applies to a random forest regressor build upon CART trees: one only needs to consider the partition obtained by intersecting all the associated tree partitions."}, {"heading": "4.2.2 Radial basis function kernel", "text": "An important class of examples is given by kernel regressors, a family of predictors containing, for instance, kernel support vector machines (see Hastie et al. (2001) Section 12.3 and Schoelkopf and Smola (2001)). In all these cases, f , the model to explain can be written in the form\nf(x) = m\u2211 i=1 \u03b1ik\u03b3i(x, \u03b6i) ,\nwhere k is a positive semi-definite kernel, \u03b3i is a positive scale parameter, \u03b1 \u2208 Rm are some real coefficients, and the \u03b6is are support points. By linearity of the explanation, we can focus on the \u03b2f with f = k\u03b3i(\u00b7, \u03b6i). For one of the most intensively used kernel, the Gaussian kernel (also called radial basis function kernel), x 7\u2192 k\u03b3(x, \u03b6) is multiplicative. Therefore, we can compute in closed-form the associated interpretable coefficients. Although, it is more challenging to obtain a statement as readable as Proposition 6, and we only give the\ncomputation of the ej,b in this case. Namely, we focus on f(x) = exp ( \u2212\u2016x\u2212\u03b6\u20162\n2\u03b32\n) for some\npositive \u03b3 and a fixed \u03b6 \u2208 Rd.\nProposition 8 (Gaussian kernel computations). For any 1 \u2264 j \u2264 d, define kj := exp ( \u2212(\u00b7\u2212\u03b6j)2\n2\u03b32\n) . Let us set\nm\u0303j,b := \u03b32\u00b5j,b + \u03c3 2 j,b\u03b6j\n\u03b32 + \u03c32j,b , and s\u03032j,b :=\n\u03c32j,b\u03b3 2\n\u03c32j,b + \u03b3 2 .\nThen, for any 1 \u2264 j \u2264 d and 1 \u2264 b \u2264 p, ej,b = ekj,b if b = b?j and e \u22121 2\u03bd2 ekj,b otherwise, where we defined\nekj,b := s\u0303j,b \u03c3j,b\n\u03a6 ( qj,b\u2212m\u0303j,b\ns\u0303j,b\n) \u2212 \u03a6 ( qj,b\u22121\u2212m\u0303j,b\ns\u0303j,b ) \u03a6 ( qj,b\u2212\u00b5j,b \u03c3j,b ) \u2212 \u03a6 ( qj,b\u22121\u2212\u00b5j,b \u03c3j,b\n) e\u2212(\u00b5j,b\u2212\u03b6j)22(\u03b32+\u03c32j,b) . (15) Proof. We see that f is multiplicative, with fj(x) = kj . Therefore we can use Proposition 5 to compute the associated interpretable coefficients. Computing ej,b requires to integrate fj with respect to the Gaussian measure. We can split the square as\n(x\u2212 \u00b5j,b)2\n2\u03c32j,b +\n(x\u2212 \u03b6j)2\n2\u03b32 =\n(x\u2212 m\u0303j,b)2\n2s\u03032j,b ,\nLet x \u223c TN(\u00b5j,b, \u03c32j,b, qj,b\u22121, qj,b). We deduce that E [ exp ( \u2212(x\u2212 \u03b6j)2\n2\u03b32\n)] = ekj,b .\nFrom the previous display we can deduce the value of e kj j,b for any 1 \u2264 j \u2264 d and 1 \u2264 b \u2264 p and conclude the proof.\nThe value of the c fj j coefficients follows, which gives us a closed-formula for \u03b2 f j . Figure 13 demonstrates how our theoretical predictions match practice in dimension 10. Without giving the explicit formula for \u03b2f , we can see that, as in the previous section, for a given j, \u03b2fj is relatively larger than the other interpretable coefficients if\nekj,b?j 1 p\u2212 1 \u2211 b 6=b?j ekj,b .\nNow, in the typical situation, \u03b3 \u03c3j,b for any 1 \u2264 j \u2264 d and 1 \u2264 b \u2264 p. Indeed, the bins are usually quite large (containing approximately 1/p-th of the training data in any given direction), whereas \u03b3 should be rather small in order to encompass the small-scale variations of the data. In this case, whenever \u00b5j,b is far away from \u03b6j with respect to \u03b3, the exponential term in Eq. (15) vanishes, and ekj,b \u2248 0. Therefore, we end up in a situation very similar to the indicator function case treated in the previous section, where the only large ekj,b coefficients are the one for which the associated bin contains \u03b6j. This has similar consequences: the explanations are rather small when \u03be is far away from \u03b6. With the same exception: when \u03bej is \u201caligned\u201d with \u03b6j , e k j,b is large and this yields artifacts in the explanation, which we demonstrate in Figure 14. Again, we do not think that this is a desirable behavior. One would prefer to see \u03b2fj pointing in the direction of \u03b6j , or at the very least small \u03b2fj since the function is very flat when far from \u03b6 with respect to \u03bd (at least in the Gaussian kernel case)."}, {"heading": "4.3 Model depending on a subset of the coordinates", "text": "As a last application of Theorem 1, we turn to the case where f depends only on a (strict) subset of the coordinates. Intuitively, this corresponds to situation where one or more coordinates are unused by the model, and we would like any interpretability method to give zero weight to this coordinate\u2014or rather, in the case of Tabular LIME, to the interpretable coefficient along this dimension. We begin with a definition.\nAssumption 3 (s-sparse function). Let s < d be a fixed integer. We say that f is s-sparse if there exists g : Rs \u2192 R such that\n\u2200x \u2208 Rd, f(x) = g(xj1 , . . . , xjs) ,\nwhere S := {j1, . . . , js} is a subset of {1, . . . , d} of cardinality s.\nOur next result shows that, indeed, Tabular LIME ignores unused coordinates.\nProposition 9 (Ignoring unused coordinates). Assume that f satisfies Assumption 3 and is bounded on S. Let S be the set of indices relevant for f and S := {1, . . . , d} \\ S. Then Theorem 1 holds with \u03b2fj = 0 for any j \u2208 S.\nWe believe that Proposition 9 encompasses a very desirable trait of any interpretability method: if a coordinate is not used by the model at all, then this coordinate has no influence on the prediction given by the model and should receive weight 0 in the explanation. We illustrate Proposition 9 in Figure 15.\nWhile Proposition 9 is true for more general weights (see Appendix J), we provide here a proof for the default weights. Our goal is, again, to demonstrate how Theorem 1 can be used to get interesting statements on the explanations provided by Tabular LIME when simple structural assumptions are made on f .\nProof. We want to specialize Theorem 1 in the specific case where f does not depend on a subset of coordinates. As we have seen before, the main challenge in using Theorem 1 is to compute the expectations E [\u03c0f(x)] and E [\u03c0zjf(x)]. The main idea of the proof is to regroup in these expectation computations the parts depending on j \u2208 S and those depending on j /\u2208 S. We will see that some cancellations happen afterwards.\nLet us turn first to the computation of E [\u03c0f(x)]. By definition of the weights (Eq. (2)), we have\nE [\u03c0f(x)] = E\n[ d\u220f\nk=1\ne \u2212(1\u2212zk)\n2\n2\u03bd2 f(x) ] .\nUsing successively Assumption 3 and the independence of the xjs, we can rewrite the previous display as \u220f\nk\u2208S\nE [ e \u2212(1\u2212zk) 2 2\u03bd2 ] \u00b7 E [\u220f k\u2208S e \u2212(1\u2212zk) 2 2\u03bd2 g(xj1 , . . . , xjs) ] .\nWe recognize the definition of the normalization constant c. Setting\nG := E [\u220f k\u2208S e \u2212(1\u2212zk) 2 2\u03bd2 g(xj1 , . . . , xjs) ] ,\nwe have proved that\nE [\u03c0f(x)] = cd\u2212s \u00b7G . (16)\nLet j \u2208 {1 . . . , d} \\ S. The computation of E [\u03c0zjf(x)] is similar. Indeed, by definition of the weights, we can write\nE [\u03c0zjf(x)] = E\n[ d\u220f\nk=1\ne \u2212(1\u2212zk)\n2\n2\u03bd2 zjf(x)\n] .\nAgain, since the xjs are independent and f satisfies Assumption 2, we rewrite the previous display as\n\u220f k\u2208S\\{j} E [ e \u2212(1\u2212zk) 2 2\u03bd2 ] \u00b7 E [ e \u2212(1\u2212zj) 2 2\u03bd2 zj ] \u00b7 E [\u220f k\u2208S e \u2212(1\u2212zk) 2 2\u03bd2 g(xj1 , . . . , xjs) ] .\nWe have proved that\nE [\u03c0zjf(x)] = cd\u2212s\u22121G\np . (17)\nFinally, according to Theorem 1,\n\u03b2fj = C \u22121 pc\npc\u2212 1\n( \u2212E [\u03c0f(x)] + pcE [\u03c0zjf(x)] ) .\nPlugging Eqs. (16) and (17) in the previous display, we obtain that\n\u03b2fj = C \u22121 pc\npc\u2212 1\n( \u2212cd\u2212s \u00b7G+ pcc d\u2212s\u22121G\np\n) = 0 ."}, {"heading": "5. Conclusion: strengths and weaknesses of Tabular LIME", "text": "In this paper, we provided a thorough analysis of Tabular LIME. In particular, we show that, in the large sample size, the interpretable coefficients provided by Tabular LIME can be obtained in an explicit way as a function of the algorithm parameters and some expectation computations related to the black-box model. Our experiments show that our theoretical analysis yields predictions that are very close to empirical results for the default implementation. This allowed us to provide very precise insights on the inner working of Tabular LIME, revealing some desirable behavior (linear models are explained linearly, unused coordinates are provably ignored, and kernel functions yield flat explanations far away from the bump), and some not so desirable properties (artifacts appear when explaining indicator and kernel functions).\nWe believe that the present work paves the way for a better theoretical understanding of Tabular LIME in numerous simple cases. Using the machinery presented here, one can check how a particular model interacts with Tabular LIME, at the price of reasonable computations. It is then possible to check whether some basic sanity checks are satisfied, helping us to decide whether to use Tabular LIME in this case. We also hope that the insights presented here can help us to design better interpretability methods by fixing the flaws of Tabular LIME. For instance, our analysis is valid for a large class of weights: for a given class of models, it could be the case that choosing non-default weights is more adequate.\nIn the future, our final goal is to tackle the text and image versions of LIME. While the sampling of the perturbed examples is similar in all three versions, there are some key differences that make the analysis even more challenging if the data is not tabular. Namely, in the text version, the TF-IDF transform (Jones, 1972) is used as data transformation between the text and the model. Its non-linear nature makes it hard to analyze. In the image version, the first step of the algorithm is to create superpixels of the image to explain. This is a complicated process that we do not know how to formalize properly."}, {"heading": "Acknowledgments", "text": "The authors want to thank Romaric Gaudel, Michael Lohaus, and Martin Pawelcyk for constructive discussions."}, {"heading": "Appendix", "text": "In this Appendix we collect all proofs and additional results. Appendix A generalizes the notation to weights generalizing the default weights. In Appendix B, C, and D we study successively \u03a3, \u0393f , and \u03b2f . The proof of Proposition 2 is presented in Appendix E, and the proof of Proposition 6 is given in Appendix F. We turn to the concentration of \u03a3\u0302n and \u0393\u0302n in Appendix G and H. Our main result is proved in Appendix I. An extension of Proposition 9 for general weights is presented in Appendix J. Finally, technical results are collected in Appendix K."}, {"heading": "Appendix A. General weights", "text": "As discussed throughout the main paper, our analysis holds for more general weights than the default weighting scheme. Indeed, it will become clear in our analysis that the proof scheme works provided that the weight \u03c0 associated to x has some multiplicative structure. More precisely, from now on we assume that\n\u03c0 := exp \u22121 2\u03bd2 d\u2211 j=1 (\u03c4j(\u03bej)\u2212 \u03c4j(xj))2  , (18)\nwhere \u03bd > 0 is, as before, the bandwidth parameter, and for any 1 \u2264 j \u2264 d, \u03c4j : R \u2192 R is an arbitrary fixed function that can depend on the input of the algorithm. We will refer to these weights as general weights in the following. We make the following assumption on the \u03c4js:\nAssumption 4. For any 1 \u2264 j \u2264 d and any x, y \u2208 [0, 1], we have\n|\u03c4j(x)\u2212 \u03c4j(y)| \u2264 1 .\nThis assumption is mainly needed to control the spectrum of \u03a3. General weights generalize two important examples, which we describe below.\nExample 1 (Weights in the default implementation). In the default implementation of LIME, for a given example xi, we have defined \u03c0i = exp ( \u2212\u20161\u2212 zi\u20162 /(2\u03bd2) ) in Eq (2). This definition of the weights amounts to taking\n\u03c4j(x) = 1x\u2208 [ qj,b?\nj \u22121,qj,b? j ] . By definition of the \u03c4js in this case, Assumption 4 is satisfied.\nExample 2 (Smooth weights). In Garreau and von Luxburg (2020), the weights were defined as \u03c0i = exp ( \u2212\u2016\u03be \u2212 xi\u20162 /(2\u03bd2) ) . This definition of the weights corresponds to taking \u03c4j(x) = x for any 1 \u2264 j \u2264 d. If the data is bounded (say by 1), then the boundedness of the \u03c4js is satisfied in this case."}, {"heading": "Appendix B. The study of \u03a3", "text": "In this section, we study the matrix \u03a3 for general weights satisfying Eq. (18). We begin by generalizing the notation e\u03c8j,b introduced at the beginning of Section 4 to general weights. For any \u03c8 : R\u2192 R, we set\ne\u03c8j,b := E [ \u03c8(xij) exp ( \u22121 2\u03bd2 (\u03c4j(\u03bej)\u2212 \u03c4j(xij))2 )\u2223\u2223\u2223\u2223bij = b] . (19)\nAs before, when \u03c8 = 1, we just write ej,b instead of e 1 j,b, and when \u03c8 = id, we write e \u00d7 j,b. We also extend the definition of the normalization constants\nc\u03c8j := 1\np p\u2211 b=1 e\u03c8j,b ,\nand cj := c 1 j . Finally, we set C := \u220fd j=1 cj .\nRemark 1. Whenever \u03c8 is regular enough, the behavior of these coefficients in the small and large bandwidth is quite straightforward. Namely:\n\u2013 if \u03bd \u2192 0, then e\u03c8j,b \u2192 0. As a consequence, c \u03c8 j \u2192 0 as well;\n\u2013 if \u03bd \u2192 +\u221e, then e\u03c8j,b \u2192 E [\u03c8(xij)|bij = b].\nIn Section 4, we computed these coefficients for the default weights. Let us redo this exercise for the weighting scheme of Garreau and von Luxburg (2020).\nExample 3 (Basic computations, smooth weights). Let us compute the ej,b when \u03c4j = id. We write\nej,b = E [ exp ( \u2212(xij \u2212 \u03bej)2\n2\u03bd2 )\u2223\u2223\u2223\u2223bij = b] (Eq. (19)) = 1\n\u03c3jb \u221a 2\u03c0 \u00b7 1 \u03a6(rj,b)\u2212 \u03a6(`j,b) \u222b qj,b qj,b\u22121 exp ( \u2212(x\u2212 \u00b5j,b)2 2\u03c32j,b + \u2212(x\u2212 \u03bej)2 2\u03bd2 ) dt (Eq. (1))\n= 1 \u03a6(rj,b)\u2212 \u03a6(`j,b) \u00b7 \u03bd e\n\u2212(\u03bej\u2212\u00b5j,b) 2\n2(\u03bd2+\u03c32 j,b )\u221a \u03bd2 + \u03c32j,b\n1 2 erf \u03bd2(x\u2212 \u00b5j,b) + \u03c32j,b(x\u2212 \u03bej) \u03bd\u03c3j,b \u221a 2(\u03bd2 + \u03c32j,b) qj,b qj,b\u22121 ,\nwhere we used Lemma 11.1 in Garreau and von Luxburg (2020) in the last display. For any 1 \u2264 j \u2264 d and 1 \u2264 b \u2264 p, let us set\nmj,b := \u03bd2\u00b5j,b + \u03c3 2 j,b\u03bej\n\u03bd2 + \u03c32j,b and s2j,b := \u03bd2\u03c32j,b \u03bd2 + \u03c32j,b .\nThen it is straightforward to show that\n\u03bd2(x\u2212 \u00b5j,b) + \u03c32j,b(x\u2212 \u03bej) \u03bd\u03c3j,b \u221a 2(\u03bd2 + \u03c32j,b) = x\u2212mj,b\u221a 2sj,b ,\nand the expression of ej,b simplifies slightly:\nej,b = 1 \u03a6(rj,b)\u2212 \u03a6(`j,b) \u00b7 \u03bd e\n\u2212(\u03bej\u2212\u00b5j,b) 2\n2(\u03bd2+\u03c32 j,b )\u221a \u03bd2 + \u03c32j,b\n[ 1\n2 erf\n( x\u2212mj,b\u221a\n2sj,b )]qj,b qj,b\u22121 .\nNow recall that Garreau and von Luxburg (2020) chose to consider \u00b5j,b = \u00b5j and \u03c3j,b = \u03c3 constant. As a consequence, mj,b does not depend on b anymore, and sj,b is a constant equal to s := \u03bd\u03c3/ \u221a \u03bd2 + \u03c32. Moreover, the qj,b are, in this case, the normalized Gaussian quantiles, and therefore\n\u03a6(rj,b)\u2212 \u03a6(`j,b) = 1\np .\nWe deduce that\nej,b = p\u03bd e\n\u2212(\u03bej\u2212\u00b5j) 2\n2(\u03bd2+\u03c32)\n\u221a \u03bd2 + \u03c32\n[ 1\n2 erf\n( x\u2212mj\u221a\n2s )]qj,b qj,b\u22121 ,\nand\ncj = \u03bd\u221a\n\u03bd2 + \u03c32 exp\n( \u2212(\u03bej \u2212 \u00b5j)2\n2(\u03bd2 + \u03c32)\n) ,\nwhich yields\nC =\n( \u03bd2\n\u03bd2 + \u03c32\n)d/2 exp ( \u2212\u2016\u03be \u2212 \u00b5\u20162\n2(\u03bd2 + \u03c32)\n) . (20)\nWe see that Eq. (20) is indeed Eq. (7.2) in Garreau and von Luxburg (2020)."}, {"heading": "B.1 Computation of \u03a3", "text": "We now have the necessary notation to turn to the computation of \u03a3. The main idea is that \u03a3 depends only on p, \u03bd, and the input parameters of the algorithm. By chance, the sampling of the new examples make \u03a3 simple enough and we can obtain a closed-form expression.\nProposition 10 (Computation of \u03a3, general weights). Recall that \u03a3\u0302n = 1 nZ >WZ and \u03a3 = E[\u03a3\u0302n], where Z was defined in Section 2.2. Then it holds that\n\u03a3 = C  1\ne1,b?1 pc1\n\u00b7 \u00b7 \u00b7 ed,b?\nd pcd\ne1,b?1 pc1\ne1,b?1 pc1\nej,b? j ek,b? k\npcjpck ...\n. . . ed,b?\nd pcd\nej,b? j ek,b? k\npcjpck\ned,b? d\npcd  Proposition 10 changes slightly if p is no longer constant across dimensions, but we do\nnot tackle this case in the present paper.\nProof. We introduce a phantom coordinate in Z since the surrogate linear model uses an offset. A straightforward computation shows that\n\u03a3\u0302 = 1\nn  \u2211n i=1 \u03c0i \u2211n i=1 \u03c0izi1 \u00b7 \u00b7 \u00b7 \u2211n i=1 \u03c0izid\u2211n i=1 \u03c0izi1 \u2211n i=1 \u03c0izi1 \u2211n i=1 \u03c0izijzik\n... . . .\u2211n i=1 \u03c0izid \u2211n i=1 \u03c0izijzik \u2211n i=1 \u03c0izi1\n . (21)\nSince the xis are i.i.d., the result follows from the computation of E [\u03c0], E [\u03c0zj ], and E [\u03c0zjzk]. This is achieved in Lemma 32.\nAs examples, we can explicit the computation of \u03a3 for default weights and smooth weights.\nExample 4 (Default implementation, computation of \u03a3). As we have seen in Section 4, in this case\nej,b =\n{ 1 if b = b?j\ne \u22121 2\u03bd2 otherwise,\nand cj = c is a constant. Therefore, according to Proposition 10, the expression of \u03a3 simplifies greatly into\n\u03a3 = cd  1 1pc \u00b7 \u00b7 \u00b7\n1 pc\n1 pc\n1 pc\n1 p2c2\n... . . .\n1 pc 1 p2c2\n1 pc\n .\nExample 5 (Computation of \u03a3, smooth weights). According to Example 3, we have that\nej,b?j pcj =\n[ 1\n2 erf\n( x\u2212mj\u221a\n2s\n)]qj,b? j\nqj,b? j \u22121\n.\nWe recover the \u03b1j coefficients (Eq. (7.3) in Garreau and von Luxburg (2020)) and as a direct consequence, Lemma 8.1:\n\u03a3 := C  1 \u03b11 \u00b7 \u00b7 \u00b7 \u03b1d \u03b11 \u03b11 \u03b1i\u03b1j ... . . .\n\u03b1d \u03b1i\u03b1j \u03b1d\n ."}, {"heading": "B.2 Computation of \u03a3\u22121", "text": "The structure of \u03a3 allows to invert it in closed-form, even in the case of general weights. This is the extent of the next result.\nProposition 11 (Computation of \u03a3\u22121, general weights). Let \u03a3 be defined as before. Then \u03a3 is invertible and\n\u03a3\u22121 = C\u22121  1 + \u2211d j=1 ej,b? j pcj\u2212ej,b? j\n\u2212pc1 pc1\u2212e1,b?1\n\u00b7 \u00b7 \u00b7 \u2212pcdpcd\u2212ed,b? d\n\u2212pc1 pc1\u2212e1,b?1\np2c21 e1,b?1 (pc1\u2212e1,b?1 ) 0\n... . . .\n\u2212pcd pcd\u2212ed,b?\nd\n0 p2c2d\ned,b? d (pcd\u2212ed,b? d )\n .\nProof. Set \u03b1j := ej,b?j /(pcj), and define \u03b1 \u2208 R d the vector of the \u03b1js. Set E := 1, F := \u03b1 >, G := \u03b1, and\nH :=  \u03b11 \u03b1j\u03b1k . . .\n\u03b1j\u03b1k \u03b1d  . Then \u03a3 is a block matrix that can be written \u03a3 = C [ E F\nG H\n] . We notice that"}, {"heading": "H \u2212GE\u22121F = diag (\u03b11(1\u2212 \u03b11), . . . , \u03b1d(1\u2212 \u03b1d)) .", "text": "Since the ej,b are positive for any j, b, the \u03b1js belong to (0, 1) and H\u2212GE\u22121F is an invertible matrix. According to the block matrix inversion formula,[\nE F\nG H\n]\u22121 = ( E\u22121 + E\u22121F (H \u2212GE\u22121F )\u22121GE\u22121 \u2212E\u22121F (H \u2212GE\u22121F )\u22121\n\u2212(H \u2212GE\u22121F )\u22121GE\u22121 (H \u2212GE\u22121F )\u22121 ) Thus\n\u03a3\u22121 = C\u22121  1 + \u2211d j=1 \u03b1j 1\u2212\u03b1j\n\u22121 1\u2212\u03b11 \u00b7 \u00b7 \u00b7\n\u22121 1\u2212\u03b1d\n\u22121 1\u2212\u03b11\n1 \u03b11(1\u2212\u03b11) 0\n... . . .\n\u22121 1\u2212\u03b1d 0\n1 \u03b1d(1\u2212\u03b1d)  . (22) The result follows from the definition of the \u03b1js.\nWe can be explicit about the computation of \u03a3\u22121 in the case of default and smooth weights.\nExample 6 (Computation of \u03a3\u22121, default implementation). Using our basic computations in this case in conjunction with Proposition 11, we obtain\n\u03a3\u22121 = c\u2212d\npc\u2212 1  pc+ d\u2212 1 \u2212pc \u00b7 \u00b7 \u00b7 \u2212pc \u2212pc p2c2 0 ... . . .\n\u2212pc 0 p2c2  . Example 7 (Computation of \u03a3\u22121, smooth weights). From the definition of \u03b1j in Example 3, we see that Eq. (22) is in fact Lemma 8.2 in Garreau and von Luxburg (2020).\nB.3 Control of \u2225\u2225\u03a3\u22121\u2225\u2225\nop Our analysis requires a control of \u2225\u2225\u03a3\u22121\u2225\u2225\nop when we want to concentrate \u03b2\u0302n (see Appendix I).\nWe show how to achieve this control when the functions \u03c4j are bounded. Proposition 12 (Upper bound on \u2225\u2225\u03a3\u22121\u2225\u2225\nop , general weights). Let \u03a3 be as before, and\nsuppose that \u03c4j satisfies Assumption 4. Then\u2225\u2225\u03a3\u22121\u2225\u2225 op \u2264 2 \u221a 2C\u22121dp2 e 2 \u03bd2 .\nProof. According to Lemma 35, we can find an upper bound for \u2225\u2225\u03a3\u22121\u2225\u2225\nop just by computing\u2225\u2225\u03a3\u22121\u2225\u2225\nF . Proposition 11 gives us\n\u2225\u2225C\u03a3\u22121\u2225\u22252 F = 1 + d\u2211 j=1 ej,b?j pcj \u2212 ej,b?j 2 + 2 d\u2211 j=1 p2c2j (pcj \u2212 ej,b?j ) 2 + d\u2211 j=1 p4c4j ej,b?j (pcj \u2212 ej,b?j ) 2 . (23)\nWe first notice that all the terms involved in Eq. (23)are positive. Moreover, we see that ej,b \u2264 1 and pcj \u2264 p for any j, b. Under Assumption 4, |\u03c4j(\u03bej)\u2212 \u03c4j(xij)| \u2264 1 almost surely. We deduce that\nej,b = E [ exp ( \u22121 2\u03bd2 (\u03c4j(\u03bej)\u2212 \u03c4j(xij))2 )\u2223\u2223\u2223\u2223bij = b] \u2265 e \u221212\u03bd2 .\nAs a consequence, pcj \u2212 ej,b?j \u2265 (p \u2212 1) e \u22121 2\u03bd2 . Plugging these bounds in Eq. (23) and using (x+ y)2 \u2264 2(x2 + y2), we obtain\n\u2225\u2225C\u03a3\u22121\u2225\u22252 F \u2264 2 ( 1 + d2 e 1 \u03bd2\n(p\u2212 1)2\n) + 2dp2 e 1 \u03bd2\n(p\u2212 1)2 + dp4 e\n4 \u03bd2\n(p\u2212 1)4 .\nFinally, we conclude using p \u2265 2 and d \u2265 1. Of course, a more precise knowledge of the weights can give a better bound for \u2225\u2225\u03a3\u22121\u2225\u2225\nop .\nFor instance, it is possible to show that\u2225\u2225\u03a3\u22121\u2225\u2225 op . c\u2212d e 1 2\u03bd2 (d+ p)\nin that case, by studying closely the spectrum of \u03a3. Since the difference with Proposition 12 is not that impressive, we keep the bound given for general weights. In particular, the dependency in \u03bd is the same, indicating that the behavior for small \u03bd is poor independently of the proof technique."}, {"heading": "Appendix C. The study of \u0393f", "text": "In this section, we turn to the study of \u0393f , the second step of our analysis. In the previous section, we dealt with the computation of \u03a3 and there was no dependency in f . This is not the case anymore. We will assume from now on that f is bounded on S, the hyperrectangle containing the training data, as it is done in Theorem 28. In particular, this assumption guarantees that all the expectations involving f are well defined (since \u03c0 and z are also bounded almost surely)."}, {"heading": "C.1 Computation of \u0393f", "text": "We begin by computing \u0393f . A straightforward computation shows that\n\u0393\u0302j =\n{ 1 n \u2211n i=1 \u03c0if(xi) if j = 0\n1 n \u2211n i=1 \u03c0izijf(xi) otherwise.\nSince the xi are i.i.d., a straightforward consequence of the previous display is\n\u0393fj = { E [\u03c0f(x)] if j = 0 E [\u03c0zjf(x)] otherwise.\n(24)\nExample 8 (Constant model, general weights). In order to get familiar with Eq. (24), let us focus on a constant model. In this case, we just need to compute E [\u03c0] and E [\u03c0zj ]. According to Lemma 32, we have\nE [\u03c0] = C and E [\u03c0zj ] = C ej,b?j pcj .\nWe have just showed that, if f = f0 a constant, then\nC\u22121\u0393f0 = f0 and C \u22121\u0393fj = ej,b?j pcj f0 \u2200j \u2265 1 . (25)"}, {"heading": "C.2 Additive f", "text": "We can specialize the computation of \u0393f when f is additive, always in the case of general weights.\nProposition 13 (Computation of \u0393f , additive f). Assume that f satisfies Assumption 1 and that f is bounded on S. Then \u0393f is such that\n\u0393f0 = d\u2211 k=1 1 pck p\u2211 b=1 efkk,b ,\nand, for any 1 \u2264 j \u2264 d,\n\u0393fj = d\u2211\nk=1\nej,b?j pcj \u00b7 1 pck p\u2211 b=1 efkk,b + 1 pcj [ e fj j,b?j \u2212 ej,b?j pcj p\u2211 b=1 e fj j,b ] .\nProof. By linearity and since f is additive, we can restrict ourselves to the case f(x) = \u03c8(xk). We first look into j = 0 in Eq. (24). Then\nE [\u03c0if(xi)] = E [\u03c0i\u03c8(xik)] = C\npck p\u2211 b=1 e\u03c8k,b\naccording to Lemma 31. Setting \u03c8 = fk in the previous display and summing for k \u2208 {1, . . . , d}, we obtain the first part of the result.\nNow we turn to the case j \u2265 1. Again, by linearity and since f is additive, we can restrict ourselves to the case f(x) = \u03c8(xk). There are two possibilities in this case: either j = k, and then\nE [\u03c0izijf(xi)] = E [\u03c0izij\u03c8(xij)] = C e\u03c8j,b?j pcj ,\naccording to Lemma 31; or j 6= k, and then\nE [\u03c0izijf(xi)] = E [\u03c0izij\u03c8(xik)] = C ej,b?j pcj 1 pck p\u2211 b=1 e\u03c8k,b ,\naccording to Lemma 31. Setting \u03c8 = fk in the last displays and summing over k \u2208 {1, . . . , d}, we obtain\nC\u22121 E [\u03c0izijf(xi)] = d\u2211\nk=1 k 6=j\nej,b?j pcj 1 pck p\u2211 b=1 e\u03c8k,b + e fj j,b?j pcj .\nRearranging the terms in the sum yields the final result.\nLet us specialize Proposition 13 for default weights.\nExample 9 (Computation of \u0393f , additive f , default weights). We can use Proposition 13 to compute \u0393f for an additive f when the weights are given by the default implementation. Indeed, recall that E [xij |bij = b] = \u00b5\u0303j,b. Since the weights are constant on each box, we can compute easily e\u00d7j,b as a function of \u00b5\u0303j,b:\ne\u00d7j,b =\n{ \u00b5\u0303j,b?j if b = b ? j\n\u00b5\u0303j,b e \u22121 2\u03bd2 otherwise.\nAlso recall that the cj are constant equal to c = 1 p + (1\u2212 1 p) e\n\u22121 2\u03bd2 . We deduce that\n(c\u2212d\u0393f )0 = d\u2211\nk=1\n1\npc p\u2211 b=1 efkk,b ,\nand, for any 1 \u2264 j \u2264 d,\n(c\u2212d\u0393f )j = d\u2211\nk=1\nej,b?j pc \u00b7 1 pc p\u2211 b=1 efkk,b + 1 pc ( e fj j,b?j \u2212 1 pc p\u2211 b=1 e fj j,b ) .\nWe can specialize Proposition 13 even further if f is linear.\nCorollary 14 (Computation of \u0393f , linear case, general weights). Assume that f(x) = f0 + f1x1 + \u00b7 \u00b7 \u00b7+ fdxd for any x \u2208 Rd. Then \u0393f is such that\n\u0393f0 = f(\u03b3) ,\nand, for any 1 \u2264 j \u2264 d, ej,b?j pcj f(\u03b3) + fj pcj [ e\u00d7j,b?j \u2212 \u03b3j \u00b7 ej,b?j ] ,\nwhere we defined, for any 1 \u2264 j \u2264 d,\n\u03b3j := 1\npcj p\u2211 b=1 e\u00d7j,b .\nProof. Again, we use the fact that \u0393f is linear as a function of f . We have already looked into the constant case, and one can check that Eq. (25) coincides with Corollary 14 when fj = 0 for all j \u2265 1. We then apply Proposition 13 with fj(x) = fj \u00b7 x for any j \u2265 1. We notice that, in this case, for any j \u2208 {1, . . . , d} and b \u2208 {1, . . . , p},\ne fj j,b = fj \u00b7 ej,b .\nSubstituting the last display yields the result.\nLet us specialize Corollary 14 for default weights and smooth weights.\nExample 10 (Computation of \u0393f , linear f , default weights). We can further specialize Example 9. The only remaining computation is\n\u03b3j = 1\npcj p\u2211 b=1 e\u00d7j,b = \u00b5\u0303j,b?j +\n\u2211 b6=b?j e \u22121 2\u03bd2 \u00b5\u0303j,b\n1 + (p\u2212 1) e \u22121 2\u03bd2\n=: \u02dc\u0303\u00b5j .\nNote that \u02dc\u0303\u00b5j is a barycenter of the \u00b5\u0303j with weight 1 for the box b ? j and e\n\u22121 2\u03bd2 for the others.\nFinally, recall that C = cd and ej,b?j = 1. We have obtained that\nc\u2212d\u0393fj = { f(\u02dc\u0303\u00b5) if j = 0 1 pcf( \u02dc\u0303\u00b5) + fj pc(\u00b5\u0303j,b?j \u2212 \u02dc\u0303\u00b5j) otherwise.\nExample 11 (Computation of \u0393f , linear f , smooth weights). We first compute\ne\u00d7j,b?j = E\n[ xij exp ( \u2212(xij \u2212 \u03bej)2\n2\u03bd2 )\u2223\u2223\u2223\u2223bij = b?j] = p\n\u03c3 \u221a 2\u03c0\n\u222b qj,b? j\nqj,b? j \u22121\nx \u00b7 exp ( \u2212(x\u2212 \u03bej)2\n2\u03bd2 + \u2212(x\u2212 \u00b5j)2 2\u03c32\n) dx (Eq. (1))\n= (\u03b1jmj \u2212 \u03b8j) \u00b7 p\u03bd\u221a \u03bd2 + \u03c32 e \u2212(\u03bej\u2212\u00b5j)\n2\n2(\u03bd2+\u03c32) ,\n(Lemma 11.2 in Garreau and von Luxburg (2020))\nwhere we set\n\u03b8j :=\n[ 1\ns \u221a 2\u03c0 exp\n( \u2212(x\u2212mj)2\n2s2\n)]qj,b? j\nqj,b? j \u22121\n.\nWe deduce that e\u00d7j,b?j pcj = \u03b1jmj \u2212 \u03b8j . Moreover, by the law of total expectation and Lemma 11.2 in Garreau and von Luxburg (2020),\n1\np p\u2211 b=1 e\u00d7j,b = mj \u00b7 \u03bd\u221a \u03bd2 + \u03c32 exp ( \u2212(\u03bej \u2212 \u00b5j)2 2(\u03bd2 + \u03c32) ) .\nFinally, since ej,b?j /(pcj) = \u03b1j and \u03b3j = mj , we find that \u0393 f 0 = f(m) and\n\u0393fj = \u03b1jf(m) + fj(\u03b1jmj \u2212 \u03b8j \u2212 \u03b1jmj) = \u03b1jf(m)\u2212 fj\u03b8j .\nWe recover Lemma 9.1 in Garreau and von Luxburg (2020)."}, {"heading": "Appendix D. The study of \u03b2f", "text": "After focusing on \u03a3 and \u0393f , we can now turn our attention to \u03b2f = \u03a3\u22121\u0393f . This section consists mostly in straightforward consequences of the computations achieved in Appendix B and C."}, {"heading": "D.1 Computation of \u03b2f", "text": "We begin by computing \u03b2f in the general case in closed-form.\nProposition 15 (Computation of \u03b2f , general f , general weights). Assume that f is bounded on S. Then it holds that\n\u03b2f0 = C \u22121 1 + d\u2211 j=1 ej,b?j pcj \u2212 ej,b?j E [\u03c0f(x)]\u2212 C\u22121 d\u2211 j=1 pcj pcj \u2212 ej,b?j E [\u03c0zjf(x)] ,\nand, for any 1 \u2264 j \u2264 d,\n\u03b2fj = C \u22121\n( \u2212pcj\npcj \u2212 ej,b?j E [\u03c0f(x)] + p2c2j ej,b?j (pcj \u2212 ej,b?j ) E [\u03c0zjf(x)]\n) .\nProof. Direct computation from Proposition 11 and Eq. (24).\nWe can easily specialize Proposition 15 for the default weights. Recall that, in this case, ej,b?j = 1 and cj = c is a constant. Furthermore, C = c d. Corollary 16 (Computation of \u03b2f , general f , default weights). Assume that f is bounded on S. Then it holds that\n\u03b2f0 = c \u2212d ( 1 + d\npc\u2212 1\n) E [\u03c0f(x)]\u2212 c\u2212d pc\npc\u2212 1 d\u2211 j=1 E [\u03c0zjf(x)] ,\nand, for any 1 \u2264 j \u2264 d, \u03b2fj = c \u2212d ( \u2212pc pc\u2212 1 E [\u03c0f(x)] + p2c2 pc\u2212 1 E [\u03c0zjf(x)] ) .\nThis last result gives the expression of \u03b2f used in our main result."}, {"heading": "D.2 Additive f", "text": "We can specialize the results of the previous section when f has a specific structure. We begin with the case of an additive f .\nProposition 17 (Computation of \u03b2f , additive f , general weights). Assume that f satisfies Assumption 1. Then \u03b2f is such that\n\u03b2f0 = d\u2211 k=1\n1\npck \u2212 ek,b?k \u2211 b6=b?k efkk,b ,\nand, for any 1 \u2264 j \u2264 d,\n\u03b2fj = pcj\nej,b?j (pcj \u2212 ej,b?j )\n( e fj j,b?j \u2212 ej,b?j pcj p\u2211 b=1 e fj j,b ) .\nProof. First let us treat the case j = 0. We write\n\u03b2f0 = 1 + d\u2211 j=1 ej,b?j pcj \u2212 ej,b?j ( d\u2211 k=1 1 pck p\u2211 b=1 efkk,b )\n\u2212 d\u2211 j=1 pcj pcj \u2212 ej,b?j\n( p\u2211\nk=1\nej,b?j p2cjck p\u2211 b=1 efkk,b + 1 pcj [ e fj j,b?j \u2212 ej,b?j pcj p\u2211 b=1 e fj j,b ])\n= d\u2211\nk=1\n1\npck p\u2211 b=1 efkk,b?k \u2212 d\u2211 j=1\n1\npcj \u2212 ej,b?j\n[ e fj j,b?j \u2212 ej,b?j pcj p\u2211 b=1 e fj j,b ]\nWe conclude after changing the indices in the sum and some algebra. As for the other terms,\n\u03b2fj = \u2212pcj\npcj \u2212 ej,b?j d\u2211 k=1 1 pck p\u2211 b=1 efkk,b\n+ p2c2j\nej,b?j (pcj \u2212 ej,b?j )\n[ d\u2211\nk=1\nej,b?j p2cjck p\u2211 b=1 efkk,b + 1 pcj ( e fj j,b?j \u2212 ej,b?j pcj p\u2211 b=1 e fj j,b )]\nand we obtain the promised result after some simplifications.\nWe can specialize Proposition 17 even further if f is linear. For any 1 \u2264 j \u2264 d, define\n\u03b3j := 1\npcj p\u2211 b=1 e\u00d7j,b .\nNote that \u03b3j = \u02dc\u0303\u00b5j when default weights are used.\nCorollary 18 (Computation of \u03b2f , linear f , general weights). Assume that for any x \u2208 Rd, f(x) = f0 + f1x1 + \u00b7 \u00b7 \u00b7+ fdxd. Then we have\n\u03b2f0 = f(\u03b3)\u2212 d\u2211 j=1\n1\npcj \u2212 ej,b?j (e\u00d7j,b?j \u2212 \u03b3j \u00b7 ej,b?j )fj ,\nand, for any 1 \u2264 j \u2264 d,\npcj ej,b?j (pcj \u2212 ej,b?j ) (e\u00d7j,b?j \u2212 \u03b3j \u00b7 ej,b?j )fj .\nLet us see how we can recover the analysis of Garreau and von Luxburg (2020) in the linear case.\nExample 12 (Computation of \u03b2f , linear f , smooth weights). We have seen before that, in this case, \u03b3 = m. Moreover,\npcj \u2212 ej,b?j = (1\u2212 \u03b1j) p\u03bd\u221a\n\u03bd2 + \u03c32 exp\n( \u2212(\u03bej \u2212 \u00b5j)2\n2(\u03bd2 + \u03c32)\n) .\nThen we write\ne\u00d7j,b?j \u2212 \u03b3jej,b?j = (\u03b1jmj \u2212 \u03b8j) p\u03bd\u221a \u03bd2 + \u03c32 exp\n( \u2212(\u03bej \u2212 \u00b5j)2\n2(\u03bd2 + \u03c32) ) \u2212mj\u03b1j\np\u03bd\u221a \u03bd2 + \u03c32 exp\n( \u2212(\u03bej \u2212 \u00b5j)2\n2(\u03bd2 + \u03c32) ) = \u2212\u03b8j\np\u03bd\u221a \u03bd2 + \u03c32 exp\n( \u2212(\u03bej \u2212 \u00b5j)2\n2(\u03bd2 + \u03c32)\n) .\nWe deduce that\ne\u00d7j,b?j \u2212 \u03b3jej,b?j\npcj \u2212 ej,b?j = \u2212\u03b8j 1\u2212 \u03b1j , (26)\nand therefore\n\u03b2f0 = f(m) + d\u2211 j=1 \u03b8jfj 1\u2212 \u03b1j .\nNow, for any given j > 0, pcj/ej,b?j = 1/\u03b1j . Combining with Eq. (26) we obtain\n\u03b2fj = \u2212\u03b8jfj\n\u03b1j(1\u2212 \u03b1j) .\nThis is the expression of \u03b2f appearing in Theorem 3.1 of Garreau and von Luxburg (2020)."}, {"heading": "D.3 Multiplicative f", "text": "When f is multiplicative (Assumption 2), we can also be more precise in the computation of \u03b2f .\nLemma 19 (Computation of \u03b2f , multiplicative f , general weights). Assume that f satisfies Assumption 2 and is bounded on S. Then\n\u03b2f0 =\n\u220fd k=1 c fk k\nC\n1 + d\u2211 j=1 ej,b?j pcj \u2212 ej,b?j 1\u2212 efjj,b?j ej,b?j \u00b7 cj c fj j  , and, for any 1 \u2264 j \u2264 d,\n\u03b2fj =\n\u220fd k=1 c fk k\nC \u00b7 pcj pcj \u2212 ej,b?j efjj,b?j ej,b?j \u00b7 cj c fj j \u2212 1  . Proof. In view of Proposition 15, we just have to compute E [\u03c0f(x)] and E [\u03c0zjf(x)] for any given 1 \u2264 j \u2264 d. We begin with the computation of E [\u03c0f(x)]:\nE [\u03c0f(x)] = E [ k\u220f k=1 exp ( \u2212(\u03c4k(xk)\u2212 \u03c4k(\u03bek))2 2\u03bd2 ) fk(xk) ] (Assumption 2 + Eq. (18))\n= d\u220f\nk=1\nE [ exp ( \u2212(\u03c4k(xk)\u2212 \u03c4k(\u03bek))2\n2\u03bd2\n) fk(xk) ] (independence)\nE [\u03c0f(x)] = d\u220f\nk=1\ncfkk (Lemma 30)\nThe second computation is very similar in spirit:\nE [\u03c0zjf(x)] = E  k\u220f k=1 k 6=j e \u2212(\u03c4k(xk)\u2212\u03c4k(\u03bek)) 2 2\u03bd2 fk(xk) \u00b7 e \u2212(\u03c4j(xj)\u2212\u03c4j(\u03bej)) 2 2\u03bd2 zjfj(xj)  (Assumption 2 + Eq. (18))\n= d\u220f\nk=1 6=j\nE [ exp ( \u2212(\u03c4k(xk)\u2212 \u03c4k(\u03bek))2\n2\u03bd2\n) fk(xk) ] \u00b7 E [ e \u2212(\u03c4j(xj)\u2212\u03c4j(\u03bej)) 2 2\u03bd2 zjfj(xj) ] (independence)\nE [\u03c0zjf(x)] = d\u220f\nk=1 k 6=j\ncfkk \u00b7 e fj j,b?j\np . (Lemma 30)\nSimple algebra concludes the proof."}, {"heading": "Appendix E. Proof of Proposition 2", "text": "In this Appendix, we prove the regularity result for Tabualr LIME, Proposition 2 of the main paper.\nProof. First let us set h := f \u2212 g and \u03b5 := \u2016h\u2016\u221e. We notice that\n\u2016\u03b2f \u2212 \u03b2g\u2016 = \u2016\u03a3\u22121\u0393f \u2212 \u03a3\u22121\u0393g\u2016 = \u2016\u03a3\u22121\u0393h\u2016 .\nLet us focus first on the first coordinate:\n(\u03a3\u22121\u0393h)0 = C \u22121 1 + d\u2211 j=1 1 pc\u2212 1 E [\u03c0h(x)] + C\u22121 d\u2211 j=1 \u2212pc pc\u2212 1 E [\u03c0zjh(x)]\n= C\u22121 E [\u03c0h(x)] + C\u22121 d\u2211 j=1 1 pc\u2212 1 E [\u03c0(1\u2212 pczj)h(x)] .\nRecall Lemma 32: \u2223\u2223C\u22121 E [\u03c0h(x)]\u2223\u2223 \u2264 \u03b5 . As for the second part, we write\nE [\u03c0 |1\u2212 pczj |h(x)] \u2264 (pcE [\u03c0zj ] + E [\u03c0])\u03b5\n= (pc \u00b7 C pc + C)\u03b5 (Lemma 32)\nE [\u03c0 |1\u2212 pczj |h(x)] \u2264 2C\u03b5\nWe deduce \u2223\u2223\u2223(\u03a3\u22121\u0393h)0\u2223\u2223\u2223 \u2264 (1 + 2d p\u2212 1 e 1 2\u03bd2 ) \u03b5 . (27)\nNow let us set j \u2265 1.\n(\u03a3\u22121\u0393h)j = C \u22121 [ \u2212pc pc\u2212 1 E [\u03c0h(x)] + p2c2 pc\u2212 1 E [\u03c0zjh(x)] ] = C\u22121pc\npc\u2212 1 E [\u03c0(pczj \u2212 1)h(x)] .\nAs before, we obtain \u2223\u2223\u2223(\u03a3\u22121\u0393h)j\u2223\u2223\u2223 \u2264 2pc\u03b5 pc\u2212 1 \u2264 2p e 1 2\u03bd2 \u03b5 p\u2212 1 . (28)\nWe then collect Eq. (27) and (28) to obtain the promised bound."}, {"heading": "Appendix F. Proof of Proposition 6", "text": "In this section, we show how to compute \u03b2f for indicator function with rectangular support (Proposition 6 of the main paper).\nProof. Our main task is to compute the e aj j,b coefficients in this specific case. By definition of the ej,b, we have\ne aj j,b = e\n\u22121b=b? j 2\u03bd2 E [ 1xj\u2208[sj ,tj ] \u2223\u2223\u2223bj 6= b] =: e\u22121b 6=b?j2\u03bd2 etj,b . (29) Since xj has support on [qj,b\u22121, qj,b] conditionally to the event {bj = b}, it is straightforward to compute etj,b with respect to the relative position of [sj , tj ] and [qj,b\u22121, qj,b]. In particular, etj,b = 0 if the intersection is empty, and we find\netj,b = \u03a6 ( tj\u2227qj,b\u2212\u00b5j,b \u03c3j,b ) \u2212 \u03a6 ( sj\u2228qj,b\u22121\u2212\u00b5j,b \u03c3j,b ) \u03a6 ( qj,b\u2212\u00b5j,b \u03c3j,b ) \u2212 \u03a6 ( qj,b\u22121\u2212\u00b5j,b \u03c3j,b\n) (30) otherwise. Now recall that we assumed that there exist 1 \u2264 b\u25e6j \u2264 p such that [sj , tj ] \u2286 [qj,b\u25e6j\u22121, qj,b\u25e6j ]. Therefore Eq. (30) simplifies and we find\netj,b =  \u03a6 ( tj\u2212\u00b5j,b \u03c3j,b ) \u2212\u03a6 ( sj\u2212\u00b5j,b \u03c3j,b ) \u03a6 ( qj,b\u2212\u00b5j,b \u03c3j,b ) \u2212\u03a6 ( qj,b\u22121\u2212\u00b5j,b \u03c3j,b ) =: \u03bbj if b = b\u25e6j , 0 otherwise.\nStraightforward computations yield\nc aj j c = etj,b?j\n+ e \u22121 2\u03bd2 \u2211\nb 6=b?j etj,b\n1 + (p\u2212 1) e \u22121 2\u03bd2\nand\npc\npc\u2212 1 ( e aj j,b?j \u00b7 c c aj j \u2212 1 ) = 1 p\u2212 1 \u2211 b 6=b?j (etj,b?j \u2212 ej,b) \u00b7 c c aj j .\nFor any 1 \u2264 k \u2264 d, there are two possible cases: either \u03bek \u2208 Ak (\u03be is aligned with A along dimension k), or \u03bek /\u2208 Ak (\u03be is not aligned). In the first case, etk,b?k = \u03bbk and all the other e t k,b are equal to zero. Therefore c ak k c = \u03bbk pc , and 1 p\u22121 \u2211 b 6=b?j (etk,b?k \u2212ek,b) = \u03bbk. In the second case,\netk,b?k = 0 and there is only one b 6= b?k such that etk,b = \u03bbk. We deduce that c ak k c =\n\u03bbk e \u22121 2\u03bd2\npc and 1p\u22121 \u2211 b 6=b?k (etk,b?k\n\u2212 ek,b) = \u2212\u03bbkp\u22121 . From this discussion, we obtain\u220fd k=1 c ak k\ncd = \u220fd k=1 \u03bbk pdcd exp ( \u2212#{k s.t. \u03bek /\u2208 Ak} 2\u03bd2 ) = I(A) pdcd exp ( \u2212d (\u03be, A) 2\u03bd2 ) .\nFinally, we can use Proposition 5 to conclude.\nWe provide in Figure 16 an illustration of Proposition 5."}, {"heading": "Appendix G. Concentration of \u03a3\u0302n", "text": "In this section, we show that \u03a3\u0302n is concentrated around \u03a3 in operator norm. The idea is to use standard results on the concentration of sum of independent random matrices (Vershynin, 2018). Indeed, \u03a3\u0302 can be written as 1n \u2211n i=1 \u03c0iZiZ > i . Since each of these matrices are bounded and identically distributed, we turn to a Hoeffding-type inequality. We borrow the following result from Tropp (2012).\nTheorem 20 (Matrix Hoeffding (Tropp, 2012)). Consider a finite sequence Mi of independent, random, symmetric matrices with common dimension D, and let Ai be a sequence of fixed symmetric matrices. Assume that each random matrix satisfies\nE [Mi] = 0 and M2i 4 A2i almost surely ."}, {"heading": "Then, for all t \u2265 0,", "text": "P ( \u03bbmax ( n\u2211 i=1 Mi ) \u2265 t ) \u2264 D \u00b7 exp ( \u2212t2 8\u03c32 ) ,\nwhere \u03c32 := \u2225\u2225\u2211n\ni=1A 2 i \u2225\u2225 op .\nWe slightly adapt this result for the situation at hand.\nCorollary 21 (Matrix Hoeffding, bounded entries). Consider a finite sequence Mi of independent, centered, random, symmetric matrices with dimension D. Assume that the entries of each matrix satisfy\n(Mi)j,k \u2208 [\u22121, 1] almost surely ."}, {"heading": "Then, for all t \u2265 0,", "text": "P \u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 Mi \u2225\u2225\u2225\u2225\u2225 op \u2265 t  \u2264 2D \u00b7 exp(\u2212nt2 8D2 ) .\nProof. Let u \u2208 RD such that \u2016u\u2016 = 1. We write\n|(Miu)j | = D\u2211 k=1 (Mi)j,kuk\n\u2264 \u2016(Mi)j\u2016 \u00b7 \u2016u\u2016 (Cauchy-Schwarz) \u2264 \u221a D . (bounded a.s. + \u2016u\u2016 = 1)\nWe deduce that \u2016Miu\u2016 \u2264 D almost surely. Since we considered an arbitrary u, we have showed that \u2016Mi\u2016op \u2264 D almost surely for any i. Thus we can apply Th. 20 with Ai = D ID to obtain\nP ( \u03bbmax ( n\u2211 i=1 Mi ) \u2265 t ) \u2264 D \u00b7 exp ( \u2212t2 8nD2 ) ,\nusing the fact that Ai commutes with Mi and thus Mi 4 Ai implies M2i 4 A 2 i . The result is a direct consequence from the last display.\nWe can now proceed to the main result of this section, the concentration of \u03a3\u0302 around its mean \u03a3.\nProposition 22 (Concentration of \u03a3\u0302, general weights). For any t \u2265 0,\nP ( \u2016\u03a3\u0302\u2212 \u03a3\u2016op \u2265 t ) \u2264 4d \u00b7 exp ( \u2212nt2\n32d2\n) .\nProof. Recall Eq. (21): the entries of \u03c0iZiZ > i belong to [0, 1] almost surely since \u03c0i \u2208 [0, 1] for any weights satisfying Eq. (18) and zij \u2208 [0, 1]. As a consequence, so do the entries of \u03a3. Let us set\nMi := \u03c0iZiZ > i \u2212 \u03a3 .\nThen Mi satisfies the assumptions of Th. 21 with D = d + 1 and the result follows since 1 n \u2211n i=1Mi = \u03a3\u0302\u2212 \u03a3."}, {"heading": "Appendix H. Concentration of \u0393\u0302", "text": "The goal of this section is the concentration of \u0393\u0302. Interestingly, we could not find a multivariate version of Hoeffding\u2019s inequality (Vershynin, 2018). We resort to a combination of Hoeffding\u2019s inequality in the univariate case and a union bound argument.\nTheorem 23 (Hoeffding\u2019s inequality). Let Mi be a finite sequence of centered random variables such that"}, {"heading": "Mi \u2208 [\u2212M,M ] almost surely .", "text": ""}, {"heading": "Then, for any t \u2265 0,", "text": "P (\u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 Mi \u2223\u2223\u2223\u2223\u2223 \u2265 t ) \u2264 2 \u00b7 exp ( \u2212nt2 2M2 ) .\nProof. This is Th. 2.8 in Boucheron et al. (2013) in our notation.\nProposition 24 (Concentration of \u0393\u0302, general weights). Suppose that f is bounded by M on S. Then, for any t \u2265 0,\nP ( \u2016\u0393\u0302\u2212 \u0393f\u2016 \u2265 t ) \u2264 4d exp ( \u2212nt2\n32Md2\n) .\nProof. The components of \u0393\u0302 are given by \u03c0if(xi) and \u03c0izi,jf(xi). Since f is bounded by M and \u03c0i, zi,j \u2208 [0, 1] for any weights satisfying Eq. (18), these quantities live in [0,M ]. We deduce that, for any i, j, {\n\u03c0if(xi)\u2212 \u0393f0 \u2208 [\u22122M, 2M ] \u03c0izi,jf(xi)\u2212 \u0393fj \u2208 [\u22122M, 2M ]\nalmost surely. We can apply Th. 23 coordinate by coordinate: for any t \u2265 0,P (\u2223\u2223\u2223 1n\u2211ni=1 \u03c0if(xi)\u2212 \u0393f0 \u2223\u2223\u2223 \u2265 t) \u2264 2 \u00b7 exp(\u2212nt28M2 ) P (\u2223\u2223\u2223 1n\u2211ni=1 \u03c0izi,jf(xi)\u2212 \u0393fj \u2223\u2223\u2223 \u2265 t) \u2264 2 \u00b7 exp(\u2212nt28M2 )\nBy a union bound argument,\nP (\u2016u\u2016 \u2265 t) \u2264 P (\nmax i |ui| \u2265 t/D\n) \u2264 D \u00b7 P (|ui| \u2265 t/D) ,\nwe deduce the result. (Note that, as usual, we used d+ 1 \u2264 2d.)\nAppendix I. Proof of the main result\nIn this section we prove that \u03b2\u0302 is concentrated around \u03b2f ."}, {"heading": "I.1 Binding lemma", "text": "We begin by a somewhat technical result, showing that we can control the behavior of \u2016\u03b2\u0302 \u2212 \u03a3\u22121\u0393f\u2016 by controlling \u2016\u03a3\u0302\u2212 \u03a3\u2016op, \u2016\u0393\u0302\u2212 \u0393f\u2016, and \u2225\u2225\u03a3\u22121\u2225\u2225 op .\nLemma 25 (Control of (ID +H) \u22121). Let H \u2208 RD\u00d7D be a matrix such that \u2016H\u2016op \u2264 \u22121 + \u221a\n7 2 (\u2248 0.32). Then ID +H is invertible and\u2225\u2225(ID +H)\u22121\u2225\u2225op \u2264 2 .\nProof. We first show that ID +H is invertible. Indeed, suppose that Ker (ID +H) 6= {0}. Then, in particular, there exists x \u2208 RD with unit norm such that Hx = \u2212x. Since \u2016H\u2016op = max\u2016x\u2016=1 \u2016Hx\u2016, we would have \u2016H\u2016op \u2265 1, which contradicts \u2016H\u2016op \u2264 0.32.\nNow according to Lemma 34,\u2225\u2225(ID +H)\u22121\u2225\u2225op = (\u03bbmin ((ID +H)>(ID +H)))\u22121/2 . (31) Let us set M := (ID +H)\n>(ID +H) and find a lower bound on \u03bbmin (M). We first notice that M is positive semi-definite. Thus we now that \u03bbmin (M) = min\u2016x\u2016=1 x >Mx. Let us fix x \u2208 SD\u22121 for now. Then\nx>Mx = x>(M \u2212 ID)x+ x>x = x>(M \u2212 ID)x+ 1 ,\nsince \u2016x\u2016 = 1. But on the other side,\n\u2212x>(M \u2212 ID)x \u2264 |\u3008x, (M \u2212 ID)x\u3009| \u2264 \u2016x\u2016 \u00b7 \u2016(M \u2212 ID)x\u2016 (Cauchy-Schwarz) \u2264 \u2016M \u2212 ID\u2016op (definition of \u2016\u00b7\u2016op + \u2016x\u2016 = 1)\nMoreover, by the triangle inequality and sub-multiplicativity of the operator norm, \u2016M \u2212 ID\u2016op = \u2225\u2225\u2225H +H> +H>H\u2225\u2225\u2225\nop \u2264 2 \u2016H\u2016op + \u2016H\u2016 2 op .\nWe deduce that \u03bbmin (M) \u2265 1\u2212 2 \u2016H\u2016op \u2212 \u2016H\u2016 2 op , (32)\nwhich is a positive quantity since we assumed \u2016H\u2016op \u2264 0.32. Plugging the lower bound (32) into Eq. (31), we obtain\u2225\u2225(ID +H)\u22121\u2225\u2225op \u2264 1\u221a\n1\u2212 2 \u2016H\u2016op \u2212 \u2016H\u2016 2 op\n.\nSet \u03c8(x) := (1\u22122x\u2212x2)\u22121/2. One can easily check that \u03c8(x) \u2264 2 for any x \u2208 [0,\u22121+ \u221a\n7/2], which concludes the proof.\nRemark 2. The numerical constant 2 in the statement of Lemma 25 can be replaced by any arbitrary constant greater than 1, at the cost of constraining further the range of \u2016H\u2016op.\nRemark 3. The Hoffman-Wielandt inequality also yields a lower bound on \u03bbmin (M), giving essentially the same result but with the Frobenius norm instead of the operator norm. Since we know how to control \u2225\u2225\u2225\u03a3\u0302\u2212 \u03a3\u2225\u2225\u2225\nop , we prefer this version of the result.\nUsing Lemma 25 we can prove something more interesting.\nLemma 26 (Control of (ID +H) \u22121\u2212 Id). Let H \u2208 RD\u00d7D be a matrix such that \u2016H\u2016op \u2264 \u22121 + \u221a\n7 2 (\u2248 0.32). Then ID +H is invertible, and\u2225\u2225(ID +H)\u22121 \u2212 ID\u2225\u2225op \u2264 2 \u2016H\u2016op .\nProof. According to Lemma 25, ID +H is an invertible matrix. Now we write\n(ID +H) \u22121 \u2212 ID = (ID +H)\u22121(ID \u2212(ID +H)) = \u2212(ID +H)\u22121H . (33)\nSince the operator norm is sub-multiplicative, Eq. (33) implies that\u2225\u2225(ID +H)\u22121 \u2212 ID\u2225\u2225op \u2264 \u2225\u2225(ID +H)\u22121\u2225\u2225op \u00b7 \u2016H\u2016op . and Lemma 25 guarantees that\n\u2225\u2225(ID +H)\u22121\u2225\u2225op \u2264 2 under our assumptions. Remark 4. It is a good surprise that the constants in Lemma 26 do not depend on the dimension. In fact, we believe that this result is nearly optimal. Indeed, in the unidimensional case, one can show that\u2223\u2223\u2223\u2223 11 + h \u2212 1\n\u2223\u2223\u2223\u2223 \u2264 2 |h| , for any h \u2208 R such that |h| \u2264 12 , showing that the inequality cannot be significantly improved.\nWe are now able to state and prove the main result of this section.\nLemma 27 (Binding lemma). Let X \u2208 RD\u00d7D such that X is invertible and Y \u2208 RD. Then, for any H \u2208 RD\u00d7D such that \u2225\u2225X\u22121H\u2225\u2225 op \u2264 0.32 and any H \u2032 \u2208 RD, it holds that\n\u2225\u2225(X +H)\u22121(Y +H \u2032)\u2212X\u22121Y \u2225\u2225 \u2264 2\u2225\u2225X\u22121\u2225\u2225 op \u2225\u2225H \u2032\u2225\u2225+ 2 \u2225\u2225X\u22121\u2225\u22252 op \u2016Y \u2016 \u2016H\u2016op . (34)\nIn particular, we achieve the promised control by setting X = \u03a3, Y = \u0393f , H = \u03a3\u0302\u2212 \u03a3, and H \u2032 = \u0393\u0302\u2212 \u0393f in Lemma 27. Namely,\u2225\u2225\u2225\u03b2\u0302 \u2212 \u03b2f\u2225\u2225\u2225 \u2264 2 \u2225\u2225\u03a3\u22121\u2225\u2225op \u2225\u2225\u2225\u0393\u0302\u2212 \u0393f\u2225\u2225\u2225+ 2 \u2225\u2225\u03a3\u22121\u2225\u22252op \u2225\u2225\u2225\u0393f\u2225\u2225\u2225\u2225\u2225\u2225\u03a3\u0302\u2212 \u03a3\u2225\u2225\u2225op . (35) Proof. We first notice that since \u2225\u2225X\u22121H\u2225\u2225 op \u2264 0.32, the matrix ID +X\u22121H is invertible according to Lemma 25. We deduce that X +H is also invertible, with\n(X +H)\u22121 = ( X(ID +X \u22121H) )\u22121 = (ID +X \u22121H)\u22121X\u22121 . (36)\nLet us split the left-hand side of Eq. (34): by the triangle inequality,\u2225\u2225(X +H)\u22121(Y +H \u2032)\u2212X\u22121Y \u2225\u2225 \u2264 \u2225\u2225(X +H)\u22121(Y +H \u2032)\u2212 (X +H)\u22121Y \u2225\u2225 + \u2225\u2225(X +H)\u22121Y \u2212X\u22121Y \u2225\u2225\n= \u2225\u2225(X +H)\u22121H \u2032\u2225\u2225+ \u2225\u2225(X +H)\u22121Y \u2212X\u22121Y \u2225\u2225 .\nLet us focus on the first term. We write\u2225\u2225(X +H)\u22121\u2225\u2225 op = \u2225\u2225(ID +X\u22121H)\u22121X\u22121\u2225\u2225op (Eq. 36)\n\u2264 \u2225\u2225(ID +X\u22121H)\u22121\u2225\u2225op \u00b7 \u2225\u2225X\u22121\u2225\u2225op (\u2016\u00b7\u2016op is sub-multiplicative)\n\u2264 2 \u2225\u2225X\u22121\u2225\u2225\nop . (Lemma 25)\nFrom the last display we deduce that\u2225\u2225(X +H)\u22121H \u2032\u2225\u2225 \u2264 2 \u2225\u2225X\u22121\u2225\u2225 op \u2225\u2225H \u2032\u2225\u2225 . (37) Now for the second term, we have\u2225\u2225(X +H)\u22121 \u2212X\u22121\u2225\u2225 op = \u2225\u2225(ID +X\u22121H)\u22121X\u22121 \u2212X\u22121\u2225\u2225op (Eq. (36))\n\u2264 \u2225\u2225(ID +X\u22121H)\u22121 \u2212 Id\u2225\u2225op \u00b7 \u2225\u2225X\u22121\u2225\u2225op (\u2016\u00b7\u2016op is sub-multiplicative) \u2264 2 \u2225\u2225X\u22121H\u2225\u2225 op \u00b7 \u2225\u2225X\u22121\u2225\u2225\nop (Lemma 26)\u2225\u2225(X +H)\u22121 \u2212X\u22121\u2225\u2225\nop \u2264 2 \u2016H\u2016op \u00b7 \u2225\u2225X\u22121\u2225\u22252 op\n(\u2016\u00b7\u2016op is sub-multiplicative)\nWe deduce that \u2225\u2225(X +H)\u22121Y \u2212X\u22121Y \u2225\u2225 \u2264 2 \u2016H\u2016op \u00b7 \u2225\u2225X\u22121\u2225\u22252op \u00b7 \u2016Y \u2016 . (38) We conclude the proof by adding Eq. (37) and Eq. (38)."}, {"heading": "I.2 Concentration of \u03b2\u0302n", "text": "We are now able to state and prove our main result, the concentration of \u03b2\u0302n around \u03b2 f for a general f and general weights satisfying Eq. 18.\nTheorem 28 (Concentration of \u03b2\u0302n, general f , general weights). Suppose that f is bounded by M on S. Let \u03b5 > 0 be a small constant , at least smaller than 1/M . Let \u03b7 \u2208 (0, 1). Then, for every\nn \u2265 max { 212Md4p4 e 4 \u03bd2 log 8d\u03b7\nC2\u03b52 , 215M2d7p8 e\n8 \u03bd2 log 8d\u03b7\nC4\u03b52\n} ,\nwe have P (\u2225\u2225\u2225\u03b2\u0302n \u2212 \u03b2f\u2225\u2225\u2225 \u2265 \u03b5) \u2264 \u03b7.\nProof. Let us define\nt1 := C\u03b5\n8 \u221a 2dp2 e 2 \u03bd2\nand t2 := C2\u03b5\n32Md5/2p4 e 4 \u03bd2\n.\nAccording to Proposition 24 and 22, we can build events \u21261 and \u21262 such that: (i) on \u21261,\u2225\u2225\u2225\u0393\u0302n \u2212 \u0393f\u2225\u2225\u2225 \u2264 t1 with probability higher than 1 \u2212 4d exp (\u2212nt21/(32Md2)), and (ii) on \u21262,\u2225\u2225\u2225\u03a3\u0302n \u2212 \u03a3\u2225\u2225\u2225 op \u2264 t2 with probability higher than 1\u2212 4d exp ( \u2212nt22/(32d2) ) . Let us define\nn1 := 212Md4p4 log e\n4 \u03bd2\n8d \u03b7\nC2\u03b52 and n2 :=\n215M2d7p8 e 8 \u03bd2\n8d \u03b7\nC4\u03b52 .\nBy assumption, n is larger than max(n1, n2). One can check that, in this case, \u21261 and \u21262 both have probability higher than 1\u2212\u03b7/2. We now work on the event \u2126 := \u21261\u2229\u21262. By the union bound, \u2126 has probability greater than 1\u2212 \u03b7. Let us show that, on \u2126, \u2225\u2225\u2225\u03b2\u0302n \u2212 \u03b2f\u2225\u2225\u2225 \u2264 \u03b5. First note that, according to Proposition 12, \u2225\u2225\u03a3\u22121\u2225\u2225 op \u2264 2sqrt2dp 2 e 1 2\u03bd2\nC . Thus, since the operator norm is sub-multiplicative, we have\n\u2225\u2225\u2225\u03a3\u22121(\u03a3\u0302n \u2212 \u03a3)\u2225\u2225\u2225 op \u2264 \u2225\u2225\u03a3\u22121\u2225\u2225 op \u2225\u2225\u2225\u03a3\u0302n \u2212 \u03a3\u2225\u2225\u2225 op\n\u2264 2 \u221a 2dp2 e 1 2\u03bd2\nC \u00b7 C\n2\u03b5\n32Md5/2p4 e 4 \u03bd2\n\u2264 \u221a 2\n16 \u00b7 \u03b5 M \u00b7 C \u00b7 1 d3/2 \u00b7 1 p2 \u00b7 e \u22122 \u03bd2 .\nSince we assumed \u03b5 < M , \u2225\u2225\u2225\u03a3\u22121(\u03a3\u0302n \u2212 \u03a3)\u2225\u2225\u2225\nop \u2264 \u221a 2/16 < 0.32. Therefore we can use Eq. (35)\nand the result follows."}, {"heading": "Appendix J. Extension of Proposition 9", "text": "We now present a generalization and a proof of Proposition 9 for general weights. That is, we show that Tabular LIME ignores unused coordinates for general weights.\nProposition 29 (Ignoring unused coordinates, general weights). Assume that f satisfies Assumption 3 and is bounded on S. Let j \u2208 S, where S is the set of indices relevant for f and S := {1, . . . , d} \\ S. Then \u03b2fj = 0.\nProof. The proof is a direct application of Theorem 28, and can be seen as a straightforward generalization of the proof of Proposition 9. We compute first\nE [\u03c0f(x)] = E\n[ d\u220f\nk=1\ne \u2212(\u03c4k(xk)\u2212\u03c4k(\u03bek))\n2\n2\u03bd2 f(x) ] (Eq. (18))\n= E\n[ d\u220f\nk=1\ne \u2212(\u03c4k(xk)\u2212\u03c4k(\u03bek))\n2\n2\u03bd2 g(xj1 , . . . , xjs)\n] (Assumption 3)\n= \u220f k\u2208S E [ e \u2212(\u03c4k(xk)\u2212\u03c4k(\u03bek)) 2 2\u03bd2 ] \u00b7 E [\u220f k\u2208S e \u2212(\u03c4k(xk)\u2212\u03c4k(\u03bek)) 2 2\u03bd2 g(xj1 , . . . , xjs) ] (independence)\nE [\u03c0f(x)] = \u220f k\u2208S ck \u00b7G , (Lemma 30)\nwhere we set\nG := E [\u220f k\u2208S e \u2212(\u03c4k(xk)\u2212\u03c4k(\u03bek)) 2 2\u03bd2 g(xj1 , . . . , xjs) ] in the last display. The other computation is similar. Recall that j /\u2208 S:\nE [\u03c0zjf(x)] = E\n[ d\u220f\nk=1\ne \u2212(\u03c4k(xk)\u2212\u03c4k(\u03bek))\n2\n2\u03bd2 zjf(x)\n] (Eq. (18))\n= \u220f\nk\u2208S\\{j}\nck \u00b7 E [ e \u2212(\u03c4j(xj)\u2212\u03c4j(\u03bej)) 2 2\u03bd2 zj ] \u00b7 E [\u220f k\u2208S e \u2212(\u03c4k(xk)\u2212\u03c4k(\u03bek)) 2 2\u03bd2 g(xj1 , . . . , xjs) ] (independence)\nE [\u03c0zjf(x)] = \u220f k\u2208S ck\ncj \u00b7 1 p ej,b?j \u00b7G .\nFinally we write\n\u03b2fj = C \u22121 pcj pcj \u2212 ej,b?j\n( \u2212E [\u03c0f(x)] + pcj\nej,b?j E [\u03c0zjf(x)]\n) (Prop. 15)\n= C\u22121 pcj\npcj \u2212 ej,b?j \u2212\u220f k\u2208S ck \u00b7G+ pcj ej,b?j \u220f k\u2208S ck cj \u00b7 1 p ej,b?j \u00b7G  \u03b2fj = 0 ."}, {"heading": "Appendix K. Technical results", "text": "In this Appendix we collect technical results used throughout the main paper."}, {"heading": "K.1 Expected values computations", "text": "We begin with the computation of the expected values needed for the computation of \u03a3 and \u0393f . We start with a generic lemma, a very common computation in our proofs which appears each time we use the independence assumption between the xij and we split the \u03c0i product.\nLemma 30 (Basic computations). Let \u03c8 : R \u2192 R be a function which is bounded on the support of xj for any 1 \u2264 j \u2264 d. ThenE [ exp ( \u2212(\u03c4j(xj)\u2212\u03c4j(\u03bej))2 2\u03bd2 ) \u03c8(xj) ] = 1p \u2211p b=1 e \u03c8 j,b E [ exp ( \u2212(\u03c4j(xj)\u2212\u03c4j(\u03bej))2\n2\u03bd2\n) zj\u03c8(xj) ] = 1pe \u03c8 j,b?j .\nIn particular,\nE [ exp ( \u22121 2\u03bd2 (\u03c4j(\u03bej)\u2212 \u03c4j(xj))2 )] = cj . (39)\nProof. Straightforward from the law of total expectation and the definition of the ej,b coefficients.\nLemma 30 is the reason why the ej,b are ubiquitous in our results. If the weights have some multiplicative structure, it is easy to extend Lemma 30 to the full weights, which we achieve in our next result.\nLemma 31 (Key computation). Suppose that \u03c0i satisfies Eq. (18). Let \u03c8 : R\u2192 R be a function bounded on the support of xj for any 1 \u2264 j \u2264 d. Then, for any given i, j, k with j 6= k,  E [\u03c0\u03c8(xj)] = Cpcj \u2211p b=1 e \u03c8 j,b E [\u03c0zj\u03c8(xj)] = Cpcj e \u03c8 j,b?j\nE [\u03c0zj\u03c8(xk)] = Cp2cjck ej,b?j \u2211p b=1 e \u03c8 k,b .\nProof. We write\nE [\u03c0\u03c8(xj)] = E [ exp ( \u22121 2\u03bd2 d\u2211 k=1 (\u03c4k(\u03bek)\u2212 \u03c4k(xk))2 ) \u00b7 \u03c8(xj) ] (Eq. (18))\n= \u220f k 6=j ck \u00b7 E [ \u03c8(xj) exp ( \u22121 2\u03bd2 (\u03c4j(\u03bej)\u2212 \u03c4j(xj))2 )] (independence + Eq. (39))\n= \u220f k 6=j ck \u00b7 1 p p\u2211 b=1 e\u03c8j,b (Lemma 30)\nE [\u03c0\u03c8(xj)] = C\npcj p\u2211 b=1 e\u03c8j,b . (definition of C)\nThe proofs of the remaining results are quite similar:\nE [\u03c0zj\u03c8(xj)] = E [ exp ( \u22121 2\u03bd2 d\u2211 k=1 (\u03c4k(\u03bek)\u2212 \u03c4k(xk))2 ) \u00b7 zj\u03c8(xj) ] (Eq. (18))\n= \u220f k 6=j ck \u00b7 E [ \u03c8(xj)zj exp ( \u22121 2\u03bd2 (\u03c4j(\u03bej)\u2212 \u03c4j(xxj ))2 )]\n(independence + Eq. (39)) = \u220f k 6=j ck \u00b7 1 p e\u03c8j,b?j (Lemma 30)\nE [\u03c0zj\u03c8(xj)] = C\npcj e\u03c8j,b?j\nE [\u03c0zj\u03c8(xk)] = E [ exp ( \u22121 2\u03bd2 d\u2211 `=1 (\u03c4`(\u03be`)\u2212 \u03c4`(x`))2 ) \u00b7 zj\u03c8(xk) ] (Eq. (18))\n= \u220f ` 6=j,k c` \u00b7 E [ zj exp ( \u22121 2\u03bd2 (\u03c4j(\u03bej)\u2212 \u03c4j(xj))2 )]\n\u00b7 E [ \u03c8(xk) exp ( \u22121 2\u03bd2 (\u03c4k(\u03bek)\u2212 \u03c4k(xk))2 )]\n(independence)\n= \u220f ` 6=j,k c` \u00b7 ej,b?j p \u00b7 1 p p\u2211 b=1 e\u03c8k,b (Lemma 30)\nE [\u03c0zj\u03c8(xk)] = C\np2cjck ej,b?j p\u2211 b=1 e\u03c8k,b . (definition of C)\nWe specialize Lemma 31 in the case \u03c8 = 1, since the ej,b coefficients are ubiquitous in our computations.\nLemma 32 (Expected values computations, zero-th order). For any j 6= k, E [\u03c0] = C E [\u03c0zj ] = C ej,b? j pcj\nE [\u03c0zjzk] = C ej,b? j\npcj\nek,b? k\npck\nProof. The first two results are a direct consequence of Lemma (31) for \u03c8 = 1. For the third one, we set \u03c8(x) = 1x\u2208[qk,b?\nk \u22121,qk,b? k ], and we notice that, in this case,\ne\u03c8k,b =\n{ ek,b?k if b = b ? k\n0 otherwise.\nThe case \u03c8 = id is also of some importance in our analysis, let us specialize Lemma 31 in that case as well.\nLemma 33 (Expected values, first order). Let j, k \u2208 {1, . . . , d} be fixed indices, with j 6= k. Then  E [\u03c0xj ] = Cpcj \u2211p b=1 e \u00d7 j,b E [\u03c0zjxj ] = Cpcj e \u00d7 j,b?j\nE [\u03c0zjxk] = Cp2cjck ej,b?j \u2211p b=1 e \u00d7 k,b\nProof. Straightforward from Lemma 31 with \u03c8 = id.\nK.2 Some facts about operator norm\nIn this section, we collect some facts about the operator norm that are used in Appendix I.\nLemma 34 (Inversion formula for the operator norm). Let M \u2208 Rd\u00d7d be an invertible matrix. Then \u2225\u2225M\u22121\u2225\u2225 op = ( \u03bbmin ( M>M ))\u22121/2 .\nProof. By the definition of the operator norm, we know that\u2225\u2225M\u22121\u2225\u22252 op = \u03bbmax ( (M\u22121)>M\u22121 ) .\nSince we are in a commutative ring, (M\u22121)> = (M>)\u22121. Additionally, for any two matrices such that AB is invertible, (AB)\u22121 = B\u22121A\u22121. Therefore\u2225\u2225M\u22121\u2225\u22252\nop = \u03bbmax\n( (MM>)\u22121 ) .\nSince MM> is a positive definite matrix, Spec ( MM> ) \u2286 R+, and \u03bbmax ( (MM>)\u22121 ) =\n\u03bbmin ( MM> )\u22121 . We can conclude since for any two matrices, AB and BA have the same spectrum.\nLemma 35 (Bounding the operator norm). For any matrix M \u2208 Rd\u00d7d, we have\n\u2016M\u2016op \u2264 \u2016M\u2016F \u2264 \u221a d \u2016M\u2016op .\nProof. We first write\n\u2016M\u2016op = \u03bbmax(M >M) \u2264 \u2211 j \u03bbj(M >M) = Tr ( M>M ) = \u2016M\u2016F .\nAs for the second part of the result, we write \u2016M\u20162F = Tr ( M>M ) (definition)\n= d\u2211 i=1 \u03bbi(M >M) (property of the trace)\n\u2264 d\u03bbmax ( M>M ) (non-negative eigenvalues)"}], "title": "Looking Deeper into Tabular LIME", "year": 2020}