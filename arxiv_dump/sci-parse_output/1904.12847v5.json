{
  "abstractText": "Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980\u2019s. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. Our experiments highlight advantages in scalability, speed, and proof of optimality.",
  "authors": [
    {
      "affiliations": [],
      "name": "Xiyang Hu"
    },
    {
      "affiliations": [],
      "name": "Cynthia Rudin"
    },
    {
      "affiliations": [],
      "name": "Margo Seltzer"
    }
  ],
  "id": "SP:13f31a7afdeaa44f1486f865e9e007d29a7a63f8",
  "references": [
    {
      "authors": [
        "E. Angelino",
        "N. Larus-Stone",
        "D. Alabi",
        "M. Seltzer",
        "C. Rudin"
      ],
      "title": "Learning certifiably optimal rule lists for categorical data",
      "venue": "Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
      "year": 2017
    },
    {
      "authors": [
        "E. Angelino",
        "N. Larus-Stone",
        "D. Alabi",
        "M. Seltzer",
        "C. Rudin"
      ],
      "title": "Learning certifiably optimal rule lists for categorical data",
      "venue": "Journal of Machine Learning Research, 18(234):1\u201378,",
      "year": 2018
    },
    {
      "authors": [
        "K. Bennett"
      ],
      "title": "Decision tree construction via linear programming",
      "venue": "Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, Utica, Illinois,",
      "year": 1992
    },
    {
      "authors": [
        "K.P. Bennett",
        "J.A. Blue"
      ],
      "title": "Optimal decision trees",
      "venue": "Technical report, R.P.I. Math Report No. 214, Rensselaer Polytechnic Institute,",
      "year": 1996
    },
    {
      "authors": [
        "D. Bertsimas",
        "J. Dunn"
      ],
      "title": "Optimal classification trees",
      "venue": "Machine Learning, 106(7):1039\u20131082,",
      "year": 2017
    },
    {
      "authors": [
        "R. Blanquero",
        "E. Carrizosa",
        "C. Molero-R\u0131o",
        "D.R. Morales"
      ],
      "title": "Optimal randomized classification trees",
      "venue": "Aug.",
      "year": 2018
    },
    {
      "authors": [
        "L. Breiman",
        "J.H. Friedman",
        "R.A. Olshen",
        "C.J. Stone"
      ],
      "title": "Classification and Regression Trees",
      "venue": "Wadsworth,",
      "year": 1984
    },
    {
      "authors": [
        "A.W. Flores",
        "C.T. Lowenkamp",
        "K. Bechtel"
      ],
      "title": "False positives, false negatives, and false analyses: A rejoinder to \u201cMachine bias: There\u2019s software used across the country to predict future criminals",
      "venue": "Federal probation, 80(2), September",
      "year": 2016
    },
    {
      "authors": [
        "A.R. Klivans",
        "R.A. Servedio"
      ],
      "title": "Toward attribute efficient learning of decision lists and parities",
      "venue": "Journal of Machine Learning Research, 7:587\u2013602,",
      "year": 2006
    },
    {
      "authors": [
        "J. Larson",
        "S. Mattu",
        "L. Kirchner",
        "J. Angwin"
      ],
      "title": "How we analyzed the COMPAS recidivism algorithm",
      "venue": "ProPublica,",
      "year": 2016
    },
    {
      "authors": [
        "N. Larus-Stone",
        "E. Angelino",
        "D. Alabi",
        "M. Seltzer",
        "V. Kaxiras",
        "A. Saligrama",
        "C. Rudin"
      ],
      "title": "Systems optimizations for learning certifiably optimal rule lists",
      "venue": "Proc. Conference on Systems and Machine Learning (SysML),",
      "year": 2018
    },
    {
      "authors": [
        "B. Letham",
        "C. Rudin",
        "T.H. McCormick",
        "D. Madigan"
      ],
      "title": "Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model",
      "venue": "The Annals of Applied Statistics, 9(3):1350\u20131371,",
      "year": 2015
    },
    {
      "authors": [
        "M. McGough"
      ],
      "title": "How bad is Sacramento\u2019s air, exactly? Google results appear at odds with reality, some say",
      "venue": "Sacramento Bee,",
      "year": 2018
    },
    {
      "authors": [
        "M. Menickelly",
        "O. G\u00fcnl\u00fck",
        "J. Kalagnanam",
        "K. Scheinberg"
      ],
      "title": "Optimal decision trees for categorical data via integer programming",
      "venue": "Preprint at arXiv:1612.03225, Jan.",
      "year": 2018
    },
    {
      "authors": [
        "N. Narodytska",
        "A. Ignatiev",
        "F. Pereira",
        "J. Marques-Silva"
      ],
      "title": "Learning optimal decision trees with SAT",
      "venue": "Proc. International Joint Conferences on Artificial Intelligence (IJCAI), pages 1362\u20131368,",
      "year": 2018
    },
    {
      "authors": [
        "S. Nijssen",
        "E. Fromont"
      ],
      "title": "Mining optimal decision trees from itemset lattices",
      "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 530\u2013539. ACM,",
      "year": 2007
    },
    {
      "authors": [
        "J.R. Quinlan. C"
      ],
      "title": "Programs for Machine Learning",
      "year": 1993
    },
    {
      "authors": [
        "S. Verwer",
        "Y. Zhang"
      ],
      "title": "Learning optimal classification trees using a binary linear program formulation",
      "venue": "33rd AAAI Conference on Artificial Intelligence,",
      "year": 2019
    },
    {
      "authors": [
        "H. Yang",
        "C. Rudin",
        "M. Seltzer"
      ],
      "title": "Scalable Bayesian rule lists",
      "venue": "International Conference on Machine Learning (ICML),",
      "year": 2017
    },
    {
      "authors": [
        "J.R. Zech",
        "M.A. Badgeley",
        "M. Liu",
        "A.B. Costa",
        "J.J. Titano",
        "E.K. Oermann"
      ],
      "title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
      "venue": "PLoS Med., 15(e1002683),",
      "year": 2018
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Interpretable machine learning has been growing in importance as society has begun to realize the dangers of using black box models for high stakes decisions: complications with confounding have haunted our medical machine learning models [22], bad predictions from black boxes have announced to millions of people that their dangerous levels of air pollution were safe [15], high-stakes credit risk decisions are being made without proper justification, and black box risk predictions have been wreaking havoc with the perception of fairness of our criminal justice system [10]. In all of these applications \u2013 medical imaging, pollution modeling, recidivism risk, credit scoring \u2013 accurate interpretable models have been created (by the Center for Disease Control and Prevention, Arnold Foundation, and others). However, such interpretable-yet-accurate models are not generally easy to construct. If we want people to replace their black box models with interpretable models, the tools to build these interpretable models must first exist.\nDecision trees are one of the leading forms of interpretable models. Despite several attempts over the last several decades to improve the optimality of decision tree algorithms, the CART [7] and C4.5 [19] decision tree algorithms (and other greedy tree-growing variants) have remained as dominant methods in practice. CART and C4.5 grow decision trees from the top down without backtracking, which means that if a suboptimal split was introduced near the top of the tree, the algorithm could spend many extra splits trying to undo the mistake it made at the top, leading to less-accurate and less-interpretable trees. Problems with greedy splitting and pruning have been known since the early 1990\u2019s, when mathematical programming tools had started to be used for creating optimal binary-split decision trees [3, 4], in a line of work [5, 6, 16, 18] until the present [20]. However, these techniques use all-purpose optimization toolboxes and tend not to scale to realistically-sized problems unless simplified to trees of a specific form. Other works [11] make overly strong assumptions (e.g., independence of all variables) to ensure optimal trees are produced using greedy algorithms.\n\u2217Authors are listed alphabetically.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n90 4.\n12 84\n7v 5\n[ cs\n.L G\n] 1\n8 Se\np 20\nWe produce optimal sparse decision trees taking a different approach than mathematical programming, greedy methods, or brute force. We find optimal trees according to a regularized loss function that balances accuracy and the number of leaves. Our algorithm is computationally efficient due to a collection of analytical bounds to perform massive pruning of the search space. Our implementation uses specialized data structures to store intermediate computations and symmetries, a bit-vector library to evaluate decision trees more quickly, fast search policies, and computational reuse. Despite the hardness of finding optimal solutions, our algorithm is able to locate optimal trees and prove optimality (or closeness of optimality) in reasonable amounts of time for datasets of the sizes used in the criminal justice system (tens of thousands or millions of observations, tens of features).\nBecause we find provably optimal trees, our experiments show where previous studies have claimed to produce optimal models yet failed; we show specific cases where this happens. We test our method on benchmark data sets, as well as criminal recidivism and credit risk data sets; these are two of the high-stakes decision problems where interpretability is needed most in AI systems. We provide ablation experiments to show which of our techniques is most influential at reducing computation for various datasets. As a result of this analysis, we are able to pinpoint possible future paths to improvement for scalability and computational speed. Our contributions are: (1) The first practical optimal binary-variable decision tree algorithm to achieve solutions for nontrivial problems. (2) A series of analytical bounds to reduce the search space. (3) Algorithmic use of a tree representation using only its leaves. (4) Implementation speedups saving 97% run time. (5) We present the first optimal sparse binary split trees ever published for the COMPAS and FICO datasets.\nThe code and the supplementary materials are available at https://github.com/xiyanghu/OSDT."
    },
    {
      "heading": "2 Related Work",
      "text": "Optimal decision trees have a quite a long history [3], so we focus on closely related techniques. There are efficient algorithms that claim to generate optimal sparse trees, but do not optimally balance the criteria of optimality and sparsity; instead they pre-specify the topology of the tree (i.e., they know a priori exactly what the structure of the splits and leaves are, even though they do not know which variables are split) and only find the optimal tree of the given topology [16]. This is not the problem we address, as we do not know the topology of the optimal tree in advance. The most successful algorithm of this variety is BinOCT [20], which searches for a complete binary tree of a given depth; we discuss BinOCT shortly. Some exploration of learning optimal decision trees is based on boolean satisfiability (SAT) [17], but again, this work looks only for the optimal tree of a given number of nodes. The DL8 algorithm [18] optimizes a ranking function to find a decision tree under constraints of size, depth, accuracy and leaves. DL8 creates trees from the bottom up, meaning that trees are assembled out of all possible leaves, which are itemsets pre-mined from the data [similarly to 2]. DL8 does not have publicly available source code, and its authors warn about running out of memory when storing all partially-constructed trees. Some works consider oblique trees [6], where splits involve several variables; oblique trees are not addressed here, as they can be less interpretable.\nThe most recent mathematical programming algorithms are OCT [5] and BinOCT [20]. Example figures from the OCT paper [5] show decision trees that are clearly suboptimal. However, as the code was not made public, the work in the OCT paper [5] is not easily reproducible, so it is not clear where the problem occurred. We discuss this in Section \u00a74. Verwer and Zhang\u2019s mathematical programming formulation for BinOCT is much faster [20], and their experiments indicate that BinOCT outperforms OCT, but since BinOCT is constrained to create complete binary trees of a given depth rather than optimally sparse trees, it sometimes creates unnecessary leaves in order to complete a tree at a given depth, as we show in Section \u00a74. BinOCT solves a dramatically easier problem than the method introduced in this work. As it turns out, the search space of perfect binary trees of a given depth is much smaller than that of binary trees with the same number of leaves. For instance, the number of different unlabeled binary trees with 8 leaves is Catalan(7) = 429, but the number of unlabeled perfect binary trees with 8 leaves is only 1. In our setting, we penalize (but do not fix) the number of leaves, which means that our search space contains all trees, though we can bound the maximum number of leaves based on the size of the regularization parameter. Therefore, our search space is much larger than that of BinOCT.\nOur work builds upon the CORELS algorithm [1, 2, 13] and its predecessors [14, 21], which create optimal decision lists (rule lists). Applying those ideas to decision trees is nontrivial. The rule list\noptimization problem is much easier, since the rules are pre-mined (there is no mining of rules in our decision tree optimization). Rule list optimization involves selecting an optimal subset and an optimal permutation of the rules in each subset. Decision tree optimization involves considering every possible split of every possible variable and every possible shape and size of tree. This is an exponentially harder optimization problem with a huge number of symmetries to consider. In addition, in CORELS, the maximum number of clauses per rule is set to be c = 2. For a data set with p binary features, there would be D = p + ( p 2 ) rules in total, and the number of distinct rule lists with dr rules is P (D, dr), where P (m, k) is the number of k-permutations of m. Therefore, the search space of CORELS is \u2211d\u22121 dr=1\nP (D, dr). But, for a full binary tree with depth dt and data with p binary features, the number of distinct trees is:\nNdt = 1\u2211 n0=1 2n0\u2211 n1=1 \u00b7 \u00b7 \u00b7 2 ndt\u22122\u2211\nndt\u22121=1\np\u00d7 ( 2n0\nn1\n) (p\u2212 1)n1 \u00d7 . . . \u00d7 ( 2ndt\u22122\nndt\u22121\n) (p\u2212 (dt \u2212 1))ndt\u22121 , (1)\nand the search space of decision trees up to depth d is \u2211d\ndt=1 Ndt . Table 1 shows how the search\nspaces of rule lists and decision trees grow as the tree depth increases. The search space of the trees is massive compared to that of the rule lists.\nApplying techniques from rule lists to decision trees necessitated new designs for the data structures, splitting mechanisms and bounds. An important difference between rule lists and trees is that during the growth of rule lists, we add only one new rule to the list each time, but for the growth of trees, we need to split existing leaves and add a new pair of leaves for each. This leads to several bounds that are quite different from those in CORELS, i.e., Theorem 3.4, Theorem 3.5 and Corollary E.1, which consider a pair of leaves rather than a single leaf. In this paper, we introduce bounds only for the case of one split at a time; however, in our implementation, we can split more than one leaf at a time, and the bounds are adapted accordingly."
    },
    {
      "heading": "3 Optimal Sparse Decision Trees (OSDT)",
      "text": "We focus on binary classification, although it is possible to generalize this framework to multiclass settings. We denote training data as {(xn, yn)}Nn=1, where xn \u2208 {0, 1}M are binary features and yn \u2208 {0, 1} are labels. Let x = {xn}Nn=1 and y = {yn}Nn=1, and let xn,m denote the m-th feature of xn. For a decision tree, its leaves are conjunctions of predicates. Their order does not matter in evaluating the accuracy of the tree, and a tree grows only at its leaves. Thus, within our algorithm, we represent a tree as a collection of leaves. A leaf set d = (p1, p2, . . . , pH) of length H \u2265 0 is an H-tuple containing H distinct leaves, where pk is the classification rule of the path from the root to leaf k. Here, pk is a Boolean assertion, which evaluates to either true or false for each datum xn indicating whether it is classified by leaf k. Here, y\u0302(leaf)k is the label for all points so classified.\nWe explore the search space by considering which leaves of the tree can be beneficially split. The leaf set d = (p1, p2, . . . , pK , pK+1, . . . , pH) is the H-leaf tree, where the first K leaves may not to be split, and the remaining H \u2212K leaves can be split. We alternately represent this leaf set as d = (dun, \u03b4un, dsplit, \u03b4split,K,H), where dun = (p1, . . . , pK) are the unchanged leaves of d, \u03b4un = (y\u0302\n(leaf) 1 , . . . , y\u0302 (leaf) K ) \u2208 {0, 1}K are the predicted labels of leaves dun, dsplit = (pK+1, . . . , pH) are the\nleaves we are going to split, and \u03b4split = (y\u0302 (leaf) K+1, . . . , y\u0302 (leaf) H ) \u2208 {0, 1}H\u2212K are the predicted labels of leaves dsplit. We call dun a K-prefix of d, which means its leaves are a size-K unchanged subset of (p1, . . . , pK , . . . , pH). If we have a new prefix d\u2032un, which is a superset of dun, i.e., d \u2032 un \u2287 dun, then\nwe say d\u2032un starts with dun. We define \u03c3(d) to be all descendents of d:\n\u03c3(d) = {(d\u2032un, \u03b4\u2032un, d\u2032split, \u03b4\u2032split,K \u2032, Hd\u2032) : d\u2032un \u2287 dun, d\u2032 \u2283 d}. (2)\nIf we have two trees d = (dun, \u03b4un, dsplit, \u03b4split,K,H) and d\u2032 = (d\u2032un, \u03b4 \u2032 un, d \u2032 split, \u03b4 \u2032 split,K \u2032, H \u2032), where H \u2032 = H + 1, d\u2032 \u2283 d, d\u2032un \u2287 dun, i.e., d\u2032 contains one more leaf than d and d\u2032un starts with dun, then we define d\u2032 to be a child of d and d to be a parent of d\u2032.\nNote that two trees with identical leaf sets, but different assignments to dun and dsplit, are different trees. Further, a child tree can only be generated through splitting leaves of its parent tree within dsplit.\nA tree d classifies datum xn by providing the label prediction y\u0302 (leaf) k of the leaf whose pk is true for xn. Here, the leaf label y\u0302 (leaf) k is the majority label of data captured by the leaf k. If pk evaluates to true for xn, we say the leaf k of leaf set dun captures xn . In our notation, all the data captured by a prefix\u2019s leaves are also captured by the prefix itself.\nLet \u03b2 be a set of leaves. We define cap(xn, \u03b2) = 1 if a leaf in \u03b2 captures datum xn, and 0 otherwise. For example, let d and d\u2032 be leaf sets such that d\u2032 \u2283 d, then d\u2032 captures all the data that d captures: {xn : cap(xn, d)} \u2286 {xn : cap(xn, d\u2032)}. The normalized support of \u03b2, denoted supp(\u03b2,x), is the fraction of data captured by \u03b2:\nsupp(\u03b2,x) = 1\nN N\u2211 n=1 cap(xn, \u03b2). (3)"
    },
    {
      "heading": "3.1 Objective Function",
      "text": "For a tree d = (dun, \u03b4un, dsplit, \u03b4split,K,Hd), we define its objective function as a combination of the misclassification error and a sparsity penalty on the number of leaves:\nR(d,x,y) = `(d,x,y) + \u03bbHd. (4)\nR(d,x,y) is a regularized empirical risk. The loss `(d,x,y) is the misclassification error of d, i.e., the fraction of training data with incorrectly predicted labels. Hd is the number of leaves in the tree d. \u03bbHd is a regularization term that penalizes bigger trees. Statistical learning theory provides guarantees for this problem; minimizing the loss subject to a (soft or hard) constraint on model size leads to a low upper bound on test error from the Occham\u2019s Razor Bound."
    },
    {
      "heading": "3.2 Optimization Framework",
      "text": "We minimize the objective function based on a branch-and-bound framework. We propose a series of specialized bounds that work together to eliminate a large part of the search space. These bounds are discussed in detail in the following paragraphs. Proofs are in the supplementary materials.\nSome of our bounds could be adapted directly from CORELS [2], namely these two: (Hierarchical objective lower bound) Lower bounds of a parent tree also hold for every child tree of that parent (\u00a73.3, Theorem 3.1). (Equivalent points bound) For a given dataset, if there are multiple samples with exactly the same features but different labels, then no matter how we build our classifier, we will always make mistakes. The lower bound on the number of mistakes is therefore the number of such samples with minority class labels (\u00a7B, Theorem B.2).\nSome of our bounds adapt from CORELS [1] with minor changes: (Objective lower bound with one-step lookahead) With respect to the number of leaves, if a tree does not achieve enough accuracy, we can prune all child trees of it (\u00a73.3, Lemma 3.2). (A priori bound on the number of leaves) For an optimal decision tree, we provide an a priori upper bound on the maximum number of leaves (\u00a7C, Theorem C.3). (Lower bound on node support) For an optimal tree, the support traversing through each internal node must be at least 2\u03bb (\u00a73.4, Theorem 3.3).\nSome of our bounds are distinct from CORELS, because they are only relevant to trees and not to lists: (Lower bound on incremental classification accuracy) Each split must result in sufficient reduction of the loss. Thus, if the loss reduction is less than or equal to the regularization term, we should still split, and we have to further split at least one of the new child leaves to search for the optimal tree (\u00a73.4, Theorem 3.4). (Leaf permutation bound) We need to consider only one\npermutation of leaves in a tree; we do not need to consider other permutations (explained in \u00a7E, Corollary E.1). (Leaf accurate support bound) For each leaf in an optimal decision tree, the number of correctly classified samples must be above a threshold. (\u00a73.4, Theorem 3.5). The supplement contains an additional set of bounds on the number of remaining tree evaluations."
    },
    {
      "heading": "3.3 Hierarchical Objective Lower Bound",
      "text": "The loss can be decomposed into two parts corresponding to the unchanged leaves and the leaves to be split: `(d,x,y) \u2261 `p(dun, \u03b4un,x,y) + `q(dsplit, \u03b4split,x,y), where dun = (p1, . . . , pK), \u03b4un = (y\u0302\n(leaf) 1 , . . . , y\u0302 (leaf) K ), dsplit = (pK+1, . . . , pHd) and \u03b4split = (y\u0302 (leaf) K+1, . . . , y\u0302 (leaf) Hd\n); `p(dun, \u03b4un,x,y) = 1 N \u2211N n=1 \u2211K k=1 cap(xn, pk) \u2227 1[y\u0302 (leaf) k 6= yn] is the proportion of data in the unchanged leaves that\nare misclassified, and `p(dsplit, \u03b4split,x,y) = 1N \u2211N n=1 \u2211Hd k=K+1 cap(xn, pk) \u2227 1[y\u0302 (leaf) k 6= yn] is the proportion of data in the leaves we are going to split that are misclassified. We define a lower bound b(dun,x,y) on the objective by leaving out the latter loss,\nb(dun,x,y) \u2261 `p(dun, \u03b4un,x,y) + \u03bbHd \u2264 R(d,x,y), (5) where the leaves dun are kept and the leaves dsplit are going to be split. Here, b(dun,x,y) gives a lower bound on the objective of any child tree of d. Theorem 3.1 (Hierarchical objective lower bound). Define b(dun,x,y) = `p(dun, \u03b4un,x,y) + \u03bbHd, as in (5). Define \u03c3(d) to be the set of all d\u2019s child trees whose unchanged leaves contain dun, as in (2). For tree d = (dun, \u03b4un, dsplit, \u03b4split,K,Hd) with unchanged leaves dun, let d\u2032 = (d\u2032un, \u03b4 \u2032 un, d \u2032 split, \u03b4 \u2032 split,K\n\u2032, Hd\u2032) \u2208 \u03c3(d) be any child tree such that its unchanged leaves d\u2032un contain dun and K \u2032 \u2265 K,Hd\u2032 \u2265 Hd, then b(dun,x,y) \u2264 R(d\u2032,x,y).\nConsider a sequence of trees, where each tree is the parent of the following tree. In this case, the lower bounds of these trees increase monotonically, which is amenable to branch-and-bound. We illustrate our framework in Algorithm 1 in Supplement A. According to Theorem 3.1, we can hierarchically prune the search space. During the execution of the algorithm, we cache the current best (smallest) objectiveRc, which is dynamic and monotonically decreasing. In this process, when we generate a tree whose unchanged leaves dun correspond to a lower bound satifying b(dun,x,y) \u2265 Rc, according to Theorem 3.1, we do not need to consider any child tree d\u2032 \u2208 \u03c3(d) of this tree whose d\u2032un contains dun. Based on Theorem 3.1, we describe a consequence in Lemma 3.2. Lemma 3.2 (Objective lower bound with one-step lookahead). Let d be a Hd-leaf tree with a K-leaf prefix and let Rc be the current best objective. If b(dun,x,y) + \u03bb \u2265 Rc, then for any child tree d\u2032 \u2208 \u03c3(d), its prefix d\u2032un starts with dun and K \u2032 > K,Hd\u2032 > Hd, and it follows that R(d\u2032,x,y) \u2265 Rc.\nThis bound tends to be very powerful in practice in pruning the search space, because it states that even though we might have a tree with unchanged leaves dun whose lower bound b(dun,x,y) \u2264 Rc, if b(dun,x,y) + \u03bb \u2265 Rc, we can still prune all of its child trees."
    },
    {
      "heading": "3.4 Lower Bounds on Node Support and Classification Accuracy",
      "text": "We provide three lower bounds on the fraction of correctly classified data and the normalized support of leaves in any optimal tree. All of them depend on \u03bb. Theorem 3.3 (Lower bound on node support). Let d\u2217 = (dun, \u03b4un, dsplit, \u03b4split,K, Hd\u2217) be any optimal tree with objective R\u2217, i.e., d\u2217 \u2208 argmindR(d,x,y). For an optimal tree, the support traversing through each internal node must be at least 2\u03bb. That is, for each child leaf pair pk, pk+1 of a split, the sum of normalized supports of pk, pk+1 should be no less than twice the regularization parameter, i.e., 2\u03bb,\n2\u03bb \u2264 supp(pk,x) + supp(pk+1,x). (6)\nTherefore, for a tree d, if any of its internal nodes capture less than a fraction 2\u03bb of the samples, it cannot be an optimal tree, even if b(dun,x,y) < R\u2217. None of its child trees would be an optimal tree either. Thus, after evaluating d, we can prune tree d. Theorem 3.4 (Lower bound on incremental classification accuracy). Let d\u2217 = (dun, \u03b4un, dsplit, \u03b4split,K,Hd\u2217) be any optimal tree with objective R\u2217, i.e., d\u2217 \u2208 argmindR(d,x,y). Let d\u2217\nhave leaves dun = (p1, . . . , pHd\u2217 ) and labels \u03b4un = (y\u0302 (leaf) 1 , . . . , y\u0302 (leaf) Hd\u2217\n). For each leaf pair pk, pk+1 with corresponding labels y\u0302(leaf)k , y\u0302 (leaf) k+1 in d \u2217 and their parent node (the leaf in the parent tree) pj and its label y\u0302(leaf)j , define ak to be the incremental classification accuracy of splitting pj to get pk, pk+1:\nak \u2261 1\nN N\u2211 n=1 {cap(xn, pk) \u2227 1[y\u0302(leaf)k = yn] + cap(xn, pk+1) \u2227 1[y\u0302 (leaf) k+1 = yn]\u2212 cap(xn, pj) \u2227 1[y\u0302 (leaf) j = yn]}. (7)\nIn this case, \u03bb provides a lower bound, \u03bb \u2264 ak.\nThus, when we split a leaf of the parent tree, if the incremental fraction of data that are correctly classified after this split is less than a fraction \u03bb, we need to further split at least one of the two child leaves to search for the optimal tree. Thus, we apply Theorem 3.3 when we split the leaves. We need only split leaves whose normalized supports are no less than 2\u03bb. We apply Theorem 3.4 when constructing the trees. For every new split, we check the incremental accuracy for this split. If it is less than \u03bb, we further split at least one of the two child leaves. Both Theorem 3.3 and Theorem 3.4 are bounds for pairs of leaves. We give a bound on a single leaf\u2019s classification accuracy in Theorem 3.5.\nTheorem 3.5 (Lower bound on classification accuracy). Let d\u2217 = (dun, \u03b4un, dsplit, \u03b4split,K,Hd\u2217) be any optimal tree with objective R\u2217, i.e., d\u2217 \u2208 argmindR(d,x,y). For each leaf (pk, y\u0302 (leaf) k ) in d\n\u2217, the fraction of correctly classified data in leaf k should be no less than \u03bb,\n\u03bb \u2264 1 N N\u2211 n=1 cap(xn, pk) \u2227 1[y\u0302(leaf)k = yn]. (8)\nThus, in a leaf we consider extending by splitting on a particular feature, if that proposed split leads to less than \u03bb correctly classified data going to either side of the split, then this split can be excluded, and we can exclude that feature anywhere further down the tree extending that leaf."
    },
    {
      "heading": "3.5 Incremental Computation",
      "text": "Much of our implementation effort revolves around exploiting incremental computation, designing data structures and ordering of the worklist. Together, these ideas save >97% execution time. We provide the details of our implementation in the supplement."
    },
    {
      "heading": "4 Experiments",
      "text": "We address the following questions through experimental analysis: (1) Do existing methods achieve optimal solutions, and if not, how far are they from optimal? (2) How fast does our algorithm converge given the hardness of the problem it is solving? (3) How much does each of the bounds contribute to the performance of our algorithm? (4) What do optimal trees look like?\nThe results of the per-bound performance and memory improvement experiment (Table 2 in the supplement) were run on a m5a.4xlarge instance of AWS\u2019s Elastic Compute Cloud (EC2). The instance has 16 2.5GHz virtual CPUs (although we run single-threaded on a single core) and 64 GB of RAM. All other results were run on a personal laptop with a 2.4GHz i5-8259U processor and 16GB of RAM.\nWe used 7 datasets: Five of them are from the UCI Machine Learning Repository [8], (Tic Tac Toe, Car Evaluation, Monk1, Monk2, Monk3). The other two datasets are the ProPublica recidivism data set [12] and the Fair Isaac (FICO) credit risk dataset [9]. We predict which individuals are arrested within two years of release (N = 7, 215) on the recidivism data set and whether an individual will default on a loan for the FICO dataset.\nAccuracy and optimality: We tested the accuracy of our algorithm against baseline methods CART and BinOCT [20]. BinOCT is the most recent publicly available method for learning optimal classification trees and was shown to outperform other previous methods. As far as we know, there is no public code for most of the other relevant baselines, including [5, 6, 16]. One of these methods, OCT [5], reports that CART often dominates their performance (see Fig. 4 and Fig. 5 in their paper). Our models can never be worse than CART\u2019s models even if we stop early, because in our implementation, we use the objective value of CART\u2019s solution as a warm start to the objective value of the current best. Figure 1 shows the training accuracy on each dataset. The time limits for both BinOCT and our algorithm are set to be 30 minutes.\nMain results: (i) We can now evaluate how close to optimal other methods are (and they are often close to optimal or optimal). (ii) Sometimes, the baselines are not optimal. Recall that BinOCT searches only for the optimal tree given the topology of the complete binary tree of a certain depth. This restriction on the topology massively reduces the search space so that BinOCT runs quickly, but in exchange, it misses optimal sparse solutions that our method finds. (iii) Our method is fast. Our method runs on only one thread (we have not yet parallelized it) whereas BinOCT is highly optimized; it makes use of eight threads. Even with BinOCT\u2019s 8-thread parallelism, our method is competitive.\nConvergence: Figure 2 illustrates the behavior of OSDT for the ProPublica COMPAS dataset with \u03bb = 0.005, for two different scheduling policies (curiosity and lower bound, see supplement). The charts show how the current best objective value Rc and the lower bound b(dun,x,y) vary as the algorithm progresses. When we schedule using the lower bound, the lower bounds of evaluated trees increase monotonically, and OSDT certifies optimality only when the value of the lower bound becomes large enough that we can prune the remaining search space or when the queue is empty, whichever is reached earlier. Using curiosity, OSDT finds the optimal tree much more quickly than when using the lower bound. Scalability: Figure 3 shows the scalability of OSDT with respect to the number of samples\nand the number of features. Runtime can theoretically grow exponentially with the number of features. However, as we add extra features that differ from those in the optimal tree, we can reach the optimum more quickly, because we are able to prune the search space more efficiently as the number of extra features grows. For example, with 4 features, it spends about 75% of the runtime to reach the optimum; with 12 features, it takes about 5% of the runtime to reach the optimum.\nmiddle-middle=x\nAblation experiments: Appendix I shows that the lookahead and equivalent points bounds are, by far, the most significant of our bounds, reducing time to optimum by at least two orders of magnitude and reducing memory consumption by more than one order of magnitude.\nTrees: We provide illustrations of the trees produced by OSDT and the baseline methods in Figures 4, 5 and 6. OSDT generates trees of any shape, and our objective penalizes trees with more leaves, thus it never introduces splits that produce a pair of leaves with the same label. In contrast, BinOCT trees are always complete binary trees of a given depth. This limitation on the tree shape can prevent BinOCT from finding the globally optimal tree. In fact, BinOCT often produces useless splits, leading to trees with more leaves than necessary to achieve the same accuracy.\nAdditional experiments: It is well-established that simpler models such as small decision trees generalize well; a set of cross-validation experiments is in the supplement demonstrating this.\nConclusion: Our work shows the possibility of optimal (or provably near-optimal) sparse decision trees. It is the first work to balance the accuracy and the number of leaves optimally in a practical amount of time. We have reason to believe this framework can be extended to much larger datasets. Theorem F.1 identifies a key mechanism for scaling these algorithms up. It suggests a bound stating that highly correlated features can substitute for each other, leading to similar model accuracies. Applications of this bound allow for the elimination of features throughout the entire execution, allowing for more aggressive pruning. Our experience to date shows that by supporting such bounds with the right data structures can potentially lead to dramatic increases in performance and scalability."
    },
    {
      "heading": "Optimal Sparse Decision Trees: Supplementary Material",
      "text": ""
    },
    {
      "heading": "A Branch and Bound Algorithm",
      "text": "Algorithm 1 shows the structure of our approach."
    },
    {
      "heading": "B Equivalent Points Bound",
      "text": "When multiple observations captured by a leaf in dsplit have identical features but opposite labels, then no tree, including those that extend dsplit, can correctly classify all of these observations. The number of misclassifications must be at least the minority label of the equivalent points.\nFor data set {(xn, yn)}Nn=1 and a set of features {sm}Mm=1, we define a set of samples to be equivalent if they have exactly the same feature values, i.e., (xi, yi) and (xj , yj) are equivalent if 1M \u2211M m=1 1[cap(xi, sm) = cap(xj , sm)] = 1. Note that a data set consists of multiple sets of equivalent points; let {eu}Uu=1 enumerate these sets. For each observation xi, it belongs to a equivalent points set eu. We denote the fraction of data with the minority label in set eu as \u03b8(eu), e.g., let eu = {xn : \u2200m \u2208 [M ], 1[cap(xn, sm) = cap(xi, sm)]}, and let qu be the minority class label among points in eu, then\n\u03b8(eu) = 1\nN N\u2211 n=1 1[xn \u2208 eu]1[yn = qu]. (9)\nWe can combine the equivalent points bound with other bounds to get a tighter lower bound on the objective function. As the experimental results demonstrate in \u00a74, there is sometimes a substantial reduction of the search space after incorporating the equivalent points bound. We propose a general equivalent points bound in Proposition B.1. We incorporate it into our framework by proposing the specific equivalent points bound in Theorem B.2. Proposition B.1 (General equivalent points bound). Let d = (dun, \u03b4un, dsplit, \u03b4split,K,H) be a tree, then R(d,x,y) \u2265 \u2211U u=1 \u03b8(eu) + \u03bbH .\nAlgorithm 1 Branch-and-bound for learning optimal decision trees. Input: Objective function R(d,x,y), objective lower bound b(dun,x,y), set of features S = {sm}Mm=1, training data (x,y) = {(xn, yn)}Nn=1, initial best known tree d0 with objective R0 = R(d0,x,y); d0 could be obtained as output from another (approximate) algorithm, otherwise, (d0, R0) = (null, 1) provides reasonable default values. The initial value of \u03b4split is the majority label of the whole dataset. Output: Provably optimal decision tree d\u2217 with minimum objective R\u2217\n(dc, Rc)\u2190 (d0, R0) . Initialize best tree and objective Q\u2190 queue( [ ((), (), (), \u03b4split, 0, 0) ] ) . Initialize queue with empty tree while Q not empty do . Stop when queue is empty\nd = (dun, \u03b4un, dsplit, \u03b4split,K,H)\u2190 Q.pop( ) . Remove tree d from the queue if b(dun,x,y) < Rc then . Bound: Apply Theorem 3.1\nR\u2190 R(d,x,y) . Compute objective of tree d if R < Rc then . Update best tree and objective\n(dc, Rc)\u2190 (d,R) end if for every possible combination of features to split dsplit do\n. Branch: Enqueue dun\u2019s children split dsplit and get new leaves dnew for each possible subset d\u2032split of dnew do\nd\u2032un = dun \u222a (dnew \\ d\u2032split) Q.push( (d\u2032un, \u03b4 \u2032 un, d \u2032 split, \u03b4 \u2032 split,K\n\u2032, H \u2032) ) end for\nend for end if\nend while (d\u2217, R\u2217)\u2190 (dc, Rc) . Identify provably optimal solution\nRecall that in our lower bound b(dun,x,y) in (5), we leave out the misclassification errors of leaves we are going to split `0(dsplit, \u03b4split, x,y) from the objective R(d,x,y). Incorporating the equivalent points bound in Theorem B.2, we obtain a tighter bound on our objective because we now have a tighter lower bound on the misclassification errors of leaves we are going to split, 0 \u2264 b0(dsplit,x,y) \u2264 `0(dsplit, \u03b4split,x,y). Theorem B.2 (Equivalent points bound). Let d be a tree with leaves dun, dsplit and lower bound b(dun,x,y), then for any tree d\u2032 \u2208 \u03c3(d) whose prefix d\u2032un \u2287 dun,\nR(d\u2032,x,y) \u2265 b(dun,x,y) + b0(dsplit,x,y), where (10)\nb0(dsplit,x,y) = 1\nN U\u2211 u=1 N\u2211 n=1 cap(xn, dsplit) \u2227 1[xn \u2208 eu]1[yn = qu]. (11)"
    },
    {
      "heading": "C Upper Bounds on Number of Leaves",
      "text": "During the branch-and-bound execution, the current best objective Rc implies an upper bound on the maximum number of leaves for those trees we still need to consider.\nTheorem C.1 (Upper bound on the number of leaves). For a dataset with M features, consider a state space of all trees. Let L(d) be the number of leaves of tree d and let Rc be the current best objective. For all optimal trees d\u2217 \u2208 argmindR(d,x,y)\nL(d\u2217) \u2264 min ( bRc/\u03bbc , 2M ) , (12)\nwhere \u03bb is the regularization parameter.\nCorollary C.2 (A priori upper bound on the number of leaves). For all optimal trees d\u2217 \u2208 argmindR(d,x,y), L(d\u2217) \u2264 min ( b1/2\u03bbc , 2M ) . (13)\nFor any particular tree d with unchanged leaves dun, we can obtain potentially tighter upper bounds on the number of leaves for all its child trees whose unchanged leaves include dun.\nTheorem C.3 (Parent-specific upper bound on the number of leaves). Let d = (dun, \u03b4un, dsplit, \u03b4split,K,H) be a tree, let d\u2032 = (d\u2032un, \u03b4 \u2032 un, d \u2032 split, \u03b4 \u2032 split,K\n\u2032, H \u2032) \u2208 \u03c3(d) be any child tree such that d\u2032un \u2287 dun, and let Rc be the current best objective. If d\u2032un has lower bound b(d \u2032 un,x, y) < R c, then\nH \u2032 < min ( H + \u230a Rc \u2212 b(dun,x,y)\n\u03bb\n\u230b , 2M ) . (14)\nTheorem C.3 can be viewed as a generalization of the one-step lookahead bound (Lemma 3.2). This is because we can view (14) as a bound on H \u2032 \u2212H , which provides an upper bound on the number of remaining splits we may need, based on the best tree we already have evaluated."
    },
    {
      "heading": "D Upper Bounds on Number of Tree Evaluations",
      "text": "In this section, based on the upper bounds on the number of leaves from \u00a7C, we give corresponding upper bounds on the number of tree evaluations made by Algorithm 1. First, in Theorem D.1, based on information about the state of Algorithm 1\u2019s execution, we calculate, for any given execution state, upper bounds on the number of additional tree evaluations needed for the execution to complete. We define the number of remaining tree evaluations as the number of trees that are currently in, or will be inserted into, the queue. We evaluate the number of tree evaluations based on current execution information of the current best objective Rc and the trees in the queue Q of Algorithm 1.\nTheorem D.1 (Upper bound on number of remaining tree evaluations). Consider the state space of all possible leaves formed from a set of M features, and consider Algorithm 1 at a particular instant during execution. Denote the current best objective as Rc, the queue as Q, and the size of prefix dun as L(dun). Denoting the number of remaining prefix evaluations as \u0393(Rc, Q), the bound is:\n\u0393(Rc, Q) \u2264 \u2211\ndun\u2208Q f(dun)\u2211 k=0 (3M \u2212 L(dun))! (3M \u2212 L(dun)\u2212 k)! , (15)\nwhere f(dun) = min (\u230a\nRc \u2212 b(dun,x,y) \u03bb\n\u230b , 3M \u2212 L(dun) ) . (16)\nThe corollary below is a na\u00efve upper bound on the total number of tree evaluations during the process of Algorithm 1\u2019s execution. It does not use algorithm execution state to bound the size of the search space like Theorem D.1, and it relies only on the number of features and the regularization parameter \u03bb. Corollary D.2 (Upper bound on the total number of tree evaluations). Define \u0393tot(S) to be the total number of trees evaluated by Algorithm 1, given the state space of all possible leaves formed from a set S of M features. For any set S of all leaves formed of M features,\n\u0393tot(S) \u2264 K\u2211 k=0 3M ! (3M \u2212 k)! ,where K = min(b1/2\u03bbc, 2 M )."
    },
    {
      "heading": "E Permutation Bound",
      "text": "If two trees are composed of the same leaves, i.e., they contain the same conjunctions of features up to a permutation, then they classify all the data in the same way and their child trees are also permutations of each other. Therefore, if we already have all children from one permutation of a tree, then there is no benefit to considering child trees generated from a different permutation. Corollary E.1 (Leaf Permutation bound). Let \u03c0 be any permutation of {1, . . . , H}, Let d = (dun, \u03b4un, dsplit, \u03b4split,K,H) and D = (Dun,\u2206un, Dsplit,\u2206split,K,H) denote trees with leaves (p1, . . . , pH) and Dun = (p\u03c0(1), . . . , p\u03c0(H)), respectively, i.e., the leaves in D correspond to a permutation of the leaves in d. Then the objective lower bounds of d and D are the same and their child trees correspond to permutations of each other.\nTherefore, if two trees have the same leaves, up to a permutation, according to Corollary E.1, either of them can be pruned. We call this symmetry-aware pruning. In Section \u00a7E.1, we demonstrate how this helps to save computation.\nE.1 Upper bound on tree evaluations with symmetry-aware pruning\nHere we give an upper bound on the total number of tree evaluations based on symmetry-aware pruning (\u00a7E). For every subset of K leaves, there are K! leaf sets equivalent up to permutation. Thus, symmetry-aware pruning dramatically reduces the search space by considering only one of them. This effects the execution of Algorithm 1\u2019s breadth-first search. With symmetry-aware pruning, when it evaluates trees of size K, for each set of trees equivalent up to a permutation, it keeps only a single tree. Theorem E.2 (Upper bound on tree evaluations with symmetry-aware pruning). Consider a state space of all trees formed from a set S of 3M leaves where M is the number of features (the 3 options correspond to having the feature\u2019s value be 1, having its value be 0, or not including the feature), and consider the branch-and-bound algorithm with symmetry-aware pruning. Define \u0393tot(S) to be the total number of prefixes evaluated. For any set S of 3M leaves,\n\u0393tot(S) \u2264 1 + K\u2211 k=1 Nk + C(M,k)\u2212 P (M,k), (17)\nwhere K = min(b1/2\u03bbc, 2M ), Nk is defined in (1).\nProof. By Corollary C.2, K \u2261 min(b1/2\u03bbc, 2M ) gives an upper bound on the number of leaves of any optimal tree. The algorithm begins by evaluating the empty tree, followed by M trees of depth k = 1, then N2 = \u22111 n0=1 \u22112n0 n1=1 M \u00d7 ( 2n0 n1 ) (M \u2212 1)n1 trees of depth k = 2. Before proceeding to length k = 3, we keep only N2+ C(M, 2)\u2212 P (M, 2) trees of depth k = 2, where Nk is defined in (1), P (M,k) denotes the number of k-permutations of M and C(M,k) denotes the number of k-combinations of M . Now, the number of length k = 3 prefixes we evaluate is N3 + C(M, 3)\u2212 P (M, 3). Propagating this forward gives (17). Pruning based on permutation symmetries thus yields significant computational savings of \u2211K k=1 P (M,k)\u2212 C(M,k). For example, when M = 10 and K = 5, the number reduced due to symmetry-aware pruning is about 35463. If M = 20 and K = 10, the number of evaluations is reduced by about 7.36891\u00d7 1011."
    },
    {
      "heading": "F Similar Support Bound",
      "text": "Here, we present the similar support bound to deal with similar trees. Let us say we are given two trees that are the same except that one internal node is split by a different feature, where this second feature is similar to the first tree\u2019s feature. By the similar support bound, if we know that one of these trees and all its child trees are worse (beyond a margin) than the current best tree, we can prune the other one and all of its child trees.\nTheorem F.1 (Similar support bound). Define d = (dun, \u03b4un, dsplit, \u03b4split,K,H) and D = (Dun,\u2206un, Dsplit,\u2206split,K,H) to be two trees which are exactly the same but one internal node split by different features. Let f1, f2 be the features used to split that node in d and D respectively. Let t1, t2 be the left subtree and the right subtree under the node f1 in d, and let T1, T2 be the left subtree and the right subtree under the node f2 in D. Denote the normalized support of data captured by only one of t1 and T1 as \u03c9, i.e.,\n\u03c9 \u2261 1 N N\u2211 n=1 [\u00ac cap(xn, t1) \u2227 cap(xn, T1) + cap(xn, t1) \u2227 \u00ac cap(xn, T1)]. (18)\nThe difference between the two trees\u2019 objectives is bounded by \u03c9 as the following:\n\u03c9 \u2265 R(d,x,y)\u2212R(D,x,y) \u2265 \u2212\u03c9, (19)\nwhere R(d,x,y) is objective of d and R(D,x,y) is the objective of D. Then, we have\n\u03c9 \u2265 min d\u2020\u2208\u03c3(d) R(d\u2020,x,y)\u2212 min D\u2020\u2208\u03c3(Dun) R(D\u2020,x,y) \u2265 \u2212\u03c9. (20)\nProof. The difference between the objectives of d and D is maximized when one of them correctly classifies all the data corresponding to \u03c9 but the other misclassifies all of them. Therefore,\n\u03c9 \u2265 R(d,x,y)\u2212R(D,x,y) \u2265 \u2212\u03c9.\nLet d\u2217 be the best child tree of d, i .e., R(d\u2217,x,y) = mind\u2020\u2208\u03c3(d)R(d \u2020,x,y), and let D\u2032 \u2208 \u03c3(Dun) be its counterpart which is exactly the same but one internal node split by a different feature. Because R(D\u2032,x,y) \u2265 minD\u2020\u2208\u03c3(Dun)R(D \u2020,x,y),\nmin d\u2020\u2208\u03c3(d)\nR(d\u2020,x,y) = R(d\u2217,x,y) \u2265 R(D\u2032,x,y)\u2212 \u03c9\n\u2265 min D\u2020\u2208\u03c3(Dun)\nR(D\u2020,x,y)\u2212 \u03c9. (21)"
    },
    {
      "heading": "Similarly, minD\u2020\u2208\u03c3(Dun)R(D",
      "text": "\u2020,x,y) + \u03c9 \u2265 mind\u2020\u2208\u03c3(d)R(d\u2020,x,y).\nWith the similar support bound, for two trees d and D as above, if we already know D and all its child trees cannot achieve a better tree than the current optimal one, and in particular, we assume we know that minD\u2020\u2208\u03c3(Dun)R(D \u2020,x,y) \u2265 Rc + \u03c9, we can then also prune d and all of its child trees, because\nmin d\u2020\u2208\u03c3(d) R(d\u2020,x,y) \u2265 min D\u2020\u2208\u03c3(Dun) R(D\u2020,x,y)\u2212 \u03c9 \u2265 Rc. (22)\nG Implementation\nWe implement a series of data structures designed to support incremental computation."
    },
    {
      "heading": "G.1 Data Structure of Leaf and Tree",
      "text": "First, we store bounds and intermediate results for both full trees and the individual leaves to support the incremental computation of the lower bound and the objective. As a global statistic, we maintain the best (minimum) observed value of the objective function and the corresponding tree. As each leaf in a tree represents a set of clauses, each leaf stores a bit-vector representing the set of samples captured by that clause and the prediction accuracy for those samples. From these values in the leaves, we can efficiently compute both the value of the objective for an entire tree and new leaf values for children formed from splitting a leaf.\nSpecifically, for the data structure of leaf l, we store:\n\u2022 A set of clauses defining the leaf. \u2022 A binary vector of length N (number of data points) indicating whether or not each point is captured\nby the leaf.\n\u2022 The number of points captured by the leaf. \u2022 A binary vector of length M (number of features) indicating the set of dead features. In a leaf, a\nfeature is dead if Theorem 3.5 does not hold.\n\u2022 The lower bound on the leaf misclassification error b0(l,x,y), which is defined in (11) \u2022 The label of the leaf. \u2022 The loss of the leaf.\n\u2022 A boolean indicating whether the leaf is dead. A leaf is dead if Theorem 3.3 does not hold; we never split a dead leaf.\nWe store additional information for entire trees:\n\u2022 A set of leaves in the tree.\n\u2022 The objective.\n\u2022 The lower bound of the objective.\n\u2022 A binary vector of length nl (number of leaves) indicating whether the leaf can be split, that is, this vector records split leaves dsplit and unchanged leaves dun. The unchanged leaves of a tree are also unchanged leaves in its child trees."
    },
    {
      "heading": "G.2 Queue",
      "text": "Second, we use a priority queue to order the exploration of the search space. The queue serves as a worklist, with each entry in the queue corresponding to a tree. When an entry is removed from the queue, we use it to generate child trees, incrementally computing the information for the child trees. The ordering of the worklist represents a scheduling policy. We evaluated both structural orderings, e.g., breadth first search and depth first search, and metric-based orderings, e.g., objective function, and its lower bound. Each metric produces a different scheduling policy. We achieve the best performance in runtime and memory consumption using the curiosity metric from CORELS [2], which is the objective\u2019s lower bound, divided by the normalized support of its unchanged leaves. For example, relative to using the objective, curiosity reduces runtime by a factor of two and memory consumption by a factor of four."
    },
    {
      "heading": "G.3 Symmetry-aware Map",
      "text": "Third, to support symmetry-aware pruning from Corollary E.1, we introduce two symmetry aware maps \u2013 a LeafCache and a TreeCache. The LeafCache ensures that we only compute values for a particular leave once; the TreeCache ensures we do not create trees equivalent to those we have already explored.\nA leaf is a set of clauses, each of which corresponds to an attribute and the value (0,1) of that attribute. As the leaves of a decision tree are mutually exclusive, the data captured by each leaf is insensitive to the order of the leaf\u2019s clauses. We encode leaves in a canonical order (sorted by attribute indexes) and use that canonical order as the key into the LeafCache. Each entry in the LeafCache represents all permutations of a set of clauses. We use a Python dictionary to map these keys to the leaf and its cached values. Before creating a leaf object, we first check if we already have that leaf in our map. If not, we create the leaf and insert it into the map. Otherwise, the permutation already exists, so we use the cached copy in the tree we are constructing.\nNext, we implement the permutation bound (Corollary E.1) using the TreeCache. The TreeCache contains encodings of all the trees we have evaluated. Like we did for the clauses in the LeafCache, we introduce a canonical order over the leaves in a tree and use that as the key to the TreeCache. If our algorithm produces a tree that is a permutation of a tree we have already evaluated, we need not evaluate it again. Before evaluating a tree, we look it up in the cache. If it\u2019s in the cache, we do nothing; if it is not, we compute the bounds for the tree and insert it into the cache."
    },
    {
      "heading": "G.4 Execution",
      "text": "Now, we illustrate how these data structures support execution of our algorithm. We initialize the algorithm with the current best objective Rc and tree dc. For unexplored trees in the queue, the scheduling policy selects the next tree d to split; we keep removing elements from the queue until the queue is empty. Then, for every possible combination of features to split dsplit, we construct a new tree d\u2032 with incremental calculation of the lower bound b(d\u2032un,x,y) and the objective R(d\u2032,x,y). If we achieve a better objective R(d\u2032,x,y), i.e., less than the current best objective Rc, we update Rc and dc. If the lower bound of the new tree d\u2032, combined with the equivalent points bound (Theorem B.2) and the lookahead bound (Theorem 3.2), is less than the current best objective, then we push it into the queue. Otherwise, according to the hierarchical lower bound (Theorem 3.1), no child of d\u2032 could possibly have an objective better than Rc, which means we do not push d\u2032 queue. When there are no more trees to explore, i.e., the queue is empty, we have finished the search of the whole space and output the (provably) optimal tree."
    },
    {
      "heading": "H Proof of Theorems",
      "text": ""
    },
    {
      "heading": "H.1 Proof of Theorem C.1",
      "text": "Proof. For an optimal tree d\u2217 with objective R\u2217,\n\u03bbL(d\u2217) \u2264 R\u2217 = R(d\u2217,x,y) = `(d\u2217,x,y) + \u03bbL(d\u2217) \u2264 Rc.\nThe maximum possible number of leaves for d\u2217 occurs when `(d\u2217,x, y) is minimized; therefore this gives bound (12).\nFor the rest of the proof, let H\u2217 = L(d\u2217) be the number of leaves of d\u2217. If the current best tree dc has zero misclassification error, then\n\u03bbH\u2217 \u2264 `(d\u2217,x,y) + \u03bbH\u2217 = R(d\u2217,x,y) \u2264 Rc = R(dc,x,y) = \u03bbH,\nand thus H\u2217 \u2264 H . If the current best tree is suboptimal, i.e., dc /\u2208 argmindR(d,x,y), then\n\u03bbH\u2217 \u2264 `(d\u2217,x,y) + \u03bbH\u2217 = R(d\u2217,x,y) < Rc = R(dc,x,y) = \u03bbH,\nin which case H\u2217 < H , i.e., H\u2217 \u2264 H \u2212 1, since H is an integer."
    },
    {
      "heading": "H.2 Proof of Theorem C.3",
      "text": "Proof. First, note that H \u2032 \u2265 H . Now recall that\nb(dun,x,y) = `p(dun, \u03b4un,x,y) + \u03bbH\n\u2264 `p(d\u2032un, \u03b4\u2032un,x,y) + \u03bbH \u2032 = b(d\u2032un,x,y),\nand that `p(dun, \u03b4un,x,y) \u2264 `p(d\u2032un, \u03b4\u2032un,x,y). Combining these bounds and rearranging gives\nb(d\u2032un,x,y) = `p(d \u2032 un, \u03b4 \u2032 un,x,y) + \u03bbH + \u03bb(H \u2032 \u2212H) \u2265 `p(dun, \u03b4un,x,y) + \u03bbH + \u03bb(H \u2032 \u2212H) = b(dun,x,y) + \u03bb(H \u2032 \u2212H). (23)\nCombining (23) with b(d\u2032un,x,y) < Rc gives (14)."
    },
    {
      "heading": "H.3 Proof of Theorem D.1",
      "text": "Proof. The number of remaining tree evaluations is equal to the number of trees that are currently in or will be inserted into queue Q. For any such tree with unchanged leaves dun, Theorem C.3 gives an upper bound on the number of leaves of a tree with unchanged leaves d\u2032un that contains dun:\nL(d\u2032un) \u2264 min ( L(dun) + \u230a Rc \u2212 b(dun,x,y)\n\u03bb\n\u230b , 2M ) \u2261 U(dun).\nThis gives an upper bound on the remaining tree evaluations:\n\u0393(Rc, Q) \u2264 \u2211\ndun\u2208Q U(dun)\u2212L(dun)\u2211 k=0 P (3M \u2212 L(dun), k) (24)\n= \u2211\ndun\u2208Q f(dun)\u2211 k=0 (3M \u2212 L(dun))! (3M \u2212 L(dun)\u2212 k)! ,\nwhere P (m, k) denotes the number of k-permutations of m."
    },
    {
      "heading": "H.4 Proof of Proposition D.2",
      "text": "Proof. By Corollary C.2, K \u2261 min(b1/2\u03bbc, 2M ) gives an upper bound on the number of leaves of any optimal tree. Since we can think of our problem as finding the optimal selection and permutation of k out of 3M leaves, over all k \u2264 K,\n\u0393tot(S) \u2264 1 + K\u2211 k=1 P (3M , k) = K\u2211 k=0 3M ! (3M \u2212 k)! ."
    },
    {
      "heading": "H.5 Proof of Theorem 3.3",
      "text": "Proof. Let d\u2217 = (dun, \u03b4un, dsplit, \u03b4split,K,H) be an optimal tree with leaves (p1, . . . , pH) and labels (y\u0302\n(leaf) 1 , . . . , y\u0302 (leaf) H ). Consider the tree d = (d \u2032 un, \u03b4 \u2032 un, d \u2032 split, \u03b4 \u2032 split,K \u2032, H \u2032) derived from d\u2217 by deleting a pair of sibling leaves pi \u2192 y\u0302(leaf)i , pi+1 \u2192 y\u0302 (leaf) i+1 and adding their parent leaf pj \u2192 y\u0302 (leaf) j , therefore d\u2032un = (p1, . . . , pi\u22121, pi+2, . . . , pH , pj) and \u03b4\u2032un = (y\u0302 (leaf) 1 , . . . , y\u0302 (leaf) i\u22121 , y\u0302 (leaf) i+2 , . . . , y\u0302 (leaf) H , y\u0302 (leaf) j ).\nWhen d misclassifies half of the data captured by pi, pi+1, while d\u2217 correctly classifies them all, the difference between d and d\u2217 would be maximized, which provides an upper bound:\nR(d,x,y) = `(d,x,y) + \u03bb(H \u2212 1) \u2264 `(d\u2217,x,y) + supp(pi,x) + supp(pi+1,x)\n\u2212 1 2 [supp(pi,x) + supp(pi+1,x)] + \u03bb(H \u2212 1)\n= R(d\u2217,x,y) + 1\n2 [supp(pi,x) + supp(pi+1,x)]\u2212 \u03bb\n= R\u2217 + 1\n2 [supp(pi,x) + supp(pi+1,x)]\u2212 \u03bb (25)\nwhere supp(pi,x), supp(pi,x) is the normalized support of pi, pi+1, defined in (3), and the regularization \u2018bonus\u2019 comes from the fact that d\u2217 has one more leaf than d.\nBecause d\u2217 is the optimal tree, we have R\u2217 \u2264 R(d,x,y), which combined with (25) leads to (6). Therefore, for each child leaf pair pk, pk+1 of a split, the sum of normalized supports of pk, pk+1 should be no less than twice the regularization parameter, i.e., 2\u03bb."
    },
    {
      "heading": "H.6 Proof of Theorem 3.4",
      "text": "Proof. Let d = (d\u2032un, \u03b4\u2032un, d\u2032split, \u03b4 \u2032 split,K \u2032, H \u2032) be the tree derived from d\u2217 by deleting a pair of leaves, pi and pi+1, and adding the their parent leaf, pj . The discrepancy between d\u2217 and d is the discrepancy between (pi, pi+1) and pj : `(d,x,y)\u2212 `(d\u2217,x,y) = ai, where ai is defined in (7). Therefore,\nR(d,x,y) = `(d,x,y) + \u03bb(K \u2212 1) = `(d\u2217,x,y) + ai + \u03bb(K \u2212 1) = R(d\u2217,x,y) + ai \u2212 \u03bb = R\u2217 + ai \u2212 \u03bb.\nThis combined with R\u2217 \u2264 R(d,x,y) leads to \u03bb \u2264 ai."
    },
    {
      "heading": "H.7 Proof of Theorem 3.5",
      "text": "Proof. Let d = (d\u2032un, \u03b4\u2032un, d\u2032split, \u03b4 \u2032 split,K \u2032, H \u2032) be the tree derived from d\u2217 by deleting a pair of leaves, pi with label y\u0302(leaf)i and pi+1 with label y\u0302 (leaf) i+1 , and adding the their parent leaf pj with label y\u0302 (leaf) j . The discrepancy between d\u2217 and d is the discrepancy between pi, pi+1 and pj : `(d,x,y) \u2212 `(d\u2217,x,y) = ai, where we defined ai in (7). According to Theorem 3.4, \u03bb \u2264 ai and\n\u03bb \u2264 1 N N\u2211 n=1 {cap(xn, pi) \u2227 1[y\u0302(leaf)i = yn]\n+ cap(xn, pi+1) \u2227 1[y\u0302(leaf)i+1 = yn]\n\u2212 cap(xn, pj) \u2227 1[y\u0302(leaf)j = yn]}. (24) For any leaf j and its two child leaves i, i+ 1, we always have\nN\u2211 n=1 cap(xn, pi) \u2227 1[y\u0302(leaf)i = yn] \u2264 N\u2211 n=1 cap(xn, pj) \u2227 1[y\u0302(leaf)j = yn],\nN\u2211 n=1 cap(xn, pi+1) \u2227 1[y\u0302(leaf)i+1 = yn] \u2264 N\u2211 n=1 cap(xn, pj) \u2227 1[y\u0302(leaf)j = yn]\nwhich indicates that ai \u2264 1N \u2211N n=1 cap(xn, pi) \u2227 1[y\u0302 (leaf) i = yn] and ai \u2264 1N \u2211N n=1 cap(xn, pi+1) \u2227 1[y\u0302 (leaf) i+1 = yn]. Therefore,\n\u03bb \u2264 1 N N\u2211 n=1 cap(xn, pi) \u2227 1[y\u0302(leaf)i = yn],\n\u03bb \u2264 1 N N\u2211 n=1 cap(xn, pi+1) \u2227 1[y\u0302(leaf)i+1 = yn]."
    },
    {
      "heading": "H.8 Proof of Proposition B.1",
      "text": "Proof. Recall that the objective is R(d,x,y) = `(d,x,y) + \u03bbH , where the misclassification error `(d,x,y) is given by\n`(d,x,y) = 1\nN N\u2211 n=1 K\u2211 k=1 cap(xn, pk) \u2227 1[y\u0302(leaf)k 6= yn].\nAny particular tree uses a specific leaf, and therefore a single class label, to classify all points within a set of equivalent points. Thus, for a set of equivalent points u, the tree d correctly classifies either points that have the majority class label, or points that have the minority class label. It follows that d misclassifies a number of points in u at least as great as the number of points with the minority class label. To translate this into a lower bound on `(d,x,y), we first sum over all sets of equivalent points, and then for each such set, count differences between class labels and the minority class label of the set, instead of counting mistakes:\n`(d,x,y)\n= 1\nN U\u2211 u=1 N\u2211 n=1 K\u2211 k=1 cap(xn, pk) \u2227 1[y\u0302(leaf)k 6= yn] \u2227 1[xn \u2208 eu]\n\u2265 1 N U\u2211 u=1 N\u2211 n=1 K\u2211 k=1 cap(xn, pk) \u2227 1[qu = yn] \u2227 1[xn \u2208 eu].\nNext, because every datum must be captured by a leaf in the tree d, \u2211K k=1 cap(xn, pk) = 1.\n`(d,x,y) \u2265 1 N U\u2211 u=1 N\u2211 n=1 1[xn \u2208 eu]1[yn = qu] = U\u2211 u=1 \u03b8(eu),\nwhere the final equality applies the definition of \u03b8(eu) in (9). Therefore, R(d,x,y) = `(d,x,y) + \u03bbK \u2265 \u2211U u=1 \u03b8(eu) + \u03bbK."
    },
    {
      "heading": "I Ablation Experiments",
      "text": "We evaluate how much each of our bounds contributes to OSDT\u2019s performance and what effect the scheduling metric has on execution. Table 2 provides experimental statistics of total execution time, time to optimum, total number of trees evaluated, number of trees evaluated to optimum, and memory consumption on the recidivism data set. The first row is the full OSDT implementation, and the others are variants, each of which removes a specific bound. While all the optimizations reduce the search space, the lookahead and equivalent points bounds are, by far, the most significant, reducing time to optimum by at least two orders of magnitude and reducing memory consumption by more than one order of magnitude. In our experiment, although the scheduling policy has a smaller effect, it is still significant \u2013 curiosity is a factor of two faster than the objective function and consumes 25% of the memory consumed when using the objective function for scheduling. All other scheduling policies, i.e., the lower bound and the entropy, are significantly worse."
    },
    {
      "heading": "J Regularized BinOCT",
      "text": "Since BinOCT always produces complete binary trees of given depth, we add a regularization term to the objective function of BinOCT. In this way, regularized BinOCT (RBinOCT) can generate the same trees as OSDT. Following the notation of [20], we provide the formulation of regularized BinOCT:\nmin \u2211 l,c el,c + \u03bb \u2211 l \u03b1l s.t. (25)\n\u2200n \u2211 f fn,f = 1\n\u2200r \u2211 l lr,l = 1\n\u2200l \u2211 c pl,c = 1\n\u2200n, f, b \u2208 bin(f) M \u00b7 fn,f + \u2211\nr\u2208lr(b) \u2211 l\u2208ll(n) lr,l + \u2211 t\u2208tl(b) M \u00b7 tn,t \u2212 \u2211 t\u2208tl(b) M \u2264M\n\u2200n, f, b \u2208 bin(f) M \u2032 \u00b7 fn,f + \u2211\nr\u2208rr(b) \u2211 l\u2208rl(n) lr,l \u2212 \u2211 t\u2208tl(b) M \u2032 \u00b7 tn,t \u2264M \u2032\n\u2200n, f M \u2032\u2032 \u00b7 fn,f + \u2211\nmaxt(f)<f(r)\n\u2211 l\u2208ll(n) lr,l + \u2211\nf(r)<mint(f)\n\u2211 l\u2208rl(n) lr,l \u2264M\u201d\n\u2200 l, c \u2211\nr:Cr=c\nlr,l \u2212M \u2032\u2032\u2032 \u00b7 pl \u2264 el,c\n\u2200 l \u2211 r lr,l \u2264 R \u00b7 \u03b1l, (26)\nwhere 1 \u2264 n \u2264 N , 1 \u2264 f \u2264 F , 1 \u2264 r \u2264 R, 1 \u2264 l \u2264 L, 1 \u2264 c \u2264 C. Variables \u03b1l, fn,f , tn,t, pl,c are binary, and el,c and lr,l are continuous (see Table 3). Compared with BinOCT, we add a penalization term \u03bb \u2211 l \u03b1l to the objective function (25) and a new constraint (26), where \u03bb is the same as that of OSDT, and \u03b1l = 1 if leaf l is not empty and \u03b1l = 0 if leaf l is empty. All the rest of the constraints are the same as those of BinOCT. We use the same notation as in the original BinOCT formulation [20].\nFigure 7 shows the trees generated by regularized BinOCT and OSDT when using the same regularization parameter \u03bb = 0.007. Although the two algorithms produce the same optimal trees, regularized BinOCT is much slower than OSDT. In our experiment, it took only 3.390 seconds to run the OSDT algorithm to optimality, while the regularized BinOCT algorithm had not finished running after 1 hour.\nIn Figure 8, we provide execution traces of OSDT and RBinOCT. OSDT converges much faster than RBinOCT. For some datasets, i.e., FICO and Monk1, the total execution time of RBinOCT was several times longer than that of OSDT."
    },
    {
      "heading": "K CART",
      "text": "We provide the trees generated by CART on COMPAS, Tic-Tac-Toe, and Monk1 datasets. With the same number of leaves as their OSDT counterparts (Figure 4, Figure 6, Figure 5), these CART trees perform much worse than the OSDT ones."
    },
    {
      "heading": "Execution Traces of OSDT, RBinOCT and CART (monk3 Dataset)",
      "text": ""
    },
    {
      "heading": "L Cross-validation Experiments",
      "text": "The difference between training and test error is probabilistic and depends on the number of observations in both training and test sets, as well as the complexity of the model class. The best learning-theoretic bound on test error occurs when the training error is minimized for each model class, which in our case, is the maximum number of leaves in the tree (the level of sparsity). By adjusting the regularization parameter throughout its full\nrange, OSDT will find the most accurate tree for each given sparsity level. Figures 10-16 show the training and test results for each of the datasets and for each fold. As indicated by theory, higher training accuacy for the same level of sparsity tends to yield higher test accuracy in general, but not always. There are some cases, like the car dataset, where OSDT\u2019s almost-uniformly-higher training accuracy leads to higher test accuracy, and other cases where all methods perform the same. In the case where all methods perform the same, OSDT provides a certificate of optimality showing that no better training performance is possible for the same level of sparsity."
    }
  ],
  "title": "Optimal Sparse Decision Trees",
  "year": 2020
}
