{
  "abstractText": "In recent years, constrained optimization has become increasingly relevant to the machine learning community, with applications including Neyman-Pearson classification, robust optimization, and fair machine learning. A natural approach to constrained optimization is to optimize the Lagrangian, but this is not guaranteed to work in the nonconvex setting, and, if using a first-order method, cannot cope with non-differentiable constraints (e.g. constraints on rates or proportions). The Lagrangian can be interpreted as a two-player game played between a player who seeks to optimize over the model parameters, and a player who wishes to maximize over the Lagrange multipliers. We propose a non-zerosum variant of the Lagrangian formulation that can cope with non-differentiable\u2014even discontinuous\u2014constraints, which we call the \u201cproxy-Lagrangian\u201d. The first player minimizes external regret in terms of easy-to-optimize \u201cproxy constraints\u201d, while the second player enforces the original constraints by minimizing swap regret. For this new formulation, as for the Lagrangian in the non-convex setting, the result is a stochastic classifier. For both the proxy-Lagrangian and Lagrangian formulations, however, we prove that this classifier, instead of having unbounded size, can be taken to be a distribution over no more than m + 1 models (where m is the number of constraints). This is a significant improvement in practical terms.",
  "authors": [
    {
      "affiliations": [],
      "name": "Andrew Cotter"
    },
    {
      "affiliations": [],
      "name": "Heinrich Jiang"
    },
    {
      "affiliations": [],
      "name": "Karthik Sridharan"
    }
  ],
  "id": "SP:afe6b57605af91525dad183171e3a850e599841f",
  "references": [
    {
      "authors": [
        "Alekh Agarwal",
        "Alina Beygelzimer",
        "Miroslav Dud\u00edk",
        "John Langford",
        "Hanna M. Wallach"
      ],
      "title": "A reductions approach to fair classification",
      "venue": "In ICML\u201918,",
      "year": 2018
    },
    {
      "authors": [
        "Sanjeev Arora",
        "Elad Hazan",
        "Satyen Kale"
      ],
      "title": "The multiplicative weights update method: a meta-algorithm and applications",
      "venue": "Theory of Computing,",
      "year": 2012
    },
    {
      "authors": [
        "Amir Beck",
        "Marc Teboulle"
      ],
      "title": "Mirror descent and nonlinear projected subgradient methods for convex optimization",
      "venue": "Oper. Res. Lett.,",
      "year": 2003
    },
    {
      "authors": [
        "Dan Biddle"
      ],
      "title": "Adverse Impact and Test Validation: A Practitioner\u2019s Guide to Valid and Defensible Employment Testing",
      "year": 2005
    },
    {
      "authors": [
        "H.F. Bohnenblust",
        "Samuel Karlin",
        "L.S. Shapley"
      ],
      "title": "Games with continuous, convex pay-off",
      "venue": "Contributions to the Theory of Games,",
      "year": 1950
    },
    {
      "authors": [
        "Robert S. Chen",
        "Brendan Lucier",
        "Yaron Singer",
        "Vasilis Syrgkanis"
      ],
      "title": "Robust optimization for non-convex objectives",
      "venue": "In Nips\u201917,",
      "year": 2017
    },
    {
      "authors": [
        "Xi Chen",
        "Xiaotie Deng"
      ],
      "title": "Settling the complexity of two-player nash equilibrium",
      "venue": "In FOCS\u201906,",
      "year": 2006
    },
    {
      "authors": [
        "Paul Christiano",
        "Jonathan A. Kelner",
        "Aleksander Madry",
        "Daniel A. Spielman",
        "Shang-Hua Teng"
      ],
      "title": "Electrical flows, Laplacian systems, and faster approximation of maximum flow in undirected graphs",
      "venue": "In STOC",
      "year": 2011
    },
    {
      "authors": [
        "Andrew Cotter",
        "Maya Gupta",
        "Jan Pfeifer"
      ],
      "title": "A Light Touch for heavily constrained SGD",
      "venue": "In 29th Annual Conference on Learning Theory,",
      "year": 2016
    },
    {
      "authors": [
        "Andrew Cotter",
        "Maya Gupta",
        "Heinrich Jiang",
        "Nathan Srebro",
        "Karthik Sridharan",
        "Serena Wang",
        "Blake Woodworth",
        "Seungil You"
      ],
      "title": "Training well-generalizing classifiers for fairness metrics and other data-dependent constraints, 2018",
      "venue": "URL https://arxiv.org/abs/1807.00028",
      "year": 2018
    },
    {
      "authors": [
        "Mark Davenport",
        "Richard G. Baraniuk",
        "Clayton D. Scott"
      ],
      "title": "Tuning support vector machines for minimax and Neyman-Pearson classification",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "year": 2010
    },
    {
      "authors": [
        "John Duchi",
        "Elad Hazan",
        "Yoram Singer"
      ],
      "title": "Adaptive subgradient methods for online learning and stochastic optimization",
      "venue": "JMLR, 12(Jul):2121\u20132159,",
      "year": 2011
    },
    {
      "authors": [
        "Elad Eban",
        "Mariano Schain",
        "Alan Mackey",
        "Ariel Gordon",
        "Rif A. Saurous",
        "Gal Elidan"
      ],
      "title": "Scalable learning of nondecomposable objectives",
      "year": 2017
    },
    {
      "authors": [
        "Dan Garber",
        "Elad Hazan"
      ],
      "title": "Playing non-linear games with linear oracles. In FOCS, pages 420\u2013428",
      "venue": "IEEE Computer Society,",
      "year": 2013
    },
    {
      "authors": [
        "Geoffrey J. Gordon",
        "Amy Greenwald",
        "Casey Marks"
      ],
      "title": "No-regret learning in convex games",
      "year": 2016
    },
    {
      "authors": [
        "Hardt",
        "Eric Price",
        "Nathan Srebro"
      ],
      "title": "Equality of opportunity in supervised learning",
      "venue": "In NIPS,",
      "year": 2008
    },
    {
      "authors": [
        "Michael Kearns",
        "Seth Neel",
        "Aaron Roth",
        "Zhiwei Steven Wu"
      ],
      "title": "Preventing fairness gerrymandering: Auditing",
      "year": 2013
    },
    {
      "authors": [
        "Diederik P. Kingma",
        "Jimmy Ba"
      ],
      "title": "Adam: A method for stochastic optimization",
      "year": 2017
    },
    {
      "authors": [
        "T. Parthasarathy",
        "T.E.S. Raghavan"
      ],
      "title": "Equilibria of continuous two-person games",
      "venue": "Pacific Journal of Mathematics,",
      "year": 1983
    },
    {
      "authors": [
        "John C. Platt"
      ],
      "title": "Fast training of support vector machines using Sequential Minimal Optimization",
      "venue": "In Advances in Kernel",
      "year": 1975
    },
    {
      "authors": [
        "Alexander Rakhlin",
        "Karthik Sridharan",
        "Ambuj Tewari"
      ],
      "title": "Online learning: Beyond regret",
      "year": 2013
    },
    {
      "authors": [
        "Nathan Srebro",
        "Karthik Sridharan",
        "Ambuj Tewari"
      ],
      "title": "On the universality of online mirror descent",
      "venue": "In NIPS\u201911,",
      "year": 2011
    },
    {
      "authors": [
        "Online",
        "Suriya Gunasekar",
        "Mesrob I. Ohannessian",
        "Nathan Srebro"
      ],
      "title": "Learning non-discriminatory",
      "year": 2018
    },
    {
      "authors": [
        "Martin Zinkevich"
      ],
      "title": "Online convex programming and generalized infinitesimal gradient ascent",
      "venue": "In ICML\u201903,",
      "year": 1920
    }
  ],
  "sections": [
    {
      "text": "ar X\niv :1\n80 4.\n06 50\n0v 2\n[ cs\n.L G\n] 2\n8 Se\nIn recent years, constrained optimization has become increasingly relevant to the machine learning community, with applications including Neyman-Pearson classification, robust optimization, and fair machine learning. A natural approach to constrained optimization is to optimize the Lagrangian, but this is not guaranteed to work in the nonconvex setting, and, if using a first-order method, cannot cope with non-differentiable constraints (e.g. constraints on rates or proportions).\nThe Lagrangian can be interpreted as a two-player game played between a player who seeks to optimize over the model parameters, and a player who wishes to maximize over the Lagrange multipliers. We propose a non-zerosum variant of the Lagrangian formulation that can cope with non-differentiable\u2014even discontinuous\u2014constraints, which we call the \u201cproxy-Lagrangian\u201d. The first player minimizes external regret in terms of easy-to-optimize \u201cproxy constraints\u201d, while the second player enforces the original constraints by minimizing swap regret.\nFor this new formulation, as for the Lagrangian in the non-convex setting, the result is a stochastic classifier. For both the proxy-Lagrangian and Lagrangian formulations, however, we prove that this classifier, instead of having unbounded size, can be taken to be a distribution over no more than m + 1 models (where m is the number of constraints). This is a significant improvement in practical terms."
    },
    {
      "heading": "1 Introduction",
      "text": "We consider the general problem of inequality constrained optimization, in which we wish to find a set of parameters \u03b8 \u2208 \u0398 minimizing an objective function subject to m functional constraints:\nmin \u03b8\u2208\u0398 g0 (\u03b8) (1)\ns.t. \u2200i \u2208 [m] .gi (\u03b8) \u2264 0\nTo highlight some of the challenges that arise in non-convex constrained optimization, consider the specific example of constraining a fairness metric. We cast the fairness problem as that of minimizing some empirical loss subject to\n\u2217acotter@google.com \u2020heinrichj@google.com \u2021sridharan@cs.cornell.edu\none or more fairness constraints. One of the simplest examples of such is the following:\nmin \u03b8\u2208\u0398\n1\n|S| \u2211\nx,y\u2208S\n\u2113 (f (x; \u03b8) , y) (2)\ns.t. 1 |S| \u2211\nx\u2208Smin\n1f(x;\u03b8)>0 \u2265 0.8 |S| \u2211\nx\u2208S\n1f(x;\u03b8)>0\nHere, f (\u00b7; \u03b8) is a classification function with parameters \u03b8, S is the training dataset, and Smin \u2286 S represents a minority population. The constraint represents a version of the so-called \u201c80% rule\u201d [e.g. Biddle, 2005, Vuolo and Levy, 2013], and forces the resulting classifier to make at least 80% of its positive predictions on the minority population\u2014 Goh et al. [2016] and Narasimhan [2018] discuss a number of useful constraints that are formulated similarly, both on fairness and non-fairness metrics. Unfortunately, several serious challenges arise when we attempt to optimize this problem:\n1. The constraint is data-dependent, and could therefore be very expensive to check. 2. The classification function f may be a badly-behaving function of \u03b8 (e.g. a deep neural network), resulting in non-convex objective and constraint functions. 3. Worse, the constraint is a linear combination of indicators, hence is not even subdifferentiable w.r.t. \u03b8.\nPerhaps the most \u201cfamiliar\u201d technique for constrained optimization is to formulate the Lagrangian:\nDefinition 1. The Lagrangian L : \u0398\u00d7 \u039b \u2192 R of Equation 1 is:\nL (\u03b8, \u03bb) := g0 (\u03b8) + m \u2211\ni=1\n\u03bbigi (\u03b8)\nwhere \u039b \u2286 Rm+ . and jointly minimize over \u03b8 \u2208 \u0398 and maximize over \u03bb \u2208 \u039b \u2286 Rm+ . By itself, using this formulation doesn\u2019t address the challenges we identified above, but we will see that, compared to the alternatives (Section 2.1), it\u2019s a good starting point for an approach that does."
    },
    {
      "heading": "1.1 Dealing with non-Convexity",
      "text": "Optimizing the Lagrangian can be interpreted as playing a two player zero-sum game: the first player chooses \u03b8 to minimize L (\u03b8, \u03bb), and the second player chooses \u03bb to maximize it. The essential difficulty is that, without strong duality\u2014 equivalently, unless the minimax theorem holds, giving that min\u03b8\u2208\u0398max\u03bb\u2208\u039b L (\u03b8, \u03bb) = max\u03bb\u2208\u039b min\u03b8\u2208\u0398L (\u03b8, \u03bb)\u2014 then the \u03b8-player, who is working on the primal (minimax) problem, and the \u03bb-player, who is working on the dual (maximin) problem, might fail to converge to a solution satisfying both players simultaneously (i.e. a pure Nash equilibrium).\nIf Equation 1 is a convex optimization problem and the action spaces \u0398 and \u039b are compact and convex, then the minimax theorem holds [von Neumann, 1928], and optimizing the Lagrangian will work. Otherwise it might not, and in fact it\u2019s quite easy to construct a counterexample: Figure 1 shows a case in which a pure Nash equilibrium of the Lagrangian game does not exist. For this reason, the standard approach for handling non-convex machine learning problems, i.e. pretending that the problem is convex and using a stochastic first order algorithm anyway, should not be expected to reliably converge to a pure Nash equilibrium\u2014even on a problem as trivial as that in Figure 1\u2014since there may be none for it to converge to.\nUnder general conditions, however, even when there is no pure Nash equilibrium, a mixed equilibrium (i.e. a pair of distributions over \u03b8 and \u03bb) does exist. Such an equilibrium defines a stochastic classifier: upon receiving an example x to classify, we would sample \u03b8 from its equilibrium distribution, and then evaluate the classification function f (x; \u03b8). Furthermore, and this is our first main contribution, this equilibrium can be taken to consist of a discrete distribution\nover at most m + 1 distinct \u03b8s (m being the number of constraints), and a single non-random \u03bb. This is a crucial improvement in practical terms, since a machine learning model consisting of e.g. a distribution over thousands (or more) of deep neural networks\u2014or worse, a continuous distribution\u2014would likely be so unwieldy as to be unusable.\n1.2 Introducing Proxy Constraints\nMost real-world machine learning implementations use first-order methods (even on non-convex problems, e.g. DNNs). To use such a method, however, one must have gradients, and gradients are unavailable for nondifferentiable constraints like that in the fairness example of Equation 2, or in the myriad of other situations in which one wishes to constrain counts or proportions instead of smooth losses (e.g. recall, coverage or churn as in Goh et al. [2016]). In all of these cases, the constraint functions are piecewise-constant, so their gradients are zero almost everywhere, and a gradient-based method cannot be expected to succeed.\nThe obvious solution is to use a surrogate. For example, we might consider replacing the indicators of Equation 2 with sigmoids, and then optimizing the Lagrangian. This solves the differentiability problem, but introduces a new one: a (mixed) Nash equilibrium would correspond to a solution satisfying the sigmoid-relaxed constraint, instead of the actual constraint. Interestingly, it turns out that we can seek to satisfy the original un-relaxed con-\nstraint, even while using a surrogate. Our proposal is motivated by the observation that, while differentiating the Lagrangian (Definition 1) w.r.t. \u03b8 requires differentiating the constraint functions gi (\u03b8), to differentiate it w.r.t. \u03bb we only need to evaluate them. Hence, a surrogate is only necessary for the \u03b8-player; the \u03bb-player can continue to use the original constraint functions.\nWe refer to a surrogate that is used by only one of the two players as a \u201cproxy\u201d, and introduce the notion of \u201cproxy constraints\u201d by taking g\u0303i (\u03b8) to be a sufficiently-smooth upper bound on gi (\u03b8) for i \u2208 [m], and formulating two functions that we call \u201cproxy-Lagrangians\u201d:\nDefinition 2. Given proxy constraint functions g\u0303i (\u03b8) \u2265 gi (\u03b8) for i \u2208 [m], the proxy-LagrangiansL\u03b8,L\u03bb : \u0398\u00d7\u039b \u2192 R of Equation 1 are:\nL\u03b8 (\u03b8, \u03bb) :=\u03bb1g0 (\u03b8) + m \u2211\ni=1\n\u03bbi+1 g\u0303i (\u03b8)\nL\u03bb (\u03b8, \u03bb) := m \u2211\ni=1\n\u03bbi+1gi (\u03b8)\nwhere \u039b := \u2206m+1 \u220b \u03bb is the (m+ 1)-dimensional simplex. As one might expect, the \u03b8-player wishes to minimize L\u03b8 (\u03b8, \u03bb), while the \u03bb-player wishes to maximize L\u03bb (\u03b8, \u03bb). Notice that the g\u0303is are only used by the \u03b8-player. Intuitively, the \u03bb-player chooses how much to weigh the proxy constraint functions, but\u2014and this is the key to our proposal\u2014does so in such a way as to satisfy the original constraints.\nUnfortunately, because the two players are optimizing different functions, this is a non-zero-sum game, and finding a (mixed) Nash equilibrium of such games is known to be PPAD-complete even in the finite setting [Chen and Deng,\n2006]. We prove, however, that a weaker type of equilibrium (a \u03a6-correlated equilibrium [Rakhlin et al., 2011], i.e. a joint distribution over \u03b8 and \u03bb w.r.t. which neither player can improve)\u2014one that we can find efficiently\u2014suffices to guarantee a nearly-optimal and nearly-feasible solution to Equation 1 in expectation."
    },
    {
      "heading": "1.3 Contributions",
      "text": "We first focus on the standard Lagrangian formulation, in the non-convex setting. In Section 3, we provide an algorithm that, given access to an approximate Bayesian optimization oracle, finds a stochastic classifier that, in expectation, is provably approximately feasible and optimal. Many previous authors have approached constrained optimization using similar techniques (see Section 2)\u2014our main contribution is to show how such a classifier can be efficiently \u201cshrunk\u201d to one that is at least as good, but is supported on only m+ 1 solutions.\nOur next major contribution is the introduction of the proxy-Lagrangian formulation, which allows us to optimize constrained problems with extremely general (even non-differentiable) constraints. In Section 4, we prove that a particular type of \u03a6-correlated equilibrium results in a stochastic classifier that is feasible and optimal, and go on to provide a novel algorithm that converges to such an equilibrium. Interestingly, to get the \u201cright\u201d sort of equilibrium, the \u03b8-player needs only minimize the usual external regret, but the \u03bb-player must minimize the swap regret. While the resulting distribution is supported on a large number of (\u03b8, \u03bb) pairs, applying the same \u201cshrinking\u201d procedure as before yields a distribution over only m+ 1 \u03b8s that is at least as good as the original.\nFinally, in Section 5, we tie everything together by describing an end-to-end recipe for provably solving a non-convex constrained optimization problem with potentially non-differentiable constraints, yielding a stochastic model that is a supported on at most m + 1 solutions. In practice, one would use SGD instead of an oracle, which results in an efficient procedure that can be easily plugged-in to existing workflows, as is experimentally verified in Section 6."
    },
    {
      "heading": "2 Related Work",
      "text": "The interpretation of constrained optimization as a two-player game has a long history: Arora et al. [2012] surveys some such work, and there are several more recent examples [e.g. Kearns et al., 2017, Narasimhan, 2018, Agarwal et al., 2018]. In particular, Agarwal et al. [2018] propose an algorithm for fair classification that is very similar to the Lagrangian-based approach that we outline in Section 3\u2014the main differences are our introduction of \u201cshrinking\u201d, and that our setting (Equation 1) is more general. The recent work of Chen et al. [2017] addresses non-convex robust optimization, i.e. problems of the form:\nmin \u03b8\u2208\u0398 max i\u2208[m] gi (\u03b8)\nLike both us and Agarwal et al. [2018], they: (i) model such a problem as a two-player game where one player chooses a mixture of objective functions, and the other player minimizes the loss of the mixture, and (ii) they find a distribution over solutions rather than a pure equilibrium. These similarities are unsurprising in light of the fact that robust optimization can be reformulated as constrained optimization via the introduction of a slack variable:\nmin \u03b8\u2208\u0398,\u03be\u2208\u039e \u03be (3)\ns.t. \u2200i \u2208 [m] .\u03be \u2265 gi (\u03b8)\nCorrespondingly, one can transform a robust problem to a constrained one at the cost of an extra bisection search [e.g. Christiano et al., 2011, Rakhlin and Sridharan, 2013]. As this relationship suggests, our main contributions can be adapted to the robust optimization setting. In particular: (i) our proposed shrinking procedure can be applied to Equation 3 to yield a distribution over only m + 1 solutions, and (ii) one could perform robust optimization over non-differentiable (even discontinuous) losses using \u201cproxy objectives\u201d, just as we use proxy constraints.\nAlgorithm 1 Optimizes the Lagrangian formulation (Definition 1) in the non-convex setting via the use of an approximate Bayesian optimization oracle O\u03c1 (Definition 3) for the \u03b8-player. The parameter R is the radius of the Lagrange multiplier space \u039b := {\n\u03bb \u2208 Rm+ : \u2016\u03bb\u20161 \u2264 R }\n, and the function \u03a0\u039b projects its argument onto \u039b w.r.t. the Euclidean norm.\nOracleLagrangian (R \u2208 R+,L : \u0398\u00d7 \u039b \u2192 R,O\u03c1 : (\u0398 \u2192 R) \u2192 \u0398, T \u2208 N, \u03b7\u03bb \u2208 R+): 1 Initialize \u03bb(1) = 0 2 For t \u2208 [T ]: 3 Let \u03b8(t) = O\u03c1 ( L ( \u00b7, \u03bb(t) ))\n// Oracle optimization\n4 Let \u2206 (t) \u03bb be a gradient of L ( \u03b8(t), \u03bb(t) ) w.r.t. \u03bb 5 Update \u03bb(t+1) = \u03a0\u039b ( \u03bb(t) + \u03b7\u03bb\u2206 (t) \u03bb )\n// Projected gradient update\n6 Return \u03b8(1), . . . , \u03b8(T ) and \u03bb(1), . . . , \u03bb(T )"
    },
    {
      "heading": "2.1 Alternative Approaches",
      "text": "Given the difficulties involved in using a Lagrangian-like formulation for non-convex problems, it\u2019s natural to ask whether one should instead favor a procedure based on entirely different principles. Unfortunately, the potential alternatives each present their own challenges.\nThe potential complexity of the constraints all but rules out approaches based on projections (e.g. projected SGD) or optimization of constrained subproblems (e.g. Frank-Wolfe, as in Hazan and Kale [2012], Jaggi [2013], Garber and Hazan [2013]). Similarly, attempting to penalize violations [e.g. Arora et al., 2012, Rakhlin and Sridharan, 2013, Mahdavi et al., 2012, Cotter et al., 2016], for example by adding \u03b3maxi\u2208[m] max {0, gi (\u03b8)} to the objective, where \u03b3 \u2208 R+ is a hyperparameter, and optimizing the resulting problem using a first order method, fails if the constraint functions are non-differentiable. Even if they are, they may still be data-dependent, so evaluating gi, or even determining whether it is positive (as is necessary for such techniques, due to the max with 0), requires enumerating over the entire dataset. Hence, unlike the Lagrangian and proxy-Lagrangian formulations, such \u201cpenalized\u201d formulations are incompatible with the use of a computationally-cheap stochastic optimizer.\nIn response to the idea of proxy constraints, it\u2019s natural to ask \u201cwhy not just relax the constraints for both players, instead of just the \u03b8-player?\u201d. This is indeed a popular approach, having been proposed e.g. for Neyman-Pearson classification [Davenport et al., 2010, Gasso et al., 2011], more general rate metrics [Goh et al., 2016], and AUC [Eban et al., 2017]. The answer is that in many cases, particularly when constraints are data dependent, they represent real-world restrictions on how the learned model is permitted to behave. For example, the \u201c80% rule\u201d of Equation 2 can be found in the HOPA Act of 1995 [Wikipedia, 2018], and it requires an 80% threshold in terms of the number of positive predictions\u2014not a relaxation\u2014which is precisely the target that the proxy-Lagrangian approach will attempt to hit.\nThis point, in turn, raises the question of generalization: satisfying the correct un-relaxed constraints on training data does not necessarily mean that they will be satisfied at evaluation time. This issue is outside the scope of this paper, but is vital. For certain specific applications, the post-training correction approach of Woodworth et al. [2017] can improve generalization performance, and Cotter et al. [2018]\u2019s more recent proposal (which is based on our proxy-Lagrangian formulation) can be applied more generally, but there is still room for future work."
    },
    {
      "heading": "3 Starting Point: Lagrangian Optimization",
      "text": "Our ultimate interest is in constrained optimization, so before we present our proposed algorithm for optimizing the Lagrangian (Definition 1) in the non-convex setting, we will characterize the relationship between an approximate Nash equilibrium of the Lagrangian game, and a nearly-optimal nearly-feasible solution to the original constrained problem (Equation 1):\nTheorem 1. Define \u039b := { \u03bb \u2208 Rm+ : \u2016\u03bb\u20161 \u2264 R } , and let \u03b8(1), . . . , \u03b8(T ) \u2208 \u0398 and \u03bb(1), . . . , \u03bb(T ) \u2208 \u039b be sequences of parameter vectors and Lagrange multipliers that comprise an approximate mixed Nash equilibrium, i.e.:\nmax \u03bb\u2217\u2208\u039b\n1\nT\nT \u2211\nt=1\nL ( \u03b8(t), \u03bb\u2217 )\n\u2212 inf \u03b8\u2217\u2208\u0398\n1\nT\nT \u2211\nt=1\nL ( \u03b8\u2217, \u03bb(t) ) \u2264 \u01eb\nDefine \u03b8\u0304 as a random variable for which \u03b8\u0304 = \u03b8(t) with probability 1/T , and let \u03bb\u0304 := (\n\u2211T t=1 \u03bb\n(t) )\n/T . Then \u03b8\u0304 is\nnearly-optimal in expectation:\nE\u03b8\u0304\n[ g0 ( \u03b8\u0304 )] \u2264 inf \u03b8\u2217\u2208\u0398:\u2200i.gi(\u03b8\u2217)\u22640 g0 (\u03b8 \u2217) + \u01eb\nand nearly-feasible:\nmax i\u2208[m] E\u03b8\u0304\n[ gi ( \u03b8\u0304 )] \u2264 \u01eb R\u2212 \u2225 \u2225\u03bb\u0304 \u2225 \u2225\n1\n(4)\nAdditionally, if there exists a \u03b8\u2032 \u2208 \u0398 that satisfies all of the constraints with margin \u03b3 (i.e. gi (\u03b8\u2032) \u2264 \u2212\u03b3 for all i \u2208 [m]), then:\n\u2225 \u2225\u03bb\u0304 \u2225 \u2225 1 \u2264 \u01eb+Bg0\n\u03b3\nwhere Bg0 \u2265 sup\u03b8\u2208\u0398 g0 (\u03b8)\u2212 inf\u03b8\u2208\u0398 g0 (\u03b8) is a bound on the range of the objective function g0.\nProof. This is a special case of Theorem 3 and Lemma 6 in Appendix A.\nThis theorem has a few differences from the more typically-encountered equivalence between Nash equilibria and optimal feasible solutions in the convex setting. First, it characterizes mixed equilibria, in that uniformly sampling from the sequences \u03b8(t) and \u03bb(t) can be interpreted as defining distributions over \u0398 and \u039b. A convexity assumption would enable us to eliminate this added complexity by appealing to Jensen\u2019s inequality to replace these sequences with their averages. Second, for the technical reason that we require compact domains in order to prove convergence rates (below), \u039b is taken to consist only of sets of Lagrange multipliers with bounded 1-norm 1.\nFinally, as a consequence of this second point, the feasibility guarantee of Equation 4 only holds if the Lagrange multipliers are, on average, smaller than the maximum 1-norm radius R. Thankfully, as is shown by the final result of Theorem 1, if there exists a point satisfying the constraints with some margin \u03b3, then there will exist Rs that are large enough to guarantee feasibility to within O(\u01eb).\nOur proposed algorithm (Algorithm 1) requires an oracle that performs approximate non-convex minimization, similarly to Chen et al. [2017]\u2019s algorithm for robust optimization and Agarwal et al. [2018]\u2019s for fair classification (the latter reference uses the terminology \u201cbest response\u201d):\nDefinition 3. A \u03c1-approximate Bayesian optimization oracle is a function O\u03c1 : (\u0398 \u2192 R) \u2192 \u0398 for which:\nf (O\u03c1 (f)) \u2264 inf \u03b8\u2217\u2208\u0398 f (\u03b8\u2217) + \u03c1\nfor any f : \u0398 \u2192 R that can be written as a nonnegative linear combination of the objective and constraint functions g0, g1, . . . , gm.\nThe \u03b8-player uses this oracle, and the \u03bb-player uses projected gradient ascent. Notice that, unlike the oracle of Chen et al. [2017], which provides a multiplicative approximation, O\u03c1 provides an additive approximation. Algorithm 1\u2019s convergence rate is:\nLemma 1. Suppose that \u039b and R are as in Theorem 1, and define the upper bound B\u2206 \u2265 maxt\u2208[T ] \u2225 \u2225 \u2225 \u2206 (t) \u03bb \u2225 \u2225 \u2225\n2 .\n1In Appendix A, this is generalized to p-norms.\nIf we run Algorithm 1 with the step size \u03b7\u03bb := R/B\u2206 \u221a 2T , then the result satisfies the conditions of Theorem 1 for:\n\u01eb = \u03c1+RB\u2206\n\u221a\n2\nT\nwhere \u03c1 is the error associated with the oracle O\u03c1.\nProof. In Appendix C.3.\nCombined with Theorem 1, we therefore have that if R is sufficiently large, then Algorithm 1 will converge to a distribution over \u0398 that is, in expectation, O(\u03c1)-far from being optimal and feasible at a O(1/ \u221a T ) rate, where \u03c1 is as in Definition 3."
    },
    {
      "heading": "3.1 Shrinking",
      "text": "Aside from the unrealistic oracle assumption (which will be partially addressed in Section 4), the main disadvantage of Algorithm 1 is that it results in a mixture of T models, which presumably would be far too many to use in practice. However, much smaller Nash equilibria exist:\nLemma 2. If \u0398 is a compact Hausdorff space, \u039b is compact, and the objective and constraint functions g0, g1, . . . , gm are continuous, then the Lagrangian game (Definition 1) has a mixed Nash equilibrium pair (\u03b8, \u03bb) where \u03b8 is a random variable supported on at most m+ 1 elements of \u0398, and \u03bb is non-random.\nProof. Follows from Theorem 5 in Appendix B.\nOf course, the mere existence of such an equilibrium is insufficient\u2014we need to be able to find it, and Algorithm 1 manifestly does not. Thankfully, we can re-formulate the problem of finding the optimal \u01eb-feasible mixture of the \u03b8(t)s as a linear program (LP) that can be solved to \u201cshrink\u201d the support set. We must first evaluate the objective and constraint functions for every \u03b8(t), yielding a T -dimensional vector of objective function values, and m such vectors of constraint function evaluations, which are then used to specify the LP:\nLemma 3. Let \u03b8(1), \u03b8(2), . . . , \u03b8(T ) \u2208 \u0398 be a sequence of T \u201ccandidate solutions\u201d of Equation 1. Define ~g0, ~gi \u2208 RT such that (~g0)t = g0 ( \u03b8(t) ) and (~gi)t = gi ( \u03b8(t) ) for i \u2208 [m], and consider the linear program:\nmin p\u2208\u2206T\n\u3008p, ~g0\u3009\ns.t. \u2200i \u2208 [m] . \u3008p, ~gi\u3009 \u2264 \u01eb\nwhere \u2206T is the T -dimensional simplex. Then every vertex p\u2217 of the feasible region\u2014in particular an optimal one\u2014 has at most m\u2217 + 1 \u2264 m+ 1 nonzero elements, where m\u2217 is the number of active \u3008p\u2217, ~gi\u3009 \u2264 \u01eb constraints.\nProof. In Appendix B.\nThis result suggests a two-phase approach to optimization. In the first phase, we apply Algorithm 1, yielding a sequence of iterates for which the uniform distribution over the \u03b8(t)s is approximately feasible and optimal. We then apply the procedure of Lemma 3 to find the best distribution over these iterates, which in particular is guaranteed to be no worse than the uniform distribution, and is supported on at most m+ 1 iterates. We\u2019ll expand upon this further in Section 5.\nAlgorithm 2 Optimizes the proxy-Lagrangian formulation (Definition 2) in the convex setting, with the \u03b8-player minimizing external regret, and the \u03bb-player minimizing swap regret. The fixM operation on line 3 results in a stationary distribution of M (i.e. a \u03bb \u2208 \u039b such that M\u03bb = \u03bb, which can be derived from the top eigenvector). The function \u03a0\u0398 projects its argument onto \u0398 w.r.t. the Euclidean norm.\nStochasticProxyLagrangian ( L\u03b8,L\u03bb : \u0398\u00d7\u2206m+1 \u2192 R, T \u2208 N, \u03b7\u03b8, \u03b7\u03bb \u2208 R+ ) :\n1 Initialize \u03b8(1) = 0, and M (1) \u2208 R(m+1)\u00d7(m+1) with Mi,j = 1/ (m+ 1) // Assumes 0 \u2208 \u0398 2 For t \u2208 [T ]: 3 Let \u03bb(t) = fixM (t) // Stationary distribution of M (t) 4 Let \u2206\u030c (t) \u03b8 be a stochastic subgradient of L\u03b8 ( \u03b8(t), \u03bb(t) ) w.r.t. \u03b8 5 Let \u2206 (t) \u03bb be a stochastic gradient of L\u03bb ( \u03b8(t), \u03bb(t) ) w.r.t. \u03bb 6 Update \u03b8(t+1) = \u03a0\u0398 ( \u03b8(t) \u2212 \u03b7\u03b8\u2206\u030c(t)\u03b8 ) // Projected SGD update 7 Update M\u0303 (t+1) = M (t) \u2299 . exp (\n\u03b7\u03bb\u2206 (t) \u03bb (\n\u03bb(t) )T )\n// \u2299 and . exp are element-wise 8 Project M\n(t+1) :,i = M\u0303 (t+1) :,i /\n\u2225 \u2225 \u2225 M\u0303 (t+1) :,i \u2225 \u2225 \u2225\n1 for i \u2208 [m+ 1] // Column-wise projection w.r.t. KL divergence\n9 Return \u03b8(1), . . . , \u03b8(T ) and \u03bb(1), . . . , \u03bb(T )"
    },
    {
      "heading": "4 Proxy-Lagrangian Optimization",
      "text": "While the Lagrangian formulation can be used to solve constrained problems in the form of Equation 1, Algorithm 1 isn\u2019t actually implementable, due to its reliance on an oracle. If one wished to apply it in practice, one would need to replace the oracle with something else, and for large-scale machine learning problems, \u201csomething else\u201d is overwhelmingly likely to be SGD [Robbins and Monro, 1951, Zinkevich, 2003] or another first-order stochastic algorithm (e.g. AdaGrad [Duchi et al., 2011] or ADAM [Kingma and Ba, 2014]).\nThis leads to the issue we raised in Section 1.2: for non-differentiable constraints like those in the fairness example of Equation 2, we cannot compute gradients, and therefore cannot use a first-order algorithm. \u201cFixing\u201d this issue by replacing the constraints with differentiable surrogates introduces a new difficulty: solutions to the resulting problem will satisfy the surrogate constraints, rather than the actual constraints.\nThe proxy-Lagrangian formulation of Definition 2 sidesteps this issue by using a non-zero-sum two-player game. The \u03bb-player chooses how much the \u03b8-player should penalize the (differentiable) proxy constraints, but does so in such a way as to satisfy the original constraints. Unfortunately, since the proxy-Lagrangian game is non-zero-sum, we cannot expect to find a Nash equilibrium, at least not efficiently. However, the analogous result to Theorem 1 requires a weaker type of equilibrium: a joint distribution over \u0398 and \u039b w.r.t. which the \u03b8-player can only make a negligible improvement compared to the best constant strategy, and the \u03bb-player compared to the best action-swapping strategy (this is a particular type of \u03a6-correlated equilibrium [Rakhlin et al., 2011]):\nTheorem 2. Define M as the set of all left-stochastic (m+ 1) \u00d7 (m+ 1) matrices, \u039b := \u2206m+1 as the (m+ 1)- dimensional simplex, and assume that each g\u0303i upper bounds the corresponding gi. Let \u03b8\n(1), . . . , \u03b8(T ) \u2208 \u0398 and \u03bb(1), . . . , \u03bb(T ) \u2208 \u039b be sequences satisfying:\n1\nT\nT \u2211\nt=1\nL\u03b8 ( \u03b8(t), \u03bb(t) )\n\u2212 inf \u03b8\u2217\u2208\u0398\n1\nT\nT \u2211\nt=1\nL\u03b8 ( \u03b8\u2217, \u03bb(t) ) \u2264\u01eb\u03b8\nmax M\u2217\u2208M\n1\nT\nT \u2211\nt=1\nL\u03bb ( \u03b8(t),M\u2217\u03bb(t) ) \u2212 1 T\nT \u2211\nt=1\nL\u03bb ( \u03b8(t), \u03bb(t) ) \u2264\u01eb\u03bb\nDefine \u03b8\u0304 as a random variable for which \u03b8\u0304 = \u03b8(t) with probability \u03bb (t) 1 / \u2211T s=1 \u03bb (s) 1 , and let \u03bb\u0304 :=\n(\n\u2211T t=1 \u03bb\n(t) )\n/T .\nThen \u03b8\u0304 is nearly-optimal in expectation:\nE\u03b8\u0304\n[ g0 ( \u03b8\u0304 )] \u2264 inf \u03b8\u2217\u2208\u0398:\u2200i.g\u0303i(\u03b8\u2217)\u22640 g0 (\u03b8 \u2217) +\n\u01eb\u03b8 + \u01eb\u03bb\n\u03bb\u03041 (5)\nand nearly-feasible:\nmax i\u2208[m] E\u03b8\u0304\n[ gi ( \u03b8\u0304 )] \u2264 \u01eb\u03bb \u03bb\u03041\n(6)\nAdditionally, if there exists a \u03b8\u2032 \u2208 \u0398 that satisfies all of the proxy constraints with margin \u03b3 (i.e. g\u0303i (\u03b8\u2032) \u2264 \u2212\u03b3 for all i \u2208 [m]), then:\n\u03bb\u03041 \u2265 \u03b3 \u2212 \u01eb\u03b8 \u2212 \u01eb\u03bb \u03b3 +Bg0\nwhere Bg0 \u2265 sup\u03b8\u2208\u0398 g0 (\u03b8)\u2212 inf\u03b8\u2208\u0398 g0 (\u03b8) is a bound on the range of the objective function g0.\nProof. This is a special case of Theorem 4 and Lemma 7 in Appendix A.\nNotice that while Equation 6 guarantees feasibility w.r.t. the original constraints, the comparator in Equation 5 is feasible w.r.t. the proxy constraints. Hence, the overall guarantee is no better than what we would achieve if we took gi := g\u0303i for all i \u2208 [m], and optimized the Lagrangian as in Section 3. However, as will be demonstrated experimentally in Section 6.2, because the feasible region w.r.t. the original constraints is larger (perhaps significantly so) than that w.r.t. the proxy constraints, the proxy-Lagrangian approach has more \u201croom\u201d to find a better solution in practice.\nOne key difference between this result and Theorem 1 is that the R parameter is absent. Instead, its role, and that of \u2225 \u2225\u03bb\u0304 \u2225 \u2225\n1 , is played by the first coordinate of \u03bb\u0304. Inspection of Definition 2 reveals that, if one or more of the constraints are violated, then the \u03bb-player would prefer \u03bb1 to be zero, whereas if they are satisfied (with some margin), then it would prefer \u03bb1 to be one. In other words, the first coordinate of \u03bb (t) encodes the \u03bb-player\u2019s belief about the feasibility of \u03b8(t), for which reason \u03b8(t) is weighted by \u03bb (t) 1 in the density defining \u03b8\u0304.\nAlgorithm 2 is motivated by the observation that, while Theorem 2 only requires that the \u03b8(t) sequence suffer low external regret w.r.t. L\u03b8 ( \u00b7, \u03bb(t) )\n, the condition on the \u03bb(t) sequence is stronger, requiring it to suffer low swap regret [Blum and Mansour, 2007]. Hence, the \u03b8-player uses SGD to minimize external regret, while the \u03bb-player uses a swap-regret minimization algorithm of the type proposed by Gordon et al. [2008], yielding the convergence guarantee:\nLemma 4. Suppose that \u0398 is a compact convex set, M and \u039b are as in Theorem 2, and that the objective and proxy constraint functions g0, g\u03031, . . . , g\u0303m are convex (but not g1, . . . , gm). Define the three upper bounds B\u0398 \u2265 max\u03b8\u2208\u0398 \u2016\u03b8\u20162, B\u2206\u030c \u2265 maxt\u2208[T ] \u2225 \u2225 \u2225 \u2206\u030c (t) \u03b8 \u2225 \u2225 \u2225\n2 , and B\u2206 \u2265 maxt\u2208[T ]\n\u2225 \u2225 \u2225 \u2206\n(t) \u03bb\n\u2225 \u2225 \u2225\n\u221e .\nIf we run Algorithm 2 with the step sizes \u03b7\u03b8 := B\u0398/B\u2206\u030c \u221a 2T and \u03b7\u03bb := \u221a\n(m+ 1) ln (m+ 1) /TB2\u2206, then the result satisfies the conditions of Theorem 2 for:\n\u01eb\u03b8 =2B\u0398B\u2206\u030c\n\u221a\n1 + 16 ln 2\u03b4 T\n\u01eb\u03bb =2B\u2206\n\u221a\n2 (m+ 1) ln (m+ 1) ( 1 + 16 ln 2\u03b4 )\nT\nwith probability 1\u2212 \u03b4 over the draws of the stochastic (sub)gradients.\nProof. In Appendix C.3.\nAlgorithm 2 is designed for the convex setting (except for the gis), for which reason it uses SGD for the \u03b8-updates. However, this convexity requirement is not innate to our approach: it\u2019s straightforward to design an oracle-based algorithm that, like Algorithm 1, doesn\u2019t require convexity 2. Our reason for presenting the SGD-based algorithm, instead of the oracle-based one, is that the purpose of proxy constraints is to substitute optimizable constraints for unoptimizable ones, and there is no need to do so if you have an oracle."
    },
    {
      "heading": "4.1 Shrinking",
      "text": "It turns out that the same existence result that we provided for the Lagrangian game (Lemma 2)\u2014of a Nash equilibrium\u2014holds for the proxy-Lagrangian:\nLemma 5. If \u0398 is a compact Hausdorff space and the objective, constraint and proxy constraint functions g0, g1, . . . , gm, g\u03031, . . . , g\u0303m are continuous, then the proxy-Lagrangian game (Definition 2) has a mixed Nash equilibrium pair (\u03b8, \u03bb) where \u03b8 is a random variable supported on at most m+1 elements of \u0398, and \u03bb is non-random.\nProof. In Appendix B.\nFurthermore, the exact same linear programming procedure of Lemma 3 can be applied (with the ~gis being defined in terms of the original\u2014not proxy\u2014constraints) to yield a solution with support size m + 1, and works equally well. This is easy to verify: since \u03b8\u0304, as defined in Theorem 2, is a distribution over the \u03b8(t)s, and is therefore feasible for the LP, the best distribution over the iterates will be at least as good."
    },
    {
      "heading": "5 Overall Procedure",
      "text": "The pieces are now in place to propose a complete two-phase optimization procedure, for both convex and nonconvex problems, with or without proxy constraints. In the first phase, we apply the appropriate algorithm to yield a distribution over the T \u201ccandidates\u201d \u03b8(1), . . . , \u03b8(T ) that is approximately feasible and optimal, according to either Theorems 1 or 2. Then, in the second phase, we construct ~g0, ~g1, . . . , ~gm \u2208 RT by evaluating the objective and constraint functions for each \u03b8(t), and then optimize the LP of Lemma 3 to find the best distribution over \u03b8(1), . . . , \u03b8(T ) (which will have support size \u2264 m + 1). If we take the \u01eb parameter to this LP to be either the RHS of Equation 4 in Theorem 1 (for the Lagrangian case), or of Equation 6 in Theorem 2 (for the proxy-Lagrangian case), then the resulting size-(m+ 1) distribution will have the same guarantees as the original.\nPractical Procedure: The approach outlined above provably works, but is still somewhat idealized. In practice, we\u2019ll dispense with the oracle O\u03c1\u2014even on non-convex problems\u2014in favor of the \u201ctypical\u201d approach: pretending that the problem is convex, and using SGD (or another cheap stochastic algorithm) for the \u03b8-updates 3. On a non-convex problem, this has no guarantees, but one would still hope that it would result in a \u201ccandidate set\u201d of \u03b8(t)s that contains enough good solutions to pass on to the LP of Lemma 3. If necessary, this candidate set can first be subsampled to make it a reasonable size. To choose the \u01eb parameter of the LP, we propose using a bisection search to find the smallest \u01eb \u2265 0 for which there exists a feasible solution. Evaluation: The ultimate result of either of these procedures is a distribution over at most m + 1 distinct \u03b8s. If the underlying problem is one of classification, with f (\u00b7; \u03b8) being the scoring function, then this distribution defines a stochastic classifier: at evaluation time, upon receiving an example x, we would sample \u03b8, and then return f (x; \u03b8). If a stochastic classifier is not acceptable (as is often the case in real-world applications), then one could heuristically convert it into a deterministic one, e.g. by weighted averaging or voting, which is made significantly easier by its small size.\n2This is Algorithm 4, with Lemma 11 being its convergence guarantee, both in Appendix C.3. 3In the Lagrangian case, this is Algorithm 3, with Lemma 10 being its convergence guarantee in the convex setting, both in Appendix C.3. In\nthe proxy-Lagrangian case, this is Algorithm 2."
    },
    {
      "heading": "6 Experiments",
      "text": "We present two experiments: the first, on the robust MNIST problem of Chen et al. [2017], tests the performance of the \u201cpractical procedure\u201d of Section 5 using the Lagrangian formulation (with the norms of the Lagrange multipliers being unbounded, i.e. R = \u221e), while the second, a fairness problem on the UCI Adult dataset [Dheeru and Karra Taniskidou, 2017], uses the proxy-Lagrangian formulation. Both were implemented in TensorFlow 4.\nIn both cases, the \u03b8 and \u03bb-updates both used AdaGrad with the same initial learning rates. In the proxy-Lagrangian case, however, the \u03bb-update (line 7 of Algorithm 2) was performed in the log domain so that it would be multiplicative. To choose the initial AdaGrad learning rate, we performed a grid search over powers-of-two, and chose the best model on a validation set. In all experiments, the optimum was in the interior of the grid.\nOur constrained optimization algorithms result in stochastic classifiers, and we report results for both the \u03b8\u0304 of Theorems 1 or 2 (in the Lagrangian or proxy-Lagrangian cases, respectively), and the optimal distribution found by the LP of Lemma 3, optimized on the training dataset."
    },
    {
      "heading": "6.1 Robust Optimization",
      "text": "In robust optimization, there are multiple objective functions g1, ..., gm : \u0398 \u2192 R, and the goal is to find a \u03b8 \u2208 \u0398 minimizing maxi\u2208[m] gi (\u03b8). As was discussed in Section 2, this can be re-written as a constrained problem by introducing a slack variable, as in Equation 3.\nThe task is the modified MNIST problem created by Chen et al. [2017], which is based on four datasets, each of which is a version of MNIST that has been corrupted in different ways. One would therefore hope that choosing gi to be an\n4Source code: https://github.com/tensorflow/tensorflow/tree/r1.10/tensorflow/contrib/constrained_optimization.\nempirical loss on the ith such dataset, and optimizing the corresponding robust problem, will result in a classifier that is \u201crobust\u201d to all four types of corruption.\nWe used a neural network with one 1024-neuron hidden layer, and ReLu activations. The four objective functions were the cross-entropy losses on the corrupted datasets. All models were trained for 50 000 iterations using a minibatch size of 100, and a \u03b8(t) was extracted every 500 iterations, yielding a sequence of length T = 100.\nBaselines: For our baselines, we trained the neural network over the union of the four datasets. We report two variants: (i) the \u201cUniform Distribution Baseline\u201d of Chen et al. [2017] is a stochastic classifier, uniformly sampled over the \u03b8(t)s (like our \u03b8\u0304 classifier), and (ii) a non-stochastic classifier taking its parameters from the last iterate \u03b8(T ).\nResults: Table 1 lists, for each of the corrupted datasets, the error rates of the compared models on both the training and testing datasets. Interestingly, although our proposed shrinking procedure is only guaranteed to give a distribution over m+ 1 solutions, in these experiments it chose only one. Hence, the \u201cLagrangian (LP)\u201d model of Table 1 is, like \u201cBaseline (\u03b8(T ))\u201d, non-stochastic.\nWhile we did not quite match the raw performance reported by Chen et al. [2017]\u2019s algorithm, our results, and theirs, tell similar stories. In particular, we can see that both of our algorithms outperformed their natural baseline equivalents. In particular, the use of shrinking not only greatly simplified the model, but also significantly improved performance."
    },
    {
      "heading": "6.2 Equal Opportunity",
      "text": "These experiments were performed on the UCI Adult dataset, which consists of census data including 14 features such as age, gender, race, occupation, and education. The goal was to predict whether income exceeds 50k/year. The dataset contains 32 561 training examples, from which we split off 20% to form a validation set, and 16 281 testing examples.\nWe dropped the \u201cfnlwgt\u201d weighting feature, and processed the remaining features as in Platt [1998], yielding 120 binary features, on which we trained linear models. The objective was to minimize the average hinge loss, subject to one 95% equal opportunity [Hardt et al., 2016] constraint in the style of Goh et al. [2016] for each \u201cprotected class\u201d: gi was defined such that gi (\u03b8) \u2264 0 iff the positive prediction rate on the set of positively-labeled examples for the associated class was at least 95% of the positive prediction rate on the set of all positively-labeled examples.\nWhen using proxy constraints, g\u0303i was taken to be a version of gi with the indicator functions defining the positive prediction rates replaced with hinge upper bounds. When not using proxy constraints, the indicator-based constraints were dropped entirely, with these upper bounds being used throughout.\nAll models were trained for 5 000 iterations with a minibatch size of 100, with a \u03b8(t) being extracted every 50 iterations, yielding a sequence of length T = 100.\nBaseline: The baseline classifier was optimized to simply minimize training hinge loss. Since this problem is unconstrained, we took the last iterate \u03b8(T ).\n\u201cBest-model\u201d Heuristic: For hyperparameter tuning using a grid search, we needed to choose the \u201cbest\u201d model on the validation set. Due to the presence of constraints, however, the \u201cbest\u201d model was not necessarily that with the lowest validation error. Instead, we used the following heuristic: the models were each ranked in terms of their objective function value, as well as the magnitude of the ith constraint violation (i.e. max {0, gi (\u03b8)}). The \u201cscore\u201d of each model was then taken to be the maximal such rank, and the model with the lowest score was chosen, with the objective function serving as a tiebreaker.\nResults: Table 2 lists the test error rates, (indicator-based) constraint function values on both the training and testing datasets, and support sizes of the stochastic classifiers, for each of the compared algorithms. The \u201cLP\u201d versions of our models, which were found using the shrinking procedure of Lemma 3, uniformly outperformed their \u03b8\u0304-analogues. We can see, however, that the generalization issue discussed in Section 2.1 caused the proxy-Lagrangian LP model to slightly violate the constraints on the testing dataset, despite satisfying them on the training dataset. The non-proxy\nalgorithms satisfied all constraints, on both the training and testing datasets, because there was sufficient \u201croom\u201d between the hinge upper bound that they actually constrained, and the true constraint, to absorb the generalization error. Inspection of the error rates, however, reveals that the relaxed constraints were so overly-conservative that satisfying them significantly damaged classification performance. In contrast, our proxy-Lagrangian approach matched the classification performance of the unconstrained baseline."
    },
    {
      "heading": "Acknowledgments",
      "text": "We thank Seungil You for initially posing the question of whether constraint functions could be relaxed for only the \u03b8-player, as well as Maya Gupta, Taman Narayan and Serena Wang for helping to develop the heuristic used to choose the \u201cbest\u201d model on the validation dataset in Section 6.2."
    },
    {
      "heading": "A Proofs of Sub{optimality,feasibility} Guarantees",
      "text": "Theorem 3. (Lagrangian Sub{optimality,feasibility}) Define \u039b = { \u03bb \u2208 Rm+ : \u2016\u03bb\u2016p \u2264 R } , and consider the Lagrangian of Equation 1 (Definition 1). Suppose that \u03b8 \u2208 \u0398 and \u03bb \u2208 \u039b are random variables such that:\nmax \u03bb\u2217\u2208\u039b E\u03b8 [L (\u03b8, \u03bb\u2217)]\u2212 inf \u03b8\u2217\u2208\u0398 E\u03bb [L (\u03b8\u2217, \u03bb)] \u2264 \u01eb (7)\ni.e. \u03b8, \u03bb is an \u01eb-approximate Nash equilibrium. Then \u03b8 is \u01eb-suboptimal:\nE\u03b8 [g0 (\u03b8)] \u2264 inf \u03b8\u2217\u2208\u0398:\u2200i\u2208[m].gi(\u03b8\u2217)\u22640 g0 (\u03b8 \u2217) + \u01eb\nFurthermore, if \u03bb is in the interior of \u039b, in the sense that \u2225 \u2225\u03bb\u0304 \u2225 \u2225\np < R where \u03bb\u0304 := E\u03bb [\u03bb], then \u03b8 is \u01eb/\n(\nR\u2212 \u2225 \u2225\u03bb\u0304 \u2225 \u2225\np\n)\n-\nfeasible: \u2225\n\u2225(E\u03b8 [g: (\u03b8)])+ \u2225 \u2225 q \u2264 \u01eb\nR\u2212 \u2225 \u2225\u03bb\u0304 \u2225 \u2225\np\nwhere g: (\u03b8) is the m-dimensional vector of constraint evaluations, and (\u00b7)+ takes the positive part of its argument, so that \u2225\n\u2225(E\u03b8 [g: (\u03b8)])+ \u2225 \u2225 q is the q-norm of the vector of expected constraint violations.\nProof. First notice that L is linear in \u03bb, so:\nmax \u03bb\u2217\u2208\u039b E\u03b8 [L (\u03b8, \u03bb\u2217)]\u2212 inf \u03b8\u2217\u2208\u0398 L ( \u03b8\u2217, \u03bb\u0304 ) \u2264 \u01eb (8)\nOptimality: Choose \u03b8\u2217 to be the optimal feasible solution in Equation 8, so that gi (\u03b8 \u2217) \u2264 0 for all i \u2208 [m], and also choose \u03bb\u2217 = 0, which combined with the definition of L (Definition 1) gives that:\nE\u03b8 [g0 (\u03b8)]\u2212 g0 (\u03b8\u2217) \u2264 \u01eb\nwhich is the optimality claim.\nFeasibility: Choose \u03b8\u2217 = \u03b8 in Equation 8. By the definition of L (Definition 1):\nmax \u03bb\u2217\u2208\u039b\nm \u2211\ni=1\n\u03bb\u2217i E\u03b8 [gi (\u03b8)]\u2212 m \u2211\ni=1\n\u03bb\u0304iE\u03b8 [gi (\u03b8)] \u2264 \u01eb\nThen by the definition of a dual norm, H\u00f6lder\u2019s inequality, and the assumption that \u2225 \u2225\u03bb\u0304 \u2225 \u2225\np < R:\nR \u2225 \u2225(E\u03b8 [g: (\u03b8)])+ \u2225 \u2225 q \u2212 \u2225 \u2225\u03bb\u0304 \u2225 \u2225 p \u2225 \u2225(E\u03b8 [g: (\u03b8)])+ \u2225 \u2225 q \u2264 \u01eb\nRearranging terms gives the feasibility claim.\nLemma 6. In the context of Theorem 3, suppose that there exists a \u03b8\u2032 \u2208 \u0398 that satisfies all of the constraints, and does so with q-norm margin \u03b3, i.e. gi (\u03b8 \u2032) \u2264 0 for all i \u2208 [m] and \u2016g: (\u03b8\u2032)\u2016q \u2265 \u03b3. Then:\n\u2225 \u2225\u03bb\u0304 \u2225 \u2225 p \u2264 \u01eb+ Bg0\n\u03b3\nwhere Bg0 \u2265 sup\u03b8\u2208\u0398 g0 (\u03b8)\u2212 inf\u03b8\u2208\u0398 g0 (\u03b8) is a bound on the range of the objective function g0.\nProof. Starting from Equation 7 (in Theorem 3), and choosing \u03b8\u2217 = \u03b8\u2032 and \u03bb\u2217 = 0:\n\u01eb \u2265E\u03b8 [g0 (\u03b8)]\u2212 E\u03bb [ g0 (\u03b8 \u2032) + m \u2211\ni=1\n\u03bbigi (\u03b8 \u2032)\n]\n\u01eb \u2265E\u03b8 [\ng0 (\u03b8)\u2212 inf \u03b8\u2032\u2208\u0398 g0 (\u03b8 \u2032)\n] \u2212 ( g0 (\u03b8 \u2032)\u2212 inf\n\u03b8\u2032\u2208\u0398 g0 (\u03b8\n\u2032)\n)\n+ \u03b3 \u2225 \u2225\u03bb\u0304 \u2225 \u2225\np\n\u01eb \u2265 \u2212Bg0 + \u03b3 \u2225 \u2225\u03bb\u0304 \u2225 \u2225\np\nSolving for \u2225 \u2225\u03bb\u0304 \u2225 \u2225\np yields the claim.\nTheorem 4. (Proxy-Lagrangian Sub{optimality,feasibility}) Let M be the set of all left-stochastic (m+ 1)\u00d7(m+ 1) matrices (i.e. M := { M \u2208 R(m+1)\u00d7(m+1) : \u2200i \u2208 [m+ 1] .M:,i \u2208 \u2206m+1 } ), and consider the \u201cproxy-Lagrangians\u201d of Equation 1 (Definition 2). Suppose that \u03b8 \u2208 \u0398 and \u03bb \u2208 \u039b are jointly distributed random variables such that:\nE\u03b8,\u03bb [L\u03b8 (\u03b8, \u03bb)]\u2212 inf \u03b8\u2217\u2208\u0398 E\u03bb [L\u03b8 (\u03b8\u2217, \u03bb)] \u2264\u01eb\u03b8 (9)\nmax M\u2217\u2208M\nE\u03b8,\u03bb [L\u03bb (\u03b8,M\u2217\u03bb)]\u2212 E\u03b8,\u03bb [L\u03bb (\u03b8, \u03bb)] \u2264\u01eb\u03bb\nDefine \u03bb\u0304 := E\u03bb [\u03bb], let (\u2126,F , P ) be the probability space, and define a random variable \u03b8\u0304 such that:\nPr { \u03b8\u0304 \u2208 S } =\n\u222b\n\u03b8\u22121(S) \u03bb1 (x) dP (x) \u222b\n\u2126 \u03bb1 (x) dP (x)\nIn words, \u03b8\u0304 is a version of \u03b8 that has been resampled with \u03bb1 being treated as an importance weight. In particular E\u03b8\u0304 [ f ( \u03b8\u0304 )] = E\u03b8,\u03bb [\u03bb1f (\u03b8)] /\u03bb\u03041 for any f : \u0398 \u2192 R. Then \u03b8\u0304 is nearly-optimal:\nE\u03b8\u0304\n[ g0 ( \u03b8\u0304 )] \u2264 inf \u03b8\u2217\u2208\u0398:\u2200i\u2208[m].g\u0303i(\u03b8\u2217)\u22640 g0 (\u03b8 \u2217) +\n\u01eb\u03b8 + \u01eb\u03bb\n\u03bb\u03041\nand nearly-feasible: \u2225\n\u2225 \u2225\n(\nE\u03b8\u0304\n[ g: ( \u03b8\u0304 )])\n+\n\u2225 \u2225 \u2225 \u221e \u2264 \u01eb\u03bb\n\u03bb\u03041\nNotice the optimality inequality is weaker than it may appear, since the comparator in this equation is not the optimal solution w.r.t. the constraints gi, but rather w.r.t. the proxy constraints g\u0303i.\nProof. Optimality: If we choose M\u2217 to be the matrix with its first row being all-one, and all other rows being all-zero, then L\u03bb (\u03b8,M\u2217\u03bb) = 0, which shows that the first term in the LHS of the second line of Equation 9 is nonnegative. Hence, \u2212E\u03b8,\u03bb [L\u03bb (\u03b8, \u03bb)] \u2264 \u01eb\u03bb, so by the definition of L\u03bb (Definition 2), and the fact that g\u0303i \u2265 gi:\nE\u03b8,\u03bb\n[\nm \u2211\ni=1\n\u03bbi+1g\u0303i (\u03b8)\n]\n\u2265 \u2212\u01eb\u03bb\nNotice that L\u03b8 is linear in \u03bb, so the first line of Equation 9, combined with the above result and the definition of L\u03b8 (Definition 2) becomes:\nE\u03b8,\u03bb [\u03bb1g0 (\u03b8)]\u2212 inf \u03b8\u2217\u2208\u0398\n(\n\u03bb\u03041g0 (\u03b8 \u2217) +\nm \u2211\ni=1\n\u03bb\u0304i+1g\u0303i (\u03b8 \u2217)\n)\n\u2264 \u01eb\u03b8 + \u01eb\u03bb (10)\nChoose \u03b8\u2217 to be the optimal solution that satisfies the proxy constraints g\u0303, so that g\u0303i (\u03b8 \u2217) \u2264 0 for all i \u2208 [m]. Then:\nE\u03b8,\u03bb [\u03bb1g0 (\u03b8)]\u2212 \u03bb\u03041g0 (\u03b8\u2217) \u2264 \u01eb\u03b8 + \u01eb\u03bb\nwhich is the optimality claim.\nFeasibility: We\u2019ll simplify our notation by defining \u21131 (\u03b8) := 0 and \u2113i+1 (\u03b8) := gi (\u03b8) for i \u2208 [m], so that L\u03bb (\u03b8, \u03bb) = \u3008\u03bb, \u2113: (\u03b8)\u3009. Consider the first term in the LHS of the second line of Equation 9:\nmax M\u2217\u2208M E\u03b8,\u03bb [L\u03bb (\u03b8,M\u2217\u03bb)] = max M\u2217\u2208M E\u03b8,\u03bb [\u3008M\u2217\u03bb, \u2113: (\u03b8)\u3009]\n= max M\u2217\u2208M E\u03b8,\u03bb\n\n\nm+1 \u2211\ni=1\nm+1 \u2211\nj=1\nM\u2217j,i\u03bbi\u2113j (\u03b8)\n\n\n=\nm+1 \u2211\ni=1\nmax M\u2217\n:,i \u2208\u2206m+1\nm+1 \u2211\nj=1\nE\u03b8,\u03bb\n[ M\u2217j,i\u03bbi\u2113j (\u03b8) ]\n=\nm+1 \u2211\ni=1\nmax j\u2208[m+1] E\u03b8,\u03bb [\u03bbi\u2113j (\u03b8)]\nwhere we used the fact that, since M\u2217 is left-stochastic, each of its columns is a (m+ 1)-dimensional multinoulli distribution. For the second term in the LHS of the second line of Equation 9, we can use the fact that \u21131 (\u03b8) = 0:\nE\u03b8,\u03bb\n[\nm+1 \u2211\ni=2\n\u03bbi\u2113i (\u03b8)\n]\n\u2264 m+1 \u2211\ni=2\nmax j\u2208[m+1] E\u03b8,\u03bb [\u03bbi\u2113j (\u03b8)]\nPlugging these two results into the second line of Equation 9, the two sums collapse, leaving:\nmax i\u2208[m+1]\nE\u03b8,\u03bb [\u03bb1\u2113i (\u03b8)] \u2264 \u01eb\u03bb\nThe definition of \u2113i then yields the feasibility claim.\nLemma 7. In the context of Theorem 4, suppose that there exists a \u03b8\u2032 \u2208 \u0398 that satisfies all of the proxy constraints with margin \u03b3, i.e. g\u0303i (\u03b8 \u2032) \u2264 \u2212\u03b3 for all i \u2208 [m]. Then:\n\u03bb\u03041 \u2265 \u03b3 \u2212 \u01eb\u03b8 \u2212 \u01eb\u03bb \u03b3 +Bg0\nwhere Bg0 \u2265 sup\u03b8\u2208\u0398 g0 (\u03b8)\u2212 inf\u03b8\u2208\u0398 g0 (\u03b8) is a bound on the range of the objective function g0.\nProof. Starting from Equation 10 (in the proof of Theorem 4), and choosing \u03b8\u2217 = \u03b8\u2032:\nE\u03b8,\u03bb [\u03bb1g0 (\u03b8)]\u2212 ( \u03bb\u03041g0 (\u03b8 \u2032) + m \u2211\ni=1\n\u03bb\u0304i+1 g\u0303i (\u03b8 \u2032)\n)\n\u2264 \u01eb\u03b8 + \u01eb\u03bb\nSince g\u0303i (\u03b8 \u2032) \u2264 \u2212\u03b3 for all i \u2208 [m]:\n\u01eb\u03b8 + \u01eb\u03bb \u2265E\u03b8,\u03bb [\u03bb1g0 (\u03b8)]\u2212 \u03bb\u03041g0 (\u03b8\u2032) + ( 1\u2212 \u03bb\u03041 ) \u03b3\n\u2265E\u03b8,\u03bb [ \u03bb1 (\ng0 (\u03b8)\u2212 inf \u03b8\u2032\u2208\u0398 g0 (\u03b8 \u2032)\n)] \u2212 \u03bb\u03041 ( g0 (\u03b8 \u2032)\u2212 inf\n\u03b8\u2032\u2208\u0398 g0 (\u03b8\n\u2032)\n)\n+ ( 1\u2212 \u03bb\u03041 ) \u03b3\n\u2265\u2212 \u03bb\u03041Bg0 + ( 1\u2212 \u03bb\u03041 ) \u03b3\nSolving for \u03bb\u03041 yields the claim."
    },
    {
      "heading": "B Proofs of Existence of Sparse Equilibria",
      "text": "Theorem 5. Consider a two player game, played on the compact Hausdorff spaces \u0398 and \u039b \u2286 Rm. Imagine that the \u03b8-player wishes to minimize L\u03b8 : \u0398 \u00d7 \u039b \u2192 R, and the \u03bb-player wishes to maximize L\u03bb : \u0398 \u00d7 \u039b \u2192 R, with both of these functions being continuous in \u03b8 and linear in \u03bb. Then there exists a Nash equilibrium \u03b8, \u03bb:\nE\u03b8 [L\u03b8 (\u03b8, \u03bb)] = min \u03b8\u2217\u2208\u0398 L\u03b8 (\u03b8\u2217, \u03bb) E\u03b8 [L\u03bb (\u03b8, \u03bb)] = max \u03bb\u2217\u2208\u039b E\u03b8 [L\u03bb (\u03b8, \u03bb\u2217)]\nwhere \u03b8 is a random variable placing nonzero probability mass on at most m + 1 elements of \u0398, and \u03bb \u2208 \u039b is non-random.\nProof. There are some extremely similar (and in some ways more general) results than this in the game theory literature [e.g. Bohnenblust et al., 1950, Parthasarathy and Raghavan, 1975], but for our particular (Lagrangian and proxyLagrangian) setting it\u2019s possible to provide a fairly straightforward proof.\nTo begin with, Glicksberg [1952] gives that there exists a mixed strategy in the form of two random variables \u03b8\u0303 and \u03bb\u0303:\nE\u03b8\u0303,\u03bb\u0303\n[ L\u03b8 ( \u03b8\u0303, \u03bb\u0303 )]\n= min \u03b8\u2217\u2208\u0398 E\u03bb\u0303\n[ L\u03b8 ( \u03b8\u2217, \u03bb\u0303 )]\nE\u03b8\u0303,\u03bb\u0303\n[ L\u03bb ( \u03b8\u0303, \u03bb\u0303 )]\n=max \u03bb\u2217\u2208\u039b E\u03b8\u0303\n[ L\u03bb ( \u03b8\u0303, \u03bb\u2217 )]\nSince both functions are linear in \u03bb\u0303, we can define \u03bb := E\u03bb\u0303\n[ \u03bb\u0303 ] , and these conditions become:\nE\u03b8\u0303\n[ L\u03b8 ( \u03b8\u0303, \u03bb )]\n= min \u03b8\u2217\u2208\u0398\nL\u03b8 (\u03b8\u2217, \u03bb) := \u2113min\nE\u03b8\u0303\n[ L\u03bb ( \u03b8\u0303, \u03bb )]\n=max \u03bb\u2217\u2208\u039b E\u03b8\u0303\n[ L\u03bb ( \u03b8\u0303, \u03bb\u2217 )]\nLet\u2019s focus on the first condition. Let p\u01eb := Pr { L\u03b8 ( \u03b8\u0303, \u03bb ) \u2265 \u2113min + \u01eb } , and notice that p1/n must equal zero for any n \u2208 {1, 2, . . . } (otherwise we would contradict the above), implying by the countable additivity of measures that Pr { L\u03b8 ( \u03b8\u0303, \u03bb ) = \u2113min } = 1. We therefore assume henceforth, without loss of generality, that the support of \u03b8\u0303 consists entirely of minimizers of L\u03b8 (\u00b7, \u03bb). Let S \u2286 \u0398 be this support set. Define G := {\n\u2207\u03bb\u0303L\u03bb (\u03b8\u2032, \u03bb) : \u03b8\u2032 \u2208 S } , and take G\u0304 to be the closure of the convex hull of G. Since\nE\u03b8\u0303\n[ \u2207\u03bb\u0303L\u03bb ( \u03b8\u0303, \u03bb )]\n\u2208 G\u0304 \u2286 Rm, we can write it as a convex combination of at most m + 1 extreme points of G\u0304, or equivalently of m+ 1 elements of G. Hence, we can take \u03b8 to be a discrete random variable that places nonzero mass on at most m+ 1 elements of S, and:\nE\u03b8\n[ \u2207\u03bb\u0303L\u03bb (\u03b8, \u03bb) ] = E\u03b8\u0303\n[ \u2207\u03bb\u0303L\u03bb ( \u03b8\u0303, \u03bb )]\nLinearity in \u03bb then implies that E\u03b8 [L\u03bb (\u03b8, \u00b7)] and E\u03b8\u0303 [ L\u03bb ( \u03b8\u0303, \u00b7 )] are the same function (up to a constant), and therefore have the same maximizer(s). Correspondingly, \u03b8 is supported on S, which contains only minimizers of L\u03b8 (\u00b7, \u03bb) by construction.\nLemma 5. If \u0398 is a compact Hausdorff space and the objective, constraint and proxy constraint functions g0, g1, . . . , gm, g\u03031, . . . , g\u0303m are continuous, then the proxy-Lagrangian game (Definition 2) has a mixed Nash equilibrium pair (\u03b8, \u03bb) where \u03b8 is a random variable supported on at most m+1 elements of \u0398, and \u03bb is non-random.\nProof. Applying Theorem 5 directly would result in a support size of m+2, rather than the desired m+1, since \u039b is (m+ 1)-dimensional. Instead, we define \u039b\u0303 = { \u03bb\u0303 \u2208 Rm+ : \u2225 \u2225 \u2225 \u03bb\u0303 \u2225 \u2225 \u2225\n1 \u2264 1 } as the space containing the last m coordinates\nof \u039b. Then we can rewrite the proxy-Lagrangian functions L\u0303\u03b8, L\u0303\u03bb : \u0398\u00d7 \u039b\u0303 \u2192 R as:\nL\u0303\u03b8 ( \u03b8, \u03bb\u0303 ) = ( 1\u2212 \u2225 \u2225 \u2225 \u03bb\u0303 \u2225 \u2225 \u2225\n1\n)\ng0 (\u03b8) +\nm \u2211\ni=1\n\u03bb\u0303ig\u0303i (\u03b8)\nL\u0303\u03bb ( \u03b8, \u03bb\u0303 ) =\nm \u2211\ni=1\n\u03bb\u0303igi (\u03b8)\nThese functions are linear in \u03bb\u0303, which is a m-dimensional space, so the conditions of Theorem 5 apply, yielding the claimed result.\nLemma 3. Let \u03b8(1), \u03b8(2), . . . , \u03b8(T ) \u2208 \u0398 be a sequence of T \u201ccandidate solutions\u201d of Equation 1. Define ~g0, ~gi \u2208 RT such that (~g0)t = g0 ( \u03b8(t) ) and (~gi)t = gi ( \u03b8(t) ) for i \u2208 [m], and consider the linear program:\nmin p\u2208\u2206T\n\u3008p, ~g0\u3009\ns.t. \u2200i \u2208 [m] . \u3008p, ~gi\u3009 \u2264 \u01eb\nwhere \u2206T is the T -dimensional simplex. Then every vertex p\u2217 of the feasible region\u2014in particular an optimal one\u2014 has at most m\u2217 + 1 \u2264 m+ 1 nonzero elements, where m\u2217 is the number of active \u3008p\u2217, ~gi\u3009 \u2264 \u01eb constraints.\nProof. The linear program contains not only the m explicit linearized functional constraints, but also, since p \u2208 \u2206T , the T nonnegativity constraints pt \u2265 0, and the sum-to-one constraint \u2211T t=1 pt = 1. Since p is T -dimensional, every vertex p\u2217 of the feasible region must include T active constraints. Letting m\u2217 \u2264 m be the number of active linearized functional constraints, and accounting for the sum-to-one constraint, it follows that at least T \u2212m\u2217\u22121 nonnegativity constraints are active, implying that p\u2217 contains at most m\u2217+1 nonzero elements."
    },
    {
      "heading": "C Proofs of Convergence Rates",
      "text": "C.1 Non-Stochastic One-Player Convergence Rates\nTheorem 6. (Mirror Descent) Let f1, f2, . . . : \u0398 \u2192 R be a sequence of convex functions that we wish to minimize on a compact convex set \u0398. Suppose that the \u201cdistance generating function\u201d \u03a8 : \u0398 \u2192 R+ is nonnegative and 1-strongly convex w.r.t. a norm \u2016\u00b7\u2016 with dual norm \u2016\u00b7\u2016\u2217. Define the step size \u03b7 = \u221a\nB\u03a8/TB2\u2207\u030c, where B\u03a8 \u2265 max\u03b8\u2208\u0398\u03a8(\u03b8) is a uniform upper bound on \u03a8, and B\u2207\u030c \u2265 \u2225 \u2225\u2207\u030cft ( \u03b8(t) ) \u2225 \u2225\n\u2217 is a uniform upper bound on the norms of the subgradients. Suppose that we perform T iterations of\nthe following update, starting from \u03b8(1) = argmin\u03b8\u2208\u0398\u03a8(\u03b8):\n\u03b8\u0303(t+1) =\u2207\u03a8\u2217 ( \u2207\u03a8 ( \u03b8(t) ) \u2212 \u03b7\u2207\u030cft ( \u03b8(t) ))\n\u03b8(t+1) =argmin \u03b8\u2208\u0398 D\u03a8\n( \u03b8 | \u03b8\u0303(t+1) )\nwhere \u2207\u030cft (\u03b8) \u2208 \u2202ft(\u03b8(t)) is a subgradient of ft at \u03b8, and D\u03a8 (\u03b8 | \u03b8\u2032) := \u03a8 (\u03b8) \u2212 \u03a8(\u03b8\u2032) \u2212 \u3008\u2207\u03a8(\u03b8\u2032), \u03b8 \u2212 \u03b8\u2032\u3009 is the Bregman divergence associated with \u03a8. Then:\n1\nT\nT \u2211\nt=1\nft\n( \u03b8(t) ) \u2212 1 T\nT \u2211\nt=1\nft (\u03b8 \u2217) \u2264 2B\u2207\u030c\n\u221a\nB\u03a8 T\nwhere \u03b8\u2217 \u2208 \u0398 is an arbitrary reference vector.\nProof. Mirror descent [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] dates back to 1983, but this particular statement is taken from Lemma 2 of Srebro et al. [2011].\nCorollary 1. (Gradient Descent) Let f1, f2, . . . : \u0398 \u2192 R be a sequence of convex functions that we wish to minimize on a compact convex set \u0398. Define the step size \u03b7 = B\u0398/B\u2207\u030c \u221a 2T , where B\u0398 \u2265 max\u03b8\u2208\u0398 \u2016\u03b8\u20162, and B\u2207\u030c \u2265 \u2225 \u2225\u2207\u030cft ( \u03b8(t) )\u2225 \u2225 2 is a uniform upper bound on the norms of the subgradients. Suppose that we perform T iterations of the following update, starting from \u03b8(1) = argmin\u03b8\u2208\u0398 \u2016\u03b8\u20162:\n\u03b8(t+1) = \u03a0\u0398\n( \u03b8(t) \u2212 \u03b7\u2207\u030cft ( \u03b8(t) ))\nwhere \u2207\u030cft (\u03b8) \u2208 \u2202ft(\u03b8(t)) is a subgradient of ft at \u03b8, and \u03a0\u0398 projects its argument onto \u0398 w.r.t. the Euclidean norm. Then:\n1\nT\nT \u2211\nt=1\nft\n( \u03b8(t) ) \u2212 1 T\nT \u2211\nt=1\nft (\u03b8 \u2217) \u2264 B\u0398B\u2207\u030c\n\u221a\n2\nT\nwhere \u03b8\u2217 \u2208 \u0398 is an arbitrary reference vector.\nProof. Follows from taking \u03a8(\u03b8) = \u2016\u03b8\u201622 /2 in Theorem 6.\nCorollary 2. Let M := { M \u2208 Rm\u0303\u00d7m\u0303 : \u2200i \u2208 [m\u0303] .M:,i \u2208 \u2206m\u0303 } be the set of all left-stochastic m\u0303 \u00d7 m\u0303 matrices, and let f1, f2, . . . : M \u2192 R be a sequence of concave functions that we wish to maximize.\nDefine the step size \u03b7 = \u221a\nm\u0303 ln m\u0303/TB2 \u2207\u0302 , where B \u2207\u0302\n\u2265 \u2225 \u2225 \u2225 \u2207\u0302ft ( M (t) ) \u2225 \u2225 \u2225\n\u221e,2 is a uniform upper bound on the norms of\nthe supergradients, and \u2016\u00b7\u2016\u221e,2 := \u221a \u2211m\u0303 i=1 \u2016M:,i\u2016 2 \u221e is the L\u221e,2 matrix norm. Suppose that we perform T iterations of the following update starting from the matrix M (1) with all elements equal to 1/m\u0303:\nM\u0303 (t+1) =M (t) \u2299 . exp ( \u03b7\u2207\u0302ft ( M (t) ))\nM (t+1) :,i =M\u0303 (t+1) :,i /\n\u2225 \u2225 \u2225 M\u0303 (t+1) :,i \u2225 \u2225 \u2225\n1\nwhere \u2212\u2207\u0302ft ( M (t) ) \u2208 \u2202 ( \u2212ft(M (t)) ) , i.e. \u2207\u0302ft ( M (t) ) is a supergradient of ft at M (t), and the multiplication and exponentiation in the first step are performed element-wise. Then:\n1\nT\nT \u2211\nt=1\nft (M \u2217)\u2212 1\nT\nT \u2211\nt=1\nft\n( M (t) )\n\u2264 2B \u2207\u0302\n\u221a\nm\u0303 ln m\u0303\nT\nwhere M\u2217 \u2208 M is an arbitrary reference matrix.\nProof. Define \u03a8 : M \u2192 R := m\u0303 ln m\u0303+\u2211i,j\u2208[m\u0303] Mi,j lnMi,j as m\u0303 ln m\u0303 plus the negative Shannon entropy, applied to its (matrix) argument element-wise (m\u0303 ln m\u0303 is added to make \u03a8 nonnegative on M). As in the vector setting, the resulting mirror descent update will be (element-wise) multiplicative.\nThe Bregman divergence satisfies:\nD\u03a8 (M |M \u2032) =\u03a8 (M)\u2212\u03a8(M \u2032)\u2212 \u3008\u2207\u03a8(M \u2032),M \u2212M \u2032\u3009\n= \u2016M \u2032\u20161,1 \u2212 \u2016M\u20161,1 + m\u0303 \u2211\ni=1\nDKL ( M:,i\u2016M \u2032:,i )\n(11)\nwhere \u2016M\u20161,1 = \u2211m\u0303 i=1 \u2016M:,i\u20161 is the L1,1 matrix norm. This incidentally shows that one projects onto M w.r.t. D\u03a8 by projecting each column w.r.t. the KL divergence, i.e. by normalizing the columns.\nBy Pinsker\u2019s inequality (applied to each column of an M \u2208 M):\n\u2016M \u2212M \u2032\u201621,2 \u2264 2 m\u0303 \u2211\ni=1\nDKL ( M:,i\u2016M \u2032:,i )\nwhere \u2016M\u20161,2 = \u221a \u2211m\u0303 i=1 \u2016M:,i\u2016 2 1 is the L1,2 matrix norm. Substituting this into Equation 11, and using the fact that \u2016M\u20161,1 = m\u0303 for all M \u2208 M, we have that for all M,M \u2032 \u2208 M:\nD\u03a8 (M |M \u2032) \u2265 1\n2 \u2016M \u2212M \u2032\u201621,2\nwhich shows that \u03a8 is 1-strongly convex w.r.t. the L1,2 matrix norm. The dual norm of the L1,2 matrix norm is the L\u221e,2 norm, which is the last piece needed to apply Theorem 6, yielding the claimed result.\nLemma 8. Let \u039b := \u2206m\u0303 be the m\u0303-dimensional simplex, define M := { M \u2208 Rm\u0303\u00d7m\u0303 : \u2200i \u2208 [m\u0303] .M:,i \u2208 \u2206m\u0303 } as the set of all left-stochastic m\u0303\u00d7 m\u0303 matrices, and take f1, f2, . . . : \u039b \u2192 R to be a sequence of concave functions that we wish to maximize.\nDefine the step size \u03b7 = \u221a\nm\u0303 ln m\u0303/TB2 \u2207\u0302 , where B \u2207\u0302\n\u2265 \u2225 \u2225 \u2225 \u2207\u0302ft ( \u03bb(t) ) \u2225 \u2225 \u2225\n\u221e is a uniform upper bound on the \u221e-norms of\nthe supergradients. Suppose that we perform T iterations of the following update, starting from the matrix M (1) with all elements equal to 1/m\u0303:\n\u03bb(t) =fixM (t)\nA(t) = ( \u2207\u0302ft ( \u03bb(t) ))( \u03bb(t) )T\nM\u0303 (t+1) =M (t) \u2299 . exp ( \u03b7A(t) )\nM (t+1) :,i =M\u0303 (t+1) :,i /\n\u2225 \u2225 \u2225 M\u0303 (t+1) :,i \u2225 \u2225 \u2225\n1\nwhere fixM is a stationary distribution of M (i.e. a \u03bb \u2208 \u039b such that M\u03bb = \u03bb\u2014such always exists, since M is left-stochastic), \u2212\u2207\u0302ft ( \u03bb(t) ) \u2208 \u2202 ( \u2212ft(\u03bb(t)) ) , i.e. \u2207\u0302ft ( \u03bb(t) ) is a supergradient of ft at \u03bb (t), and the multiplication and exponentiation of the third step are performed element-wise. Then:\n1\nT\nT \u2211\nt=1\nft\n( M\u2217\u03bb(t) ) \u2212 1 T\nT \u2211\nt=1\nft\n( \u03bb(t) )\n\u2264 2B \u2207\u0302\n\u221a\nm\u0303 ln m\u0303\nT\nwhere M\u2217 \u2208 M is an arbitrary left-stochastic reference matrix.\nProof. This algorithm is an instance of that contained in Figure 1 of Gordon et al. [2008].\nDefine f\u0303t (M) := ft ( M (t)\u03bb(t) ) . Observe that since \u2207\u0302ft ( \u03bb(t) ) is a supergradient of ft at \u03bb (t), and M (t)\u03bb(t) = \u03bb(t):\nft\n( M\u0303\u03bb(t) ) \u2264ft ( M (t)\u03bb(t) ) + \u2329 \u2207\u0302ft ( \u03bb(t) ) , M\u0303\u03bb(t) \u2212M (t)\u03bb(t) \u232a\n\u2264ft ( M (t)\u03bb(t) ) +A(t) \u00b7 ( M\u0303 \u2212M (t) )\nwhere the matrix product on the last line is performed element-wise. This shows that A(t) is a supergradient of f\u0303t at M (t), from which we conclude that the final two steps of the update are performing the algorithm of Corollary 2, so:\n1\nT\nT \u2211\nt=1\nf\u0303t (M \u2217)\u2212 1\nT\nT \u2211\nt=1\nf\u0303t\n( M (t) )\n\u2264 2B \u2207\u0302\n\u221a\nm\u0303 ln m\u0303\nT\nwhere the B \u2207\u0302\nof Corollary 2 is a uniform upper bound on the L\u221e,2 matrix norms of the A (t)s. However, by the\ndefinition of A(t) and the fact that \u03bb(t) \u2208 \u2206m\u0303, we can instead take B \u2207\u0302 to be a uniform upper bound on\n\u2225 \u2225 \u2225 \u2207\u0302(t) \u2225 \u2225 \u2225\n\u221e .\nSubstituting the definition of f\u0303t and again using the fact that M (t)\u03bb(t) = \u03bb(t) then yields the claimed result.\nC.2 Stochastic One-Player Convergence Rates\nTheorem 7. (Stochastic Mirror Descent) Let \u03a8, \u2016\u00b7\u2016, D\u03a8 and B\u03a8 be as in Theorem 6, and let f1, f2, . . . : \u0398 \u2192 R be a sequence of convex functions that we wish to minimize on a compact convex set \u0398.\nDefine the step size \u03b7 = \u221a\nB\u03a8/TB2\u2206\u030c, where B\u2206\u030c \u2265 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225 \u2217 is a uniform upper bound on the norms of the\nstochastic subgradients. Suppose that we perform T iterations of the following stochastic update, starting from \u03b8(1) = argmin\u03b8\u2208\u0398\u03a8(\u03b8):\n\u03b8\u0303(t+1) = \u2207\u03a8\u2217 ( \u2207\u03a8 ( \u03b8(t) ) \u2212 \u03b7\u2206\u030c(t) )\n\u03b8(t+1) = argmin \u03b8\u2208\u0398 D\u03a8\n( \u03b8|\u03b8\u0303(t+1) )\nwhere E [ \u2206\u030c(t) | \u03b8(t) ] \u2208 \u2202ft(\u03b8(t)), i.e. \u2206\u030c(t) is a stochastic subgradient of ft at \u03b8(t). Then, with probability 1 \u2212 \u03b4 over the draws of the stochastic subgradients:\n1\nT\nT \u2211\nt=1\nft\n( \u03b8(t) ) \u2212 1 T\nT \u2211\nt=1\nft (\u03b8 \u2217) \u2264 2B\u2207\u030c\n\u221a\n2B\u03a8 ( 1 + 16 ln 1\u03b4 )\nT\nwhere \u03b8\u2217 \u2208 \u0398 is an arbitrary reference vector.\nProof. This is nothing more than the usual transformation of a uniform regret guarantee into a stochastic one via the Hoeffding-Azuma inequality\u2014we include a proof for completeness.\nDefine the sequence:\nf\u0303t (\u03b8) = ft\n( \u03b8(t) ) + \u2329 \u2206\u030c(t), \u03b8 \u2212 \u03b8(t) \u232a\nThen applying non-stochastic mirror descent to the sequence f\u0303t will result in exactly the same sequence of iterates \u03b8 (t) as applying stochastic mirror descent (above) to ft. Hence, by Theorem 6 and the definition of f\u0303t (notice that we can take B\u2207\u030c = B\u2206\u030c):\n1\nT\nT \u2211\nt=1\nf\u0303t\n( \u03b8(t) ) \u2212 1 T\nT \u2211\nt=1\nf\u0303t (\u03b8 \u2217) \u22642B\u2207\u030c\n\u221a\nB\u03a8 T\n1\nT\nT \u2211\nt=1\nft\n( \u03b8(t) ) \u2212 1 T\nT \u2211\nt=1\nft (\u03b8 \u2217) \u22642B\u2207\u030c\n\u221a\nB\u03a8 T + 1 T\nT \u2211\nt=1\n(\nf\u0303t (\u03b8 \u2217)\u2212 ft (\u03b8\u2217)\n)\n\u22642B\u2207\u030c \u221a\nB\u03a8 T + 1 T\nT \u2211\nt=1\n\u2329 \u2206\u030c(t) \u2212 \u2207\u030cft ( \u03b8(t) ) , \u03b8\u2217 \u2212 \u03b8(t) \u232a\n(12)\nwhere the last step follows from the convexity of the fts. Consider the second term on the RHS. Observe that, since the \u2206\u030c(t)s are stochastic subgradients, each of the terms in the sum is zero in expectation (conditioned on the past), and the partial sums therefore form a martingale. Furthermore, by H\u00f6lder\u2019s inequality:\n\u2329 \u2206\u030c(t) \u2212 \u2207\u030cft ( \u03b8(t) ) , \u03b8\u2217 \u2212 \u03b8(t) \u232a \u2264 \u2225 \u2225 \u2225 \u2206\u030c(t) \u2212 \u2207\u030cft ( \u03b8(t) )\u2225 \u2225 \u2225\n\u2217\n\u2225 \u2225 \u2225 \u03b8\u2217 \u2212 \u03b8(t) \u2225 \u2225\n\u2225 \u2264 4B\u2206\u030c\n\u221a\n2B\u03a8\nthe last step because \u2225 \u2225\u03b8\u2217 \u2212 \u03b8(t) \u2225 \u2225 \u2264 \u2225 \u2225\u03b8\u2217 \u2212 \u03b8(1) \u2225 \u2225+ \u2225 \u2225\u03b8(t) \u2212 \u03b8(1) \u2225 \u2225 \u2264 2 sup\u03b8\u2208\u0398 \u221a 2D\u03a8 ( \u03b8 | \u03b8(1) ) \u2264 2 \u221a 2B\u03a8, using the fact that D\u03a8 is 1-strongly convex w.r.t. \u2016\u00b7\u2016, and the definition of \u03b8(1). Hence, by the Hoeffding-Azuma inequality:\nPr\n{\n1\nT\nT \u2211\nt=1\n\u2329 \u2206\u030c(t) \u2212 \u2207\u030cft ( \u03b8(t) ) , \u03b8\u2217 \u2212 \u03b8(t) \u232a \u2265 \u01eb } \u2264 exp ( \u2212 T \u01eb 2\n64B\u03a8B2\u2206\u030c\n)\nequivalently:\nPr\n\n\n\n1\nT\nT \u2211\nt=1\n\u2329 \u2206\u030c(t) \u2212 \u2207\u030cft ( \u03b8(t) ) , \u03b8\u2217 \u2212 \u03b8(t) \u232a \u2265 8B\u2206\u030c\n\u221a\nB\u03a8 ln 1 \u03b4\nT\n\n\n\n\u2264 \u03b4\nsubstituting this into Equation 12, and applying the inequality \u221a a+ \u221a b \u2264 \u221a 2a+ 2b, yields the claimed result.\nCorollary 3. (Stochastic Gradient Descent) Let f1, f2, . . . : \u0398 \u2192 R be a sequence of convex functions that we wish to minimize on a compact convex set \u0398. Define the step size \u03b7 = B\u0398/B\u2206\u030c \u221a 2T , where B\u0398 \u2265 max\u03b8\u2208\u0398 \u2016\u03b8\u20162, and B\u2206\u030c \u2265 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225 2 is a uniform upper bound on the norms of the stochastic subgradients. Suppose that we perform T iterations of the following stochastic update, starting from \u03b8(1) = argmin\u03b8\u2208\u0398 \u2016\u03b8\u20162:\n\u03b8(t+1) = \u03a0\u0398\n( \u03b8(t) \u2212 \u03b7\u2206\u030c(t) )\nwhere E [ \u2206\u030c(t) | \u03b8(t) ] \u2208 \u2202ft(\u03b8(t)), i.e. \u2206\u030c(t) is a stochastic subgradient of ft at \u03b8(t), and \u03a0\u0398 projects its argument onto \u0398 w.r.t. the Euclidean norm. Then, with probability 1\u2212 \u03b4 over the draws of the stochastic subgradients:\n1\nT\nT \u2211\nt=1\nft\n( \u03b8(t) ) \u2212 1 T\nT \u2211\nt=1\nft (\u03b8 \u2217) \u2264 2B\u0398B\u2207\u030c\n\u221a\n1 + 16 ln 1\u03b4 T\nwhere \u03b8\u2217 \u2208 \u0398 is an arbitrary reference vector.\nProof. Follows from taking \u03a8(\u03b8) = \u2016\u03b8\u201622 /2 in Theorem 7.\nCorollary 4. Let M := { M \u2208 Rm\u0303\u00d7m\u0303 : \u2200i \u2208 [m\u0303] .M:,i \u2208 \u2206m\u0303 } be the set of all left-stochastic m\u0303 \u00d7 m\u0303 matrices, and let f1, f2, . . . : M \u2192 R be a sequence of concave functions that we wish to maximize.\nDefine the step size \u03b7 = \u221a\nm\u0303 ln m\u0303/TB2 \u2206\u0302 , where B\u2206\u0302 \u2265 \u2225 \u2225 \u2225 \u2206\u0302(t) \u2225 \u2225 \u2225 \u221e,2 is a uniform upper bound on the norms of the\nstochastic supergradients, and \u2016\u00b7\u2016\u221e,2 := \u221a \u2211m\u0303 i=1 \u2016M:,i\u2016 2 \u221e is the L\u221e,2 matrix norm. Suppose that we perform T iterations of the following stochastic update starting from the matrix M (1) with all elements equal to 1/m\u0303:\nM\u0303 (t+1) =M (t) \u2299 . exp ( \u03b7\u2206\u0302(t) )\nM (t+1) :,i =M\u0303 (t+1) :,i /\n\u2225 \u2225 \u2225 M\u0303 (t+1) :,i \u2225 \u2225 \u2225\n1\nwhere E [ \u2212\u2206\u0302(t) | M (t) ] \u2208 \u2202 ( \u2212ft(M (t)) ) , i.e. \u2206\u0302(t) is a stochastic supergradient of ft at M (t), and the multiplication and exponentiation in the first step are performed element-wise. Then with probability 1 \u2212 \u03b4 over the draws of the stochastic supergradients:\n1\nT\nT \u2211\nt=1\nft (M \u2217)\u2212 1\nT\nT \u2211\nt=1\nft\n( M (t) )\n\u2264 2B\u2206\u0302\n\u221a\n2 (m\u0303 ln m\u0303) ( 1 + 16 ln 1\u03b4 )\nT\nwhere M\u2217 \u2208 M is an arbitrary reference matrix.\nProof. The same reasoning as was used to prove Corollary 2 from Theorem 6 applies here (but starting from Theorem 7).\nLemma 9. Let \u039b := \u2206m\u0303 be the m\u0303-dimensional simplex, define M := { M \u2208 Rm\u0303\u00d7m\u0303 : \u2200i \u2208 [m\u0303] .M:,i \u2208 \u2206m\u0303 } as the set of all left-stochastic m\u0303\u00d7 m\u0303 matrices, and take f1, f2, . . . : \u039b \u2192 R to be a sequence of concave functions that we wish to maximize.\nDefine the step size \u03b7 = \u221a\nm\u0303 ln m\u0303/TB2 \u2206\u0302 , where B\u2206\u0302 \u2265 \u2225 \u2225 \u2225 \u2206\u0302(t) \u2225 \u2225 \u2225 \u221e is a uniform upper bound on the \u221e-norms of the\nstochastic supergradients. Suppose that we perform T iterations of the following update, starting from the matrix M (1) with all elements equal to 1/m\u0303:\n\u03bb(t) =fixM (t)\nA(t) =\u2206\u0302(t) ( \u03bb(t) )T\nM\u0303 (t+1) =M (t) \u2299 . exp ( \u03b7A(t) )\nM (t+1) :,i =M\u0303 (t+1) :,i /\n\u2225 \u2225 \u2225 M\u0303 (t+1) :,i \u2225 \u2225 \u2225\n1\nwhere fixM is a stationary distribution of M (i.e. a \u03bb \u2208 \u039b such that M\u03bb = \u03bb\u2014such always exists, since M is left-stochastic), E [ \u2212\u2206\u0302(t) | \u03bb(t) ] \u2208 \u2202 ( \u2212ft(\u03bb(t)) ) , i.e. \u2206\u0302(t) is a stochastic supergradient of ft at \u03bb (t), and the multiplication and exponentiation of the third step are performed element-wise. Then with probability 1 \u2212 \u03b4 over the draws of the stochastic supergradients:\n1\nT\nT \u2211\nt=1\nft\n( M\u2217\u03bb(t) ) \u2212 1 T\nT \u2211\nt=1\nft\n( \u03bb(t) )\n\u2264 2B\u2206\u0302\n\u221a\n2 (m\u0303 ln m\u0303) ( 1 + 16 ln 1\u03b4 )\nT\nwhere M\u2217 \u2208 M is an arbitrary left-stochastic reference matrix.\nProof. The same reasoning as was used to prove Lemma 8 from Corollary 2 applies here (but starting from Corollary 4).\nC.3 Two-Player Convergence Rates\nLemma 1. (Algorithm 1) Suppose that \u039b and R are as in Theorem 1, and define the upper bound B\u2206 \u2265 maxt\u2208[T ] \u2225 \u2225 \u2225 \u2206 (t) \u03bb \u2225 \u2225 \u2225\n2 .\nIf we run Algorithm 1 with the step size \u03b7\u03bb := R/B\u2206 \u221a 2T , then the result satisfies the conditions of Theorem 1 for:\n\u01eb = \u03c1+RB\u2206\n\u221a\n2\nT\nwhere \u03c1 is the error associated with the oracle O\u03c1.\nAlgorithm 3 Optimizes the Lagrangian formulation (Definition 1) in the convex setting. The parameter R is the radius of the Lagrange multiplier space \u039b := {\n\u03bb \u2208 Rm+ : \u2016\u03bb\u20161 \u2264 R }\n, and the functions \u03a0\u0398 and \u03a0\u039b project their arguments onto \u0398 and \u039b (respectively) w.r.t. the Euclidean norm.\nStochasticLagrangian (R \u2208 R+,L : \u0398\u00d7 \u039b \u2192 R, T \u2208 N, \u03b7\u03b8, \u03b7\u03bb \u2208 R+): 1 Initialize \u03b8(1) = 0, \u03bb(1) = 0 // Assumes 0 \u2208 \u0398 2 For t \u2208 [T ]: 3 Let \u2206\u030c\n(t) \u03b8 be a stochastic subgradient of L ( \u03b8(t), \u03bb(t) ) w.r.t. \u03b8\n4 Let \u2206 (t) \u03bb be a stochastic gradient of L ( \u03b8(t), \u03bb(t) ) w.r.t. \u03bb 5 Update \u03b8(t+1) = \u03a0\u0398 ( \u03b8(t) \u2212 \u03b7\u03b8\u2206\u030c(t)\u03b8 )\n// Projected SGD updates . . .\n6 Update \u03bb(t+1) = \u03a0\u039b\n(\n\u03bb(t) + \u03b7\u03bb\u2206 (t) \u03bb\n)\n// . . .\n7 Return \u03b8(1), . . . , \u03b8(T ) and \u03bb(1), . . . , \u03bb(T )\nProof. Applying Corollary 1 to the optimization over \u03bb gives:\n1\nT\nT \u2211\nt=1\nL ( \u03b8(t), \u03bb\u2217 ) \u2212 1 T\nT \u2211\nt=1\nL ( \u03b8(t), \u03bb(t) ) \u2264 B\u039bB\u2206 \u221a 2\nT\nBy the definition of O\u03c1 (Definition 3):\n1\nT\nT \u2211\nt=1\nL ( \u03b8(t), \u03bb\u2217 )\n\u2212 inf \u03b8\u2217\u2208\u0398\n1\nT\nT \u2211\nt=1\nL ( \u03b8\u2217, \u03bb(t) ) \u2264 \u03c1+B\u039bB\u2206 \u221a 2\nT\nUsing the linearity of L in \u03bb, the fact that B\u039b = R, and the definitions of \u03b8\u0304 and \u03bb\u0304, yields the claimed result.\nLemma 10. (Algorithm 3) Suppose that \u0398 is a compact convex set, \u039b and R are as in Theorem 1, and that the objective and constraint functions g0, g1, . . . , gm are convex. Define the three upper bounds B\u0398 \u2265 max\u03b8\u2208\u0398 \u2016\u03b8\u20162, B\u2206\u030c \u2265 maxt\u2208[T ] \u2225 \u2225 \u2225 \u2206\u030c (t) \u03b8 \u2225 \u2225 \u2225\n2 , and B\u2206 \u2265 maxt\u2208[T ]\n\u2225 \u2225 \u2225 \u2206\n(t) \u03bb\n\u2225 \u2225 \u2225\n2 .\nIf we run Algorithm 3 with the step sizes \u03b7\u03b8 := B\u0398/B\u2206\u030c \u221a 2T and \u03b7\u03bb := R/B\u2206 \u221a 2T , then the result satisfies the conditions of Theorem 1 for:\n\u01eb = 2 (B\u0398B\u2206\u030c +RB\u2206)\n\u221a\n1 + 16 ln 2\u03b4 T\nwith probability 1\u2212 \u03b4 over the draws of the stochastic (sub)gradients.\nProof. Applying Corollary 3 to the two optimizations (over \u03b8 and \u03bb) gives that with probability 1\u2212 2\u03b4\u2032 over the draws of the stochastic (sub)gradients:\n1\nT\nT \u2211\nt=1\nL ( \u03b8(t), \u03bb(t) ) \u2212 1 T\nT \u2211\nt=1\nL ( \u03b8\u2217, \u03bb(t) )\n\u22642B\u0398B\u2206\u030c\n\u221a\n1 + 16 ln 1\u03b4\u2032\nT\n1\nT\nT \u2211\nt=1\nL ( \u03b8(t), \u03bb\u2217 ) \u2212 1 T\nT \u2211\nt=1\nL ( \u03b8(t), \u03bb(t) ) \u22642B\u039bB\u2206\n\u221a\n1 + 16 ln 1\u03b4\u2032\nT\nAdding these inequalities, taking \u03b4 = 2\u03b4\u2032, using the linearity of L in \u03bb, the fact that B\u039b = R, and the definitions of \u03b8\u0304 and \u03bb\u0304, yields the claimed result.\nAlgorithm 4 Optimizes the proxy-Lagrangian formulation (Definition 2) in the non-convex setting via the use of an approximate Bayesian optimization oracle O\u03c1 (Definition 3, but with g\u0303is instead of gis in the linear combination defining f ) for the \u03b8-player, with the \u03bb-player minimizing swap regret. The fixM operation on line 3 results in a stationary distribution of M (i.e. a \u03bb \u2208 \u039b such that M\u03bb = \u03bb, which can be derived from the top eigenvector).\nOracleProxyLagrangian ( L\u03b8,L\u03bb : \u0398\u00d7\u2206m+1 \u2192 R,O\u03c1 : (\u0398 \u2192 R) \u2192 \u0398, T \u2208 N, \u03b7\u03bb \u2208 R+ ) :\n1 Initialize M (1) \u2208 R(m+1)\u00d7(m+1) with Mi,j = 1/ (m+ 1) 2 For t \u2208 [T ]: 3 Let \u03bb(t) = fixM (t) // Stationary distribution of M (t) 4 Let \u03b8(t) = O\u03c1 ( L\u03b8 ( \u00b7, \u03bb(t) )) // Oracle optimization 5 Let \u2206 (t) \u03bb be a gradient of L\u03bb ( \u03b8(t), \u03bb(t) ) w.r.t. \u03bb 6 Update M\u0303 (t+1) = M (t) \u2299 . exp (\n\u03b7\u03bb\u2206 (t) \u03bb (\n\u03bb(t) )T )\n// \u2299 and . exp are element-wise 7 Project M\n(t+1) :,i = M\u0303 (t+1) :,i /\n\u2225 \u2225 \u2225 M\u0303 (t+1) :,i \u2225 \u2225 \u2225\n1 for i \u2208 [m+ 1] // Column-wise projection w.r.t. KL divergence\n8 Return \u03b8(1), . . . , \u03b8(T ) and \u03bb(1), . . . , \u03bb(T )\nLemma 11. (Algorithm 4) Suppose that M and \u039b are as in Theorem 2, and define the upper bound B\u2206 \u2265 maxt\u2208[T ] \u2225 \u2225 \u2225 \u2206 (t) \u03bb \u2225 \u2225 \u2225\n\u221e .\nIf we run Algorithm 4 with the step size \u03b7\u03bb := \u221a (m+ 1) ln (m+ 1) /TB2\u2206, then the result satisfies satisfies the conditions of Theorem 2 for:\n\u01eb\u03b8 =\u03c1\n\u01eb\u03bb =2B\u2206\n\u221a\n(m+ 1) ln (m+ 1)\nT\nwhere \u03c1 is the error associated with the oracle O\u03c1.\nProof. Applying Lemma 8 to the optimization over \u03bb (with m\u0303 := m+ 1) gives:\n1\nT\nT \u2211\nt=1\nL\u03bb ( \u03b8(t),M\u2217\u03bb(t) ) \u2212 1 T\nT \u2211\nt=1\nL\u03bb ( \u03b8(t), \u03bb(t) ) \u2264 2B\u2206 \u221a (m+ 1) ln (m+ 1)\nT\nBy the definition of O\u03c1 (Definition 3):\n1\nT\nT \u2211\nt=1\nL\u03b8 ( \u03b8(t), \u03bb(t) )\n\u2212 inf \u03b8\u2217\u2208\u0398\n1\nT\nT \u2211\nt=1\nL\u03b8 ( \u03b8\u2217, \u03bb(t) ) \u2264 \u03c1\nUsing the definitions of \u03b8\u0304 and \u03bb\u0304 yields the claimed result.\nLemma 4. (Algorithm 2) Suppose that \u0398 is a compact convex set, M and \u039b are as in Theorem 2, and that the objective and proxy constraint functions g0, g\u03031, . . . , g\u0303m are convex (but not g1, . . . , gm). Define the three upper bounds B\u0398 \u2265 max\u03b8\u2208\u0398 \u2016\u03b8\u20162, B\u2206\u030c \u2265 maxt\u2208[T ] \u2225 \u2225 \u2225 \u2206\u030c (t) \u03b8 \u2225 \u2225 \u2225\n2 , and B\u2206 \u2265 maxt\u2208[T ]\n\u2225 \u2225 \u2225 \u2206\n(t) \u03bb\n\u2225 \u2225 \u2225\n\u221e .\nIf we run Algorithm 2 with the step sizes \u03b7\u03b8 := B\u0398/B\u2206\u030c \u221a 2T and \u03b7\u03bb := \u221a\n(m+ 1) ln (m+ 1) /TB2\u2206, then the result satisfies the conditions of Theorem 2 for:\n\u01eb\u03b8 =2B\u0398B\u2206\u030c\n\u221a\n1 + 16 ln 2\u03b4 T\n\u01eb\u03bb =2B\u2206\n\u221a\n2 (m+ 1) ln (m+ 1) ( 1 + 16 ln 2\u03b4 )\nT\nwith probability 1\u2212 \u03b4 over the draws of the stochastic (sub)gradients.\nProof. Applying Corollary 3 to the optimization over \u03b8, and Lemma 9 to that over \u03bb (with m\u0303 := m + 1), gives that with probability 1\u2212 2\u03b4\u2032 over the draws of the stochastic (sub)gradients:\n1\nT\nT \u2211\nt=1\nL\u03b8 ( \u03b8(t), \u03bb(t) ) \u2212 1 T\nT \u2211\nt=1\nL\u03b8 ( \u03b8\u2217, \u03bb(t) ) \u22642B\u0398B\u2206\u030c\n\u221a\n1 + 16 ln 1\u03b4\u2032\nT\n1\nT\nT \u2211\nt=1\nL\u03bb ( \u03b8(t),M\u2217\u03bb(t) ) \u2212 1 T\nT \u2211\nt=1\nL\u03bb ( \u03b8(t), \u03bb(t) ) \u22642B\u2206\n\u221a\n2 (m+ 1) ln (m+ 1) ( 1 + 16 ln 1\u03b4\u2032 )\nT\nTaking \u03b4 = 2\u03b4\u2032, and using the definitions of \u03b8\u0304 and \u03bb\u0304, yields the claimed result."
    }
  ],
  "title": "Two-Player Games for Efficient Non-Convex Constrained Optimization",
  "year": 2018
}
