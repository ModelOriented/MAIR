{
  "abstractText": "Explainability has been a challenge in AI for as long as AI has existed. With the recently increased use of AI in society, it has become more important than ever that AI systems would be able to explain the reasoning behind their results also to end-users in situations such as being eliminated from a recruitment process or having a bank loan application refused by an AI system. Especially if the AI system has been trained using Machine Learning, it tends to contain too many parameters for them to be analysed and understood, which has caused them to be called \u2018black-box\u2019 systems. Most Explainable AI (XAI) methods are based on extracting an interpretable model that can be used for producing explanations. However, the interpretable model does not necessarily map accurately to the original black-box model. Furthermore, the understandability of interpretable models for an end-user remains questionable. The notions of Contextual Importance and Utility (CIU) presented in this paper make it possible to produce human-like explanations of black-box outcomes directly, without creating an interpretable model. Therefore, CIU explanations map accurately to the black-box model itself. CIU is completely model-agnostic and can be used with any black-box system. In addition to feature importance, the utility concept that is well-known in Decision Theory provides a new dimension to explanations compared to most existing XAI methods. Finally, CIU can produce explanations at any level of abstraction and using different vocabularies and other means of interaction, which makes it possible to adjust explanations and interaction according to the context and to the target users.",
  "authors": [
    {
      "affiliations": [],
      "name": "A PREPRINT"
    },
    {
      "affiliations": [],
      "name": "Kary Fr\u00e4mling"
    }
  ],
  "id": "SP:2864dcfa081d5439ee0de4d1448ec2b114171a1c",
  "references": [
    {
      "authors": [
        "Edward H. Shortliffe",
        "Randall Davis",
        "Stanton G. Axline",
        "Bruce G. Buchanan",
        "C.Cordell Green",
        "Stanley N. Cohen"
      ],
      "title": "Computer-based consultations in clinical therapeutics: Explanation and rule acquisition capabilities of the mycin system",
      "venue": "Computers and Biomedical Research,",
      "year": 1975
    },
    {
      "authors": [
        "Tim Miller",
        "Piers Howe",
        "Liz Sonenberg"
      ],
      "title": "Explainable AI: Beware of inmates running the asylum",
      "venue": "IJCAI",
      "year": 2017
    },
    {
      "authors": [
        "Tim Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "Artificial Intelligence,",
      "year": 2019
    },
    {
      "authors": [
        "M. Westberg",
        "A. Zelvelder",
        "A. Najjar"
      ],
      "title": "A historical perspective on cognitive science and its influence on xai research",
      "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),",
      "year": 2019
    },
    {
      "authors": [
        "Ralph Keeney",
        "Howard Raiffa"
      ],
      "title": "Decisions with Multiple Objectives: Preferences and Value Trade-Offs",
      "year": 1976
    },
    {
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Salvatore Ruggieri",
        "Franco Turini",
        "Fosca Giannotti",
        "Dino Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM Computing Surveys (CSUR),",
      "year": 2018
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier, 2016",
      "year": 2016
    },
    {
      "authors": [
        "Kary Fr\u00e4mling"
      ],
      "title": "Mod\u00e9lisation et apprentissage des pr\u00e9f\u00e9rences par r\u00e9seaux de neurones pour l\u2019aide \u00e0 la d\u00e9cision multicrit\u00e8re",
      "venue": "Phd thesis, INSA de Lyon,",
      "year": 1996
    },
    {
      "authors": [
        "Thomas L. Saaty"
      ],
      "title": "Decision Making for Leaders: The Analytic Hierarchy Process for Decisions in a Complex World",
      "venue": "RWS Publications,",
      "year": 1999
    },
    {
      "authors": [
        "Sylvain Kubler",
        "J\u00e9r\u00e9my Robert",
        "William Derigent",
        "Alexandre Voisin",
        "Yves Le Traon"
      ],
      "title": "A state-of the-art survey & testbed of fuzzy ahp (fahp) applications",
      "venue": "Expert Systems with Applications, 65:398\u2013422,",
      "year": 2016
    },
    {
      "authors": [
        "Kary Fr\u00e4mling",
        "Didier Graillot"
      ],
      "title": "Extracting Explanations from Neural Networks",
      "year": 1995
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Explainability has been a challenge in AI for as long as AI has existed. Shortliffe et al pointed out already in 1975 that \u2018It is our belief, therefore, that a consultation program will gain acceptance only if it serves to augment rather than replace the physician\u2019s own decision making processes.\u2019 [1]. The system described in that paper was MYCIN, an expert system that was capable of advising physicians who request advice regarding selection of appropriate antimicrobial therapy for hospital patients with bacterial infections. Great emphasis was put into the interaction with the end-user, in this case a skilled physician.\nWith the recently increased use of AI in society, it has become more important than ever that AI systems should be able to explain the reasoning behind their results also to end-users, in situations such as being eliminated from a recruitment process or having a bank loan application refused. Meanwhile, many XAI researchers have pointed out that it is rare that current XAI research would truly take \u2018normal\u2019 end-users into consideration. For instance, Miller et al. illustrate the phenomenon in their article entitled \u2018Explainable AI: Beware of Inmates Running the Asylum\u2019 for expressing the tendency that current XAI methods mainly help AI researchers to understand their own results and models [2].\nMany XAI researchers also point out that it is fair to say that most XAI work uses only the researchers\u2019 intuition of what constitutes a \u2018good\u2019 explanation, while ignoring the vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations [3][4]. Another domain that seems neglected in current XAI work is Decision Theory and related sub-domains such as Multiple Criteria Decision Making (MCDM) [5]. Decision Theory is tightly connected with the other mentioned domains because methods of\nar X\niv :2\n00 9.\n13 99\n6v 1\n[ cs\n.A I]\n2 9\nSe p\nDecision Theory are intended to produce Decision Support Systems (DSS) that are understood and used by humans when taking decisions. Decision Theory and MCDM provide clear definitions of what is meant by the importance of an input, as well as what is the utility of a given input value towards the outcome of the DSS. A simple linear DSS model is the weighted sum, where a numerical weight expresses the importance of an input and a numerical score expresses the utility of different possible input values for different outcomes of the DSS, i.e. how good or favorable a value is.\nContextual Importance and Utility (CIU) extends this linear definition of importance and utility towards non-linear models such as those produced by typical ML methods. In many (or most) real-life situations the importance of an input and the utility of different input values changes depending on values of other inputs. For instance, the outdoor temperature has a great importance on a person\u2019s comfort level as long as the person is outdoors. When the person goes inside, the situation (context) changes and the outdoor temperature then only has an indirect (if any) importance for the person\u2019s comfort level. Regarding utility, both a very cold and a very warm outdoor temperature might be good or bad depending on the context. For instance, a \u221220\u25e6C temperature tends to be uncomfortable if wearing a T-shirt, whereas a 20\u25e6C temperature is uncomfortable if wearing winter clothes. The utility of different temperature values changes when adding or removing clothes, and vice versa, the utility of different clothes changes when the temperature changes.\nAfter this Introduction, Section 2 goes through the most relevant state-of-the-art of XAI methods. Section 3 presents the formal definition of CIU. Experimental results are shown in Section 4. Open questions and future research are presented in Section 5, followed by conclusions in Section 6."
    },
    {
      "heading": "2 Background",
      "text": "There does not seem to be a clear agreement in XAI literature on the meaning of the terms interpretable versus explainable. For the rest of this paper, interpretable model will be used to signify models whose behaviour humans can understand to some extent, such as rules or linear models. Explanation will be used to signify what is actually presented to a user for a specific prediction or outcome.\nXAI methods can be classified into categories model explanation, outcome explanation and model inspection according to [6]. Model explanation signifies providing a global explanation of the black-box model through an interpretable and transparent model. This model should be able to mimic the entire behavior of the black-box and it should also be understandable by humans. Rule extraction methods and estimation of global feature importance are examples of model explanation methods, as well as decision tree, attention model, etc.\nOutcome explanation consists in providing an explanation of the outcome of the black-box for a specific instance (or context) and can therefore be considered local. It is not required to explain the underlying logic of the entire black-box but only the reason for the outcome on a specific input instance. Model inspection is not truly a XAI category, it mainly refers to how model or outcome explanations are presented to users (visual or textual for instance) for understanding the black-box model or its outcome.\nMost (or all) current outcome explanation methods are so-called post-hoc methods, i.e. they require creating an intermediate interpretable model to provide explanations. The Local Interpretable Model-agnostic Explanations (LIME) method presented in 2016 [7] might be considered a cornerstone regarding post-hoc outcome explanation. LIME belongs to the family of additive feature attribution methods [8] that are based on the assumption that a locally linear model that represents the gradient around the current context is sufficient for outcome explanation purposes. Other methods that belong to the same family are for instance Shapley values, DeepLIFT and Layer-Wise Relevance Propagation [8].\nA major challenge of all methods that use an intermediate interpretable model (the \u2018explanation model\u2019 in [8]) is to what extent the interpretable model actually corresponds to the black-box model. A rising concern among XAI researchers is that current XAI methods themselves tend to be black-boxes whose behaviour is as difficult to understand as that of the explained AI black-boxes, which causes challenges to assess to what extent XAI explanations can be trusted. Furthermore, it is not evident whether a gradient-based, locally linear model is adequate or accurate for interpreting or explaining black-box behaviour. CIU differs radically from the existing state-of-the-art in XAI because CIU does not create or use an intermediate interpretable model."
    },
    {
      "heading": "3 Contextual Importance and Utility (CIU)",
      "text": "The underlying idea behind CIU is to use a similar approach to explanation as humans do when explaining or justifying a decision to other humans. In a XAI context, the explainer is a (X)AI system that justifies or explains its decisions or actions and the explainee is a human (one or many) that is the target of the explanation [3]. Human explainers tend to identify what were the most important aspects that influenced their decision and start their explanation with them. Human explainers also adapt the abstraction level and vocabulary used in the explanation to their expectations about\nwhat is best understood and accepted by the explainee. It is generally not enough to explain only the taken decision, it is also often necessary to justify why another decision wasn\u2019t taken instead.\nCIU was initially developed in a MCDM context [9]. In MCDM, importance and utility concepts are clearly defined. The Analytic Hierarchy Process (AHP) [10] that was originally developed in the 1970\u2019s seems to have become the most popular MCDM method in research and practice [11]. AHP is essentially based on a weighted sum, where the global output can be broken into intermediate concepts in a hierarchical manner. The importance of different criteria (features, inputs of the model) is expressed by numeric weights. The utility expresses how good, favorable or typical a value is for the output of the model. For a car selection problem, importance and utility can be used for giving explanations such as \u2018This car is good because it has a good size, decent performances and a reasonable price, which are very important features\u2019, where words indicating utilities are underlined and only the most important features are presented. The use of a linear model makes the meaning of importance and utility quite understandable to humans, as illustrated in Figure 1a.\nRule-based systems, as well as classification trees for instance, are a way of overcoming the linearity limitation but tends to lead to step-wise models as illustrated in Figure 1b. Non-linear models such as neural nets can learn smooth and non-linear functions as illustrated in Figure 1c. Even though CIU can deal with all three kinds of models, the focus here is on the kind of non-linear functions in Figure 1c. We will begin the formal definition of CIU by providing a set of definitions. Definition 1 (Black-box model). A black-box model is a mathematical transformation f that maps inputs #\u00bbx to outputs #\u00bby according to #\u00bby = f( #\u00bbx ).\nDefinition 2 (Context). A Context #\u00bb\nC defines the input values #\u00bbx that describe the current situation or instance to be explained. Definition 3 (Pre-defined output range). The value range [absminj , absmaxj ] that an output yj can take by definition.\nIn classification tasks, the Pre-defined output range is typically [0, 1]. In regression tasks the minimum and maximum output values present in a training set used for Machine Learning can usually be used as an estimate of [absminj , absmaxj ]. Definition 4 (Set of studied inputs for CIU). The index set {i} defines the indices of inputs #\u00bbx for which CIU is calculated. Definition 5 (Estimated output range). [Cminj( #\u00bb C, {i}), Cmaxj( #\u00bb\nC, {i})] is the range of values that an output yj can take in the Context #\u00bb\nC when modifying the values of inputs x{i}.\nThe values used for the inputs x{i} should be \u2018representative\u2019 or realistic within the Context #\u00bb\nC . The meaning of \u2018representative\u2019 is discussed further down in this paper.\nWe are now ready to provide the first definition of Contextual Importance, using a Pre-defined output range, followed by the definition of Contextual Utility.\nDefinition 6 (Contextual Importance). Contextual Importance CIj( #\u00bb\nC, {i}) is a numeric value that expresses to what extent variations in one or several inputs {i} affect the value of an output j of a black-box model f , according to\nCIj( #\u00bb C, {i}) = Cmaxj( #\u00bb C, {i})\u2212 Cminj( #\u00bb C, {i}) absmaxj \u2212 absminj\n(1)\nDefinition 7 (Contextual Utility). Contextual Utility CUj( #\u00bb\nC, {i}) is a numeric value that expresses to what extent the current input values #\u00bb C are favorable for the output yj( #\u00bb C) of a black-box model, according to\nCUj( #\u00bb C, {i}) = yj( #\u00bb C)\u2212 Cminj( #\u00bb C, {i}) Cmaxj( #\u00bb C, {i})\u2212 Cminj( #\u00bb C, {i}) (2)\nCI and CU are illustrated in Figure 2 for the non-linear function in Figure 1c. With ( #\u00bb C) = (0.1, 0.2), CI1( #\u00bb\nC, {1}) = 0.5 and CI1( #\u00bb\nC, {2}) = 0.5, which signifies that both inputs are exactly as important for the output value. For the utilities, CU1( #\u00bb C, {1}) = 0, 316 and CU1( #\u00bb\nC, {2}) = 0.04, so even though the x2 value is higher than the x1 value, the utility of the x1 value is higher than the utility of the x2 value for the result y.\nThe estimation of the range [Cminj( #\u00bb C, {i}), Cmaxj( #\u00bb\nC, {i})] is the only part of CIU that requires more than one #\u00bby = f( #\u00bbx ) calculation. It is also the most critical part of CIU for producing explanations that truly correspond to and explain the behaviour of the black-box. Even though it might be possible to calculate or estimate [Cminj( #\u00bb C, {i}), Cmaxj( #\u00bb\nC, {i})] directly for some models, that is not the case for generic black-box models. One possible approach is to generate a Set of representative input vectors.\nDefinition 8 (Set of representative input vectors). S( #\u00bb\nC, {i}) is an N \u00d7M matrix, where M is the length of #\u00bbx and N is a parameter that gives the number of input vectors to generate for obtaining an adequate estimate of the Estimated output range [Cminj( #\u00bb C, {i}), Cmaxj( #\u00bb C, {i})].\nA simple way to construct S( #\u00bb C, {i}) is to set all input vectors in S( #\u00bbC, {i}) to #\u00bbC and then replace the values of inputs {i} with random values from a pre-defined value range that may be different for every input xi. N is the only adjustable parameter of CIU and needs to be determined based on the complexity of the function learned by the model. More efficient approaches than random values certainly exist but that remains a topic of future research. Furthermore, random values do not guarantee that the generated input vectors are \u2018representative\u2019. It might even result in input vectors that are impossible in reality. There is also a risk to have input vectors that are not even close to the examples that were included in the training set of the black-box. This challenge can be addressed at least in the following ways:\n1. Use a black-box model that has some guarantees that [Cminj( #\u00bb C, {i}), Cmaxj( #\u00bb\nC, {i})] does not go out-ofbounds even with \u2018non-representative\u2019 input vectors. In a classification task, for instance, \u2018non-representative\u2019 input vectors are not a problem for models whose outputs do not go under zero and do not go over one under any conditions .\n2. Eliminate or correct input vectors that are impossible in reality or that are too far from those included in the training set. One way of doing this could be to remove all rows in S( #\u00bb\nC, {i}) that are too far from any example in the training set. One example of non-realistic input vectors that are straightforward to correct is if there are one-hot encoded inputs, where only one of the concerned inputs is allowed to be TRUE in every input vector.\n3. Use \u2018non-representative\u2019 input vectors on purpose for potentially detecting inconsistencies in the learned model.\nNow that we have studied how to estimate CI and CU of one or more inputs {i} for any output outj , we will introduce the notion of Intermediate Concept.\nDefinition 9 (Intermediate Concept). An Intermediate Concept names a given set of inputs {i}.\nAs defined by Equations (1) and (2), CIU can be estimated for any set of inputs {i}. Intermediate concepts make it possible to specify vocabularies that can be used for producing explanations on any level of abstraction. Different names can be used for the same Intermediate Concept (as well as for input features) and change the concept name used according to the current context and the target explainee(s).\nIn addition to using Intermediate Concepts for explaining y values, Intermediate Concept values can be explained using more specific Intermediate Concepts or input features. The following defines Generalized Contextual Importance for explaining Intermediate Concepts.\nDefinition 10 (Generalized Contextual Importance).\nCIj( #\u00bb C, {i}, {I}) = Cmaxj( #\u00bb C, {i})\u2212 Cminj( #\u00bb C, {i}) Cmaxj( #\u00bb C, {I})\u2212 Cminj( #\u00bb C, {I}) (3)\nwhere {I} is the set of input indices that correspond to the Intermediate Concept that we want to explain and {i} \u2208 {I}.\nEquation 3 is similar to Equation 1 when {I} is the set of all inputs, i.e. the range [absminj , absmaxj ] has been replaced by the range [Cminj( #\u00bb C, {I}, Cmaxj( #\u00bb\nC, {I}]. Equation 2 for CU does not change by the introduction of Intermediate Concepts. In other words, Equation 3 allows the explanation of the outputs yj as well as the explanation of any Intermediate Concept that leads to yj ."
    },
    {
      "heading": "4 Experimental Results",
      "text": "Experimental results are shown for two well-know benchmark data sets: Iris flowers and Boston Housing. Iris flowers is a classification task, whereas Boston Housing is a regression task. This choice of using simple but well-known data sets signifies that it is relatively easy to understand the learned models and also to assess what \u2018correct\u2019 explanations might look like.\nBar plot visualisations are used in this paper for illustrating CIU. The length on the bar corresponds to the CI value. A configurable threshold value of CUneutral = 0.5 has been used for dividing the CU range [0, 1] into \u2018defavorable\u2019 and \u2018favorable\u2019 ranges. A red-yellow-green color scale visualises the CU value, where CU \u2208 [0, CUneutral] gives a continuous transition from red to yellow. CU \u2208 [CUneutral, 1] gives a continuous transition from yellow to dark green. Future analysis and validation with more data-sets and real-life applications will be needed in order to assess whether CUneutral needs to be adjusted in practice.\nA set S( #\u00bb\nC, {i}) with N = 1000 has been used for all results reported here, which gives a negligible calculation time using RStudio Version 1.2 on a MacBook Pro from 2017 with a 2,8 GHz Quad-Core Intel Core i7 processor and 16 GB 2133 MHz LPDDR3 memory."
    },
    {
      "heading": "4.1 Iris Results",
      "text": "The neural net described in [12] was used for the Iris data set, which has the useful property of converging towards the mean output value when the input values go towards infinity. Therefore the range [Cminj( #\u00bb C, {i}), Cmaxj( #\u00bb\nC, {i})] should remain within reasonable bounds. A specific Iristest instance is studied that is quite a typical Virginica with values #\u00bb\nC = (7, 3.2, 6, 1.8) for the input features \u2018Sepal Length\u2019, \u2018Sepal Width\u2019, \u2018Petal Length\u2019, \u2018Petal Width\u2019. The trained neural network gives us #\u00bby = (0.012, 0.158, 0.830) for the three outputs classes \u2018Setosa\u2019, \u2018Versicolor\u2019, and \u2018Virginica\u2019, so it is clearly a Virginica. Table 1 shows the corresponding CIU values for Iristest.\nSome questions that could be asked are \u2018Why is it a Virginica?\u2019 but also \u2018Why is it not a Versicolor or a Setosa?\u2019. Figure 3a shows bar plot explanations for the three Iris classes. It is clear that Iristest is not a Setosa because none of the features is typical for a Setosa and modifying any of the values will not change the situation. On the other hand, all features are typical for a Virginica. Petal length is clearly the most important feature for the classification of Iristest. Figure 4 shows how the output value (estimated class probability) changes for Versicolor and Virginica as a function of\nthe four input features. These graphs confirm that Petal Length is the feature that discriminates Versicolor and Virginica the most from each other.\nFor showing the use of Intermediate Concepts, a small vocabulary was developed. The vocabulary specifies that \u2018Sepal size and shape\u2019 is the combination of features \u2018Sepal Length\u2019 and \u2018Sepal Width\u2019. \u2018Petal size and shape\u2019 is the combination of features \u2018Petal Length\u2019 and \u2018Petal Width\u2019. When studying the results using the Intermediate Concepts \u2018Sepal size and shape\u2019 and \u2018Petal size and shape\u2019, we get the bar plot explanation in Figure 3b.\nFinally, Figure 3c answers questions such as \u2018why is Petal size and shape not so typical for Versicolor?\u2019 and \u2018why is Petal size and shape typical for Virginica?\u2019. These bar plots express what can be observed also in the 3D graphs of Figure 4, where we can see that the combination of \u2018Petal Length\u2019 and \u2018Petal Width\u2019 could be even more typical for Virginica than what it is for Iristest.\n4.5 5.5 6.5 7.5\n0. 0\n0. 4\n0. 8\nVersicolor\nSepal Length\nO ut\npu t v\nal ue\n2.0 2.5 3.0 3.5 4.0\n0. 0\n0. 4\n0. 8\nVersicolor\nSepal width\nO ut\npu t v\nal ue\n1 2 3 4 5 6\n0. 0\n0. 4\n0. 8\nVersicolor\nPetal Length\nO ut\npu t v\nal ue\n0.5 1.0 1.5 2.0 2.5\n0. 0\n0. 4\n0. 8\nVersicolor\nPetal width\nO ut\npu t v\nal ue\n4.5 5.5 6.5 7.5\n0. 0\n0. 4\n0. 8\nVirginica\nSepal Length\nO ut\npu t v\nal ue\n2.0 2.5 3.0 3.5 4.0\n0. 0\n0. 4\n0. 8\nVirginica\nSepal width\nO ut\npu t v\nal ue\n1 2 3 4 5 6\n0. 0\n0. 4\n0. 8\nVirginica\nPetal Length\nO ut\npu t v\nal ue\n0.5 1.0 1.5 2.0 2.5\n0. 0\n0. 4\n0. 8\nVirginica\nPetal width\nO ut\npu t v\nal ue"
    },
    {
      "heading": "4.2 Boston Housing Results",
      "text": "A gradient boosting model was used for the Boston Housing data set. It learned the mapping from the 13 input variables to the Median value (medv) of owner-occupied homes in $1000\u2019s. The resulting CIU bar plot is shown in Figure 5 for instances #406 (medv=5, lowest), and #370 (medv=50, highest). CIU clearly identifies what are the most important features for the different instances, as well as identifying which input values are favorable for a high \u2018medv\u2019 value. These results are consistently similar over different runs.\nWhen using a neural net like in Section 4.1 the learned model also becomes slightly different than with the gradient boosting model. These differences in the underlying black-box model are well reflected also in the CIU explanations, with differences notably in CI values. CU values and the overall balance between red and green in the bar plots remains similar. This illustrates how CIU can also be used as a tool for assessing the credibility and trust in different black-box models."
    },
    {
      "heading": "5 Future Work",
      "text": "Two simple benchmark data sets were used in this paper for reasons of illustration and to enable human readers to assess the validity of the results. Experience from more data sets and use cases might lead to further extensions of CIU. For instance, use cases involving one-hot coding will require using Intermediate Concepts for aggregating the concerned one-hot inputs into one single explainable feature.\nCIU provides many topics for future research. For instance, is it always better to use CI values as such or normalise them to one? What ways of visualising CIU are the best understood by humans? Research is ongoing in these directions\nbut how to best interact with human explainees is a vast domain. Research has also been initiated for using CIU together with Reinforcement and Unsupervised learning."
    },
    {
      "heading": "6 Conclusions",
      "text": "CIU is a model-agnostic method that allows producing explanations from any black-box model (no matter how \u2018black\u2019 or not it is), without producing an intermediate interpretable model. Therefore CIU does not have the same challenges of black-box model fidelity as most other XAI methods do. Compared to other output explanation methods, CIU allows for more flexibility in how explanations can be produced and presented to explainees due to the possibility to apply CIU to sets of features and Intermediate Concepts. Intermediate Concepts enable the use of different vocabularies depending on the context and on the explainee. The Contextual Utility concept also allows to produce explanations in a more human-like way than other XAI methods.\nBy not using an intermediate interpretable model, CIU does not fit into any of the existing categories presented by major XAI survey articles. CIU has only one adjustable parameter, i.e. the number of samples in S( #\u00bb\nC, {i}), which might be possible to eliminate or automate in the future. Therefore, CIU establishes a new category of XAI methods that will hopefully help to solve at least some of the many challenges that AI and XAI are currently facing."
    }
  ],
  "title": "EXPLAINABLE AI WITHOUT INTERPRETABLE MODEL",
  "year": 2020
}
