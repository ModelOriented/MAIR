{
  "abstractText": "This paper proposes a theoretical analysis of one approach to the eXplainable AI (XAI) problem, using post-hoc explanation-by-example, that relies on the twinning of artificial neural networks (ANNs) with case-based reasoning (CBR) systems; so-called ANN-CBR twins. It surveys these systems to advance a new theoretical interpretation of previous work and define a road map for CBR\u2019s further role in XAI. A systematic survey of 1102 papers was conducted to identify a fragmented literature on this topic and trace its influence to more recent work involving deep neural networks (DNNs). The twin-system approach is advanced as one possible coherent, generic solution to the XAI problem. The paper concludes by road-mapping future directions for this XAI solution, considering (i) further tests of feature-weighting techniques, (ii) how explanatory cases might be deployed (e.g., in counterfactuals, a fortori cases), and (iii) the unwelcome, much-ignored issue of user evaluation.",
  "authors": [
    {
      "affiliations": [],
      "name": "Mark T. Keane"
    },
    {
      "affiliations": [],
      "name": "Eoin M. Kenny"
    }
  ],
  "id": "SP:07bc27b5e2f829ae102f31dbd783899993717039",
  "references": [
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)",
      "venue": "IEEE Access, 6, 52138-52160",
      "year": 2018
    },
    {
      "authors": [
        "B. Goodman",
        "S. Flaxman"
      ],
      "title": "European Union regulations on algorithmic decision-making and a \u201cright to explanation",
      "venue": "AI Magazine, 38(3), 50-57",
      "year": 2017
    },
    {
      "authors": [
        "S. Wachter",
        "B. Mittelstadt",
        "L. Floridi"
      ],
      "title": "Why a right to explanation of automated decision-making does not exist in the general data protection regulation",
      "venue": "International Data Privacy Law, 7(2), 76-99",
      "year": 2017
    },
    {
      "authors": [
        "E.M. Kenny",
        "Keane",
        "Mark T."
      ],
      "title": "Twin-systems for explaining ANNs using CBR: Experiments using feature-weighting schemes to retrieve explanatory cases",
      "venue": "Under review",
      "year": 2019
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "F. Turini",
        "F. Giannotti",
        "D. Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM Computing Surveys, 51(5), p.93",
      "year": 2018
    },
    {
      "authors": [
        "F. Doshi-Velez",
        "B. Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608",
      "year": 2017
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "Queue, 16(3), 30",
      "year": 2018
    },
    {
      "authors": [
        "T. Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "Artificial Intelligence, 267, 1-38",
      "year": 2019
    },
    {
      "authors": [
        "A. Abdul",
        "J. Vermeulen",
        "D. Wang",
        "B.Y. Lim",
        "M. Kankanhalli"
      ],
      "title": "Trends and trajectories for explainable, accountable and intelligible systems: An HCI research agenda",
      "venue": "Proceedings 2018 CHI Conference on Human Factors in Computing Systems (p. 582). ACM",
      "year": 2018
    },
    {
      "authors": [
        "O. Biran",
        "C. Cotton"
      ],
      "title": "Explanation and justification in machine learning: A survey",
      "venue": "IJCAI-17 workshop on explainable AI (XAI) (Vol. 8) p.1",
      "year": 2017
    },
    {
      "authors": [
        "F. S\u00f8rmo",
        "J. Cassens",
        "A. Aamodt"
      ],
      "title": "Explanation in case-based reasoning\u2013perspectives and goals",
      "venue": "Artificial Intelligence Review, 24(2), 109-143",
      "year": 2005
    },
    {
      "authors": [
        "A.J. Johs",
        "M. Lutts",
        "R.O. Weber"
      ],
      "title": "Measuring Explanation Quality in XCBR",
      "venue": "ICCBR18, p.75",
      "year": 2018
    },
    {
      "authors": [
        "N. Tintarev",
        "J. Masthoff"
      ],
      "title": "A survey of explanations in recommender systems",
      "venue": "2007 IEEE 23rd international conference on data engineering workshop, 801-810. IEEE",
      "year": 2007
    },
    {
      "authors": [
        "G.H. Harman"
      ],
      "title": "The inference to the best explanation",
      "venue": "The philosophical review, 74(1), 8895",
      "year": 1965
    },
    {
      "authors": [
        "W.C. Salmon"
      ],
      "title": "Scientific explanation and the causal structure of the world",
      "venue": "Princeton University Press",
      "year": 1984
    },
    {
      "authors": [
        "B.C. Van Fraassen"
      ],
      "title": "The scientific image",
      "venue": "Oxford University Press",
      "year": 1980
    },
    {
      "authors": [
        "F.C. Keil"
      ],
      "title": "Explanation and understanding",
      "venue": "Annual Rev. of Psychology, 57, 227-254",
      "year": 2006
    },
    {
      "authors": [
        "D.B. Leake"
      ],
      "title": "CBR in context: The present and future",
      "venue": "Case-based reasoning: Experiences, lessons, and future directions, pp.3-30",
      "year": 1996
    },
    {
      "authors": [
        "D. Leake",
        "D. McSherry"
      ],
      "title": "Introduction to the special issue on explanation in case-based reasoning",
      "venue": "Artificial Intelligence Review, 24(2), 103-108",
      "year": 2005
    },
    {
      "authors": [
        "R. Caruana",
        "H. Kangarloo",
        "J.D. Dionisio",
        "U. Sinha",
        "D. Johnson"
      ],
      "title": "Case-based explanation of non-case-based learning methods",
      "venue": "Proceedings of the AMIA Symposium, p. 212. American Medical Informatics Association",
      "year": 1999
    },
    {
      "authors": [
        "B. Kim",
        "C. Rudin",
        "J.A. Shah"
      ],
      "title": "The Bayesian case model: A generative approach for case-based reasoning and prototype classification",
      "venue": "Adv. in NIPs, pp. 1952-1960",
      "year": 2014
    },
    {
      "authors": [
        "D. Pedreschi",
        "F. Giannotti",
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "F. Turini"
      ],
      "title": "Meaningful explanations of Black Box AI decision systems",
      "venue": "Proceedings of AAAI-19",
      "year": 2019
    },
    {
      "authors": [
        "S. Haykin"
      ],
      "title": "Neural networks (Vol",
      "venue": "2). New York: Prentice Hall",
      "year": 1994
    },
    {
      "authors": [
        "I. Goodfellow",
        "Y. Bengio",
        "A. Courville"
      ],
      "title": "Deep learning",
      "venue": "MIT Press",
      "year": 2016
    },
    {
      "authors": [
        "J. Kolodner"
      ],
      "title": "Case based reasoning",
      "venue": "Morgan Kaufmann",
      "year": 2014
    },
    {
      "authors": [
        "A. Aamodt",
        "E. Plaza"
      ],
      "title": "Case-based reasoning: Foundational issues, methodological variations, and system approaches",
      "venue": "AI communications, 7(1), 39-59",
      "year": 1994
    },
    {
      "authors": [
        "R.L. De Mantaras",
        "D. McSherry",
        "D. Bridge",
        "D. Leake",
        "B. Smyth",
        "S. Craw",
        "B. Faltings",
        "M.L. Maher",
        "M.T. Cox",
        "K. Forbus",
        "M.T. Keane"
      ],
      "title": "Retrieval, reuse, revise and retention in CBR",
      "venue": "Knowledge Engineering Review, 20(3), 215-240",
      "year": 2006
    },
    {
      "authors": [
        "S. Sahin",
        "M.R. Tolun",
        "R. Hassanpour"
      ],
      "title": "Hybrid expert systems: A survey of current approaches and applications",
      "venue": "Expert systems with applications, 39(4), 4609-4617",
      "year": 2012
    },
    {
      "authors": [
        "M. Negnevitsky"
      ],
      "title": "Artificial intelligence",
      "venue": "Pearson education",
      "year": 2005
    },
    {
      "authors": [
        "L.R. Medsker"
      ],
      "title": "Hybrid neural network and expert systems",
      "venue": "Springer",
      "year": 2012
    },
    {
      "authors": [
        "C. Szegedy",
        "W. Zaremba",
        "I. Sutskever",
        "J. Bruna",
        "D. Erhan",
        "I. Goodfellow",
        "R. Fergus"
      ],
      "title": "Intriguing properties of neural networks",
      "venue": "arXiv preprint arXiv:1312.6199",
      "year": 2013
    },
    {
      "authors": [
        "A. Krizhevsky",
        "I. Sutskever",
        "G.E. Hinton"
      ],
      "title": "Imagenet classification with deep convolutional neural networks",
      "venue": "Advances in NIPs, pp. 1097-1105",
      "year": 2012
    },
    {
      "authors": [
        "Witten",
        "Ian H.",
        "Eibe Frank",
        "Mark A. Hall",
        "Christopher J. Pal."
      ],
      "title": "Data Mining: Practical machine learning tools and techniques",
      "venue": "Morgan Kaufmann",
      "year": 2016
    },
    {
      "authors": [
        "Y. LeCun",
        "Y. Bengio",
        "G. Hinton"
      ],
      "title": "Deep learning",
      "venue": "Nature, 521(7553), p.436",
      "year": 2015
    },
    {
      "authors": [
        "J.D. Olden",
        "D.A. Jackson"
      ],
      "title": "Illuminating the \u201cblack box",
      "venue": "Ecological modelling, 154(12), pp.135-150",
      "year": 2002
    },
    {
      "authors": [
        "R.R. Selvaraju",
        "M. Cogswell",
        "A. Das",
        "R. Vedantam",
        "D. Parikh",
        "D. Batra"
      ],
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision. pp. 618-626",
      "year": 2017
    },
    {
      "authors": [
        "L.H. Gilpin",
        "D. Bau",
        "B.Z. Yuan",
        "A. Bajwa",
        "M. Specter",
        "L. Kagal"
      ],
      "title": "Explaining explanations: an approach to evaluating interpretability of machine learning",
      "venue": "arXiv preprint arXiv:1806.00069",
      "year": 2018
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "European conference on computer vision, pp. 818-833. Springer",
      "year": 2014
    },
    {
      "authors": [
        "He",
        "Kaiming",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "title": "Deep residual learning for image recognition",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778",
      "year": 2016
    },
    {
      "authors": [
        "D. Erhan",
        "Y. Bengio",
        "A. Courville",
        "P. Vincent"
      ],
      "title": "Visualizing higher-layer features of a deep network",
      "venue": "University of Montreal, 1341(3), p.1",
      "year": 2009
    },
    {
      "authors": [
        "M.T. Keane"
      ],
      "title": "Analogical asides on case-based reasoning",
      "venue": "European Workshop on CaseBased Reasoning, pp. 21-32, Springer, Berlin, Heidelberg",
      "year": 1993
    },
    {
      "authors": [
        "I. Nunes",
        "D. Jannach"
      ],
      "title": "A systematic review and taxonomy of explanations in decision support and recommender systems",
      "venue": "UMUAI, 27(3-5), 393-444",
      "year": 2017
    },
    {
      "authors": [
        "L. Becker",
        "K. Jazayeri"
      ],
      "title": "A connectionist approach to case-based reasoning",
      "venue": "Proceedings of the Case-Based Reasoning Workshop, pp. 213-217. Morgan Kaufmann",
      "year": 1989
    },
    {
      "authors": [
        "P. Thrift"
      ],
      "title": "A neural network model for case-based reasoning",
      "venue": "Proceedings of the CaseBased Reasoning Workshop, pp. 334-337. Morgan Kaufmann",
      "year": 1989
    },
    {
      "authors": [
        "M. Hilario",
        "C. Pellegrini",
        "F. Alexandre"
      ],
      "title": "Modular integration of connectionist and symbolic processing in knowledge-based systems",
      "venue": "C.de R. en Informatique de Nancy",
      "year": 1994
    },
    {
      "authors": [
        "C.K. Shin",
        "S.C. Park"
      ],
      "title": "Memory and neural network based expert system",
      "venue": "Expert Systems with Applications, 16(2), 145-155",
      "year": 1999
    },
    {
      "authors": [
        "C.K. Shin",
        "U.T. Yun",
        "H.K. Kim",
        "S.C. Park"
      ],
      "title": "A hybrid approach of neural network and memory-based learning to data mining",
      "venue": "IEEE Transactions on Neural Networks, 11(3), 637-646",
      "year": 2000
    },
    {
      "authors": [
        "C.K. Shin",
        "S.C. Park"
      ],
      "title": "A machine learning approach to yield management in semiconductor manufacturing",
      "venue": "International Journal of Production Research,",
      "year": 2000
    },
    {
      "authors": [
        "J.H. Park",
        "C.K. Shin",
        "K.H. Im",
        "S.C. Park"
      ],
      "title": "A local weighting method to the integration of neural network and case based reasoning",
      "venue": "Neural Networks for Signal Processing XI: Proceedings of the 2001 IEEE SPS Workshop, pp. 33-42. IEEE",
      "year": 2001
    },
    {
      "authors": [
        "C.K. Shin",
        "S.C. Park"
      ],
      "title": "Towards integration of memory based learning and neural networks",
      "venue": "In Soft computing in case based reasoning (pp. 95-114). Springer, London",
      "year": 2001
    },
    {
      "authors": [
        "J.H. Park",
        "K.H. Im",
        "C.K. Shin",
        "S.C. Park"
      ],
      "title": "MBNR: case-based reasoning with local feature weighting by neural network",
      "venue": "Applied Intelligence, 21(3), pp.265-276",
      "year": 2004
    },
    {
      "authors": [
        "S.C. Park",
        "J.W. Kim",
        "K.H. Im"
      ],
      "title": "Feature-weighted CBR with NN for symbolic features",
      "venue": "International Conference on Intelligent Computing, 1012-1020. Springer",
      "year": 2006
    },
    {
      "authors": [
        "H. Im",
        "S.C. Park"
      ],
      "title": "Case-based reasoning and neural network based expert system for personalization",
      "venue": "Expert Systems with Applications, 32(1), pp.77-85",
      "year": 2007
    },
    {
      "authors": [
        "S. Ha"
      ],
      "title": "A personalized counseling system using case-based reasoning with neural symbolic feature weighting (CANSY)",
      "venue": "Applied intelligence, 29(3), pp. 279-288",
      "year": 2008
    },
    {
      "authors": [
        "E.B. Reategui",
        "J.A. Campbell",
        "B.F. Leao"
      ],
      "title": "Combining a neural network with case-based reasoning in a diagnostic system",
      "venue": "Artificial Intelligence in Medicine, 9(1), 5-27",
      "year": 1997
    },
    {
      "authors": [
        "B.S. Yang",
        "T. Han",
        "Y.S. Kim"
      ],
      "title": "Integration of ART-Kohonen NN and CBR for intelligent fault diagnosis",
      "venue": "Expert Systems with Applications, 26(3), 387-395",
      "year": 2004
    },
    {
      "authors": [
        "Y. Rodriguez",
        "M.M. Garcia",
        "B. De Baets",
        "C. Morell",
        "R. Bello"
      ],
      "title": "A connectionist fuzzy case-based reasoning model",
      "venue": "Mexican International Conference on Artificial Intelligence, pp. 176-185. Springer, Berlin, Heidelberg",
      "year": 2006
    },
    {
      "authors": [
        "K. Amin",
        "S. Kapetanakis",
        "K.D. Althoff",
        "A. Dengel",
        "M. Petridis"
      ],
      "title": "Answering with cases: A CBR Approach to Deep Learning",
      "venue": "International Conference on Case-Based Reasoning, pp. 15-27. Springer, Cham.",
      "year": 2018
    },
    {
      "authors": [
        "J.M. Corchado",
        "N. Rees",
        "B. Lees",
        "J. Aiken"
      ],
      "title": "Data mining using example-based methods in oceanographic forecast models",
      "venue": "IEE Colloquium on Knowledge Discovery and Data Mining (Digest No. 1998/310) (pp. 7-1). IET",
      "year": 1998
    },
    {
      "authors": [
        "J.M. Corchado",
        "B. Lees"
      ],
      "title": "A hybrid case-based model for forecasting",
      "venue": "Applied Artificial Intelligence, 15(2), 105-127",
      "year": 2001
    },
    {
      "authors": [
        "F. Fdez-Riverola",
        "J.M. Corchado",
        "J.M. Torres"
      ],
      "title": "An automated hybrid CBR system for forecasting",
      "venue": "European Conf. on Case-Based Reasoning, pp. 519-533. Springer",
      "year": 2002
    },
    {
      "authors": [
        "R. Jothikimar",
        "N. Shivakumar",
        "P.S. Ramesh",
        "Suganthan",
        "A. Suresh"
      ],
      "title": "Heart Disease Prediction System Using ANN, RBF and CBR",
      "venue": "International Journal of Pure and Applied Mathematics, 117(21), 199-217",
      "year": 2017
    },
    {
      "authors": [
        "R.R. Kouser",
        "T. Manikandan",
        "V.V. Kumar"
      ],
      "title": "Heart Disease Prediction System Using Artificial Neural Network, Radial Basis Function and Case Based Reasoning",
      "venue": "Journal of Computational and Theoretical Nanoscience, 15(9-10), 2810-2817",
      "year": 2018
    },
    {
      "authors": [
        "R. Weber",
        "J.M. Proctor",
        "I. Waldstein",
        "A. Kriete"
      ],
      "title": "CBR for modeling complex systems",
      "venue": "International Conference on Case-Based Reasoning, pp. 625-639. Springer",
      "year": 2005
    },
    {
      "authors": [
        "Y. Peng",
        "L. Zhuang"
      ],
      "title": "A case-based reasoning with feature weights derived by BP network",
      "venue": "Intelligent Information Technology Application, pp. 26-29. IEEE",
      "year": 2007
    },
    {
      "authors": [
        "S.K. Biswas",
        "N. Sinha",
        "B. Purakayastha",
        "L. Marbaniang"
      ],
      "title": "Hybrid expert system using case based reasoning and neural network for classification",
      "venue": "Biologically Inspired Cognitive Architectures, 9, 57-70",
      "year": 2014
    },
    {
      "authors": [
        "S.K. Biswas",
        "B. Baruah",
        "N. Sinha",
        "B. Purkayastha"
      ],
      "title": "A hybrid CBR classification model by integrating ANN into CBR",
      "venue": "International Journal of Services Technology and Management, 21(4-6), 272-293",
      "year": 2015
    },
    {
      "authors": [
        "S.K. Biswas",
        "M. Chakraborty",
        "H.R. Singh",
        "D. Devi",
        "B. Purkayastha",
        "A.K. Das"
      ],
      "title": "Hybrid case-based reasoning system by cost-sensitive neural network for classification",
      "venue": "Soft Computing, 21(24), 7579-7596",
      "year": 2017
    },
    {
      "authors": [
        "G.F. Cooper",
        "C.F. Aliferis",
        "R. Ambrosino",
        "J. Aronis",
        "B.G. Buchanan",
        "R. Caruana",
        "M.J. Fine",
        "C. Glymour",
        "G. Gordon",
        "B.H. Hanusa",
        "J.E. Janosky"
      ],
      "title": "An evaluation of machinelearning methods for predicting pneumonia mortality",
      "venue": "Artificial intelligence in medicine, 9(2), 107-138",
      "year": 1997
    },
    {
      "authors": [
        "R. Caruana",
        "Y. Lou",
        "J. Gehrke",
        "P. Koch",
        "M. Sturm",
        "N. Elhadad"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1721-1730. ACM",
      "year": 2015
    },
    {
      "authors": [
        "N. Papernot",
        "McDaniel",
        "P. Deep k-nearest neighbours"
      ],
      "title": "Towards confident, interpretable and robust deep learning",
      "venue": "arXiv preprint arXiv:1803.04765",
      "year": 2018
    },
    {
      "authors": [
        "B. Mittelstadt",
        "C. Russell",
        "S. Wachter"
      ],
      "title": "Explaining Explanations in AI",
      "venue": "Proceedings of Conference on Fairness, Accountability, and Transparency (FAT*-19)",
      "year": 2019
    },
    {
      "authors": [
        "P. Cunningham",
        "D. Doyle",
        "J. Loughrey"
      ],
      "title": "An evaluation of the usefulness of case-based explanation",
      "venue": "International Conf. on Case-Based Reasoning, 122-130. Springer",
      "year": 2003
    },
    {
      "authors": [
        "C. Nugent",
        "P. Cunningham"
      ],
      "title": "A case-based explanation system for black-box systems",
      "venue": "Artificial Intelligence Review, 24(2), 163-178",
      "year": 2005
    },
    {
      "authors": [
        "C. Nugent",
        "P. Cunningham",
        "D. Doyle"
      ],
      "title": "The best way to instill confidence is by being right",
      "venue": "International Conference on Case-Based Reasoning, pp. 368-381. Springer",
      "year": 2005
    },
    {
      "authors": [
        "C. Nugent",
        "D. Doyle",
        "P. Cunningham"
      ],
      "title": "Gaining insight through case-based explanation",
      "venue": "Journal of Intelligent Information Systems, 32(3), 267-295",
      "year": 2009
    },
    {
      "authors": [
        "D. Doyle",
        "P. Cunningham",
        "D. Bridge",
        "Y. Rahman"
      ],
      "title": "Explanation oriented retrieval",
      "venue": "In European Conference on Case-Based Reasoning, pp. 157-168. Springer",
      "year": 2004
    },
    {
      "authors": [
        "R. Andrews",
        "J. Diederich",
        "A.B. Tickle"
      ],
      "title": "Survey and critique of techniques for extracting rules from trained artificial neural networks",
      "venue": "Knowledge-based systems, 8, 373-389",
      "year": 1995
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should I trust you?: Explaining the predictions of any classifier",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144. ACM",
      "year": 2016
    },
    {
      "authors": [
        "T. Olsson",
        "D. Gillblad",
        "P. Funk",
        "N. Xiong"
      ],
      "title": "Case-based reasoning for explaining probabilistic machine learning",
      "venue": "International Journal of Computer Science and Information Technology, 6(2), 87-101",
      "year": 2014
    },
    {
      "authors": [
        "Y. Zharov",
        "D. Korzhenkov",
        "P. Shvechikov",
        "A. Tuzhilin"
      ],
      "title": "YASENN: Explaining Neural Networks via Partitioning Activation Sequences",
      "venue": "arXiv preprint arXiv:1811.02783",
      "year": 2018
    },
    {
      "authors": [
        "C. Chen",
        "O. Li",
        "A. Barnett",
        "J. Su",
        "C. Rudin"
      ],
      "title": "This looks like that: deep learning for interpretable image recognition",
      "venue": "arXiv preprint arXiv:1806.10574",
      "year": 2018
    },
    {
      "authors": [
        "O. Li",
        "H. Liu",
        "C. Chen",
        "C. Rudin"
      ],
      "title": "Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence. AAAI",
      "year": 2018
    },
    {
      "authors": [
        "J.R. Zilke",
        "E.L. Menc\u00eda",
        "F. Janssen"
      ],
      "title": "DeepRED\u2013Rule extraction from deep neural networks",
      "venue": "Internat. Conf. on Discovery Science, pp. 457-473. Springer",
      "year": 2016
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On pixelwise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PloS one, 10(7), p.e0130140.",
      "year": 2015
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Taly",
        "Q. Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume, 70, 33193328. JMLR. Org",
      "year": 2017
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Kundaje"
      ],
      "title": "Learning important features through propagating activation differences",
      "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume 70, 3145-3153. JMLR. Org",
      "year": 2017
    },
    {
      "authors": [
        "C.L. Zhang",
        "J.H. Luo",
        "X.S. Wei",
        "J. Wu"
      ],
      "title": "In defense of fully connected layers in visual representation transfer",
      "venue": "Pacific Rim Conf. on Multimedia (pp. 807-817). Springer",
      "year": 2017
    },
    {
      "authors": [
        "P. Myllym\u00e4ki",
        "H. Tirri"
      ],
      "title": "November",
      "venue": "Massively parallel case-based reasoning with probabilistic similarity metrics. In: European Workshop on Case-Based Reasoning (pp. 144154). Springer, Berlin, Heidelberg",
      "year": 1993
    },
    {
      "authors": [
        "A. Kofod-Petersen",
        "H. Langseth",
        "A. Aamodt"
      ],
      "title": "Explanations in Bayesian networks using provenance through case-based reasoning",
      "venue": "Workshop Proceedings, p. 79",
      "year": 2010
    },
    {
      "authors": [
        "E. Wallace",
        "S. Feng",
        "J. Boyd-Graber"
      ],
      "title": "Interpreting Neural Networks with Nearest Neighbours",
      "venue": "arXiv preprint arXiv:1809.02847",
      "year": 2018
    },
    {
      "authors": [
        "D. Card",
        "M. Zhang",
        "Smith",
        "N.A. January. Deep Weighted Averaging Classifiers. In"
      ],
      "title": "Proceedings of Conf",
      "venue": "on Fairness, Accountability & Trust (pp. 369-378). ACM",
      "year": 2019
    }
  ],
  "sections": [
    {
      "text": "Keywords: CBR, Explanation, Artificial Neural Networks, XAI, Deep Learning"
    },
    {
      "heading": "1 Introduction",
      "text": "As AI systems impact our everyday lives, jobs, and leisure time, the issue of explaining how these systems actually work has become more acute, the so-called eXplainable AI (XAI) problem. In the last few years, almost every major AI/ML conference has targeted this problem either as a major theme or as a focus for thematic workshops (e.g., NIPS-16, IJCAI-17, IJCAI/ECAI-18, IJCNN-17, ICCBR-18, ICCBR-19) along with the emergence of meetings dedicated solely to it (FAT-ML, FAT*19; see [1]). The urgency of this effort is not just academic, as it is played out against a backdrop of increasing regulatory interest by governments in AI (e.g., GDPR in the EU [2, 3]). This paper surveys one particular solution to the XAI problem, where an opaque, black-box AI system is explained by a more interpretable, white-box AI system; the so-called twin-systems approach [4]. This survey is used to advance a new theoretical interpreta-\ntion of previous work and define a road map for CBR\u2019s further role in XAI. Specifically, we review the pairing of artificial neural networks1 (ANNs) with case-based reasoning (CBR) systems where the explanatory cases of the latter are used to interpret the opaque outputs of the former; so-called ANN-CBR twins. For example, imagine an ANN that accurately predicts house prices given different feature-descriptions of houses (e.g., size, location, no-of-rooms), but like all ANNs is opaque and, thus, cannot explain its predictions. If we twin this ANN with a CBR system, using the same dataset, extract the ANN\u2019s feature-weights and apply them in the CBR-system to retrieve neighboring cases to the query-case, then we can use the latter to explain the predictions of the former (see Fig. 1). We have discovered a fragmented literature on this topic that deserves to be brought together, if only to avoid unnecessary re-invention. In the next sub-section, we lay out our orientation to \u201cexplanation\u201d and the motivation for the present systematic survey."
    },
    {
      "heading": "1.1 \u201cExplanation\u201d Needs Explanation",
      "text": "As an area, XAI has many issues; foremost amongst these, perhaps, is some clarity on what \u201cexplanation\u201d actually means. Several recent XAI reviews have pointedly noted the lack of clear definitions for the notions of explanation, interpretability and transparency [5-10], echoing long-standing discussions in CBR [11, 12], recommender systems [13], Philosophy [14-16] and Psychology [17]. While the exact meaning of these terms remains a matter of debate, these reviews make useful taxonomic distinctions. For example, S\u00f8rmo et al.\u2019s [11] review reports the distinction between explaining how the\n1 Here, ANN is used to label all neural network techniques; older neural networks will be la-\nbelled as multi-layered perceptrons (MLP) and newer deep learning techniques will be called \u201cdeep neural networks\u201d (DNNs; following [5]).\nsystem reached some answer (which they call transparency) and explaining why the answer is good (justification). More recently, this distinction is echoed by dividing interpretability into (i) transparency (or simulatability) which tries to reflect how the AI system produced its outputs, and (ii) post-hoc interpretability which is more about why the AI reached its outputs, providing some after-the-fact rationale/evidence for system outputs [7]. CBR systems are notable in this respect, through their use of examples/cases/precedents to explain system outputs [7, 10, 11, 12, 18, 19]. So, here, when a CBR system\u2019s cases are used to explain an ANN\u2019s opaque predictions, it is classed as \u201cpost-hoc explanation-by-example\u201d. As such, twin-systems are just one possible solution to interpretability in the XAI problem, but one, we argue, that deserves more attention."
    },
    {
      "heading": "1.2 Motivation for a Systematic Review",
      "text": "There are several reasons why a systematic review of ANN-CBR twins for XAI is both timely and necessary. First, if citation patterns are any indication, there is clear evidence that the literature on twin-systems is fragmented. For instance, many recent reviews of XAI make little or no reference to key twin-systems papers in the CBR literature [1, 6, 7], while referencing papers outside CBR canon [20, 21]. Second, if we do not know the literature on this CBR-solution to XAI then, arguably, we are doomed to re-invent its findings and mistakes. Third, the XAI area requires a systematic, general framework to bring the literature together and focus future efforts. As Pedreschi et al. [22] point out \u201cthe state of the art to date still exhibits ad-hoc, scattered results, mostly hardwired to specific models\u2026[and]\u2026 a widely applicable, systematic approach has not emerged yet\u201d. The twin-system idea represents one possible general solution to a broad class of systems. Fourth, a systematic survey should allow us to know where we currently stand, and then to strategically road map future directions for this XAI solution. Hence, in the remainder of this paper we review the literature on ANN-CBR twins as solutions to the XAI problem. In Section 2, twin-systems are defined more precisely. In Section 3, our systematic survey methodology is described using the twin-system definition along with descriptive statistics. In Section 4, using the literature found, the history of ANNCBR twins is outlined. Finally, in Section 5, future directions are road mapped."
    },
    {
      "heading": "2. Defining ANN-CBR Twins",
      "text": "ANN-CBR twin-systems can be found at the intersection between research on ANNs [23, 24], CBR [25-27], and hybrid systems [28-30] when explanation is a major taskrequirement of the system.\nArtificial Neural Networks (ANNs). Biologically inspired, these AI systems typically consist of layers of nodes with non-linear activation functions and a bias term, which are connected by weights [23, 24]. Here, we distinguish between traditional neural networks of the multilayer perceptron (MLP) or backpropagation (BP) variety, and deep neural networks (DNNs) which include a wide variety of techniques; such as, recurrent neural networks (RNNs), convolutional neural networks (CNNs) and generative adversarial networks (GANs) [31-34]. MLPs typically consist of three layers, an input feature layer, a hidden layer (aka, its latent features), and an output layer. At their simplest, these ANNs learn an input-output mapping over a training set, so that when a test-case is presented, its features are used to accurately predict/classify at the MLP output layer.\nSignificantly, the model\u2019s learning of an input-output mapping depends on modifying the weights connecting the nodes in these layers and the bias terms within the nodes. DNNs are a menagerie of many different techniques; notably, they advance beyond MLPs by being able to learn features in unstructured data (such as images or video). However, the non-linear nature of all of these ANNs make them difficult to interpret and poor at explaining their outputs [35-37]. Attempts to make ANNs more interpretable use many different \u201cexplanation methods\u201d that are often specific to a given architecture (see reviews [5, 9, 37]). Arguably, DNNs are even less interpretable than MLPs, because of their complexity and difficulties in surfacing their extracted features. Currently, major efforts at \u201cexplaining\u201d DNNs hinge on visualizing what specific neurons have learned or indicating \u201cwhere the DNN is looking\u201d in an image using saliency maps [36, 38-40]. However, these methods are often quite specific to particular DNNtechniques and do not reflect the model\u2019s \u201creasoning process\u201d [83]. So, a key question for the field is whether any approach can explain all ANNs \u2013 both MLPs and DNNs \u2013 in a general, unified way [5]; arguably, ANN-CBR twins are one possible solution [4].\nCase-Based Reasoning (CBR). These systems preform a type of reasoning from examples or cases using a retrieval, reuse, revise, and re-train cycle [25-27, 41]. At its simplest, in CBR, when a query-case is presented the most similar cases to it are retrieved before being adapted (or used directly) to make a prediction/classification. Typically, the retrieval step finds cases by matching the features of the query-case and cases in the case base using k-nearest neighbor (k-NN). Retrieval accuracy (and, hence, the success of the system) can depend heavily on the weights given to these features, weights that reflect their importance in the domain. Notably, CBR is claimed to have a \u201cnatural\u201d transparency as its reasoning-from-precedent or -example parallels what human experts sometimes do [18, 25]; though these claims have not always been extensively user-tested [11]. Accordingly, CBR as an area has a substantial literature on explanation [11, 12, 19], as does its sister area of recommender systems [13, 42]."
    },
    {
      "heading": "2.1 ANN-CBR Twins",
      "text": "ANN-CBR twins are a special-case of a hybrid system, that combines ANN and CBR modules, when both accuracy and interpretability are primary requirements of the overall system. Though ANNs and CBR were coupled as early hybrid systems [43-45], it is not really until the late-1990s that \u201ctrue\u201d twins emerge [20, 46-54]. Fig. 1 shows one simple example of an ANN-CBR twinning. The task, here, is the prediction of house prices, where one has some dataset of training examples (i.e., a case base of prior cases) describing houses and their prices from previous years. The ANN accurately learns to predict the price of unseen houses (i.e., query-cases) having computed the input-output mapping from house-features to their price using the training set. To explain the ANN\u2019s prediction, its feature-weights are (in some way) extracted and used in the CBR\u2019s k-NN retrieval-step, to identify a nearest-neighbor case (or cases) to \u201cexplain\u201d the ANN\u2019s prediction. In essence, the explanation step is asserting: price-\ud835\udc65 is predicted, because these other houses, that have very similar features, have these prices (that are close to the predicted one). Of course, the success of this whole enterprise depends on a number of factors: (i) the ANN has to be reasonably accurate in its predictions, (ii) the featureweights extracted from the ANN have a high fidelity to the ANN\u2019s function (iii) the nearest neighbors found do not bear an overly complex relationship to the query-case (iv) and the user has sufficient expertise to easily relate these explanatory nearest neighbors to the query case (e.g., as in Fig 1 and Fig. 2; see also [11]) and so on.\nDefinition of ANN-CBR Twin. Accordingly, we can define an ANN-CBR twin-system, precisely, as a system with:\n\u2022 Two Techniques. A hybrid system in which (at least) two techniques2 \u2013 ANN (MLP or DNN) and CBR techniques (notably, k-NN) \u2013 are combined to meet system requirements of accuracy and interpretability.\n\u2022 Separate Modules. Where these techniques are run as separate modules, independently but, as it were, side-by-side.\n\u2022 Common Dataset. The two techniques are run on the same dataset (i.e., they are twinned by this common usage).\n\u2022 Feature-Weight Mapping. Some description of the ANN\u2019s functionality, typically described as its feature-weights, that \u201creflect\u201d what the ANN has learned, is mapped to the k-NN retrieval step of the CBR-system.\n\u2022 Bipartite Division of Labor. In the ANN and CBR modules, the former delivers predictions and the latter provides interpretability, explaining the ANN\u2019s outputs (for classification or regression).\nAs we shall see in our subsequent survey, though this is quite a simple definition, it excludes many hybrid systems that combine ANNs and CBR, as well as many CBR systems that do explanation without any ANN-aspect. For example, there are many systems that combine ANNs and CBR in a pipelined way where the ANN is used to extract features or feature weights that then improve the CBR\u2019s performance, using both MLPs [55-57] and DNNs [58]; these are not twin-systems because the CBR module is making the predictions (though the ANN improves these predictions), and the CBR system is not explaining the ANN\u2019s predictions. Similarly, there are some systems that use ANN and CBR modules in a single system, where both make predictions [59- 61]. For instance, several agent-based systems for predicting oceanographic events (e.g., sea temperature, oils spills, red tides) alternate between ANN and CBR sub-systems, where the predictions from both are monitored to ensure continuing accuracy over time [59-61]; here, both systems are tasked with accuracy and the CBR sub-system is not specifically tasked with explanation (i.e., there is no mapping of feature weights). In the next section, we survey the somewhat abandoned regions of the hybrid-systems literature, relating to true ANN-CBR twins that specifically address explanation."
    },
    {
      "heading": "3 A Systematic Review: Methodology",
      "text": "A systematic search of the literature on ANN-CBR twins for explanation was done with a number of top-down searches using relevant keywords, supplemented by bottom-up, citation-based searches from key papers (see Table 1). In total 1,102 papers were checked (title and abstract) and filtered down to 379 papers; from this latter set a close reading of 90 papers was carried out to identify all the ANN-CBR twins in the literature.\n2 Note, there are many other systems that combine CBR with other techniques, that are not\nconsidered here (e.g., with Genetic Algorithms, Rule-Based systems, Bayesian techniques)."
    },
    {
      "heading": "3.1 Method: Search Procedure",
      "text": "Five systematic searches were carried out on https://scholar.google.com between January 6th, 2019 and March 24th, 2019: four top-down searches using keywords and one bottom-up search through papers that cited key articles. Table 1 shows the string searches used in each of the top-down searches with (i) the number of results considered for a given search (these results were checked by reading the title and abstract, and checking the google anchor-text on which the strings matched), (ii) the unique papers selected across these searches that were considered further (N = 379; these papers were generally downloaded and string-searched for the part of the paper that discussed their hybrid status and \u201cexplanation\u201d). From the latter set, a final set of papers (N=90) were selected to be read in full to determine if the paper described a twin-system, as defined. In all searches, review papers were excluded as we wanted original system papers. We also tried not to double-count cases of twin-systems; for example, groups often produce several papers in different venues for the same system, so where the papers were essentially identical we did not double count them (though we did count cases where essentially the same system was applied to a new domain/task)."
    },
    {
      "heading": "3.2 Results Summary",
      "text": "In total 1,102 distinct GoogleScholar results were initially checked and filtered down to 379 potentially relevant papers, that were downloaded and examined for evidence of being twin-systems. From this set, only 90 were read in full to see if they match the twin-system definition. Of these 90 papers, only 34 were identified as true ANN-CBR twin-systems (n.b., only 21 of these papers report unique systems). Many systems combine ANN and CBR techniques, but fail to meet some key property of the twin-system definition (e.g., they were a pipeline, they did not work over the same dataset, or explanation was not a major concern). There was some indication that the top-down searches identified a fairly complete set of relevant papers because (i) many of the same papers recurred across searches, (ii) several, apparently, plagiarized papers were found, where the same paper was published with non-overlapping author names [62, 63], and (iii) many of the papers found in top-down searches cited the key papers on the twinning topic in the bottom-up search. The character and profile of the identified papers is discussed as a history in the next section."
    },
    {
      "heading": "4 A History of ANN-CBR Twins",
      "text": "Our survey of the landscape of ANN-CBR twins reveals a fragmentary and subdued development of the twinning idea. A close reading of 90 articles on hybrid ANN-CBR systems that made some mention of explanation, found only 34 papers (21 unique systems) that were true twin-systems. The remaining papers tend to be ANN-CBR pipelines where the ANN is used to compute features and/or feature-weights that are then used in the CBR\u2019s k-NN for predictive purposes. Even though the idea of combining ANN and CBR is first referenced in 1989 [43, 44], it is not until 1999 that the first true twin-systems emerge [20, 46]. From this beginning, there is a very modest development of the idea over the intervening 20 years. Indeed, citation patterns are quite inconsistent and lookbacks from more recent papers are patchy. The history divides into three distinct periods: (i) a major piece of work by a Korean Group in the late-1990s (with a parallel proposal in the USA), (ii) a significant addition by an Irish Group through the mid-2000s and, then (iii) more recent work in deep learning that revisits related approaches, often with no or poor reference to the prior literature."
    },
    {
      "heading": "4.1 Korean Developments (1999-2007): Feature-Weighting Tests of Twins",
      "text": "Around 1999, a South Korean group working at the Korean Advanced Institute of Science and Technology (KAIST), explored a range of feature-weighting techniques in comparative tests of the twin-system idea, in a framework they called \u201cMemory Based Neural Reasoning\u201d [46-54]. Shin and colleagues [46, 47, 51] paired MLPs with CBR operating over the same dataset, proposing that this hybrid system \u201ccan give examplebased explanations together with prediction values of the neural network\u201d [47, p.637]. Initially, they tested these twin-systems on a semiconductor-yield dataset before moving on to tests on many benchmark datasets (e.g., Iris and Wisconsin Diagnostic Breast Cancer datasets) for classification and regression tasks. In this work, they perform competitive tests of four different feature-weighting schemes for capturing the MLPs activation patterns (i.e., sensitivity, activity, relevance and saliency). For example, in sensitivity a feature\u2019s weight is calculated by taking the absolute difference between the normal prediction of the MLP and its prediction with that input feature set to zero; this is repeated and summed for the entire training set, and the final figure is then divided by the number of instances in the training data to normalize it for the final featureweight value [46]. For each weighting scheme, the feature-weights were used in the kNN to retrieve cases, matching the prediction for the query-case. There are three significant contributions in this Korean work: (i) the researchers explicitly talk about a division of labor, where the ANN provides accuracy, using its feature-weights, and the CBR system provides explanations using nearest-neighbor cases, (ii) they recognize that there are many different ways to describe the ANN (i.e., different feature-weighting methods) that need to be tested3, and (iii) they understand that there are two classes of feature-weighting methods (global and local). Shin and colleagues [46, 47, 51] appear to be the first in the literature, to explicitly pair ANNs and CBR systems in a twinned way for purposes of explanation and to perform comparative tests of different feature-weighting schemes. Shin and colleagues [46, 47] propose that the sensitivity and activity measures seemed to perform best, (though conclusions are different for different datasets) arguing that their fidelity to the 3 A fact overlooked in most papers, even very recent ones.\nfunction being computed by the MLP was better. Park et al. [51] extend the earlier tests with a new feature-weighting scheme based on the important distinction between global and local weighting schemes. Global feature-weighting assumes the input space is isotropic, deriving a single ubiquitous feature-weight vector for the entire domain (i.e., weights do not change for different query cases), whereas local feature-weighting weights each specific query-case (and sometimes each training case) differently to help case retrieval. Park et al. [51] find that local-weighting schemes perform markedly better than global-weighting methods, presumably because the former captures information about a specific area of the input space for a given query in a more fine-grained way. However, their local-weighting technique is not applicable to post-hoc explanations in MLPs, as it is specifically trained to generate query-specific feature-weightsets rather than giving predictions. Hence, it cannot be considered to be a twin-system; however, it does show the potential for local-weighting methods. Later work extends global weighting schemes to datasets having symbolic feature-values [53, 54]. Overall, the global methods tend to produce very similar results for different values of k but the results from local methods are found to be demonstrably better. These nine Korean papers did not attract huge levels of attention; together they have a total of 269 GoogleScholar citations (M=30, Max=91, Min=5). Indeed, many of these citations are not specifically to the twinning idea, but reference other aspects of the work (e.g., the domains used). However, more recently, several papers have referenced their work. Weber et al. [64] claim a philosophic overlap with the Korean work in an ANN-CBR hybrid; yet the details of the feature-weighting used are not clear. Peng and Zhuang [65] propose a different feature-weighting scheme for an ANN-CBR twinning, that replaces feature-values of a case using the MLPs weights (but does not reference the Korean work). However, it is only in the last few years, that the Korean work has been seriously revisited. Biswas and colleagues [66-68] revisited the sensitivity measure and several limitations of earlier weighting schemes; they transform the MLP into an AND/OR graph from which weights are extracted for use in the CBR system. On applying this graph technique to several new domains, they find that it does better than other methods. Biswas et al. [68] also revisit global weighing-techniques in the context of class-imbalanced datasets, showing that a cost-sensitive learning algorithm displays improvements for such datasets. Finally, recently the importance of the global-local distinction to XAI in deep learning has been emphasized [5, 6], without referencing the Korean work."
    },
    {
      "heading": "4.2 A Parallel Discovery in L.A.: Caruana et al. (1999)",
      "text": "Around the same time as the Korean Group\u2019s work, another group working at UCLA reported an extension to an earlier system [69] that provided case-based explanations ([20] cited 33 times in GoogleScholar). Caruana et al. [20] describe a system for medical domains in which a \u201cnon-case-based learning method\u201d (an MLP) could generate a distance metric over a training set, that could then be used to find an explanatory case that was most similar to a query-case. Caruana et al.\u2019s MLP predicted pneumonia mortality and proposed case-based explanations based on a query-case\u2019s hidden-layer activation-vector (i.e., its latent features), by computing the Euclidean distance between the query-vector and all training-cases, thus enabling them to find the explanatory cases with the most similar latent features. The paper does not provide detailed results on the success of this method and neither does it report user trials, though it does discuss the issues surrounding how cases might be deployed to explain the ANN\u2019s predictions (for\nrecent related work see [70]). Caruana et al.\u2019s [20] feature-weighting method differs from those examined by the Korean group, that were based on input space weightings rather than latent space weightings (as well as being a local-weighting technique). This latent-space approach has not been pursued actively, perhaps, because it appears to be less transparent than input-space approaches (see [4]). Recently, in the deep learning literature reviews of XAI, [20] is regularly cited as the case-based explanation paper [7, 12, 71, 72], in the absence of references to the Korean work that, arguably, is more complete; a fact that, perhaps, indicates some discontinuities in the XAI literature."
    },
    {
      "heading": "4.3 An Irish Departure (mid-2000s): Local Feature-Weighting Tests of Twins",
      "text": "The next major step in the development of the twinning idea came in the mid-2000s, from a group of Irish researchers, largely, at University College Dublin [73-77]. This group also saw a role for CBR in explaining the opaque, but accurate outputs of MLPs, arguing that \u201cthe use of actual training data, cases from the case base, as evidence in support of a particular prediction, is a powerful and convincing form of explanation\u201d (p. 164, [75]). The Irish researchers, who did not cite the Korean work, proposed a new and intriguing local feature-weighting method [74-76]. Nugent and Cunningham [74] were concerned with capturing the function being computed by MLPs in the local region around a given query-case for a blood-alcohol dataset. So, they systematically perturbed the features of the query-case, queried labels for these perturbed cases from the MLP, and then built a linear model from the results of these tests. The coefficients of this linear model were then used to weight the k-NN search in the CBR system that shared its case-base with the MLP\u2019s training set. Nugent et al. [76, 77] also considered more complex use of cases than just providing nearest neighbors, by selecting a fortori cases; the idea being to use a case that is closest to the decision boundary for the querycase, which may not, necessarily, be the nearest neighbor. Finally, [73] did user tests to show that the retrieved cases have some explanatory value in these domains. There are, at least, three significant contributions from this work: it explores (i) a very different approach to the computation of local feature-weights which, in contrast to the Korean local-weighting method, can be used for post-hoc explanations, (ii) a more complex scheme for selecting explanatory cases, beyond the simple use of nearest neighbors, (iii) it showed that this type of twin-system had some validity for human users by using case-based explanations over feature-based ones. These five papers have received moderate attention in the literature; between them they have a total of 236 GoogleScholar citations (M=47, max = 99, min=8). However, few of these citations are about the twin-system idea (i.e., often about user tests). Notably, though the linear-model idea has advanced significantly in the literature on interpretable classifiers [78, 79], few papers specifically cite this Irish work. For instance, the Local Interpretable Model-Agnostic Explanations (LIME) [79] technique also perturbs query-cases to build local linear models but does not cite [79]. Although, Olsson et al. [80] do, in a related approach to case similarity using logistic regression \u2013 the principle of interchangeability \u2013 and the notion of local accuracy to handle the identification of explanatory cases. More recently, there is some recognition of these papers in reviews [10, 81] but for the most part they are passed over [6, 7, 37, 72] with all CBR-solutions being attributed to Caruana et al. [20] and Kim, Rudin and Shah [21]."
    },
    {
      "heading": "4.4 Recent DNN-CBR Twinning",
      "text": "We have already established that there is a disconnect between more recent DNN research and the older twin-system literature in CBR. Yet, in the last two decades, a huge amount of work has been done on different ways to describe the functions of opaque ANN and ML systems. Recently, the focus of some of this work has shifted to the casebased explanation-by-example of DNNs. For instance, Chen et al. [82] and Li et al. [83] both build CBR into the DNN architecture itself, mainly to avoid the need for post-hoc explanations. Although these are not twin-systems, they do combine CBR and ANN techniques for the purpose of interpretability and explanations (though they fail to cite much previous work done). A review by Gilpin et al. [37] proposes that DNNs have often been explained by using simpler \u201cproxy systems\u201d4 of which they identify four types: linear models [79], decision trees [84], automatic rule extraction [78], and saliency mapping [85-87]. Two of these approaches \u2013 linear models and saliency mapping \u2013 have resonances with the twin-systems literature reviewed here.\nLinear Models. As in Nugent and Cunningham\u2019s work, a currently popular approach to explaining ANNs (and indeed any ML model) is to use local linear models built by perturbing an input in the neighborhood of a query. LIME [79] is the prime example of this approach as it finds relative feature-weightings for a given query-case. Recently, LIME has been used in comparative tests of several twin-systems ([4] influenced by Nugent and Cunningham) and found that it not appreciably better than popular globalweighting techniques (including the sensitivity method used by the Korean group). Saliency Mapping. Another popular technique looks at the contribution of inputs to an ANN\u2019s output, deriving saliency maps by backpropagating contribution scores from a given activation in the network (usually in the output layer), to a previous layer (usually the input one). Amongst others, these methods include Layer-wise Relevance Propagation [85], Integrated Gradients [86], and DeepLIFT [87]. This saliency mapping is typically used to highlight important pixels in a CNN\u2019s classification of an image, however it has other uses and has recently been applied to MLP-CBR and CNN-CBR twin-\n4 The proxy system is meant to behave similarly to the black-box system but is simpler for\nexplanatory purposes (so, the CBR in ANN-CBR twin is one type of proxy model).\nsystems (notably, with multiple fully connected dense layers using image transfer learning [88]) in comparative tests (see [4] and also Fig 2).\nOther DNN Options. There are also a handful of other DNN options that are arguably twin-systems though of quite a different ilk. Work on the extraction of prototypes from the analysis of DNNs have been cast as case-based approaches, though with a Bayesian aspect [21, 83]. These proposals look like a different type of twinning \u2013 Bayesian-CBR twins \u2013 that perhaps have other precursors in the CBR literature [89, 90]. Another approach tries to map the layers of a DNN onto particular exemplar cases using Deep kNearest neighbors (DkNN) [71, 91, 92]. However, it still remains to be seen whether these are to be accommodated as twin-systems."
    },
    {
      "heading": "5 Future Directions: Road Mapping",
      "text": "In the present paper, we have reviewed the history of how CBR has been used in a twinning fashion to explain the outputs of ANNs. The significance and importance of this survey is that it shows there are generalizations about XAI to be gleaned from the twin-system approach. Such generalizations may help us avoid the current scattered fragmentary development of XAI solutions [5]. This review also suggests a research road map for future work in this area, along at least three paths:\n\u2022 Feature-Weighting Schemes. It is clear that there is a large space of featureweighting schemes that could be explored (especially, more recent ones in DNNs); this exploration needs to be done in a controlled and comparative fashion to determine which ones are best for which domains and tasks. \u2022 The Deployment of Cases. CBR work has shown in twin-systems there are many different ways cases can be used for explanation (e.g., a fortiori usage, counterfactual cases, near misses, nearest unlike neighbors and so on; more needs to be done on these ideas in the context of ANNs, and especially DNNs). \u2022 The Embarrassment of User Testing. In all the papers we examined we found less than a handful (i.e., < 5) that performed any adequate user testing of the proposal that cases improved the interpretability of models; this gap needs to be rectified.\nIn conclusion, notwithstanding the citation gaps in the literature, it is clear that there are many fruitful directions in which the CBR-twin idea can be taken to answer the interpretability problems we currently face in XAI."
    },
    {
      "heading": "95 (1965)",
      "text": "15. Salmon, W.C.: Scientific explanation and the causal structure of the world. Princeton University Press (1984)\n16. Van Fraassen, B.C.: The scientific image. Oxford University Press (1980) 17. Keil, F.C.: Explanation and understanding. Annual Rev. of Psychology, 57, 227-254 (2006) 18. Leake, D.B.: CBR in context: The present and future. Case-based reasoning: Experiences,\nlessons, and future directions, pp.3-30 (1996) 19. Leake, D. and McSherry, D.: Introduction to the special issue on explanation in case-based\nreasoning. Artificial Intelligence Review, 24(2), 103-108 (2005) 20. Caruana, R., Kangarloo, H., Dionisio, J.D., Sinha, U. and Johnson, D.: Case-based explana-\ntion of non-case-based learning methods. In: Proceedings of the AMIA Symposium, p. 212. American Medical Informatics Association (1999)\n21. Kim, B., Rudin, C. and Shah, J.A.: The Bayesian case model: A generative approach for case-based reasoning and prototype classification. In: Adv. in NIPs, pp. 1952-1960 (2014) 22. Pedreschi, D., Giannotti, F., Guidotti, R., Monreale, A., Ruggieri, S. and Turini, F.: Meaningful explanations of Black Box AI decision systems. In: Proceedings of AAAI-19 (2019)\n23. Haykin, S.: Neural networks (Vol. 2). New York: Prentice Hall (1994) 24. Goodfellow, I., Bengio, Y. and Courville, A.: Deep learning. MIT Press (2016) 25. Kolodner, J.: Case based reasoning. Morgan Kaufmann (2014) 26. Aamodt, A. and Plaza, E.: Case-based reasoning: Foundational issues, methodological var-\niations, and system approaches. AI communications, 7(1), 39-59 (1994) 27. De Mantaras, R.L., McSherry, D., Bridge, D., Leake, D., Smyth, B., Craw, S., Faltings, B.,\nMaher, M.L., Cox, M.T., Forbus, K. and Keane, M.T.: Retrieval, reuse, revise and retention in CBR. Knowledge Engineering Review, 20(3), 215-240 (2006)\n28. Sahin, S., Tolun, M.R. and Hassanpour, R.: Hybrid expert systems: A survey of current approaches and applications. Expert systems with applications, 39(4), 4609-4617 (2012) 29. Negnevitsky, M.: Artificial intelligence. Pearson education (2005) 30. Medsker, L.R.: Hybrid neural network and expert systems. Springer (2012) 31. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus,\nR.: Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013)\n32. Krizhevsky, A., Sutskever, I. and Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in NIPs, pp. 1097-1105 (2012)\n33. Witten, Ian H., Eibe Frank, Mark A. Hall, and Christopher J. Pal.: Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann (2016)\n34. LeCun, Y., Bengio, Y. and Hinton, G.: Deep learning. Nature, 521(7553), p.436 (2015) 35. Olden, J.D. and Jackson, D.A.: Illuminating the \u201cblack box\u201d. Ecological modelling, 154(1-\n2), pp.135-150 (2002) 36. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D. and Batra, D.: Grad-cam:\nVisual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 618-626 (2017)\n37. Gilpin, L.H., Bau, D., Yuan, B.Z., Bajwa, A., Specter, M. and Kagal, L.: Explaining explanations: an approach to evaluating interpretability of machine learning. arXiv preprint arXiv:1806.00069 (2018)\n38. Zeiler, M.D. and Fergus, R.: Visualizing and understanding convolutional networks. In: European conference on computer vision, pp. 818-833. Springer (2014) 39. He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778 (2016)\n40. Erhan, D., Bengio, Y., Courville, A. and Vincent, P.: Visualizing higher-layer features of a deep network. University of Montreal, 1341(3), p.1 (2009)\n41. Keane, M.T.: Analogical asides on case-based reasoning. In: European Workshop on CaseBased Reasoning, pp. 21-32, Springer, Berlin, Heidelberg (1993)\n42. Nunes, I. and Jannach, D.: A systematic review and taxonomy of explanations in decision support and recommender systems. UMUAI, 27(3-5), 393-444 (2017)\n43. Becker, L. and Jazayeri, K.: A connectionist approach to case-based reasoning. In: Proceedings of the Case-Based Reasoning Workshop, pp. 213-217. Morgan Kaufmann (1989)\n44. Thrift, P.: A neural network model for case-based reasoning. In: Proceedings of the CaseBased Reasoning Workshop, pp. 334-337. Morgan Kaufmann (1989)\n45. Hilario, M., Pellegrini, C., & Alexandre, F. Modular integration of connectionist and symbolic processing in knowledge-based systems. C.de R. en Informatique de Nancy (1994)\n46. Shin, C. K., & Park, S. C.: Memory and neural network based expert system. Expert Systems with Applications, 16(2), 145-155 (1999)\n47. Shin, C. K., Yun, U. T., Kim, H. K., & Park, S. C.: A hybrid approach of neural network and memory-based learning to data mining. IEEE Transactions on Neural Networks, 11(3),"
    },
    {
      "heading": "637-646 (2000)",
      "text": "48. Shin, C.K. and Park, S.C., A machine learning approach to yield management in semiconductor manufacturing. International Journal of Production Research, 38, 4261-4271 (2000)\n49. Park, J.H., Shin, C.K., Im, K.H. and Park, S.C.: A local weighting method to the integration of neural network and case based reasoning. In: Neural Networks for Signal Processing XI: Proceedings of the 2001 IEEE SPS Workshop, pp. 33-42. IEEE (2001)\n50. Shin, C.K. and Park, S.C.: Towards integration of memory based learning and neural networks. In Soft computing in case based reasoning (pp. 95-114). Springer, London (2001)\n51. Park, J.H., Im, K.H., Shin, C.K. and Park, S.C.: MBNR: case-based reasoning with local feature weighting by neural network. Applied Intelligence, 21(3), pp.265-276 (2004)\n52. Park, S.C., Kim, J.W. and Im, K.H.: Feature-weighted CBR with NN for symbolic features. In: International Conference on Intelligent Computing, 1012-1020. Springer (2006)\n53. Im, H. and Park, S.C.: Case-based reasoning and neural network based expert system for personalization. Expert Systems with Applications, 32(1), pp.77-85 (2007)\n54. Ha, S.: A personalized counseling system using case-based reasoning with neural symbolic feature weighting (CANSY). Applied intelligence, 29(3), pp. 279-288 (2008)\n55. Reategui, E.B., Campbell, J.A. and Leao, B.F.: Combining a neural network with case-based reasoning in a diagnostic system. Artificial Intelligence in Medicine, 9(1), 5-27 (1997)\n56. Yang, B.S., Han, T. and Kim, Y.S.: Integration of ART-Kohonen NN and CBR for intelligent fault diagnosis. Expert Systems with Applications, 26(3), 387-395 (2004)\n57. Rodriguez, Y., Garcia, M.M., De Baets, B., Morell, C. and Bello, R.: A connectionist fuzzy case-based reasoning model. In: Mexican International Conference on Artificial Intelligence, pp. 176-185. Springer, Berlin, Heidelberg (2006) 58. Amin, K., Kapetanakis, S., Althoff, K.D., Dengel, A. and Petridis, M.: Answering with cases: A CBR Approach to Deep Learning. In: International Conference on Case-Based Reasoning, pp. 15-27. Springer, Cham. (2018)\n59. Corchado, J.M., Rees, N., Lees, B. and Aiken, J.: Data mining using example-based methods in oceanographic forecast models. In: IEE Colloquium on Knowledge Discovery and Data Mining (Digest No. 1998/310) (pp. 7-1). IET (1998) 60. Corchado, J.M. and Lees, B.: A hybrid case-based model for forecasting. Applied Artificial Intelligence, 15(2), 105-127 (2001)\n61. Fdez-Riverola, F., Corchado, J.M. and Torres, J.M.: An automated hybrid CBR system for forecasting. In: European Conf. on Case-Based Reasoning, pp. 519-533. Springer (2002)\n62. Jothikimar, R., Shivakumar, N., Ramesh, P.S., Suganthan, and Suresh, A.: Heart Disease Prediction System Using ANN, RBF and CBR. International Journal of Pure and Applied Mathematics, 117(21), 199-217 (2017)\n63. Kouser, R.R., Manikandan, T. and Kumar, V.V.: Heart Disease Prediction System Using Artificial Neural Network, Radial Basis Function and Case Based Reasoning. Journal of Computational and Theoretical Nanoscience, 15(9-10), 2810-2817 (2018)\n64. Weber, R., Proctor, J.M., Waldstein, I. and Kriete, A.: CBR for modeling complex systems. In: International Conference on Case-Based Reasoning, pp. 625-639. Springer (2005) 65. Peng, Y. and Zhuang, L.: A case-based reasoning with feature weights derived by BP network. In: Intelligent Information Technology Application, pp. 26-29. IEEE (2007)\n66. Biswas, S. K., Sinha, N., Purakayastha, B., & Marbaniang, L.: Hybrid expert system using case based reasoning and neural network for classification. Biologically Inspired Cognitive Architectures, 9, 57-70 (2014)\n67. Biswas, S. K., Baruah, B., Sinha, N., & Purkayastha, B.: A hybrid CBR classification model by integrating ANN into CBR. International Journal of Services Technology and Management, 21(4-6), 272-293 (2015)\n68. Biswas, S. K., Chakraborty, M., Singh, H. R., Devi, D., Purkayastha, B., & Das, A. K.: Hybrid case-based reasoning system by cost-sensitive neural network for classification. Soft Computing, 21(24), 7579-7596 (2017)\n69. Cooper, G.F., Aliferis, C.F., Ambrosino, R., Aronis, J., Buchanan, B.G., Caruana, R., Fine, M.J., Glymour, C., Gordon, G., Hanusa, B.H. and Janosky, J.E.: An evaluation of machinelearning methods for predicting pneumonia mortality. Artificial intelligence in medicine, 9(2), 107-138 (1997)\n70. Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M. and Elhadad, N.: Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1721-1730. ACM (2015)\n71. Papernot, N., & McDaniel, P. Deep k-nearest neighbours: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765 (2018)\n72. Mittelstadt, B., Russell, C. and Wachter, S.: Explaining Explanations in AI. In: Proceedings of Conference on Fairness, Accountability, and Transparency (FAT*-19) (2019)\n73. Cunningham, P., Doyle, D., and Loughrey, J.: An evaluation of the usefulness of case-based explanation. In: International Conf. on Case-Based Reasoning, 122-130. Springer (2003)\n74. Nugent, C., and Cunningham, P.: A case-based explanation system for black-box systems. Artificial Intelligence Review, 24(2), 163-178 (2005)\n75. Nugent, C., Cunningham, P., and Doyle, D.: The best way to instill confidence is by being right. In: International Conference on Case-Based Reasoning, pp. 368-381. Springer (2005)\n76. Nugent, C., Doyle, D., and Cunningham, P.: Gaining insight through case-based explanation. Journal of Intelligent Information Systems, 32(3), 267-295 (2009)\n77. Doyle, D., Cunningham, P., Bridge, D. and Rahman, Y.: Explanation oriented retrieval. In European Conference on Case-Based Reasoning, pp. 157-168. Springer (2004)\n78. Andrews, R., Diederich, J. and Tickle, A.B.: Survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-based systems, 8, 373-389 (1995)\n79. Ribeiro, M.T., Singh, S. and Guestrin, C.: Why should I trust you?: Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144. ACM (2016)\n80. Olsson, T., Gillblad, D., Funk, P. and Xiong, N.: Case-based reasoning for explaining probabilistic machine learning. International Journal of Computer Science and Information Technology, 6(2), 87-101 (2014)\n81. Zharov, Y., Korzhenkov, D., Shvechikov, P. and Tuzhilin, A.: YASENN: Explaining Neural Networks via Partitioning Activation Sequences. arXiv preprint arXiv:1811.02783 (2018)\n82. Chen, C., Li, O., Barnett, A., Su, J. and Rudin, C.: This looks like that: deep learning for interpretable image recognition. arXiv preprint arXiv:1806.10574 (2018)\n83. Li, O., Liu, H., Chen, C., and Rudin, C.: Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. In: Thirty-Second AAAI Conference on Artificial Intelligence. AAAI (2018) 84. Zilke, J.R., Menc\u00eda, E.L. and Janssen, F.: DeepRED\u2013Rule extraction from deep neural networks. In: Internat. Conf. on Discovery Science, pp. 457-473. Springer (2016)\n85. Bach, S., Binder, A., Montavon, G., Klauschen, F., M\u00fcller, K.R. and Samek, W.: On pixelwise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7), p.e0130140. (2015)\n86. Sundararajan, M., Taly, A. and Yan, Q.: Axiomatic attribution for deep networks. In: Proceedings of the 34th International Conference on Machine Learning-Volume, 70, 3319- 3328. JMLR. Org (2017)\n87. Shrikumar, A., Greenside, P. and Kundaje, A.: Learning important features through propagating activation differences. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70, 3145-3153. JMLR. Org (2017)\n88. Zhang, C.L., Luo, J.H., Wei, X.S. and Wu, J.: In defense of fully connected layers in visual representation transfer. In: Pacific Rim Conf. on Multimedia (pp. 807-817). Springer (2017)\n89. Myllym\u00e4ki, P. and Tirri, H.: November. Massively parallel case-based reasoning with probabilistic similarity metrics. In: European Workshop on Case-Based Reasoning (pp. 144- 154). Springer, Berlin, Heidelberg (1993)\n90. Kofod-Petersen, A., Langseth, H., and Aamodt, A.: Explanations in Bayesian networks using provenance through case-based reasoning. In: Workshop Proceedings, p. 79 (2010) 91. Wallace, E., Feng, S. and Boyd-Graber, J.: Interpreting Neural Networks with Nearest Neighbours. arXiv preprint arXiv:1809.02847 (2018)\n92. Card, D., Zhang, M. and Smith, N.A. January. Deep Weighted Averaging Classifiers. In: Proceedings of Conf. on Fairness, Accountability & Trust (pp. 369-378). ACM (2019)"
    }
  ],
  "title": "How Case-Based Reasoning Explains Neural Networks: A Theoretical Analysis of XAI Using Post-Hoc Explanation- by-Example from a Survey of ANN-CBR Twin-Systems",
  "year": 2019
}
