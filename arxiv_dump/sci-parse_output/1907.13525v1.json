{"abstractText": "As machine learning becomes an important part of many real world applications affecting human lives, new requirements, besides high predictive accuracy, become important. One important requirement is transparency, which has been associated with model interpretability. Many machine learning algorithms induce models difficult to interpret, named black box. Black box models are difficult to validate. Moreover, people have difficulty to trust models that cannot be explained. Explainable artificial intelligent is an active research area. In particular for machine learning, many groups are investigating new methods able to explain black box models. These methods usually look inside the black models to explain their inner work. By doing so, they allow the interpretation of the decision making process used by black box models. Among the recently proposed model interpretation methods, there is a group, named local estimators, which are designed to explain how the label of particular instance is predicted. For such, they induce interpretable models on the neighborhood of the instance to be explained. Local estimators have been successfully used to explain specific predictions. Although they provide some degree of model interpretability, it is still not 1 ar X iv :1 90 7. 13 52 5v 1 [ cs .L G ] 3 1 Ju l 2 01 9 clear what is the best way to implement and apply them. Open questions include: how to best define the neighborhood of an instance? How to control the trade-off between the accuracy of the interpretation method and its interpretability? How to make the obtained solution robust to small variations on the instance to be explained? To answer to these questions, we propose and investigate two strategies: (i) using data instance properties to provide improved explanations, and (ii) making sure that the neighborhood of an instance is properly defined by taking the geometry of the domain of the feature space into account. We evaluate these strategies in a regression task and present experimental results that show that they can improve local explanations.", "authors": [{"affiliations": [], "name": "Tiago Botari"}, {"affiliations": [], "name": "Rafael Izbicki"}, {"affiliations": [], "name": "Andre C. P. L. F. de Carvalho"}], "id": "SP:0c12a4db0041ee9beb6d721a530c82da249d870c", "references": [{"authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "title": "ImageNet: A Large-Scale Hierarchical Image Database", "venue": "In CVPR09,", "year": 2009}, {"authors": ["Joaquin Vanschoren", "Jan N. van Rijn", "Bernd Bischl", "Luis Torgo"], "title": "Openml: Networked science in machine learning", "venue": "SIGKDD Explorations,", "year": 2013}, {"authors": ["Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "title": "TensorFlow: Large-scale machine learning", "venue": "on heterogeneous systems,", "year": 2015}, {"authors": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "title": "Scikit-learn: Machine learning in Python", "venue": "Journal of Machine Learning Research,", "year": 2011}, {"authors": ["Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer"], "title": "Automatic differentiation in pytorch", "venue": "NIPS-W,", "year": 2017}, {"authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "title": "Show, attend and tell: Neural image caption generation with visual attention", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,", "year": 2015}, {"authors": ["Rich Caruana", "Yin Lou", "Johannes Gehrke", "Paul Koch", "Marc Sturm", "Noemie Elhadad"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30day readmission", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2015}, {"authors": ["Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang"], "title": "End to end learning for self-driving cars", "venue": "arXiv preprint arXiv:1604.07316,", "year": 2016}, {"authors": ["Leilani H Gilpin", "David Bau", "Ben Z Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal"], "title": "Explaining explanations: An overview of interpretability of machine learning", "venue": "IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA),", "year": 2018}, {"authors": ["Christoph Molnar"], "title": "Interpretable Machine Learning. 2019", "venue": "https://christophm. github.io/interpretable-ml-book/", "year": 2019}, {"authors": ["Zachary C Lipton"], "title": "The mythos of model interpretability", "venue": "arXiv preprint arXiv:1606.03490,", "year": 2016}, {"authors": ["Jerome H Friedman"], "title": "Greedy function approximation: a gradient boosting machine", "venue": "Annals of statistics,", "year": 2001}, {"authors": ["Aaron Fisher", "Cynthia Rudin", "Francesca Dominici"], "title": "All models are wrong but many are useful: Variable importance for black-box, proprietary, or misspecified prediction models, using model class reliance", "venue": "arXiv preprint arXiv:1801.01489,", "year": 2018}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data", "year": 2016}, {"authors": ["Scott Lundberg", "Su-In Lee"], "title": "An unexpected unity among methods for interpreting model predictions", "venue": "arXiv preprint arXiv:1611.07478,", "year": 2016}, {"authors": ["Erik \u0160trumbelj", "Igor Kononenko"], "title": "Explaining prediction models and individual predictions with feature contributions", "venue": "Knowledge and information systems,", "year": 2014}, {"authors": ["Anil Aswani", "Peter Bickel", "Claire Tomlin"], "title": "Regression on manifolds: Estimation of the exterior derivative", "venue": "The Annals of Statistics,", "year": 2011}, {"authors": ["Ann B Lee", "Rafael Izbicki"], "title": "A spectral series approach to high-dimensional nonparametric regression", "venue": "Electronic Journal of Statistics,", "year": 2016}, {"authors": ["Rafael Izbicki", "Ann B Lee"], "title": "Nonparametric conditional density estimation in a highdimensional regression setting", "venue": "Journal of Computational and Graphical Statistics,", "year": 2016}, {"authors": ["Rafael Izbicki", "Ann B Lee"], "title": "Converting high-dimensional regression to highdimensional conditional density estimation", "venue": "Electronic Journal of Statistics,", "year": 2017}, {"authors": ["Art B Owen"], "title": "A robust hybrid of lasso and ridge regression", "venue": "Contemporary Mathematics,", "year": 2007}, {"authors": ["David Alvarez-Melis", "Tommi S Jaakkola"], "title": "On the robustness of interpretability methods", "venue": "arXiv preprint arXiv:1806.08049,", "year": 2018}, {"authors": ["Larry Wasserman"], "title": "Topological data analysis", "venue": "Annual Review of Statistics and Its Application,", "year": 2018}, {"authors": ["Herbert Edelsbrunner", "David Kirkpatrick", "Raimund Seidel"], "title": "On the shape of a set of points in the plane", "venue": "IEEE Transactions on information theory,", "year": 1983}, {"authors": ["Herbert Edelsbrunner"], "title": "Alpha shapesa survey", "venue": "Tessellations in the Sciences,", "year": 2010}, {"authors": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "title": "A global geometric framework for nonlinear dimensionality reduction", "year": 2000}], "sections": [{"text": "As machine learning becomes an important part of many real world applications affecting human lives, new requirements, besides high predictive accuracy, become important. One important requirement is transparency, which has been associated with model interpretability. Many machine learning algorithms induce models difficult to interpret, named black box. Black box models are difficult to validate. Moreover, people have difficulty to trust models that cannot be explained. Explainable artificial intelligent is an active research area. In particular for machine learning, many groups are investigating new methods able to explain black box models. These methods usually look inside the black models to explain their inner work. By doing so, they allow the interpretation of the decision making process used by black box models. Among the recently proposed model interpretation methods, there is a group, named local estimators, which are designed to explain how the label of particular instance is predicted. For such, they induce interpretable models on the neighborhood of the instance to be explained. Local estimators have been successfully used to explain specific predictions. Although they provide some degree of model interpretability, it is still not\nar X\niv :1\n90 7.\n13 52\n5v 1\n[ cs\nclear what is the best way to implement and apply them. Open questions include: how to best define the neighborhood of an instance? How to control the trade-off between the accuracy of the interpretation method and its interpretability? How to make the obtained solution robust to small variations on the instance to be explained? To answer to these questions, we propose and investigate two strategies: (i) using data instance properties to provide improved explanations, and (ii) making sure that the neighborhood of an instance is properly defined by taking the geometry of the domain of the feature space into account. We evaluate these strategies in a regression task and present experimental results that show that they can improve local explanations."}, {"heading": "Introduction", "text": "Machine learning (ML) algorithms have shown high predictive capacity for model inference in several application domains. This is mainly due to recent technological advances, increasing number and size of public dataset repositories, and development of powerful frameworks for ML experiments1\u20136 Application domains where ML algorithms have been successfully used include image recognition,7 natural language processing8 and speech recognition.9 In many of these applications, the safe use of machine learning models and the users\u2019 right to know how decisions affect their life make the interpretability of the models a very important issue. Many currently used machine learning algorithms induce models difficult to interpret and understand how they make decisions, named black boxes.\nThis occurs because several algorithms produce highly complex models in order to better\ndescribe the patterns in a dataset.\nMost ML algorithms with high predictive performance induce black box models, leading to inexplicable decision making processes. Black box models reduce the confidence of practitioners in the model predictions, which can be a obstacle in many real world applications, such as medical diagnostics,10 science, autonomous driving,11 and others sensitive domains. In these applications, it is therefore important that predictive models are easy to interpret.\nTo overcome these problems, many methods that are able to improve model interpretation have been recently proposed; see e.g.12,13 for details. These methods aim at providing further information regarding the predictions obtained from predictive models. In these methods, interpretability can occur at different levels: (i) on the dataset; (ii) after the model is induced; and (iii) before the model is induced.14 We will focus our discussion on methods for model interpretability that can be applied after the induction of a predictive model by a ML algorithm; these are known as agnostic methods.\nModel-agnostic interpretation methods are a very promising approach to solve the problem of trust and to uncover the full potential of ML algorithms. These methods can be applied to explain predictions made by models induced by any ML algorithm. Some well known model-agnostic interpretation methods are described in.15\u201319 Perhaps the most well known interpretation method is LIME,17 which allows local explanations for classification and regression models. LIME has been shown to present a very good capability to create\nlocal explanations. As a result, LIME has been used to interpret models induced by ML algorithms in different application domains. However, it it still not clear how to make some decisions when implementing and applying LIME and related methods. Some questions that arise are:\ni How to best define the neighborhood of an instance?\nii How to control the trade-off between the accuracy of the interpretation model and its\ninterpretability?\niii How to make the obtained solution robust to small variations on the instance to be\nexplained?\nA good local explanation for a given instance x\u2217 needs to have high fidelity to the model induced by a ML algorithm in the neighborhood of x\u2217. Although this neighborhood is typically defined in terms of Euclidean distances, ideally it should be supported by the dataset. Thus, the sub-domain used to fit the local explanation model (i.e., a model used to explain the black box model) should reflect the domain where the black model model was induced from. For instance, high-dimensional datasets often lie on a submanifold of Rd, in which case defining neighborhoods in terms of the Euclidean distance is not appropriate.20\u201323 To deal with this deficiency, we address issue (i) by creating a technique that samples training points for the explanation model along the submanifold where the dataset lies on (as opposed to Euclidean neighborhoods). We experimentally show that this technique provides a solution to (iii).\nIn order to address (ii), we observe that a good local explanation is not necessarily a direct map of the feature space. For some cases, the appropriate local description of the explanation lies on specific properties of the instance. These instance properties can be obtained through a transformation of the feature space. Thus, we address issue (ii) by creating local explanations on a transformed space of the feature space. This spectrum of questions should be elaborated by the specialists of the specific application domain.\nIn this work, we focus on performing these modifications for regression tasks. However, these modifications can be easily adapted for classification tasks. In Section , we discuss the use of instance properties, how to deal with the trade-off between explanation complexity and the importance of employing a robust method as an explanatory model. In Section , we describe how to improve the local explanation method using the estimation of the domain of feature space. In Section , we apply our methodology to a toy example. Finally, Section presents the main conclusions from our work and describes possible future directions.\nModel Interpretation Methods"}, {"heading": "Local Explanation Through Instance Properties", "text": "A crucial aspect for providing explanations to predictive models induced by ML algorithms is the relevant information to the specific knowledge domain. In some cases, a direct representation of the original set of features of an instance does not reflect the best local behavior of a prediction process. Hence, other instance properties can be used to create clear decision explanations. These properties can be generated through a map of the original features space, i.e., a function of the input x. Moreover, these instance properties can increase the local fidelity of the explanation with the predictive model. This can be easily verified when the original feature space is highly limited and providing poor information on the neighborhood of a specific point. This case is illustrated by Figure 1 (a).\nIn order to provide a richer environment to obtain a good explanation, the interpretable model should be flexible to possible questions that an user want to instigate the ML model. Given that the possible explanations are mapped using specific functions of the feature space, we can create an interpretable model using\ng(x) = \u03b10 + N\u2211 i=1 \u03b1ifi(x) (1)\nwhere x represents the original vector of features, \u03b1i are the coefficients of the linear regression that will be used as an explanation, and fi(.) are known functions that map x to the properties (that is, questions) that have a meaningful value for explaining a prediction, or that are necessary to obtain an accurate explanation.\nOnce fi\u2019s are created, the explainable method should choose which of these functions better represent the predictions made by the original model locally. This can be achieved by introducing an L1 regularization in the square error loss function. More precisely, let h be a black-box model induced by a ML algorithm and consider the task of explaining the prediction made by h at a new instance x. Let x\u20321, . . . ,x \u2032 M be a sample generated on a neighborhood of x. The local explanation can be found by minimizing (in \u03b1)\nL = M\u2211 k=1 (h(x\u2032k)\u2212 g(x\u2032k))2 + N\u2211 i=1 \u03bbi|\u03b1i| , (2)\nwhere the first term is the standard square error between the induced model and the explanatory model and the second term is the penalization over the explanatory terms. The value of \u03bbi can be set to control the trade-off among the explanatory terms. For instance, if some explanatory terms (fi) are more difficult to interpret, then a larger value can be assigned to \u03bbi.\nIn order to set the objective function (Equation 2), one must be able to sample in a neighborhood of x. To keep consistency over random sampling variations on the neighborhood of x, we decided to use a linear robust method that implements the L1 regularization (see24). This robust linear regression solves some of the problems of instability of local explanations.25\nAdditionally, a relevant question is how to define a meaningful neighborhood around x.\nIn the next section we discuss how this question can be answered in an effective way."}, {"heading": "Defining meaningful neighborhoods", "text": ""}, {"heading": "Feature Space", "text": "The training data used by a ML algorithm defines the domain of the feature space. In order to obtain a more reliable explanation model, we can use the estimated domain of the feature space for sampling the data needed to obtain this model via Equation 2, x\u20321, . . . ,x \u2032 M . This approach improves the fidelity and accuracy to the model when compared to standard Euclidean neighborhoods used by other methods.17 The estimation of the feature domain is closely related to the manifold estimation problem.26 Here, we show how this strategy works by using the \u03b1-shape technique27,28 to estimate the domain of the feature space.\n\u03b1-shape\nThe \u03b1-shape is a formal mathematical definition of the polytope concept of a set of points on the Euclidean space. Given a set of points S \u2282 Rd and a real value \u03b1 \u2208 [0,\u221e), it is possible to uniquely define this polytope that enclose S. The \u03b1 value defines an open hypersphere H of radius \u03b1. For \u03b1 \u2192 0, H is a point, while for \u03b1 \u2192 \u221e, H is an open half-space. Thus, an \u03b1-shape is defined by all k-simplex, {k \u2208 Z|0 \u2264 k \u2264 d}, defined by a set of points s \u2208 S where there exist an open hypersphere H that is empty, H \u2229S = \u2205, and \u2202H \u2229 s = s. In this way, the \u03b1 value controls the polytope details. For \u03b1\u2192 0, the \u03b1-shape recovered is the set of points S itself, and for \u03b1\u2192\u221e, the convex hull of the set S is recovered.27,28 We define the neighborhood of an instance x to be the intersection of an Euclidean ball around x and the space defined by polytope obtained from the \u03b1-shape. In practice, we obtain the instances used in Equation 2 by sampling new points around x that belong to the space defined by polytope obtained from the \u03b1-shape."}, {"heading": "Results for a Toy Model: Length of a Spiral", "text": "In this section, we present an application of our proposed methodology for a toy model in which the data is generated along a spiral. For such, we use the Cartesian coordinates of the spiral on the plane as features."}, {"heading": "Definition", "text": "We explore the toy model described by\nx1 = \u03b8 cos(\u03b8) + 1 x2 = \u03b8 sin(\u03b8) + 2 (3)\ny = 1\n2\n[ \u03b8 \u221a 1 + \u03b82 + sinh\u22121 \u03b8 ]\nwhere x1 and x2 are the values that form the feature vector x = (x1, x2), \u03b8 is a independent variable, i, i \u2208 {1, 2}, is a random noise, and the target value is given by y, the length of the spiral. This toy model presents some interesting features for our analysis, such as the feature domain over the spiral and the substantial variance of the target value when varying one of the features coordinate while keeping the other one fixed."}, {"heading": "Instances for Investigation", "text": "We investigate the explanation for 3 specific instances of our toy model: x1 = (0.0, 14.5), x2 = (10.0, 10.0) and x3 = (\u221216.0, 0.0). For the first point, x1, we have that the target value (the length of the spiral) will locally depend on the value of x1 , and thus explanation methods should indicate that the most important feature is x1. For the second value, x 2, the features x1 and x2 have the same contribution for explaining such target. Finally, for\nthe third point, x3, the second feature should be the most important feature to explain the target."}, {"heading": "Data Generation:", "text": "Using the model described in Equation 4, we generated 80 thousand data points. These data was generated according to \u03b8 \u223c Unif[0, 8\u03c0], an uniform distribution. The values of random noise were selected from 1 \u223c N (0, 0.4) and 2 \u223c N (0, 0.4), where N (\u00b5, \u03c3) is a normal distribution with mean \u00b5 and standard deviation \u03c3. The feature space and the target value are shown in Figure 2 (a). The generated data was split into two sets in which 90% used for training and 10% for testing. Additionally, we test the explanation methods by sampling three sets of data in the neighborhoods of x1, x2, and x3.\nModel induction using a ML algorithm:\nWe used a decision tree induction algorithm (DT) in the experiments. We used the Classification and Regression Trees (CART) algorithm implementation provided by the scikit-learn5 library. The model induced by this algorithm using the previously described dataset had as predictive performance MSE = 24.00 and R2 = 0.997."}, {"heading": "Determining the \u03b1-shape of the data:", "text": "For this example, we applied the \u03b1-shape technique using \u03b1 = 1.0. The value of \u03b1 can be optimized for the specific dataset at hand; see28 for details. The estimation of the domain using the \u03b1-shape is illustrated by Figure 2 (b)."}, {"heading": "Local Explanation", "text": "The local explanation was generated though a linear regression fitted to a data generated over the neighborhood of the point for which the explanation was requested (xexp). We use the linear robust method available on the scikit-learn package.5\nExplanation for instance x1 = (0.0, 14.5):\nThe obtained explanation using the standard sampling approach (hereafter normal sampling) presents low agreement with true value of the spiral length (Figure 3(a)). We also noticed that this explanation is unstable with respect to sampling variations (even though we use a robust method to create the interpretation), and indicates that the best feature to explain the ML algorithm locally is x2 (Figure 3(b)). This description is inaccurate (see discussion in Section Instances for Investigation). On the other hand, when the sampling strategy is performed over the correct domain of the feature space (hereafter selected sampling), we obtain an explanation method with high predictive accuracy (i.e., that accurately reproduces the true target value - Figure 3(c)). Moreover, the feature that best explains such prediction is x1 (Figure 3(d)), which is in agreement with our expectation.\nExplanation for instances x2 = (10.0, 10.0) and x3 = (16.0, 0.0):\nWe also analyzed the other two points to demonstrate the capability of the selected sampling to capture the correct feature importance. For the instance x2, the features importance is almost equally divided between the two features (Figure 4). For the instance x3, the most important feature is x2, with importance of \u22121.0 (figure 5). In the case of x3, the normal sampling strategy produced a good explanation (figure 5(b)). However, we noticed that this result is unstable due to random variation in the sampling. All results presented here are in agreement with our discussion in Section Instances for Investigation."}, {"heading": "Robustness of Explanations", "text": "Good explanation models for x\u2217 should be stable to small perturbations around x\u2217. To illustrate the stability of our method, we generated explanations for instances in the neighborhood of x1: x1a = (\u22122.0, 14.5), x1b = (1.0, 14.0) and x1c = (0.5, 13.7). Table 1 shows that the explanations created for these points using selected sampling are compatible with those for x1. On the other hand, the normal sampling strategy is unstable. These results demonstrate that using the domain defined by the feature space can improve the robustness of a local explanation of an instance."}, {"heading": "Conclusion", "text": "In order to increase trust and confidence on black box models induced by ML algorithms, explanation methods must be reliable, reproducible and flexible with respect to the nature of the questions asked. Local agnostic-model explanations methods have many advantages that\nare aligned with these points. Besides, they can be applied to any ML algorithm. However, the standard of the existing agnostic methods present problems in producing reproducible explanation, while maintaining accuracy to the original model. To overcome these limitations, we developed new strategies to overcome them. For such, the proposed strategies address the following issues: (i) estimation of the domain of the feature space in order to provide meaningful neighborhoods; (ii) use of different penalization level on explanatory terms; and (iii) employment of robust techniques for fitting the explanatory method.\nThe estimation of the domain of the features space should be performed and used during the sampling step of local interpretation methods. This strategy increases the accuracy of the local explanation. Additionally, using robust regression methods to create the explainable models is beneficial to obtain stable solutions. However, our experiments show that robust methods are not enough; the data must be sampled taking the domain of the feature space into account, otherwise the generated explanations can be meaningless.\nFuture work includes testing other methods for estimating manifolds such as diffusion maps29 and isomaps,30 extending these ideas to classification problems, and investigating the performance of our approach on real datasets."}, {"heading": "Acknowledgments", "text": "The authors would like to thank CAPES and CNPq (Brazilian Agencies) for their financial support. T.B. acknowledges support by Grant 2017/06161-7, Sa\u0303o Paulo Research Foundation (FAPESP). R. I. acknowledges support by Grant 2017/03363 (FAPESP) and Grant 306943/2017-4 (CNPq). The authors acknowledge Grant 2013/07375-0 - CeMEAI - Center for Mathematical Sciences Applied to Industry from Sa\u0303o Paulo Research Foundation (FAPESP). T.B. thanks Rafael Amatte Bizao for review and comments."}], "title": "Local Interpretation Methods to Machine Learning Using the Domain of the Feature Space", "year": 2019}