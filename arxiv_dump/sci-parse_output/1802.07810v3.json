{"abstractText": "With the increased use of machine learning in decision-making scenarios, there has been a growing interest in creating human-interpretable machine learning models. While many such models have been proposed, there have been relatively few experimental studies of whether these models achieve their intended effects, such as encouraging people to follow the model\u2019s predictions when the model is correct and to deviate when it makes a mistake. We present a series of randomized, pre-registered experiments comprising 3,800 participants in which people were shown functionally identical models that varied only in two factors thought to influence interpretability: the number of input features and the model transparency (clear or black-box). Predictably, participants who were shown a clear model with a small number of features were better able to simulate the model\u2019s predictions. However, contrary to what one might expect when manipulating interpretability, we found no improvements in the degree to which participants followed the model\u2019s predictions when it was beneficial to do so. Even more surprisingly, increased transparency hampered people\u2019s ability to detect when the model makes a sizable mistake and correct for it, seemingly due to information overload. These counterintuitive results suggest that decision scientists creating interpretable models should harbor a healthy skepticism of their intuitions and empirically verify that interpretable models achieve their intended effects.", "authors": [{"affiliations": [], "name": "Forough Poursabzi-Sangdeh"}, {"affiliations": [], "name": "Daniel G. Goldstein"}, {"affiliations": [], "name": "Jake M. Hofman"}, {"affiliations": [], "name": "Jennifer Wortman Vaughan"}, {"affiliations": [], "name": "Hanna Wallach"}], "id": "SP:c3acc44f8b28d46d34feea55398ea8adb10c754b", "references": [{"authors": ["L Russell"], "title": "Ackoff. Management misinformation systems", "venue": "Management Science,", "year": 1967}, {"authors": ["Oscar Alvarado", "Annika Waern"], "title": "Towards algorithmic experience: Initial efforts for social media contexts", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,", "year": 2018}, {"authors": ["Julia Angwin", "Jeff Larson", "Surya Mattu", "Lauren Kirchner"], "title": "Machine bias: There\u2019s software used across the country to predict future criminals. and it\u2019s biased against blacks", "venue": "ProPublica,", "year": 2016}, {"authors": ["Thomas \u00c5stebro", "Samir Elhedhli"], "title": "The effectiveness of simple decision heuristics: Forecasting commercial success for early-stage ventures", "venue": "Management Science,", "year": 2006}, {"authors": ["Douglas Bates", "Martin M\u00e4chler", "Ben Bolker", "Steve Walker"], "title": "Fitting linear mixed-effects models using lme4", "venue": "Journal of Statistical Software,", "year": 2015}, {"authors": ["Max H Bazerman"], "title": "Norms of distributive justice in interest arbitration", "venue": "ILR Review,", "year": 1985}, {"authors": ["James Bennett", "Stan Lanning"], "title": "The netflix prize", "venue": "In Proceedings of KDD Cup and Workshop,", "year": 2007}, {"authors": ["Reuben Binns", "Max Van Kleek", "Michael Veale", "Ulrik Lyngs", "Jun Zhao", "Nigel Shadbolt"], "title": "it\u2019s reducing a human being to a percentage\u2019: Perceptions of justice in algorithmic decisions", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,", "year": 2018}, {"authors": ["Robert C Blattberg", "Stephen J Hoch"], "title": "Database models and managerial intuition", "venue": "manager. Management Science,", "year": 1990}, {"authors": ["Taina Bucher"], "title": "The algorithmic imaginary: exploring the ordinary affects of facebook algorithms", "venue": "Information, Communication & Society,", "year": 2017}, {"authors": ["Michael Buhrmester", "Tracy Kwang", "Samuel D Gosling"], "title": "Amazon\u2019s mechanical turk: A new source of inexpensive, yet high-quality, data", "venue": "Perspectives on psychological science,", "year": 2011}, {"authors": ["Rich Caruana", "Yin Lou", "Johannes Gehrke", "Paul Koch", "Marc Sturm", "Noemie Elhadad"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "venue": "In Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "year": 2015}, {"authors": ["Krista Casler", "Lydia Bickel", "Elizabeth Hackett"], "title": "Separate but equal? a comparison of participants and data gathered via amazon\u2019s mturk, social media, and face-to-face behavioral testing", "venue": "Computers in Human Behavior,", "year": 2013}, {"authors": ["Alexandra Chouldechova"], "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments", "venue": "Big data,", "year": 2017}, {"authors": ["Eric Colson"], "title": "Using human and machine processing in recommendation systems", "venue": "In First AAAI Conference on Human Computation and Crowdsourcing,", "year": 2013}, {"authors": ["Alexander Coppock"], "title": "Generalizing from survey experiments conducted on mechanical turk: A replication approach", "venue": "Political Science Research and Methods,", "year": 2019}, {"authors": ["Jeffrey Dastin"], "title": "Amazon scraps secret AI recruiting tool that showed bias against women", "year": 2018}, {"authors": ["Robyn M Dawes"], "title": "The robust beauty of improper linear models in decision making", "venue": "American psychologist,", "year": 1979}, {"authors": ["Robyn M Dawes", "David Faust", "Paul E Meehl"], "title": "Clinical versus actuarial", "venue": "judgment. Science,", "year": 1989}, {"authors": ["Berkeley J. Dietvorst", "Joseph P. Simmons", "Cade Massey"], "title": "Algorithm aversion: People erroneously avoid algorithms after seeing them err", "venue": "Journal of Experimental Psychology: General,", "year": 2015}, {"authors": ["Berkeley J Dietvorst", "Joseph P Simmons", "Cade Massey"], "title": "Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them", "venue": "Management Science,", "year": 2016}, {"authors": ["Jaap J Dijkstra"], "title": "User agreement with incorrect expert system advice", "venue": "Behaviour & Information Technology,", "year": 1999}, {"authors": ["Jaap J Dijkstra", "Wim BG Liebrand", "Ellen Timminga"], "title": "Persuasiveness of expert systems", "venue": "Behaviour & Information Technology,", "year": 1998}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["Mary T Dzindolet", "Linda G Pierce", "Hall P Beck", "Lloyd A Dawe"], "title": "The perceived utility of human and automated aids in a visual detection", "venue": "task. Human Factors,", "year": 2002}, {"authors": ["Motahhare Eslami", "Sneha R Krishna Kumaran", "Christian Sandvig", "Karrie Karahalios"], "title": "Communicating algorithmic process in online behavioral advertising", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,", "year": 2018}, {"authors": ["Vivian Giang"], "title": "The potential hidden bias in automated hiring systems", "venue": "Accessed at https://www.fastcompany.com/40566971/ the-potential-hidden-bias-in-automated-hiring-systems/,", "year": 2018}, {"authors": ["Gerd Gigerenzer", "Daniel G Goldstein"], "title": "Reasoning the fast and frugal way: models of bounded rationality", "venue": "Psychological review,", "year": 1996}, {"authors": ["Francesca Gino", "Don A. Moore"], "title": "Effects of task difficulty on use of advice", "venue": "Journal of Behavioral Decision Making,", "year": 2007}, {"authors": ["Alyssa Glass", "Deborah L McGuinness", "Michael Wolverton"], "title": "Toward establishing trust in adaptive agents", "venue": "In Proceedings of the 13th International Conference on Intelligent User Interfaces (IUI),", "year": 2008}, {"authors": ["William M Grove", "David H Zald", "Boyd S Lebow", "Beth E Snitz", "Chad Nelson"], "title": "Clinical versus mechanical prediction: a meta-analysis", "venue": "Psychological assessment,", "year": 2000}, {"authors": ["Todd M Gureckis", "Jay Martin", "John McDonnell", "Alexander S Rich", "Doug Markant", "Anna Coenen", "David Halpern", "Jessica B Hamrick", "Patricia Chan"], "title": "psiTurk: An open-source framework for conducting replicable behavioral experiments online", "venue": "Behavior Research Methods,", "year": 2016}, {"authors": ["David J Hand", "William E Henley"], "title": "Statistical classification methods in consumer credit scoring: a review", "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),", "year": 1997}, {"authors": ["Jacob Jacoby"], "title": "Perspectives on information overload", "venue": "Journal of consumer research,", "year": 1984}, {"authors": ["Jongbin Jung", "Connor Concannon", "Ravi Shro", "Sharad Goel", "Daniel G. Goldstein"], "title": "Simple rules for complex decisions", "venue": "arXiv preprint arXiv:1702.04690,", "year": 2017}, {"authors": ["Kevin Lane Keller", "Richard Staelin"], "title": "Effects of quality and quantity of information on decision effectiveness", "venue": "Journal of consumer research,", "year": 1987}, {"authors": ["Jon Kleinberg", "Himabindu Lakkaraju", "Jure Leskovec", "Jens Ludwig", "Sendhil Mullainathan"], "title": "Human decisions and machine predictions", "venue": "The quarterly journal of economics,", "year": 2017}, {"authors": ["Pang Wei Koh", "Percy Liang"], "title": "Understanding black-box predictions via influence functions", "venue": "In Proceedings of the 34th International Conference on Machine Learning (ICML),", "year": 2017}, {"authors": ["Igor Kononenko"], "title": "Machine learning for medical diagnosis: history, state of the art and perspective", "venue": "Artificial Intelligence in medicine,", "year": 2001}, {"authors": ["Josua Krause", "Aritra Dasgupta", "Jordan Swartz", "Yindalon Aphinyanaphongs", "Enrico Bertini"], "title": "A workflow for visual diagnostics of binary classifiers using instance-level explanations", "venue": "In Proceedings of IEEE Conference and Visual Analytics Science and Technology,", "year": 2017}, {"authors": ["Isaac Lage", "Andrew Slavin Ross", "Been Kim", "Samuel J. Gershman", "Finale Doshi-Velez"], "title": "Human-in-the-loop interpretability prior", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["Himabindu Lakkaraju", "Ece Kamar", "Rich Caruana", "Jure Leskovec"], "title": "Interpretable & explorable approximations of black box models", "venue": "In FATML Workshop,", "year": 2017}, {"authors": ["Cynthia CS Liem", "Markus Langer", "Andrew Demetriou", "Annemarie MF Hiemstra", "Achmadnoer Sukma Wicaksana", "Marise Ph Born", "Cornelius J K\u00f6nig"], "title": "Psychology meets machine learning: Interdisciplinary perspectives on algorithmic job candidate screening", "venue": "In Explainable and Interpretable Models in Computer Vision and Machine Learning,", "year": 2018}, {"authors": ["Brian Y Lim", "Anind K Dey"], "title": "Design of an intelligible mobile context-aware application", "venue": "In Proceedings of the 13th international conference on human computer interaction with mobile devices and services,", "year": 2011}, {"authors": ["Brian Y Lim", "Anind K Dey", "Daniel Avrahami"], "title": "Why and why not explanations improve the intelligibility of context-aware intelligent systems", "venue": "In Proceedings of the 2009 CHI Conference on Human Factors in Computing Systems,", "year": 2009}, {"authors": ["Zachary C Lipton"], "title": "The mythos of model interpretability", "venue": "arXiv preprint arXiv:1606.03490,", "year": 2016}, {"authors": ["John DC Little"], "title": "Models and managers: The concept of a decision calculus", "venue": "Management Science,", "year": 1970}, {"authors": ["Jia Liu", "Olivier Toubia"], "title": "A semantic approach for estimating consumer content preferences from online search queries", "venue": "Marketing Science,", "year": 2018}, {"authors": ["Jennifer M. Logg"], "title": "Theory of machine: When do people rely on algorithms", "venue": "Harvard Business School NOM Unit Working Paper No", "year": 2017}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke"], "title": "Intelligible models for classification and regression", "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "year": 2012}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke", "Giles Hooker"], "title": "Accurate intelligible models with pairwise interactions", "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "year": 2013}, {"authors": ["Scott Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Winter Mason", "Siddharth Suri"], "title": "Conducting behavioral research on amazon\u2019s mechanical turk", "venue": "Behavior research methods,", "year": 2012}, {"authors": ["Rishabh Mehrotra", "James McInerney", "Hugues Bouchard", "Mounia Lalmas", "Fernando Diaz"], "title": "Towards a fair marketplace: Counterfactual evaluation of the trade-off between relevance, fairness & satisfaction in recommendation systems", "venue": "In Proceedings of the 27th ACM International Conference on Information and Knowledge Management,", "year": 2018}, {"authors": ["Tim Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "Artificial Intelligence,", "year": 2018}, {"authors": ["Tim Miller", "Piers Howe", "Liz Sonenberg"], "title": "Explainable AI: Beware of inmates running the asylum", "venue": "arXiv preprint arXiv:1712.00547,", "year": 2017}, {"authors": ["Safiya Umoja Noble"], "title": "Algorithms of Oppression: How search engines reinforce racism", "year": 2018}, {"authors": ["Dilek \u00d6nkal", "Paul Goodwin", "Mary Thomson", "Sinan G\u00f6n\u00fcl"], "title": "The relative influence of advice from human experts and statistical methods on forecast adjustments", "venue": "Journal of Behavioral Decision Making,", "year": 2009}, {"authors": ["Gabriele Paolacci", "Jesse Chandler"], "title": "Inside the turk: Understanding mechanical turk as a participant pool", "venue": "Current Directions in Psychological Science,", "year": 2014}, {"authors": ["Marianne Promberger", "Jonathan Baron"], "title": "Do patients trust computers", "venue": "Journal of Behavioral Decision Making,", "year": 2006}, {"authors": ["Emilee Rader", "Rebecca Gray"], "title": "Understanding user beliefs about algorithmic curation in the facebook news feed", "venue": "In Proceedings of the 2015 CHI Conference on Human Factors in Computing Systems,", "year": 2015}, {"authors": ["Emilee Rader", "Kelley Cotter", "Janghee Cho"], "title": "Explanations as mechanisms for supporting algorithmic transparency", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,", "year": 2018}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should I trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "year": 2016}, {"authors": ["Christian Rudder"], "title": "Dataclysm: Love, Sex, Race, and Identity\u2013What Our Online Lives", "venue": "Tell Us about Our Offline Selves. Crown,", "year": 2014}, {"authors": ["Cynthia Rudin", "Berk Ustun"], "title": "Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice", "venue": "Applied Analytics,", "year": 2018}, {"authors": ["Paul JH Schoemaker", "C Carter Waid"], "title": "An experimental comparison of different approaches to determining weights in additive utility models", "venue": "Management Science,", "year": 1982}, {"authors": ["Richard Tomsett", "David Braines", "Dan Harborne", "Alun Preece", "Supriyo Chakraborty"], "title": "Interpretable to whom? A role-based model for analyzing interpretable machine learning systems", "venue": "Workshop on Human Interpretability in Machine Learning,", "year": 2018}, {"authors": ["Amos Tversky", "Daniel Kahneman"], "title": "Judgment under uncertainty", "venue": "Heuristics and biases. science,", "year": 1974}, {"authors": ["Berk Ustun", "Cynthia Rudin"], "title": "Supersparse linear integer models for optimized medical scoring systems", "venue": "Machine Learning Journal,", "year": 2016}, {"authors": ["Sara Wachter-Boettcher"], "title": "Why you can\u2019t trust ai to make unbiased hiring decisions", "venue": "Accessed at http://time", "year": 2017}, {"authors": ["Martin Wattenberg", "Fernanda Vi\u00e9gas", "Moritz Hardt"], "title": "Attacking discrimination with smarter machine learning", "venue": "Accessed at https://research.google.com/bigpicture/ attacking-discrimination-in-ml/,", "year": 2016}, {"authors": ["Ilan Yaniv"], "title": "Receiving other people\u2019s advice: Influence and benefit", "venue": "Organizational Behavior and Human Decision Processes,", "year": 2004}], "sections": [{"heading": "1 Introduction", "text": "Machine learning models are used to make decisions in critical domains like medical diagnosis [39], credit risk assessment [33], judicial sentencing and bail [3, 14, 35, 37], and hiring [43]. In addition, these models influence decisions about what news people see on social media platforms (e.g., Twitter and Facebook, [2, 10, 48, 61]), provide recommendations for what movies to watch (e.g., Netflix, [7]) or what music to listen to on online streaming platforms (e.g., Spotify, [54]), help people shop for clothing (e.g., Stitch Fix, [15]), and recommend whom to date on online dating platforms (e.g., OKCupid, [64]). More and more often, humans and machines decide jointly where humans used to decide alone. Because of the impact that model-aided decision-making has on business and policy outcomes, the importance of these interactions has been emphasized in the management science literature for decades [9, 47].\nMachine learning models are often evaluated based on their predictive performance on held-out data sets, measured, for example, in terms of accuracy, precision, or recall. On these metrics, aggregated over many studies, machines often have\nar X\niv :1\n80 2.\n07 81\n0v 3\n[ cs\n.A I]\n8 N\nthe advantage over people [31]. However, good performance on held-out data alone may not be sufficient to convince decision makers that a model is trustworthy, reliable, justifiable, ethical, or fair in practice. Historically, studies have shown that people are resistant to allowing decisions to be made entirely by algorithms [6, 18, 20, 60], though they can be supportive of using algorithmic decision aids provided they are first given the opportunity to review and potentially modify the predictions [21]. Recently, this resistance has been amplified by the difficulty of understanding how a potentially complex model has made a decision or how this decision can be improved. For example, it has been argued that certain blackbox models are systematically unfair [27, 57, 70], sometimes causing managers to refrain from deploying them [17].\nTo address this problem as well as the General Data Protection Regulation\u2019s requirements to provide \u201cmeaningful information about the logic involved\u201d in decisions made by algorithms, a new line of research has emerged that focuses on developing interpretable decision-making models. There are two common approaches. Since there is evidence that simple, transparent models can be as accurate as more complex, black-box models [4, 18, 28, 35, 65, 66], the first approach is to create inherently simple models, such as point systems that can be memorized [35, 69] or generative additive models in which the impact of each feature on the model\u2019s prediction should be relatively easy to understand [12, 50, 51]. The second is to provide post-hoc explanations for (potentially complex) models. One thread of research in this direction looks at how to explain individual predictions by learning simple local approximations of a model around particular data points [42, 52, 63] or estimating the influence of training examples [38], while another focuses on visualization of model output or properties [40, 71]. Both simple models and post-hoc explanations are attempts at creating more interpretable decisions aids.\nBut what is interpretability? Despite the progress in this area, there is still no consensus about how to define, quantify, or measure the interpretability of a machine learning model [24]. Different notions of interpretability, such as simulatability, trustworthiness, and simplicity, are often conflated [46]. This problem is exacerbated by the fact that there are different types of users of machine learning systems and these users may have different needs in different scenarios [67]. The approach that works best for a regulator who wants to understand why a particular person was denied a loan may be different from the approach that works best for a data scientist debugging a machine learning model or a manager using a model to make a business decision.\nWe take the perspective that the difficulty in assessing the value of interpretability stems from the fact that interpretability is not something that can be directly manipulated or measured. Rather, interpretability is a latent property that can be influenced by different manipulable factors (such as the number of features, the complexity of the model, the transparency of the model, or even the user interface) and that impacts different measurable outcomes (such as people\u2019s tendency to follow the model\u2019s predictions or people\u2019s ability to simulate or debug the model\u2019s predictions). Different factors may influence these outcomes in different ways. As such, we argue that to understand interpretability, it is necessary to directly manipulate and measure the influence that different factors have on people\u2019s abilities to complete various tasks. What is or is not \u201cinterpretable\u201d should be defined by user behavior, not by what appeals to the modeler\u2019s intuition [55, 56].\nWe build on decision-making research on human trust in algorithmic (also called actuarial or mechanical) decision making [19\u201321, 49, 58, 60, 73] and similar efforts in computer science [8, 26, 30, 41, 44, 45, 62, 63] and present a sequence of pre-registered experiments (N = 3, 800) in which we vary factors thought to make models more or less interpretable and measure how these changes impact people\u2019s decision making. Based on a structured review of the literature on effective decision aids and interpretable machine learning, we focus on two factors that are often assumed to influence interpretability, but rarely studied formally\u2014the number of features (i.e., predictors) and the model transparency (i.e., whether the model internals are clear or a black box)\u2014and investigate how these factors affect three outcomes commonly mentioned in the literature: simulatability, deviation, and error detection. Specifically, we look at the following questions:\n\u2022 How well can people estimate what a model will predict? Here we find that people can better simulate a model\u2019s predictions when presented with a clear model with few features compared to the other experimental conditions. \u2022 To what extent do people follow a model\u2019s predictions when it is beneficial to do so? Contrary to what one might expect when manipulating factors associated with interpretability, we do not find a significant improvement in the degree to which people follow the predictions of a clear model with few features compared to the other experimental conditions. \u2022 How well can people detect when a model has made a sizable mistake? Even more surprisingly, we find that transparency can hamper people\u2019s ability to detect when a model makes serious mistakes.\nIn each of our experiments, participants were asked to predict the prices of apartments in a single neighborhood in New York City with the help of a machine learning model. We conducted our experiments on laypeople, as they represent a large class of individuals who might potentially use or be impacted by machine learning models, and chose the task of predicting housing prices because many people have considered purchasing a house or apartment, making the setting both familiar and potentially interesting to participants. Each apartment was represented in terms of eight features: number of bedrooms, number of bathrooms, square footage, total rooms, days on the market, maintenance fee, distance from the subway, and distance from a school. All participants saw the same set of apartments (i.e., the same feature values) and, crucially, the same model prediction for each apartment, which came from a linear regression model. What varied between the experimental conditions was only the presentation of the model. As a result, any observed differences in the participants\u2019 behavior between conditions could be attributed entirely to the model presentation. This is a key feature of our experimental design.\nIn our first experiment (Section 2), we tested people on a sequence of 12 apartments. The first 10 apartments had typical configurations whereas the last two had unusual combinations of features (more bathrooms than bedrooms). For each apartment, participants first saw its features alongside the model and were asked to estimate what the model would predict for the apartment\u2019s selling price. They were then shown the model\u2019s prediction and asked for their own estimate of what the apartment sold for.\nWe hypothesized that participants who were shown a clear model with a small number of features would be better able to simulate the model\u2019s predictions and be less likely to deviate from (that is, more likely to follow) the model\u2019s predictions. We also hypothesized that the assigned conditions would affect participants\u2019 abilities to correct the model\u2019s inaccurate predictions about the unusual apartments. As predicted, participants who were shown a clear model with a small number of features were better able to simulate the model\u2019s predictions. However, we did not find that people followed the clear, simple model more closely than the complex, opaque model. This is of concern because people\u2019s predictions were less accurate than the model\u2019s, a familiar finding in the literature on human versus statistical prediction [19, 31]. Even more surprisingly, participants who were shown a clear model were less likely to correct the model\u2019s inaccurate predictions on the unusual apartments. To account for these unexpected results, we ran three additional experiments.\nIn our second experiment (Section 3), we scaled down the apartment prices and maintenance fees to match median housing prices in the U.S. in order to determine whether the findings from our first experiment were merely an artifact of New York City\u2019s high prices. With scaled-down prices and fees, the findings from our first experiment replicated quite closely.\nOur third experiment (Section 4) was designed to measure weight of advice\u2014commonly used in the literature on advice-taking [29, 72] and subsequently used in the context of algorithmic predictions by Logg [49]\u2014to quantify the extent to which people update their belief towards advice they are given. We calculated weight of advice by eliciting two predictions from participants: one before being introduced to the model and one after. Similar to deviation, we found\nno meaningful difference in weight of advice between the clear, two-feature model and the black-box, eight-feature model. Interestingly, however, participants in the clear conditions were better able to correct the model\u2019s mistakes than in the previous two experiments. One possible reason for this improvement is that making predictions before being exposed to the clear model\u2019s overwhelming level of detail increased the likelihood that participants noticed unusual combinations of features and adjusted their estimates accordingly.\nThis motivated our fourth and final experiment (Section 5), which focused on whether the observed differences in error detection between conditions could be explained by people in the clear conditions failing to notice the configurations of the unusual apartments due to information overload. We returned to the setup from the first experiment but added an \u201coutlier focus\u201d condition in which some people were shown additional text highlighting apartments with peculiar configurations as outliers. Without this manipulation we again found that participants who were shown a clear model were less likely to correct inaccurate predictions on examples for which the model made a sizable mistake. However, this difference essentially disappeared for participants in the outlier focus condition, consistent with the idea that transparency can be overwhelming and cause users to overlook unusual cases. Seeing this effect, we revisited the results of our first two experiments with an eye towards information overload in a post-hoc (i.e., not pre-registered) analysis. We found that the condition that provides users with the most information (CLEAR-8) scored worst on simulation error, deviation, and prediction error compared to the other conditions, suggesting that it may be detrimental to present people with models that are both transparent and complex. We present these results in the discussions of Experiments 1 and 2.\nIn summary, across several experiments involving several thousand participants we saw that factors thought to improve interpretability often had negligible effects and even had detrimental effects in certain instances. While one might expect that exposing model internals by default could only help decision makers, our experiments suggest otherwise. Taken together, these results emphasize the importance of user testing over intuition in the design of interpretable models to assist decision making.\nIn the remainder of the paper we provide further details for each of these experiments and present our results in greater depth. We conclude by discussing limitations of and possible extensions to our work, as well as implications for the design of interfaces to decision aids."}, {"heading": "2 Experiment 1: Predicting prices", "text": "Our first experiment was designed to measure the influence of the number of features and model transparency (clear or black box) on three behaviors that our literature search revealed to be commonly associated with interpretability: laypeople\u2019s abilities to predict (or simulate) the model\u2019s predictions, follow a model\u2019s prediction when it would be beneficial to do so, and detect when a model makes mistakes. Before running the experiment, we posited and pre-registered three hypotheses, stated informally here:1\nH1. Simulation. A clear model with a small number of features will be easiest for participants to simulate. H2. Deviation. On typical examples participants will be more likely to follow (that is, less likely to deviate from) the\npredictions of a clear model with a small number of features than the predictions of a black-box model with a large number of features. H3. Detection of mistakes. Participants in different conditions will exhibit varying abilities to correct the model\u2019s inaccurate predictions on unusual examples.\n1Pre-registered hypotheses are available at https://aspredicted.org/xy5s6.pdf.\nWe test the first hypothesis by showing people an apartment configuration, asking them to estimate what the model will predict for its selling price, and comparing this estimate with the model\u2019s prediction. A small difference between these two quantities would indicate that people have a good understanding of how the model works. For the second hypothesis, we quantify the degree to which participants follow a model by showing them the model\u2019s prediction for each apartment, asking them to estimate the actual selling price, and measuring the deviation between this estimate and the model\u2019s prediction.\nWe use the same deviation measure for the third hypothesis, but applied to unusual apartments for which the model makes erroneously high predictions. Here, a large deviation from the model\u2019s predictions would imply that people are able to correct the model\u2019s mistakes.\nFor the unusual examples, we did not make directional predictions about which conditions would make participants more or less able to correct inaccurate predictions. On the one hand, if a participant understands the model better, she may be better equipped to correct examples on which the model makes mistakes. On the other hand, a participant may follow a model more if she understands how it arrives at its predictions.\nWe additionally pre-registered our intent to analyze participants\u2019 prediction error in each condition, but intentionally refrained from making directional predictions."}, {"heading": "2.1 Experimental design", "text": "As explained in the previous section, we asked participants to predict apartment prices with the help of a machine learning model. We manipulated the presentation of the model in a 2\u00d7 2 design:\n\u2022 Participants were randomly assigned to see either a model that uses only two features (number of bathrooms and square footage\u2014the two most predictive features) or a model that uses all eight features. \u2022 Participants were randomly assigned to either see the model internals (i.e., a linear regression model with visible coefficients) or see the model as a black box.\nWe showed all participants the same set of apartments and the same model prediction for each apartment, regardless of their randomly assigned experimental condition. This key feature of our experimental design is what enabled us to run tightly controlled experiments. The only thing that varied between the primary conditions was model presentation.\nScreenshots from each of the four primary experimental conditions are shown in Figure 1. Note that all eight feature values were visible to participants in all conditions. Furthermore, the accuracies of the two- and eight-feature models were nearly identical, as described below, and their (rounded) predictions were identical on all apartments used in the experiment (see Appendix .2). We additionally included a baseline condition in which there was no model available.\nWe ran the experiment on Amazon Mechanical Turk using psiTurk [32], an open-source platform for designing online experiments and the experiment was IRB-approved. We recruited 1,250 participants, all located in the U.S., with Mechanical Turk approval ratings greater than 97%.2 The participants were randomly assigned to the five conditions (CLEAR-2, n = 248; CLEAR-8, n = 247; BB-2, n = 247; BB-8, n = 256; and NO-MODEL, n = 252) and each participant received a flat payment of $2.50.\n2Multiple studies show that data from high reputation Mechanical Turk workers are comparable to data from commercial panels and university pools when assessing factors such as attentiveness, honesty, and effort [11, 13, 16, 53, 59].\nParticipants were first shown detailed instructions, including, in the clear conditions, a simple English description of the corresponding two- or eight-feature linear regression model (Appendix .1). To be sure they understood these instructions, participants were required to answer a multiple choice question on the number of features used by the model before proceeding with the experiment in two phases. The training phase familiarized participants with both the housing domain and the model\u2019s predictions. Participants were shown ten apartments in a random order. In the four primary experimental conditions, participants were shown the model\u2019s prediction of each apartment\u2019s price, asked to make their own prediction, and then shown the apartment\u2019s actual price. In the baseline condition, participants were asked to predict the price of each apartment and then shown the actual price.\nIn the testing phase, participants were shown twelve apartments they had not previously seen. The order of the first ten was randomized, while the remaining two always appeared last, for reasons described below. In the four primary experimental conditions, participants were asked to guess what the model would predict for each apartment (i.e., simulate the model) and to indicate how confident they were in this guess on a five-point scale (Figure 2a). They were\nthen shown the model\u2019s prediction and asked to indicate how confident they were that the model was correct (Figure 2b). Finally, they were asked to make their own prediction of the apartment\u2019s price and to indicate how confident they were in this prediction (Figure 2c). In the baseline condition, participants were asked to predict the price of each apartment and to indicate their confidence.\nThe apartments shown to participants were selected from a data set of apartments sold between 2013 and 2015 on the Upper West Side of New York City, and were taken from StreetEasy.com, a popular real estate website. To create the models for the four primary experimental conditions, we first trained a two-feature linear regression model on our data set, rounding coefficients for readability.3 To keep the models as similar as possible, we fixed the coefficients for number of bathrooms and square footage and the intercept of the eight-feature model to match those of the two-feature model, and then trained a linear regression model with the remaining six features, following the same rounding procedure. The models explain 82% of the variance in the apartment prices. When presenting the model predictions to participants, we rounded predictions to the nearest $100,000. The model coefficients are shown in Figure 1.\nWe used the same set of apartments for each participant since randomizing the selection of apartments would introduce additional noise and reduce the power of the experiments, making it harder to spot differences between conditions. To enable comparisons across experimental conditions, the ten apartments used in the training phase and the first ten apartments used in the testing phase were selected from those in our data set for which the rounded predictions of the two- and eight-feature models agreed. They were chosen to cover a representative range of the models\u2019 prediction errors. Details on the apartment configurations and how they were selected are provided in Appendix .2.\nThe last two apartments used in the testing phase were chosen to test our third hypothesis, namely, that participants will differ in how much they deviate from the model\u2019s inaccurate predictions on unusual examples. To test this hypothesis, we would ideally have used an apartment with strange or misleading features that caused the two- and eight-feature models to make the same bad prediction. Unfortunately, there was no such apartment in our data set, so we chose two examples to test different aspects of our hypothesis. Both of these examples exploited the models\u2019 large coefficient ($350,000) for the number of bathrooms. The first (apartment 11) was a one-bedroom, two-bathroom apartment from our data set for which both models made high, but different, predictions. Comparisons between the two- and eightfeature conditions were therefore impossible, but we could examine differences in prediction error between the clear and black-box conditions. The second (apartment 12) was a synthetically generated one-bedroom, three-bathroom apartment for which both models made the same (high) prediction, allowing comparisons between all conditions, but ruling out prediction error comparisons since there was no ground truth. These apartments were always shown last to avoid the phenomenon in which people trust a model less after seeing it make a mistake [20]."}, {"heading": "2.2 Results", "text": "3For each estimated coefficient, we found a round number that was within one quarter of a standard error of the estimate.\nExperiment 1: New York City prices\nExperiment 2: Representative U.S. prices\nHaving run our experiment, we compared participants\u2019 behavior across the conditions.4 Doing so required us to compare multiple responses from multiple participants, which was complicated by possible correlations among any given participant\u2019s responses. For example, some people might consistently overestimate apartment prices regardless of the condition they are assigned to, while others might consistently provide underestimates. We addressed this by fitting a mixed-effects model for each measure of interest to capture differences across conditions while controlling for participant-level effects\u2014a standard approach for analyzing repeated measures experimental designs [5]. We derived all statistical tests from these models. Bar plots and the mean outcomes in the density plots show averages (\u00b1 one standard error) by condition from the fitted models. To test for our specific hypotheses, we ran contrasts and report degrees of freedom, test statistics, and p-values under these models. Unless otherwise noted, all plots and statistical tests correspond to just the first ten apartments from the testing phase. The results corresponding to the pre-registered hypotheses are as follows:\nH1. Simulation. We defined a participant\u2019s simulation error to be |m\u2212 um|, the absolute deviation between the model\u2019s prediction m and the participant\u2019s guess for that prediction um. Figure 3a shows the simulation error in the testing phase. Participants in the CLEAR-2 condition had lower simulation error, on average, than participants in the other conditions (t(994) = \u221212.06, p < 0.001). This indicates that, as hypothesized, participants who saw a clear model with a small number of features were better able to predict what the model would predict.\nH2. Deviation. We quantified a participant\u2019s deviation from the model as |m\u2212 ua|, the absolute difference between the model\u2019s prediction m and the participant\u2019s prediction of the apartment\u2019s price ua. Smaller values indicate that participants follow the model\u2019s predictions more closely. Figure 3b shows that contrary to our second hypothesis, we found no significant difference in participants\u2019 deviation from the model between CLEAR-2 and BB-8 (t(994) = 0.67, p = 0.5).\nH3. Detection of mistakes. We used the last two apartments in the testing phase (apartments 11 and 12) to test our third hypothesis. The models err and make overly high predictions on these examples because they do not consider the ratio of the number of bathrooms to the number of bedrooms. For both apartments participants in the four primary conditions overestimated the apartments\u2019 prices compared to participants in the baseline condition. We suspect that this is due to an anchoring effect around the models\u2019 predictions. For apartment 11, we found no significant difference in participants\u2019 deviation from the model\u2019s prediction between the four primary conditions (F (3, 994) = 1.03, p = 0.379 under a one-way ANOVA). For apartment 12, a one-way ANOVA revealed a significant difference between the four primary conditions (F (3, 994) = 4.42, p = 0.004). Participants in the clear conditions deviated from the model\u2019s prediction less, on average, than participants in the black-box conditions (F (1, 994) = 8.81 , p = 0.003 for the main effect of model transparency, see Figure 5a), resulting in even worse final predictions of the apartment\u2019s price. Participants\u2019 poor abilities to detect the model\u2019s mistakes in clear conditions contradicts the common intuition that transparency enables users to detect when a model is making a mistake. We explore this finding in more detail later.\nWe additionally performed some post-hoc analyses. First, we checked participants\u2019 self-reported confidence in the model\u2019s predictions. Though this was not a primary outcome that we pre-registered a hypothesis for, it showed an interesting difference between participants\u2019 stated confidence and revealed behavior. Specifically, even though participants reported that they were more confident in the predictions of the model in the CLEAR-2 condition than in the BB-8 condition (a difference of .25 on average on a five-point scale from \u201cI\u2019m confident the model got it wrong\u201d to \u201cI\u2019m confident the model got it right\u201d, t(994) = 4.27, p < 0.001), their decisions did not reflect this. Predictions in\n4For all of our experiments, we report all sample sizes, conditions, data exclusions, and measures for the main analyses mentioned in the pre-registration documents. We determined the sample size for our first experiment based on estimates from a small pilot experiment to enable detecting a difference of at least $50,000 in deviation between the CLEAR-2 and BB-8 conditions with 80% power. For the subsequent experiments, we adjusted the sample size to target a power of 80% or more. Full distributions of collected responses are in Appendix .3 and details of statistical tests are in Appendix .4. All data and code will be made available upon publication.\nboth conditions deviated from the model by the same amount, on average.\nThe second post-hoc analysis concerned prediction error, which we measured as |a\u2212 ua|, the absolute difference between the apartment\u2019s actual price a, and the participant\u2019s prediction of the apartment\u2019s price, ua. Figure 6a shows that on apartments with a normal configuration, participants in the primary conditions had higher average error than the model itself, but fared better than those in the baseline condition (t(1245) = 15.28, p < 0.001 for the comparison of the baseline with the four primary conditions), indicating that the use of a model is highly advantageous in this setting. On the other hand, participants\u2019 prediction error on apartment 11 revealed a reversed pattern: participants in the primary conditions had lower prediction error than the model but higher prediction error than those in the baseline condition (t(1245) = \u22127.99, p < 0.001). Having a model helped for typical apartments but was detrimental on an apartment with an unusual configuration for which the model made a highly inaccurate prediction. Transparency further hampered participants\u2019 prediction quality: participants who were shown a clear model made worse predictions of the apartment\u2019s selling price than those who were shown a black-box model (F (1, 994) = 31.98, p < 0.001 for the main effect of model transparency under a two-way ANOVA).\nAs predicted, we found that participants who were shown a clear model with a small number of features were better able to simulate the model\u2019s predictions. However, we did not find that they followed the model\u2019s predictions more closely when it would have been beneficial to do so. We also found that, contrary to intuition, participants who were shown a clear model were less able to correct the model when it made a mistake. Finally, we found no meaningful difference in prediction error between the four primary conditions. We found that participants who had the help of a model were more accurate than those in the baseline condition who did not. Having a model mattered but participants would have been better off simply following the model, on average, than adjusting its predictions.\nExperiment 1: New York City prices\nExperiment 2: Representative U.S. prices"}, {"heading": "3 Experiment 2: Scaled-down prices", "text": "One potential critique of our first experiment is that participants\u2019 lack of familiarity with New York City\u2019s unusually high apartment prices may influence their willingness to follow the model and ability to detect when the model makes a mistake. Our second experiment was designed as a robustness check to address this potential concern by replicating our first experiment with apartment prices and maintenance fees scaled down to match median housing prices in the U.S. Before running this experiment we pre-registered three hypotheses. The first two hypotheses (H4 and H5) are identical to H1 and H2 from the first experiment. We made the third hypothesis (H6) more precise than H3 to reflect the results of the first experiment and the results of a small pilot with scaled-down prices:\nH6. Detection of mistakes. Participants will be less likely to correct inaccurate predictions on unusual examples of a clear model compared to a black-box model, and this effect will be more prominent the more unusual an example is."}, {"heading": "3.1 Experimental design", "text": "We first scaled down the apartment prices and maintenance fees from our first experiment by a factor of ten. To account for this change, we also scaled down all regression coefficients (except for the coefficient for maintenance fee) by a factor of ten. Apart from the description of the neighborhood from which the apartments were selected, the experimental design was unchanged. We again ran the experiment on Amazon Mechanical Turk. We excluded people who had participated in our first experiment, and recruited 750 new participants all of whom satisfied the selection criteria from the first experiment. The participants were randomly assigned to the five conditions (CLEAR-2, n = 150; CLEAR-8, n = 150; BB-2, n = 147; BB-8, n = 151; and NO-MODEL, n = 152) and each participant received a flat payment of $2.50."}, {"heading": "3.2 Results", "text": "The results of this experiment closely replicated those of Experiment 1.\nH4. Simulation. As hypothesized, and shown in Figure 4a, participants in the CLEAR-2 condition had significantly lower simulation error, on average, than participants in the other primary conditions (t(594) = \u221210.41, p < 0.001). This is in line with the finding from the first experiment.\nH5. Deviation. In line with the first experiment, and contrary to our original hypothesis, we found no significant difference in participants\u2019 deviation from the model on typical apartments between CLEAR-2 and BB-8 (t(594) = 0.49, p = 0.626, see Figure 4b).\nH6. Detection of mistakes. For apartment 12, as hypothesized and in line with our findings from Experiment 1, a one-way ANOVA revealed a significant difference across the four primary conditions (F (3, 594) = 7.96, p < 0.001). In particular, participants in the clear conditions deviated from the model\u2019s prediction less, on average, than participants in the black-box conditions for apartment 12, resulting in even worse final predictions of the apartment\u2019s price (t(594) = \u22124.16, p < 0.001, see Figure 5b).\nFor apartment 11, while a one-way ANOVA revealed a significant difference across the four primary conditions (F (3, 594) = 3.00 , p = 0.03), as in Experiment 1, we found no significant difference in participants\u2019 deviation from the model\u2019s prediction between the clear and black-box conditions (t(594) = \u22121.82, p = 0.069), perhaps because the configuration is not sufficiently unusual. These findings both suggest that New York City\u2019s high apartment prices do not explain participants\u2019 poor abilities to correct inaccurate predictions.\nAs in Experiment 1, we conducted some post-hoc analyses. For self-reported confidence, unlike in Experiment 1, we found no significant difference between CLEAR-2 and BB-8 (t(594) = 1.03, p = 0.303). We note that the effect size of this difference in Experiment 1 was small (Cohen\u2019s d of 0.23) and even smaller in Experiment 2 (Cohen\u2019s d of 0.07), which was a nearly identical experiment except for the prices. We also note that despite the difference in self-reported confidence, there was no significant difference in deviation from the model in either experiment.\nResults for prediction error are in line with those in Experiment 1. For the typical apartments, participants in the primary conditions had higher average error than the model itself, but fared better than participants in the baseline condition (t(745) = 10.62, p < 0.001). A one-way ANOVA revealed a significant difference in prediction error across the four primary conditions (F (3, 594) = 8.60, p < 0.001). Figure 6b shows that the prediction error in the CLEAR-8 condition is particularly high. Looking at the results from Experiment 1, we see a similar pattern: prediction error in the\nCLEAR-8 condition was higher than in the other three primary conditions (t(994) = 2.68, p = 0.007 for Experiment 1, t(594) = 4.78, p < 0.001 for Experiment 2, see Figure 6).\nContrary to the case with typical apartments, having a model was detrimental for predicting the price of the atypical apartment 11. Participants in the primary conditions made better predictions than the model but worse predictions than those in the baseline condition (t(745) = \u22126.41, p < 0.001). As in Experiment 1, transparency further hampered people\u2019s predictions on apartment 11: participants who were shown a clear model made worse predictions of the apartment\u2019s selling price than those who were shown a black-box model (t(594) = 2.63, p = 0.008 for the contrast of CLEAR-2 and CLEAR-8 with BB-2 and BB-8). These findings, coupled with the difficulty that people had in simulating the CLEAR-8 model compared to the other conditions (t(994) = 7.96, p < 0.001 for Experiment 1, t(594) = 7.23, p < 0.001 for Experiment 2, see Figures 3a and 4a), and the higher deviation of participants\u2019 predictions from the model\u2019s predictions in the CLEAR-8 condition compared to other conditions (t(994) = 2.37, p = 0.018 for Experiment 1, t(594) = 2.49, p = 0.012 for Experiment 2, see Figures 3b and 4b), suggest potential downsides of decision aids that are both transparent and complex.\nExperiment 2 replicated the results of Experiment 1, suggesting that they were not an artifact of participants\u2019 lack of familiarity with New York City\u2019s high prices. Both experiments showed that participants who used a simple, transparent model were better able to simulate the model\u2019s predictions. However, they were not more inclined to follow these predictions, or to detect when the model was making a sizable mistake."}, {"heading": "4 Experiment 3: Weight of Advice", "text": "In our first two experiments, participants were no more likely to follow the predictions of a clear model with a small number of features than the predictions of a black-box model with a large number of features, as indicated by the deviation of their own predictions from the model\u2019s prediction. However, perhaps another measure would reveal differences between the conditions. In this section, we therefore present our third experiment, which was designed to allow us to compare a measure frequently used in the literature on advice-taking (weight of advice, [29, 49, 72]) across the conditions.\nWeight of advice quantifies the degree to which people update their beliefs (e.g., predictions made before seeing the model\u2019s predictions) toward advice they are given (e.g., the model\u2019s predictions). In the context of our experiment, it is defined as |u2 \u2212 u1| / |m\u2212 u1|, where m is the model\u2019s prediction, u1 is the participant\u2019s initial prediction of the apartment\u2019s price before seeing m, and u2 is the participant\u2019s final prediction of the apartment\u2019s price after seeing m. It is equal to 1 if the participant\u2019s final prediction matches the model\u2019s prediction and equal to 0.5 if the participant averages their initial prediction and the model\u2019s prediction.\nTo understand the benefits of comparing weight of advice across the conditions, consider the scenario in which a participant\u2019s final prediction u2 is close to the model\u2019s prediction m. There are different reasons why this might happen. On the one hand, it could be the case that the initial prediction u1 was far from m and the participant made a significant update to their initial prediction based on the model. On the other hand, it could be the case that the initial prediction u1 was already close to m and the participant did not update her prediction at all. These two scenarios are indistinguishable in terms of the participant\u2019s deviation from the model\u2019s prediction. In contrast, weight of advice would be high in the first case and low in the second.\nWe additionally used this experiment to check whether participants\u2019 behavior would differ if they were told that the\npredictions were made by a \u201chuman expert\u201d instead of a model. Previous studies have examined this question from different perspectives with differing results [20, 22, 23, 25, 58]. Most closely related to our experiment is work by Logg [49], which found that when people have no information about the quality of predictions shown to them, they follow the predictions that appear to come from an algorithm more than those that appear to come from a human. We were curious to see if this finding extended to a setting where people were first given the chance to gauge the quality of predictions before deciding how closely to follow them. We pre-registered four hypotheses:5\nH7. Deviation. Participants\u2019 predictions will deviate less from the predictions of a clear model with a small number of features than the predictions of a black-box model with a large number of features. H8. Weight of advice. Weight of advice will be higher for participants who see a clear model with a small number of features than for those who see a black-box model with a large number of features. H9. Humans vs. machines. Participants\u2019 deviation and weight of advice measures will differ depending on whether the predictions come from a black-box model with a large number of features or a human expert. H10. Detection of mistakes. Participants in different conditions will exhibit varying abilities to correct the model\u2019s inaccurate predictions on unusual examples.\nThe first two hypotheses are variations on H2 from our first experiment, while the last hypothesis is identical to H3.\n5Pre-registered hypotheses are available at https://aspredicted.org/795du.pdf."}, {"heading": "4.1 Experimental design", "text": "We returned to using the original New York City housing prices and used the same four primary experimental conditions as in the first two experiments plus a new condition, EXPERT, in which participants saw the same information as in BB-8, but with the black-box model labeled as \u201cHuman Expert\u201d instead of \u201cModel.\u201d We did not include a baseline condition because the most natural baseline would have been to simply ask participants to predict apartment prices (i.e., the first step of the testing phase described below).\nWe again ran the experiment on Amazon Mechanical Turk. We excluded people who had participated in our first two experiments, and recruited 1,000 new participants all of whom satisfied the selection criteria from our first two experiments. The participants were randomly assigned to the five conditions (CLEAR-2, n = 202; CLEAR-8, n = 200; BB-2, n = 202; BB-8, n = 198; and EXPERT, n = 197) and each participant received a flat payment of $1.50. We excluded data from one participant who reported technical difficulties.\nWe asked participants to predict apartment prices for the same set of apartments used in the first two experiments. However, in order to calculate weight of advice, we modified the experiment so that participants were asked for two predictions for each apartment during the testing phase: an initial prediction before being shown the model\u2019s prediction and a final prediction after being shown the model\u2019s prediction. To ensure that participants\u2019 initial predictions were not influenced by the condition they were assigned to, we asked for their initial predictions for all twelve apartments before introducing them to the model or human expert and before informing them that they would be able to update their predictions.6\nParticipants were first shown detailed instructions (which intentionally did not include any information about the corresponding model or human expert), before proceeding with the experiment in two phases. In the (short) training phase, participants were shown three apartments, asked to predict each apartment\u2019s price, and shown the apartment\u2019s actual price. The testing phase consisted of two steps. In the first step, participants were shown another twelve apartments. The order of all twelve apartments was randomized. Participants were asked to predict the price of each apartment (Figure 7a). In the second step, participants were introduced to the model or human expert before revisiting the twelve apartments (Figure 7b). As in the first two experiments, the order of the first ten apartments was randomized, while the remaining two (apartments 11 and 12) always appeared last. For each apartment, participants were first reminded of their initial prediction, next shown the model or expert\u2019s prediction, and only then asked to make their final prediction of the apartment\u2019s price. To simplify the experiment, we did not ask for participants\u2019 confidence in their and the model\u2019s predictions.\nExperiment 3: Weight of advice"}, {"heading": "4.2 Results", "text": "H7. Deviation. Contrary to the hypothesis, and in line with the findings from the first two experiments, there was no significant difference in participants\u2019 deviation from the model between CLEAR-2 and BB-8 (t(798) = \u22120.87 , p = 0.384, see Figure 8a).\n6 We initially piloted a version of this study where participants were first shown an apartment and asked to predict its price, then shown the model and its prediction for that apartment, and finally asked to update their own prediction before moving on to the next apartment. We expected that the distribution of initial predictions for any given apartment would be the same regardless of which model participants were shown, but this turned out not be the case. Instead, we found that participants in the CLEAR-2 condition submitted initial predictions that were closer to the model\u2019s prediction\u2014which they had not yet seen\u2014compared to participants in other conditions (t(239) = \u22123.42, p < 0.001). We suspect this is because the CLEAR-2 condition is the easiest to simulate. Specifically, when making a prediction for a new apartment, participants in the CLEAR-2 condition could have internalized the model coefficients they saw in previous examples (i.e., $350,000 per bathroom and $1,000 per square foot) and used this to guide their initial prediction. This is an interesting potential benefit of transparent, simple models, but also poses a threat to validity for measuring the weight of advice given to different model presentations. To be able to compare weight of advice in different conditions, participants\u2019 initial predictions should not be influenced by the conditions they were assigned to, which is why we modified the design to collect initial predictions for all twelve apartments before participants were introduced to the model.\nH8. Weight of advice. Weight of advice is not well defined when a participant\u2019s initial prediction matches the model\u2019s prediction (i.e., u1 = m). For each condition, we therefore calculated the mean weight of advice over all participant\u2013apartment pairs for which the participant\u2019s initial prediction did not match the model\u2019s prediction.7 This can be viewed as calculating the mean conditional on there being a difference between the participant\u2019s and the model\u2019s predictions. Contrary to the hypothesis, and in line with the findings for deviation in the first two experiments, there was no significant difference in participants\u2019 weight of advice between the CLEAR-2 and BB-8 conditions (t(819) = 1.27, p = 0.205, see Figure 8b).\nH9. Humans vs. machines. Contrary to the hypothesis, there was not a significant difference in participants\u2019 deviation from the model (t(994) = 0.45 , p = 0.655) or in their weight of advice (t(1005) = \u22120.38 , p = 0.704) between the BB-8 and EXPERT conditions. We expect that the difference between our results and those in [49] is due to participants getting more experience with the model (or expert) and its predictions over the course of twelve apartments in our experiment.\nH10. Detection of mistakes. In contrast to the first two experiments, we did not find that participants in the clear conditions were less able to correct inaccurate predictions (t(798) = \u22120.96, p = 0.337 and t(798) = \u22120.19, p = 0.847 for the contrast of CLEAR-2 and CLEAR-8 with BB-2 and BB-8 for apartment 11 and apartment 12, respectively). We will investigate this further in the next experiment.\nThis experiment confirmed the findings from the first two experiments regarding deviation. That is, we again found that participants were no more likely to follow the predictions of a clear model with a small number of features than the predictions of a black-box model with a large number of features, this time as indicated by their weight of advice as well as by the deviation of their own predictions from the model\u2019s prediction. We also found that participants were no more or less likely to follow the predictions of a human expert than a black-box model. Finally, in contrast to the findings from the first two experiments, we did not find that participants in the clear conditions were less able to correct inaccurate predictions on unusual examples, a result that motivated our final experiment."}, {"heading": "5 Experiment 4: Outlier focus and the detection of mistakes", "text": "Recall that, contrary to expectations, participants in the clear conditions in our first two experiments were less likely to notice when the model overpriced an apartment compared with black-box conditions (Figures 5a and 5b). In seeming contradiction, our third experiment revealed practically no difference across conditions in terms of participants\u2019 abilities to detect the model\u2019s sizable mistakes. In this section, we propose an explanation for these findings and support it with an additional experiment. The explanation rests on two conjectures. First, visual displays with a great amount of numerical information (e.g., model coefficients) might cause users not to notice peculiarities of a case under consideration due to information overload [1, 34, 36]. Second, when past predictions are visible, they might exert an anchoring effect [21, 68] on subsequent predictions, swaying new predictions in the direction of the past predictions.\nIn the first two experiments, participants made their final prediction while seeing their simulation of the model\u2019s prediction (Figure 2c). In contrast, participants in the third experiment made their final prediction while seeing their own initial guess of the price (Figure 7b). That is, there were different anchor values in the first and second experiments compared with the third.\n7Between conditions, we found no significant difference in the fraction of times that participants\u2019 initial predictions matched the model\u2019s predictions.\nFurthermore, within the first two experiments, anchor values differed by condition because they were influenced by the model presentation. Participants in the CLEAR-2 condition could simulate the model rather well relative to other conditions (Figures 3a and 4a). When the model overprices an apartment, simulating the model well might actually anchor the participant on a value that is too high. In addition, since clear models present more information, participants in these conditions might be unlikely to notice unusual apartment configurations. Together, these factors could explain their overly high final predictions.\nIn contrast, participants in the black-box conditions could not simulate the model easily, but, undistracted by model internals, might be more likely to notice unusual apartment configurations. Interestingly, participants in these conditions apparently (incorrectly) assumed the model would also take the unusual configurations into account and thus reported simulated values that were too low (relative to the model). When making their final predictions, participants in the black-box conditions therefore could have had two factors working in their favor: they were not overwhelmed by the model internals and they were anchored on the low values they had previously stated. Both of these factors could lower their final estimate for the unusual apartments and thus increase their predictive accuracy.\nIn brief, being overwhelmed by the internals of the clear models and possibly anchoring on prior estimates could together account for the observed differences in noticing unreasonable predictions in our first two experiments as opposed to our third experiment. To test this account, our fourth experiment was designed to remove possible sources of anchoring and then measure the influence of an \u201coutlier focus\u201d message that encouraged participants to take note of the configurations of the unusual apartments. Before running this experiment, we pre-registered three hypotheses:8\nH11. Effect of outlier focus. Participants in the outlier focus conditions will exhibit different abilities to correct the model\u2019s mistakes compared to those in the control conditions (who are not shown outlier focus messages). H12. Effect of model transparency within control conditions. Participants in the control conditions will exhibit different abilities to correct the model\u2019s mistakes based on whether they are shown a clear model or a black-box model. H13. Effect of model transparency within outlier focus conditions. Participants in the outlier focus conditions will exhibit different abilities to correct the model\u2019s mistakes based on whether they are shown a clear model or a black-box model."}, {"heading": "5.1 Experimental design", "text": "Similar to the first two experiments, we asked people to predict apartment prices with the help of a model. We considered four experimental conditions in a 2\u00d7 2 design:\n\u2022 Participants were randomly assigned to see the model internals (CLEAR), or see the model as a black box (BB). \u2022 Participants were randomly assigned to see an explicit message concerning the unusual configurations of\napartments (FOCUS), or not (NO-FOCUS).\nSince there did not appear to be a strong effect of the number of features on people\u2019s abilities to detect the model\u2019s mistakes in our previous experiments, we only considered the 2-feature conditions. A screenshot of an apartment displayed for participants in the outlier focus condition is shown in Figure 9.\nWe again ran the experiment on Amazon Mechanical Turk. We excluded people who had participated in our first three 8Pre-registered hypotheses are available at https://aspredicted.org/5xy8y.pdf.\nexperiments, and recruited 800 new participants all of whom satisfied the selection criteria from our first three experiments. The participants were randomly assigned to the four conditions (CLEAR-FOCUS, n = 202; CLEAR-NO-FOCUS, n = 195; BB-FOCUS, n = 201; and BB-NO-FOCUS, n = 202) and each participant received a flat payment of $1.00.\nWe shortened the experiment by limiting the training phase and the first part of the testing phase to only five of the original ten apartments in each. We then added three more apartments to the end of the testing phase: two synthetically generated apartments with unusual configurations, with an apartment with a normal configuration (one-bedroom, one-bathroom, 788 square feet) in between. The first synthetically generated apartment was apartment 12 from our previous experiments (one-bedroom, three-bathroom, 726 square feet, referred to as apartment 6 in this experiment). The second synthetically generated apartment had an even more unusual configuration (one-bedroom, three-bathroom, 350 square feet, referred to as apartment 8 in this experiment), for which the model makes an unreasonably high prediction of the price. The order of the two synthetically generated apartments was randomized, while the apartment with the normal configuration (apartment 7) was always shown in the middle.\nParticipants went through the exact same training phase as in the first two experiments. For each apartment during the testing phase, participants were first shown the model\u2019s prediction and were asked to indicate how confident they were that the model was correct. Then they were asked to make their own prediction of the apartment\u2019s price and to indicate how confident they were in this prediction. To simplify the design and remove potential sources of anchoring, we did not ask participants to simulate the model\u2019s prediction in this experiment."}, {"heading": "5.2 Results", "text": "Figure 10 shows the mean predictions of the prices of apartment 6 and apartment 8 from our fourth experiment. To test our hypotheses, we pre-registered the difference between a participant\u2019s prediction (ua) and the model\u2019s prediction (m) as the dependent variable of interest: ua \u2212m.\nH11. Effect of outlier focus. Participants in the outlier focus conditions differed from the model\u2019s prediction more, on average, than participants in the control conditions for both apartment 6 (t(791) = \u22124.72, p < 0.001) and apartment 8 (t(795) = \u22125.00, p < 0.001). That is, providing an outlier focus message improved participants\u2019 predictions.\nH12. Effect of model transparency within control conditions. In the control conditions, predictions in the clear condition differed from the model\u2019s prediction less, on average, than predictions in the black-box condition for both apartment 6 (t(393) = \u22123.65, p < 0.001) and apartment 8 (t(395) = \u22123.51, p < 0.001). That is, in the absence of an outlier focus message, participants in the clear condition again performed worse by deviating less from the model\u2019s unreasonably high predictions.\nH13. Effect of model transparency within outlier focus conditions. With an outlier focus message, we found no significant effect of model transparency on how much participants\u2019 predictions differed from the model\u2019s predictions (t(401) = \u22120.004, p = 0.996 for apartment 6 and t(394) = \u22121.64, p = 0.101 for apartment 8). This result is consistent with the idea that an outlier focus message can help participants react to the peculiarities that get obscured by the overwhelming information presented by clear models.\nTaken together, these results suggest a simple explanation consistent with our information overload conjecture for why participants who were shown clear models performed poorly on apartments with unusual configurations. Without an outlier focus message, exposing the details of a model\u2019s internals can cause people to miss unusual inputs to the model. With such a message, however, this effect disappears. This both highlights an unintended downside of clear models and offers a simple intervention to neutralize the effect."}, {"heading": "6 Discussion and future work", "text": "Our experiments yielded several surprising results. On typical examples, we saw no significant difference between a transparent model with few features and a black-box model with many features in terms of how closely participants followed the model\u2019s predictions. We also saw that people would have been better off simply following the models rather than adjusting their predictions. Even more surprisingly, we found that transparent models had the unwanted effect of impairing people\u2019s ability to correct inaccurate predictions, seemingly due to people being overwhelmed by the additional information that the transparent model presented. When we tested an intervention to counter information overload, this unwanted effect was mitigated, consistent with the idea that the information provided in a transparent model inhibited people\u2019s ability to notice unusual inputs. Several empirical findings from post-hoc analyses were also consistent with the idea that presenting too much information can be detrimental. In Experiments 1 and 2, the decision aid that presented participants with the most information\u2014the clear, eight-feature model\u2014led to the worst performance on simulation, deviation, and prediction of all the models we tested.\nThese findings suggest new ways to present models to decision makers. When technically possible, it may be helpful for decision aids to alert users that the case under consideration may be an outlier. This could be achieved by training an auxiliary model to detect such cases. In addition, it may be prudent to have practitioners provide their own predictions before seeing the details or predictions of a model. Doing so could encourage decision makers to inspect the details of each case, increasing the likelihood that they notice any unusual inputs. Lastly, despite potential benefits of transparent models, it may be detrimental to expose model internals by default, as doing so might cause people to miss important details. Instead, model internals could be hidden until the user requests to see them. Testing these suggestions empirically would be a natural direction for future research.\nNone of this is to say that transparency and parsimony should be ignored or avoided when designing decision aids. Instead, our results underscore the point that there are many possible goals in designing interpretable models, and that one should rely on rigorous experimentation over intuition to assess whether those goals are met.\nA limitation of our work is that the experiments focused on just one model, in one domain, for a subset of scenarios in which interpretability might matter. Future extensions to other tasks (e.g., classification), models (e.g, decision trees or rule lists), users (e.g., data scientists or domain experts), and domains (e.g., medicine, hiring, judicial decisions) may yield different results than those found here. Even within the design of our experiments, although we have controlled for many potential confounds, we cannot rule out that there could have been specifics about the particular models we used beyond simply the number of features and transparency that influenced our results. For instance, people might have found the combination of features used in the two-feature model (i.e., bathrooms and square feet) to be less intuitive than other possible two-feature combinations (e.g., bedrooms and bathrooms) or a three-feature model (e.g., bedrooms, bathrooms, and square feet). Also, those who saw a two-feature model were in the unique position of having access to more features than the model did. For this reason, people might have thought a model that uses two out of eight features was overly simplistic. Perhaps if they were told that incorporating the remaining six features would not improve the model\u2019s performance, they would have viewed the two-feature model differently. Conversely, it could have been the case that specifics of the eight-feature model led people to question its quality. For example, the negative weight this model placed on the total number of rooms (to account for correlations with the number of bedrooms and bathrooms) might have been confusing or mistakenly viewed as wrong, leading people to follow it less closely than they would have otherwise. Although these factors make it difficult to isolate exactly why we do not see differences in deviation or prediction quality across these different models, it still remains the case that using the intuitively simpler and more transparent model did not produce higher quality decisions than using the more complex, opaque one and\neven hampered people\u2019s ability to correct models\u2019 inaccurate predictions.\nAt the same time, there is still a long list of reasons why interpretability might be desirable. First, in some domains, interpretability may play an important role in people\u2019s willingness to adopt a model on ethical grounds. For instance, if a model is used to make judicial decisions, policy makers may demand transparency to be assured that these decisions are not being made on the basis of disallowed criteria like race or proxies for race. Second, access to model internals allows people to better anticipate the inputs for which a model might go wrong. In fact, we leveraged this aspect of our own linear regression model for apartment prices to design the unusual apartment configurations used in our experiments, since we could easily see that it would place unreasonably high value on additional bathrooms while keeping other features fixed; detailed debugging and analysis would have been more difficult with a black-box model. Third, in settings where it is desirable to have a model that is easy to simulate (e.g., decision rules used by field workers), our experiments suggest that simple, transparent models had an advantage: the clear, two-feature model won out on being easiest to simulate. Fourth, while we did not investigate adoption of decision aids, it might be the case that people are more likely to adopt simple models over complex ones because they find them more appealing [35]. Given that simple, transparent models improved predictions as effectively as any other model over the baseline of not having a model at all, if it is the case that people are more willing to adopt them, there could be substantial benefits in terms of decision quality. The results of our experiments should be considered jointly with these observations, some of which are quite encouraging for the use of simple, transparent models as decision aids.\nIn light of recent advances, it is likely that in the future more and more decisions will be made with the aid of machine learning. As we transition to this future, it is also likely there will be a demand for models that people can interpret. We hope that the often surprising results in this paper will reinforce that the behavior of decision makers, and not only the intuitions of modelers, should guide the creation of interpretable decisions aids."}], "title": "Manipulating and Measuring Model Interpretability", "year": 2019}