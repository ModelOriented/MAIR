{"abstractText": "The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees. We show that our \u201cFair Forest\u201d retains the benefits of the treebased approach, while providing both greater accuracy and fairness than other alternatives, for both \u201cgroup fairness\u201d and \u201cindividual fairness.\u201d We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.", "authors": [{"affiliations": [], "name": "Edward Raff"}], "id": "SP:a38ffd3189709b40a80bd4a792c3054468dd68f3", "references": [{"authors": ["Bechavod", "Y. Ligett 2017 Bechavod", "K. Ligett"], "title": "Learning Fair Classifiers: A Regularization-Inspired Approach", "venue": "In FAT ML Workshop", "year": 2017}, {"authors": ["Berk"], "title": "A Convex Framework for Fair Regression", "venue": "In FAT ML Workshop", "year": 2017}, {"authors": ["Breiman"], "title": "Classification and Regression Trees", "year": 1984}, {"authors": ["Calders", "T. Verwer 2010 Calders", "S. Verwer"], "title": "Three Naive Bayes Approaches for Discrimination-free Classification", "venue": "Data Min. Knowl. Discov. 21(2):277\u2013292", "year": 2010}, {"authors": ["Calders"], "title": "Controlling Attribute Effect in Linear Regression", "venue": "IEEE 13th International Conference on Data Mining,", "year": 2013}, {"authors": ["Chen", "T. Guestrin 2016 Chen", "C. Guestrin"], "title": "XGBoost: Reliable Large-scale Tree Boosting System", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "year": 2016}, {"authors": ["Dwork"], "title": "Fairness Through Awareness", "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,", "year": 2012}, {"authors": ["Dwork"], "title": "Decoupled classifiers for fair and efficient machine learning", "venue": "In FAT ML Workshop", "year": 2017}, {"authors": ["Edwards", "H. Storkey 2016 Edwards", "A. Storkey"], "title": "Censoring Representations with an Adversary", "venue": "In International Conference on Learning Representations (ICLR)", "year": 2016}, {"authors": ["Fern\u00e1ndez-Delgado"], "title": "Do we Need Hundreds of Classifiers to Solve Real World Classification Problems", "venue": "Journal of Machine Learning Research", "year": 2014}, {"authors": ["Garc\u00eda-Mart\u00edn", "E. Lavesson 2017 Garc\u00eda-Mart\u00edn", "N. Lavesson"], "title": "Is it ethical to avoid error analysis", "venue": "In FAT ML Workshop", "year": 2017}, {"authors": ["Hall", "P. Gill 2017 Hall", "N. Gill"], "title": "Debugging the Black-Box COMPAS Risk Assessment Instrument to Diagnose and Remediate Bias", "year": 2017}, {"authors": ["Price Hardt", "M. Srebro 2016 Hardt", "E. Price", "N. Srebro"], "title": "Equality of Opportunity in Supervised Learning", "venue": "In Advances in Neural Information Processing Systems", "year": 2016}, {"authors": ["Kamiran", "F. Calders 2009 Kamiran", "T. Calders"], "title": "Classifying without discriminating", "venue": "In 2009 2nd International Conference on Computer, Control and Communication,", "year": 2009}, {"authors": ["Akaho Kamishima", "T. Sakuma 2011 Kamishima", "S. Akaho", "J. Sakuma"], "title": "Fairness-aware Learning Through Regularization Approach", "venue": "In Proceedings of the 2011 IEEE 11th International Conference on Data Mining Workshops,", "year": 2011}, {"authors": ["Landeiro", "V. Culotta 2016 Landeiro", "A. Culotta"], "title": "Robust Text Classification in the Presence of Confounding Bias", "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "year": 2016}, {"authors": ["Louizos"], "title": "The Variational Fair Autoencoder", "venue": "In International Conference on Learning Representations (ICLR)", "year": 2016}, {"authors": ["Louppe"], "title": "Understanding variable importances in forests of randomized trees", "venue": "Advances in Neural Information Processing Systems", "year": 2013}, {"authors": ["Ruggieri Luong", "B.T. Turini 2011 Luong", "S. Ruggieri", "F. Turini"], "title": "k-NN As an Implementation of Situation Testing for Discrimination Discovery and Prevention", "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2011}, {"authors": ["Ruggieri Pedreshi", "D. Turini 2008 Pedreshi", "S. Ruggieri", "F. Turini"], "title": "Discrimination-aware Data Mining", "venue": "In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2008}, {"authors": ["Skirpan", "M. Gorelick 2017 Skirpan", "M. Gorelick"], "title": "The Authority of \u201cFair", "venue": "Machine Learning. In FAT ML Workshop", "year": 2017}, {"authors": ["Zemel"], "title": "Learning Fair Representations", "venue": "Proceedings of the 30th International Conference on Machine Learning,", "year": 2013}], "sections": [{"text": "ar X\niv :1\n71 2.\n08 19\n7v 1\n[ st\nat .M\nL ]\n2 1\nD ec\n2 01"}, {"heading": "1 Introduction", "text": "As applications of Machine Learning becomes more pervasive in society, it is important to consider the fairness of such models. We consider a model to be fair with respect to some protected attribute ap (such as age or gender), if it\u2019s predicted label y\u0302 with respect to a datumn x is unaffected by changes to ap. Removing ap from x is not sufficient to meet this goal in practice, as ap\u2019s effect is still present as a latent variable (Pedreshi, Ruggieri, and Turini 2008). In this work, we look at adapting decision trees, specifically Random Forests, to this problem. Given an attribute ap that we wish to protect, we will show how to induce a \u201cFair Forest\u201d that provides improved fairness and accuracy compared to existing approaches.\nDecision Trees have become one of the most widely used classes of machine learning algorithms. In particular, C4.5 (Quinlan 1993) and CART (Breiman et al. 1984) tree induction approaches, combined with ensembling approaches like Random Forests (Breiman 2001) and Gradient Boosting (Friedman 2002), have proven to be potent and effective across a broad spectrum of needs and tasks. These methods\nCopyright \u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nare one of the few to be simultaneously interpretable, nonlinear, and easy-to-use.\nRandom Forests have proven to be particularly effective. In a study of over one-hundred datasets, Random Forests were found to be one of the best performing approaches \u2014 even when no hyperparameter tuning is done (Fern\u00e1ndezDelgado et al. 2014). XGBoost, a variant of gradient boosting, has been used in the winning solutions to over half of recent Kaggle competitions (Chen and Guestrin 2016).\nTree-based algorithms also provide a rare degree of interpretability. Single trees within an ensemble can be printed in a human-readable form, allowing the immediate extraction of the decision process. Further still, there are numerous ways to extract feature importance scores from any treebased approach (Louppe et al. 2013; Breiman 2003). Being able to understand how a model reaches its decision is of special utility when we desire fair decision algorithms, as it gives us a method to double-check that the model appears to be making reasonable judgments. This interpretability has already been exploited in prior work to understand black-box models (Hall and Gill 2017).\nGiven the wide-ranging benefits and successes of treebased learning, it is surprising that no prior work has focused on designing fair decision tree induction methods. Other methods for constructing fair models will be reviewed in section 2. In section 3 we propose, to the best of our knowledge, the first fair decision tree induction method. Our design is simple, requiring only minimal changes to existing tree induction code, thereby retaining the desirable property that the trees tend to \u201cjust work\u201d without hyperparameter tuning. Our experimental methodology is discussed in section 4, including the introduction of novel fairness measures which are suitable for use with multinomial and continuous attributes. Finally, experimental results are summarized in section 5, including a new experimental procedure to evaluate fair algorithms against all possible features rather than single protected attributed. We end with our conclusions in section 6.\n2 Related Work\nOne approach to building fair classifiers is based on data alteration, where the original corpus is altered to remove or mask information about the protected attribute. Some of the first work in fairness learning followed this approach, and\nattempted to make the minimum number of changes that removed the discriminative protective information (Kamiran and Calders 2009). Others have attempted to re-label the data points to ensure a fair determination (Luong, Ruggieri, and Turini 2011).\nAnother approach is to regularize the model in such a way that it is penalized for keeping information that allows it to discriminate against the protected feature. Some of the earliest work was to develop a fair version of Naive Bayes algorithm (Calders and Verwer 2010). Others have taken to creating a differentiable regularization term, and applying it to models such as Logistic and Linear Regression (Kamishima, Akaho, and Sakuma 2011; Bechavod and Ligett 2017; Berk et al. 2017; Calders et al. 2013). Our new fair induction algorithm is a member of this group of regularization-based approaches, but unlike prior works has no parameters to tune.\nOne final group of related approaches is to build new representations, which mask the protected attribute (Dwork et al. 2012). The use of neural networks have become popular for this task, such as variational auto encoders (Louizos et al. 2016) and adversarial networks (Edwards and Storkey 2016). One of the seminal works in this field used an autoencoder with three separate terms in the loss (Zemel et al. 2013), and provides one of the largest comparisons on three now-standard datasets. We replicate their evaluation procedure in this work.\nThere is an important commonality in all of these prior works. The research is done with respect to datasets and attributes where there is a prior normative expectation of fairness. These are problems usually of social importance, and protected attributes are intrinsic characteristics like age, gender and nationality. But what if focusing on such problems has inadvertently biased the development of fair research? The mechanism for inducing fairness should work for any attribute, not just those that align with current societal norms, and must not be over-fit to the protected attributes used in research. We evaluate our approach with respect to every possible feature choice, to ensure that the mechanism of producing fairness is not over-fit to the data."}, {"heading": "3 Fair Forests", "text": "We propose a simple regularization approach to constructing a fair decision tree induction algorithm. This is done by altering the way we measure the information gain G(T, a), where T is a set of training examples, and a is the attribute to split on. We will denote the set of points in each of the k branchs of the tree as Ti...k. This normally is combined with an impurity measure I(T ), to give us\nG(T, a) = I(T )\u2212 \u2211\n\u2200Ti\u2208splits(a)\n|Ti| |T | \u00b7 I(Ti) (1)\nThe information gain scores the quality of a splitting attribute a by how much it reduces impurity compared to the current impurity. The larger the gain, the more pure the class labels have become, and thus, should improve classification performance. In the CART algorithm, the Gini impurity (2)\nis normally used for categorical targets.\nIGini(T ) = 1\u2212 \u2211\n\u2200Ti\u2208splits(label)\n(\n|Ti|\n|T |\n)2\n(2)\nThis greedy approach to feature selection has proven effective for decades, helping to cement the place of treebased algorithms as one of the most popular learning methods. However, this does not take into account any notion of fairness, which we desire to add. In this work we do so by altering the information gain scoring itself, leaving the whole of the tree induction process unaltered.\nWe begin by noting we need to make two slight alterations for our approach. First, we will use the Impurity score to measure both the class label, and now additionally the protected attribute under consideration. We will denote these two cases as I l, and Ia, and the Gain with respect to the label and protected attribute as Gl and Ga respectively. Additionally, we will impose the constraint that the impurity measure must return a value normalized to the range of [0, 1]. For the Gini measure this becomes\nIaGini(T ) = 1\u2212\n\u2211\n\u2200Ti\u2208splits(a)\n(\n|Ti| |T |\n)2\n1\u2212 |splits(a)| \u22121 (3)\nWe require that the impurity score Ia(\u00b7) produce a normalized score so that we can compare scores on a similar scale range, regardless of which features are selected. We then use this to define a new fair gain measure Gfair(T, a), which seeks to balance predictive accuracy with the fairness goal with respect to some protected attribute af .\nGfair(T, b) = G l(T, b)\u2212Gaf (T, b) (4)\nIntuitively, (4) will discourage the selection of any feature correlated with both the protected attribute and the target label. It remains possible for such a feature to still be selected if no other feature is better suited."}, {"heading": "3.1 Gain for Numeric Features", "text": "To our knowledge, no work has yet explored making a continuous feature the protected attribute. We can derive this naturally in our new fair induction framework. In CART, trees\u2019 numeric target variables are optimized by finding the binary split that minimizes the weighted variance between each split. We use this same notion to define a gain Gr(T, a) that is used when either the predictor or protected attribute is continuous.\nBecause we are interested in fairness, we look at changes in the mean value of the splits compared to their parent. Even if variances differ, if they retain similar means the impact on the fairness is minimal. To produce a scaled value, we look at the number of standard deviations from the previous mean is for each of the new splits, and assume that being more than three standard deviations is the maximum violation. This gain is defined in (5), where \u03c3b,Ti indicates the standard deviation of attribute b for all datums in the set Ti, and \u00b5b,Ti has the same meaning but for the mean of the subset.\nGr(T, b) = 1\u2212 1\n3\n\u2211\nTi\u2208split\n|Ti| |T | min\n(\n|\u00b5b,T \u2212 \u00b5b,Ti |\n\u03c3b,T , 3\n)\n(5)\nWe emphasize that the standard deviation of the parent T is used, not that of any sub-population Ti. This is because we want to measure drift with respect to the current status. Re-writing the continuous splitting criteria in this fashion also produces a score normalized to the range [0, 1]. We can now continue to use the Gfair(T, b) function with continuous attributes as either the label target, or the protected attribute.\nThis framework now gives us a means to induce decision trees, and thus build Random Forests, for all scenarios: classification and regression problems, and protected features either nominal or numeric. We emphasize that this approach to regularizing the information gain has no tunable parameters as given. This is to keep with the general utility of decision trees in that they often \u201cjust work.\u201d\nWhile adjusting hyperparameters such as maximum tree depth may be used to improve classification accuracy, the results of a decision tree are often effective without any kind of parameter tunning. This is important for practical use and adoption. Many fairness based systems require an additional two to three hyperparameters to tune (Kamishima, Akaho, and Sakuma 2011; Bechavod and Ligett 2017; Zemel et al. 2013), on top of whatever hyperparameters come with the original model. This increases the computational requirements in practice, especially when used with a classic gridsearch approach."}, {"heading": "4 Methodology", "text": "There is currently considerable discussion about what it means for a machine learning model to be fair, which metrics should be used, and whether or not they can be completely optimized (Skirpan and Gorelick 2017; Garc\u00eda-Mart\u00edn and Lavesson 2017; Hardt, Price, and Srebro 2016).\nWe choose to use the same evaluation procedure laid out by Zemel et al. (2013). This makes our results comparable with a larger body of work, as their approach and metrics have been widely used through the literature (Landeiro and Culotta 2016; Bechavod and Ligett 2017; Dwork et al. 2017; Calders et al. 2013). We present both of their metrics \u2014 Discrimination and Inconsistency1 \u2014 in a manner compatible with both classification and regression problems, while also extending Discrimination to a broader set of scenarios. We will also discuss the datasets used, their variants tested, and the models we will evaluate."}, {"heading": "4.1 Metrics", "text": "The first metric we will consider is the Discrimination of the model, measured by the average difference between the average predicted scores for each attribute value.\nDiscrimination =\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2211 xi\u2208Tap y\u0302i |Tap | \u2212 \u2211 xi\u2208T\u00acap y\u0302i |T\u00acap | \u2223 \u2223 \u2223 \u2223 \u2223\n(6)\nDiscrimination measures a macro-level quality of fairness, as such it is sometimes termed \u201cgroup fairness.\u201d However,\n1Zemel et al. refer to their metric as \u2018consistency,\u2019 but define it in a way that only makes sense for classification. We use Inconsistency = 1 \u2212 Consistency. This form is applicable to both classification and regression tasks.\nthe definition in (6) is limited to only binary protected attributes. For this work, we will also look at a generalization of Discrimination to k-way categorical variables. This is done by re-formulating Discrimination to consider the subpopulation differences from the global mean. This is equivalent to the original definition when k = 2, and is given by (7). (See the Appendix for a proof of equivalence.)\nDiscrimination = 2\nk\nk \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2211 xj\u2208T y\u0302j |T | \u2212 \u2211 xj\u2208Ti y\u0302j\n|Ti|\n\u2223 \u2223 \u2223 \u2223 \u2223 (7)\nWe will also consider the discrimination with respect to a continuous variable. With ap denoting a protected continuous attribute, let xi(ap) be the value of feature ap for datum xi. We will then define our new Maximum Discrimination (MaxD) metric as the largest discrimination score achieved for some binary split of ap by some threshold t. This is given in equation (8), and gives us a concise definition extending Discrimination to regression tasks. When a continuous attribute is manually discretized into a binary problem, as is done in prior work, we obtain by definition that MaxD \u2265 Discrimination.\nMaxD = argmax t\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2211 xi(ap)<t y\u0302i |xi(ap) < t| \u2212 \u2211 xi(ap)\u2265t y\u0302i |xi(ap) \u2265 t| \u2223 \u2223 \u2223 \u2223 \u2223 (8)\nGiven our novel extensions of the Discrimination scores (7) and (8), we can evaluate this property for any feature. Importantly though, these metrics are population level measures of fairness. Satisfying the Discrimination metric does not guarantee that no bias exists. To measure the potential for bias within sub-populations of the data set, we look at the Inconsistency metric (9).\nInconsistency = 1\nN\nN \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 y\u0302i \u2212 1 k\n\u2211\nj\u2208k-NN(xi)\ny\u0302j\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n(9)\nInconsistency compares the prediction of the model with that of nearby points, and is sometimes referred to as \u201cindividual fairness.\u201d This is under the assumption that nearby points should produce similar predictions, and is optimized when the score is as close to zero as possible.\nDiscrimination and Inconsistency are both evaluating the fairness of a model, and hence do not consider the true supervised label y. Maximizing fairness involves minimizing these two scores, at a potential cost to the model\u2019s predictive utility. We measure the predictive utility of each model with accuracy or Root Mean Squared Error (RMSE) for classification and regression problems respectively. For classification problems, we also consider the Delta metric, where Delta = Accuracy \u2212 Discrimination.\nFor corpora with a test set, these metrics will all be evaluated on the given test set. Otherwise, we will evaluate these scores on 10-fold cross validation. For Inconsistency, we will measure it using nearest neighbors from all folds, but using the predicted scores obtained from cross validation. This is in keeping with prior work (Zemel et al. 2013)."}, {"heading": "4.2 Data Sets", "text": "To evaluate our work, we will use three classification datasets used by Zemel et al. (2013), the German Credit, Adult, and Heritage Health datasets. For regression we will also use the Health dataset, which was originally a regression problem (how many days will someone stay in the hospital?) that was converted to classification (will they stay one or more days?).\nTable 1 summarizes the size, protected attribute, feature count, and task type for each dataset. For the German and Health datasets, the protected attribute age is originally encoded as a numeric feature, but, because prior work did not support continuous protected attributes, they converted it to a binary categorical feature. We replicate this in our work, but will also investigate using the original continuous version of age."}, {"heading": "4.3 Models Evaluated", "text": "When listing results, we will compare with standard CART decision trees (DT) and Random Forests (RF). Our fair variants of these methods will be denoted as DTF and RFF .\nSince our new fair tree induction can directly protect the original non-discretized form, we also evaluate in that manner. Models DTFc and RF F c indicate a fair decision tree and Random Forest trained to protect the continuous age attribute. When we do this, we will continue to evaluate the models\u2019 Discrimination with the originally proposed threshold.\nFrom Zemel et al. (2013), we compare against their proposed Learning Fair Representations (LFR) approach and their baseline approaches: Logistic Regression, fair Logistic Regression (LRF ) (Kamishima, Akaho, and Sakuma 2011) and fair Naive Bayes (NBF ) (Kamiran and Calders 2009)."}, {"heading": "5 Experiments", "text": "In this section we present the results of our experiments. We remind the reader that for all experiments, we perform no parameter turning for any of our tree-based models. This is in line with practical use, and is a benefit for users in both runtime and simplicity. In these experiments we will show our Fair Forests can be used in the standard classification scenario with a binary protected attribute. In addition, we can use a continuous protected attribute and achieve similar results, and apply both methods to a numeric prediction target. All code was written in Java using the JSAT library (Raff 2017)."}, {"heading": "5.1 Binary Target, Binary Protected", "text": "For the classification tasks, the results for our various decision tree variants can be seen compared to the baselines in\nTable 2. We can see that our new Fair Forests win in almost every metric.\nLooking at just the tree-based results, we can make two interesting observations. First, that the ensembling and random feature sub-sampling used by Random Forests appears to improve the fairness of CART trees, both when they do and do not consider our fairness regularization. This is a positive indication for the general use of Random Forests compared to single decision trees. Second, that our fairness regularizer can actually improve accuracy. This was observed on the German and Health datasets. We do not observe this phenomena with any other fairness approach. While a positive result, we caution that this should not be a general expectation. It is always possible that the protected attribute may truly be predictive of the target task. In such cases we would expect performance to decrease, which we observe on the Adult income dataset.\nThis result is also important when we contrast to the Logistic Regression model and its fair variant. On every dataset tested, the fair variant of LR has worse predictive accuracy than the standard model. Our fair trees do not suffer in the same way, indicating they are a more robust approach to building fair models.\nThe baseline results shown from Zemel et al. (2013) required a grid-search, and were selected to maximize the Delta score. In this regard our Fair Forests almost dominate the table. The Fair Forest is second best only once to NBF on the Adult Income corpus, with a relative difference of merely 2.3%. NBF achieves this by obtaining higher accuracies, but also a higher Discrimination.\nOn both measures of fairness, Discrimination and Inconsistency, our fair Random Forest dominates the table with empirical zeros. The best non-tree approach in this regard is the LFR algorithm, which obtains empirical zeros on the Health dataset and near-zeros for Discrimination on the German and Adult datasets. However, LFR\u2019s Inconsistency increases to 0.06 and 0.19 for each respectively."}, {"heading": "5.2 Binary Target, Continuous Protected", "text": "In all prior literature we are aware of, the protected attribute is always presented as a binary feature. Our fair tree induction approach allows us to mark a continuous feature as protected directly, without first having to discretize it. We can test this ability with the German and Health datasets, where the protected attribute (age) is originally a numeric feature. The results comparing this approach with the classic binary age attribute are shown in Table 3. In this table Discrimination is based on the original thresholds used to binarize the age.\nHere we can see that the Random Forest using the continuous age (RFFc ) and the one using binary age (RF\nF ) have equivalent performance. This appears to be a net effect of the added fairness Random Forests naturally provide. In this case it becomes more informative to look at the results from the standard decision tree, where non-zero Discrimination still occurs.\nFor both DTF and DTFc , we can see they continue to reduce the Discrimination and Inconsistency with respect to\nthe original decision tree approach. In these cases, DTFc appears to uniformly outperform DTF in regards to the fairness metrics, with only a 0.003 change in accuracy. On the German dataset we can also see that DTFc has improved upon the MaxD score from the DTF , dropping from 0.0216 down to 0.0054. This is reasonable to expect, as DTF is optimizing fairness with respect to a specific value of age, where DTFc is attempting to be fair with respect to all age values.\nFurther reading of the table indicates the MaxD score for DTFc (0.005) is smaller than the Discrimination score of the DTF approach (0.008). This means DTFc has a greater degree of fairness with respect to age for all possible age splits, than DTF does with regard to its single age split of interest. We explain this result by noting that DTF \u2019s single split focus at age \u2265 25 means discrimination can occur in nearby age ranges (e.g., 26-30, or 21-24), and this permissible \u201cborder\u201d discrimination can generalize into the test set. Ultimately, while protecting the binary age attribute works well compared to the naive DT in Table 2, these results demonstrate the benefit of protecting the original numeric attribute: we can provide better fairness with respect to the threshold of interest, as well as every possible other threshold."}, {"heading": "5.3 Continuous Target", "text": "One of the benefits of Decision Trees or Random Forests is that they can be applied to both classification and regression problems. In this section we will show that our fair induction strategy improves fairness in such scenarios, both when protecting on a continuous or a binary attribute. We do this using the original version of the Health dataset (see Table 4).\nWhen phrased as a regression problem, we see lower Discrimination scores for both the standard Decision Tree and the Random Forest, which leaves little room for improvement. When comparing Discrimination against the binary age threshold (age \u2265 65), and the Maximum Discrimination against age, we see the fair variants of our algorithms perform better than their non-fair counterparts. While the DTF and DTFc happen to perform slightly better than their counterparts RFF and RFFc , the differences are in an epsilon range. Either way, these results show that we can use our approach for regression problems and protect both categorical and continuous attributes."}, {"heading": "5.4 Visualizing the Impact of Fairness", "text": "One of the benefits of tree-based approaches to prediction is the ability to interpret the models. In particular, we note that one can measure the relative importance of a feature using a variety of approaches. Using the Mean Decrease in Impurity (MDI) measure (Louppe et al. 2013), we show the relative importance of features on the German and Adult datasets. These are shown in Figure 1 and Figure 2 respectively, where \u201cFair\u201d is the relative importance of features used by our Fair Forest induction algorithm and \u201cStandard\u201d indicates the normal Random Forest induction process using CART-style trees. These results allow us to see that our simple regularizer can have a wide range of impact, depending on the dataset and protected attribute.\nOn the German dataset in Figure 1, we see a dramatic change in what the model considers importance, with the the most important variable being checking-status under the Standard model but housing under the Fair model. For almost all features in this corpus, we see a reversing of importance: if it was important under the naive model, it becomes less-so under the Fair model, and vice versa. The only exception to this being the savings-status attribute, and to some degree, property-magnitude.\nThe Adult dataset has a markedly different and surprising behavior. Under both the Fair and Naive model, the relationship attribute continues to be the most important. However, the Fair model dramatically reduces the relative importance of most other features. Many of these (e.g. capital-loss, capital-gain, education) would likely be features we expect to reliably predict the target attribute, Income. While our intuition may be that these variables should be unbiased and naturally fair predictors, the underlying distribution of this dataset indicates they were too highly correlated with the protected Gender attribute, and thus were rarely selected for use.\nWe expect that the ability to perform such investigation into feature importance pre/post fairness will become a valuable tool for those who wish to build fair models in production environments. Changes in feature importance can give us underlying insights into non-linear correlations that would escape simple analysis. The information itself may allow a decision maker to discover deficiencies or unintended biases in their data collection process, based on these unexpected changes. For example, the non-use of the capitalgain/loss features may tell us that we need to collect more data specifically from women with capital investments."}, {"heading": "5.5 Fairness vs the Mechanism", "text": "We now evaluate the ability of our model to reduce Discrimination for every attribute individually, across each dataset. This helps us to determine that our approach is not overly specific to the choice of attributes such as age and gender. To our knowledge this is the first such evaluation in the fairness literature.\nFirst we train a standard Random Forest, and measure the Discrimination for each attribute using (7) or (8) as appropriate. From these we record the average and standard deviation of the \u201cRaw\u201d discrimination. Then we train a new Fair Forest D times for D features, testing the model when each feature is selected as the protected attribute. We then measure the Discrimination of the protected feature and the accuracy of the resulting model. The mean and standard deviation are then calculated from the protected feature Discriminations. The results of this are shown in Table 5.\nAcross all three datasets and every feature, the Fair Forest approach was always able to decrease the Discrimination with respect to the protected attribute. For the German and Health datasets, it is able to reduce the Discrimination to zero for all features, and always results in the same accuracy. For the Adult dataset, the original protected attribute of Gender was the only attribute which could be reduced to a Discrimination of zero. The Adult dataset is the only one producing a wide impact in the amount of Discrimination removed, and the resulting accuracy of the model (decreasing from 0.85 down to 0.80 on average)."}, {"heading": "6 Conclusion", "text": "We have developed, to the best of our knowledge, the first fair variant of the Random Forest algorithm. This Fair Forest can be used for classification and regression problems, and protected k-category features as well as numeric attributes, a first in the fairness literature. In doing so we have extended the measure of discrimination to these cases. We have shown our method produces state-of-the art results on three common benchmark datasets, while requiring no parameter tuning to use, and is able to uniformly reduce Discrimination across any feature in each corpus."}], "title": "Fair Forests: Regularized Tree Induction to Minimize Model Bias", "year": 2018}