{
  "abstractText": "The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable AI systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of Explainable AI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of Explainable AI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of Explainable AI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of Explainable AI design goals and evaluation methods. Our categorization presents the mapping between design goals for different Explainable AI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary Explainable AI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in Explainable AI research.",
  "authors": [
    {
      "affiliations": [],
      "name": "SINA MOHSENI"
    },
    {
      "affiliations": [],
      "name": "NILOOFAR ZAREI"
    },
    {
      "affiliations": [],
      "name": "ERIC D. RAGAN"
    }
  ],
  "id": "SP:77c864d6c835ac92d651b190a3ca32b581a09afd",
  "references": [
    {
      "authors": [
        "Ashraf Abdul",
        "Jo Vermeulen",
        "Danding Wang",
        "Brian Y Lim",
        "Mohan Kankanhalli"
      ],
      "title": "Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda",
      "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Amina Adadi",
        "Mohammed Berrada"
      ],
      "title": "Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)",
      "venue": "IEEE Access",
      "year": 2018
    },
    {
      "authors": [
        "Julius Adebayo",
        "Justin Gilmer",
        "Michael Muelly",
        "Ian Goodfellow",
        "Moritz Hardt",
        "Been Kim"
      ],
      "title": "Sanity checks for saliency maps",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Yongsu Ahn",
        "Yu-Ru Lin"
      ],
      "title": "Fairsight: visual analytics for fairness in decision making",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "year": 2019
    },
    {
      "authors": [],
      "title": "Task-driven comparison of topic models",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 22,",
      "year": 2015
    },
    {
      "authors": [
        "Saleema Amershi",
        "Maya Cakmak",
        "William Bradley Knox",
        "Todd Kulesza"
      ],
      "title": "Power to the people: The role of humans in interactive machine learning",
      "venue": "AI Magazine 35,",
      "year": 2014
    },
    {
      "authors": [
        "Saleema Amershi",
        "Dan Weld",
        "Mihaela Vorvoreanu",
        "Adam Fourney",
        "Besmira Nushi",
        "Penny Collisson",
        "Jina Suh",
        "Shamsi Iqbal",
        "Paul N Bennett",
        "Kori Inkpen"
      ],
      "title": "Guidelines for human-AI interaction",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM,",
      "year": 2019
    },
    {
      "authors": [
        "Dario Amodei",
        "Chris Olah",
        "Jacob Steinhardt",
        "Paul Christiano",
        "John Schulman",
        "Dan Man\u00e9"
      ],
      "title": "Concrete problems in AI safety",
      "year": 2016
    },
    {
      "authors": [
        "Mike Ananny",
        "Kate Crawford"
      ],
      "title": "Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability",
      "venue": "New Media & Society 20,",
      "year": 2018
    },
    {
      "authors": [
        "Stavros Antifakos",
        "Nicky Kern",
        "Bernt Schiele",
        "Adrian Schwaninger"
      ],
      "title": "Towards improving trust in contextaware systems by displaying system confidence",
      "venue": "In Proceedings of the 7th International Conference on Human Computer Interaction with Mobile Devices & Services",
      "year": 2005
    },
    {
      "authors": [
        "Alejandro Barredo Arrieta",
        "Natalia D\u00edaz-Rodr\u00edguez",
        "Javier Del Ser",
        "Adrien Bennetot",
        "Siham Tabik",
        "Alberto Barbado",
        "Salvador Garc\u00eda",
        "Sergio Gil-L\u00f3pez",
        "Daniel Molina",
        "Richard Benjamins"
      ],
      "title": "Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
      "venue": "Information Fusion",
      "year": 2020
    },
    {
      "authors": [
        "Jimmy Ba",
        "Volodymyr Mnih",
        "Koray Kavukcuoglu"
      ],
      "title": "Multiple object recognition with visual attention",
      "venue": "arXiv preprint arXiv:1412.7755",
      "year": 2014
    },
    {
      "authors": [
        "Sebastian Bach",
        "Alexander Binder",
        "Gr\u00e9goire Montavon",
        "Frederick Klauschen",
        "Klaus-Robert M\u00fcller",
        "Wojciech Samek"
      ],
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PloS one 10,",
      "year": 2015
    },
    {
      "authors": [
        "David Baehrens",
        "Timon Schroeter",
        "Stefan Harmeling",
        "Motoaki Kawanabe",
        "Katja Hansen",
        "Klaus-Robert M\u00fcller"
      ],
      "title": "How to explain individual classification decisions",
      "venue": "Journal of Machine Learning Research",
      "year": 2010
    },
    {
      "authors": [
        "Gagan Bansal",
        "Besmira Nushi",
        "Ece Kamar",
        "Walter S Lasecki",
        "Daniel S Weld",
        "Eric Horvitz"
      ],
      "title": "Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance",
      "venue": "In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,",
      "year": 2019
    },
    {
      "authors": [
        "Victoria Bellotti",
        "Keith Edwards"
      ],
      "title": "Intelligibility and accountability: human considerations in context-aware systems",
      "venue": "Human\u2013Computer Interaction",
      "year": 2001
    },
    {
      "authors": [
        "Shlomo Berkovsky",
        "Ronnie Taib",
        "Dan Conway"
      ],
      "title": "How to Recommend?: User Trust Factors in Movie Recommender Systems",
      "venue": "In Proceedings of the 22nd International Conference on Intelligent User Interfaces (IUI \u201917)",
      "year": 2017
    },
    {
      "authors": [
        "Daniel M Best",
        "Alex Endert",
        "Daniel Kidwell"
      ],
      "title": "7 key challenges for visualization in cyber network defense",
      "venue": "In Proceedings of the Eleventh Workshop on Visualization for Cyber Security",
      "year": 2014
    },
    {
      "authors": [
        "Mustafa Bilgic",
        "Raymond J Mooney"
      ],
      "title": "Explaining recommendations: Satisfaction vs. promotion",
      "venue": "In Beyond Personalization Workshop, IUI,",
      "year": 2005
    },
    {
      "authors": [
        "Reuben Binns",
        "Max Van Kleek",
        "Michael Veale",
        "Ulrik Lyngs",
        "Jun Zhao",
        "Nigel Shadbolt"
      ],
      "title": "It\u2019s Reducing a Human Being to a Percentage\u201d: Perceptions of Justice in Algorithmic Decisions",
      "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Philip Bobko",
        "Alex J Barelka",
        "Leanne M Hirshfield"
      ],
      "title": "The construct of state-level suspicion: A model and research agenda for automated and information technology (IT) contexts",
      "venue": "Human Factors 56,",
      "year": 2014
    },
    {
      "authors": [
        "Mariusz Bojarski",
        "Anna Choromanska",
        "Krzysztof Choromanski",
        "Bernhard Firner",
        "Larry J Ackel",
        "Urs Muller",
        "Phil Yeres",
        "Karol Zieba"
      ],
      "title": "Visualbackprop: Efficient visualization of cnns for autonomous driving",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "year": 2018
    },
    {
      "authors": [
        "Engin Bozdag",
        "Jeroen van den Hoven"
      ],
      "title": "Breaking the filter bubble: democracy and design",
      "venue": "Ethics and Information Technology 17,",
      "year": 2015
    },
    {
      "authors": [
        "Nicholas Bryan",
        "Gautham Mysore"
      ],
      "title": "An efficient posterior regularized latent variable model for interactive sound source separation",
      "venue": "In International Conference on Machine Learning",
      "year": 2013
    },
    {
      "authors": [
        "Andrea Bunt",
        "Matthew Lount",
        "Catherine Lauzon"
      ],
      "title": "Are explanations always important?: a study of deployed, low-cost intelligent interactive systems",
      "venue": "In Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces",
      "year": 2012
    },
    {
      "authors": [
        "Adrian Bussone",
        "Simone Stumpf",
        "Dympna O\u2019Sullivan"
      ],
      "title": "The role of explanations on trust and reliance in clinical decision support systems",
      "venue": "In International Conference on Healthcare Informatics (ICHI)",
      "year": 2015
    },
    {
      "authors": [
        "Angel Cabrera",
        "Will Epperson",
        "Fred Hohman",
        "Minsuk Kahng",
        "Jamie Morgenstern",
        "Duen Horng Chau"
      ],
      "title": "FairVis: visual analytics for discovering intersectional bias in machine learning",
      "venue": "IEEE Conference on Visual Analytics Science and Technology (VAST)",
      "year": 2019
    },
    {
      "authors": [
        "B\u00e9atrice Cahour",
        "Jean-Fran\u00e7ois Forzy"
      ],
      "title": "Does projection into use improve trust and exploration? An example with a cruise control system",
      "venue": "Safety Science 47,",
      "year": 2009
    },
    {
      "authors": [
        "Carrie J Cai",
        "Jonas Jongejan",
        "Jess Holbrook"
      ],
      "title": "The effects of example-based explanations in a machine learning interface",
      "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces",
      "year": 2019
    },
    {
      "authors": [
        "Carrie J Cai",
        "Emily Reif",
        "Narayan Hegde",
        "Jason Hipp",
        "Been Kim",
        "Daniel Smilkov",
        "Martin Wattenberg",
        "Fernanda Viegas",
        "Greg S Corrado",
        "Martin C Stumpe"
      ],
      "title": "Human-centered tools for coping with imperfect algorithms during medical decision-making",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Rich Caruana",
        "Yin Lou",
        "Johannes Gehrke",
        "Paul Koch",
        "Marc Sturm",
        "Noemie Elhadad"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2015
    },
    {
      "authors": [
        "Jiahao Chen",
        "Nathan Kallus",
        "Xiaojie Mao",
        "Geoffry Svacha",
        "Madeleine Udell"
      ],
      "title": "Fairness under unawareness: Assessing disparity when protected class is unobserved",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Jaegul Choo",
        "Hanseung Lee",
        "Jaeyeon Kihm",
        "Haesun Park"
      ],
      "title": "iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction",
      "venue": "In Visual Analytics Science and Technology (VAST),",
      "year": 2010
    },
    {
      "authors": [
        "Jaegul Choo",
        "Shixia Liu"
      ],
      "title": "Visual analytics for explainable deep learning",
      "venue": "IEEE Computer Graphics and Applications 38,",
      "year": 2018
    },
    {
      "authors": [
        "Alexandra Chouldechova"
      ],
      "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
      "venue": "Big data 5,",
      "year": 2017
    },
    {
      "authors": [
        "Michael Chromik",
        "Malin Eiband",
        "Sarah Theres V\u00f6lkel",
        "Daniel Buschek"
      ],
      "title": "Dark patterns of explainability, transparency, and user control for intelligent systems",
      "venue": "In IUI Workshops",
      "year": 2019
    },
    {
      "authors": [
        "Lingyang Chu",
        "Xia Hu",
        "Juhua Hu",
        "Lanjun Wang",
        "Jian Pei"
      ],
      "title": "Exact and consistent interpretation for piecewise linear neural networks: A closed form solution",
      "venue": "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "year": 2018
    },
    {
      "authors": [
        "Miruna-Adriana Clinciu",
        "Helen Hastie"
      ],
      "title": "A survey of explainable AI terminology",
      "venue": "In Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence",
      "year": 2019
    },
    {
      "authors": [
        "Sven Coppers",
        "Jan Van den Bergh",
        "Kris Luyten",
        "Karin Coninx",
        "Iulianna Van der Lek-Ciudin",
        "Tom Vanallemeersch",
        "Vincent Vandeghinste"
      ],
      "title": "Intellingo: An intelligible translation environment",
      "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Enrico Costanza",
        "Joel E Fischer",
        "James A Colley",
        "Tom Rodden",
        "Sarvapali D Ramchurn",
        "Nicholas R Jennings"
      ],
      "title": "Doing the laundry with agents: a field trial of a future smart energy system in the home",
      "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "year": 2014
    },
    {
      "authors": [
        "William Curran",
        "Travis Moore",
        "Todd Kulesza",
        "Weng-Keen Wong",
        "Sinisa Todorovic",
        "Simone Stumpf",
        "Rachel White",
        "Margaret Burnett"
      ],
      "title": "Towards recognizing cool: can end users help computer vision recognize subjective attributes of objects in images",
      "venue": "In Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces",
      "year": 2012
    },
    {
      "authors": [
        "Abhishek Das",
        "Harsh Agrawal",
        "C. Lawrence Zitnick",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions",
      "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP). https://computing.ece.vt.edu/~abhshkdz/vqa-hat/",
      "year": 2016
    },
    {
      "authors": [
        "Abhishek Das",
        "Harsh Agrawal",
        "Larry Zitnick",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "title": "Human attention in visual question answering: Do humans and deep networks look at the same regions",
      "venue": "Computer Vision and Image Understanding",
      "year": 2017
    },
    {
      "authors": [
        "Amit Datta",
        "Michael Carl Tschantz",
        "Anupam Datta"
      ],
      "title": "Automated experiments on ad privacy settings",
      "venue": "Proceedings on Privacy Enhancing Technologies 2015,",
      "year": 2015
    },
    {
      "authors": [
        "Nicholas Diakopoulos"
      ],
      "title": "Algorithmic-Accountability: the investigation of Black Boxes",
      "venue": "Tow Center for Digital Journalism",
      "year": 2014
    },
    {
      "authors": [
        "Nicholas Diakopoulos"
      ],
      "title": "Enabling Accountability of Algorithmic Media: Transparency as a Constructive and Critical Lens",
      "venue": "In Transparent Data Mining for Big and Small Data",
      "year": 2017
    },
    {
      "authors": [
        "Jonathan Dodge",
        "Sean Penney",
        "Andrew Anderson",
        "Margaret M Burnett"
      ],
      "title": "What Should Be in an XAI Explanation? What IFT Reveals",
      "venue": "In IUI Workshops",
      "year": 2018
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Been Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608",
      "year": 2017
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Mason Kortz",
        "Ryan Budish",
        "Christopher Bavitz",
        "Samuel J Gershman",
        "David O\u2019Brien",
        "Stuart Shieber",
        "Jim Waldo",
        "David Weinberger",
        "Alexandra Wood"
      ],
      "title": "Accountability of AI Under the Law: The Role of Explanation",
      "venue": "Berkman Center Research Publication Forthcoming",
      "year": 2017
    },
    {
      "authors": [
        "James K Doyle",
        "Michael J Radzicki",
        "W Scott Trees"
      ],
      "title": "Measuring change in mental models of complex dynamic systems. In Complex Decision Making. Springer, 269\u2013294",
      "venue": "ACM Trans. Interact. Intell. Syst.,",
      "year": 2008
    },
    {
      "authors": [
        "Fan Du",
        "Catherine Plaisant",
        "Neil Spring",
        "Kenyon Crowley",
        "Ben Shneiderman"
      ],
      "title": "EventAction: A Visual Analytics Approach to Explainable Recommendation for Event Sequences",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS) 9,",
      "year": 2019
    },
    {
      "authors": [
        "Mengnan Du",
        "Ninghao Liu",
        "Qingquan Song",
        "Xia Hu"
      ],
      "title": "Towards explanation of dnn-based prediction with guided feature inversion",
      "venue": "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "year": 2018
    },
    {
      "authors": [
        "M. Du",
        "N. Liu",
        "F. Yang",
        "X. Hu"
      ],
      "title": "Learning credible deep neural networks with rationale regularization",
      "venue": "IEEE International Conference on Data Mining (ICDM)",
      "year": 2019
    },
    {
      "authors": [
        "John J Dudley",
        "Per Ola Kristensson"
      ],
      "title": "A review of user interface design for interactive machine learning",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)",
      "year": 2018
    },
    {
      "authors": [
        "Malin Eiband",
        "Daniel Buschek",
        "Alexander Kremer",
        "Heinrich Hussmann"
      ],
      "title": "The Impact of Placebic Explanations on Trust in Intelligent Systems",
      "venue": "In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. ACM,",
      "year": 2019
    },
    {
      "authors": [
        "Malin Eiband",
        "Hanna Schneider",
        "Mark Bilandzic",
        "Julian Fazekas-Con",
        "Mareike Haug",
        "Heinrich Hussmann"
      ],
      "title": "Bringing transparency design into practice",
      "venue": "In 23rd International Conference on Intelligent User Interfaces (IUI \u201918)",
      "year": 2018
    },
    {
      "authors": [
        "A Endert",
        "W Ribarsky",
        "C Turkay",
        "BL Wong",
        "Ian Nabney",
        "I D\u00edaz Blanco",
        "F Rossi"
      ],
      "title": "The state of the art in integrating machine learning into visual analytics",
      "venue": "In Computer Graphics Forum,",
      "year": 2017
    },
    {
      "authors": [
        "Motahhare Eslami",
        "Aimee Rickman",
        "Kristen Vaccaro",
        "Amirhossein Aleyasen",
        "Andy Vuong",
        "Karrie Karahalios",
        "Kevin Hamilton",
        "Christian Sandvig"
      ],
      "title": "I always assumed that I wasn\u2019t really that close to [her]: Reasoning about Invisible Algorithms in News Feeds",
      "venue": "In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems",
      "year": 2015
    },
    {
      "authors": [
        "Motahhare Eslami",
        "Kristen Vaccaro",
        "Karrie Karahalios",
        "Kevin Hamilton"
      ],
      "title": "Be careful; things can be worse than they appear\u201d: Understanding Biased Algorithms and Users\u2019 Behavior around Them in Rating Platforms",
      "venue": "In Eleventh International AAAI Conference on Web and Social Media",
      "year": 2017
    },
    {
      "authors": [
        "Raquel Florez-Lopez",
        "Juan Manuel Ramon-Jeronimo"
      ],
      "title": "Enhancing accuracy and interpretability of ensemble strategies in credit risk assessment. A correlated-adjusted decision forest proposal",
      "venue": "Expert Systems with Applications 42,",
      "year": 2015
    },
    {
      "authors": [
        "Ruth C Fong",
        "Andrea Vedaldi"
      ],
      "title": "Interpretable explanations of black boxes by meaningful perturbation",
      "venue": "In Proceedings of the IEEE International Conference on Computer",
      "year": 2017
    },
    {
      "authors": [
        "Fatih Gedikli",
        "Dietmar Jannach",
        "Mouzhi Ge"
      ],
      "title": "How should I explain? A comparison of different explanation types for recommender systems",
      "venue": "International Journal of Human-Computer Studies 72,",
      "year": 2014
    },
    {
      "authors": [
        "Amirata Ghorbani",
        "James Wexler",
        "James Y Zou",
        "Been Kim"
      ],
      "title": "Towards automatic concept-based explanations",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Leilani H Gilpin",
        "David Bau",
        "Ben Z Yuan",
        "Ayesha Bajwa",
        "Michael Specter",
        "Lalana Kagal"
      ],
      "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
      "venue": "IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)",
      "year": 2018
    },
    {
      "authors": [
        "Alyssa Glass",
        "Deborah L McGuinness",
        "Michael Wolverton"
      ],
      "title": "Toward establishing trust in adaptive agents",
      "venue": "In Proceedings of the 13th International Conference on Intelligent User Interfaces",
      "year": 2008
    },
    {
      "authors": [
        "John Goodall",
        "Eric D Ragan",
        "Chad A Steed",
        "Joel W Reed",
        "G David Richardson",
        "Kelly MT Huffer",
        "Robert A Bridges",
        "Jason A Laska"
      ],
      "title": "Situ: Identifying and Explaining Suspicious Behavior in Networks",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "year": 2018
    },
    {
      "authors": [
        "Bryce Goodman",
        "Seth Flaxman"
      ],
      "title": "European Union regulations on algorithmic decision-making and a \u201cright to explanation",
      "venue": "AI Magazine 38,",
      "year": 2017
    },
    {
      "authors": [
        "Colin M Gray",
        "Yubo Kou",
        "Bryan Battles",
        "Joseph Hoggatt",
        "Austin L Toombs"
      ],
      "title": "The dark (patterns) side of UX design",
      "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Shirley Gregor",
        "Izak Benbasat"
      ],
      "title": "Explanations from Intelligent Systems: Theoretical Foundations and Implications for Practice",
      "venue": "Management Information Systems Quarterly 23,",
      "year": 1999
    },
    {
      "authors": [
        "Alex Groce",
        "Todd Kulesza",
        "Chaoqiang Zhang",
        "Shalini Shamasunder",
        "Margaret Burnett",
        "Weng-Keen Wong",
        "Simone Stumpf",
        "Shubhomoy Das",
        "Amber Shinsel",
        "Forrest Bice"
      ],
      "title": "You are the only possible oracle: Effective test selection for end users of interactive machine learning systems",
      "venue": "IEEE Transactions on Software Engineering",
      "year": 2014
    },
    {
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Salvatore Ruggieri",
        "Franco Turini",
        "Fosca Giannotti",
        "Dino Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM Computing Surveys (CSUR) 51,",
      "year": 2018
    },
    {
      "authors": [
        "David Gunning"
      ],
      "title": "Explainable artificial intelligence (XAI). Defense Advanced Research Projects Agency (DARPA) (2017)",
      "venue": "ACM Trans. Interact. Intell. Syst.,",
      "year": 2017
    },
    {
      "authors": [
        "Aniko Hannak",
        "Piotr Sapiezynski",
        "Arash Molavi Kakhki",
        "Balachander Krishnamurthy",
        "David Lazer",
        "Alan Mislove",
        "Christo Wilson"
      ],
      "title": "Measuring personalization of web search",
      "venue": "In Proceedings of the 22nd International Conference on World Wide Web",
      "year": 2013
    },
    {
      "authors": [
        "Steven R Haynes",
        "Mark A Cohen",
        "Frank E Ritter"
      ],
      "title": "Designs for explaining intelligent agents",
      "venue": "International Journal of Human-Computer Studies 67,",
      "year": 2009
    },
    {
      "authors": [
        "Jeffrey Heer"
      ],
      "title": "Agency plus automation: Designing artificial intelligence into interactive systems",
      "venue": "Proceedings of the National Academy of Sciences 116,",
      "year": 2019
    },
    {
      "authors": [
        "Lisa Anne Hendricks",
        "Kaylee Burns",
        "Kate Saenko",
        "Trevor Darrell",
        "Anna Rohrbach"
      ],
      "title": "Women also snowboard: Overcoming bias in captioning models",
      "venue": "In European Conference on Computer",
      "year": 2018
    },
    {
      "authors": [
        "Jonathan L Herlocker",
        "Joseph A Konstan",
        "John Riedl"
      ],
      "title": "Explaining collaborative filtering recommendations",
      "venue": "In Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work",
      "year": 2000
    },
    {
      "authors": [
        "Bernease Herman"
      ],
      "title": "The Promise and Peril of Human Evaluation for Model Interpretability",
      "venue": "arXiv preprint arXiv:1711.07414",
      "year": 2017
    },
    {
      "authors": [
        "Robert Hoffman",
        "Tim Miller",
        "Shane T Mueller",
        "Gary Klein",
        "William J Clancey"
      ],
      "title": "Explaining explanation, part 4: a deep dive on deep nets",
      "venue": "IEEE Intelligent Systems 33,",
      "year": 2018
    },
    {
      "authors": [
        "Robert R Hoffman"
      ],
      "title": "Theory concepts measures but policies metrics. In Macrocognition Metrics and Scenarios",
      "year": 2017
    },
    {
      "authors": [
        "Robert R Hoffman",
        "John K Hawley",
        "Jeffrey M Bradshaw"
      ],
      "title": "Myths of automation, part 2: Some very human consequences",
      "venue": "IEEE Intelligent Systems 29,",
      "year": 2014
    },
    {
      "authors": [
        "Robert R Hoffman",
        "Matthew Johnson",
        "Jeffrey M Bradshaw",
        "Al Underbrink"
      ],
      "title": "Trust in automation",
      "venue": "IEEE Intelligent Systems 28,",
      "year": 2013
    },
    {
      "authors": [
        "Robert R Hoffman",
        "Gary Klein"
      ],
      "title": "Explaining explanation, part 1: theoretical foundations",
      "venue": "IEEE Intelligent Systems 32,",
      "year": 2017
    },
    {
      "authors": [
        "Robert R Hoffman",
        "Shane T Mueller",
        "Gary Klein"
      ],
      "title": "Explaining explanation, part 2: empirical foundations",
      "venue": "IEEE Intelligent Systems 32,",
      "year": 2017
    },
    {
      "authors": [
        "Robert R Hoffman",
        "Shane T Mueller",
        "Gary Klein",
        "Jordan Litman"
      ],
      "title": "Metrics for explainable AI: challenges and prospects",
      "year": 2018
    },
    {
      "authors": [
        "Fred Hohman",
        "Haekyu Park",
        "Caleb Robinson",
        "Duen Horng Polo Chau"
      ],
      "title": "Summit: scaling deep learning interpretability by visualizing activation and attribution summarizations",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 26,",
      "year": 2019
    },
    {
      "authors": [
        "Fred Hohman",
        "Arjun Srinivasan",
        "Steven M. Drucker"
      ],
      "title": "TeleGam: combining visualization and verbalization for interpretable machine learning",
      "venue": "IEEE Visualization Conference (VIS)",
      "year": 2019
    },
    {
      "authors": [
        "Fred Matthew Hohman",
        "Minsuk Kahng",
        "Robert Pienta",
        "Duen Horng Chau"
      ],
      "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "year": 2018
    },
    {
      "authors": [
        "Daniel Holliday",
        "Stephanie Wilson",
        "Simone Stumpf"
      ],
      "title": "User trust in intelligent systems: A journey over time",
      "venue": "In Proceedings of the 21st International Conference on Intelligent User Interfaces",
      "year": 2016
    },
    {
      "authors": [
        "Kristina H\u00f6\u00f6k"
      ],
      "title": "Steps to take before intelligent user interfaces become real",
      "venue": "Interacting with Computers 12,",
      "year": 2000
    },
    {
      "authors": [
        "Philip N Howard",
        "Bence Kollanyi"
      ],
      "title": "Bots, #StrongerIn, and #Brexit: computational propaganda during the UK-EU referendum",
      "year": 2016
    },
    {
      "authors": [
        "Yuening Hu",
        "Jordan Boyd-Graber",
        "Brianna Satinoff",
        "Alison Smith"
      ],
      "title": "Interactive topic modeling",
      "venue": "Machine Learning 95,",
      "year": 2014
    },
    {
      "authors": [
        "Shagun Jhaver",
        "Yoni Karpfen",
        "Judd Antin"
      ],
      "title": "Algorithmic anxiety and coping strategies of Airbnb hosts",
      "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Jiun-Yin Jian",
        "Ann M Bisantz",
        "Colin G Drury"
      ],
      "title": "Foundations for an empirically determined scale of trust in automated systems",
      "venue": "International Journal of Cognitive Ergonomics",
      "year": 2000
    },
    {
      "authors": [
        "Minsuk Kahng",
        "Pierre Y Andrews",
        "Aditya Kalro",
        "Duen Horng Polo Chau"
      ],
      "title": "ActiVis: visual exploration of industry-scale deep neural network models",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 24,",
      "year": 2018
    },
    {
      "authors": [
        "Matthew Kay",
        "Tara Kola",
        "Jessica R Hullman",
        "Sean A Munson"
      ],
      "title": "When (ish) is my bus?: User-centered visualizations of uncertainty in everyday, mobile predictive systems",
      "venue": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems",
      "year": 2016
    },
    {
      "authors": [
        "Frank C Keil"
      ],
      "title": "Explanation and understanding",
      "venue": "Annu. Rev. Psychol",
      "year": 2006
    },
    {
      "authors": [
        "Been Kim",
        "Rajiv Khanna",
        "Oluwasanmi O Koyejo"
      ],
      "title": "Examples are not enough, learn to criticize! criticism for interpretability",
      "venue": "ACM Trans. Interact. Intell. Syst.,",
      "year": 2016
    },
    {
      "authors": [
        "Been Kim",
        "Martin Wattenberg",
        "Justin Gilmer",
        "Carrie Cai",
        "James Wexler",
        "Fernanda Viegas"
      ],
      "title": "Interpretability beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "venue": "In International Conference on Machine Learning",
      "year": 2018
    },
    {
      "authors": [
        "Jaedeok Kim",
        "Jingoo Seo"
      ],
      "title": "Human understandable explanation extraction for black-box classification models based on matrix factorization",
      "year": 2017
    },
    {
      "authors": [
        "Pieter-Jan Kindermans",
        "Sara Hooker",
        "Julius Adebayo",
        "Maximilian Alber",
        "Kristof T Sch\u00fctt",
        "Sven D\u00e4hne",
        "Dumitru Erhan",
        "Been Kim"
      ],
      "title": "The (un) reliability of saliency methods. In Explainable AI: Interpreting",
      "year": 2019
    },
    {
      "authors": [
        "Gary Klein"
      ],
      "title": "Explaining explanation, part 3: The causal landscape",
      "venue": "IEEE Intelligent Systems 33,",
      "year": 2018
    },
    {
      "authors": [
        "Rafal Kocielnik",
        "Saleema Amershi",
        "Paul N Bennett"
      ],
      "title": "Will you accept an imperfect ai? exploring designs for adjusting end-user expectations of ai systems",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Johannes Kraus",
        "David Scholz",
        "Dina Stiegemeier",
        "Martin Baumann"
      ],
      "title": "The more you know: Trust dynamics and calibration in highly automated driving and the effects of take-overs, system malfunction, and system transparency",
      "venue": "Human Factors",
      "year": 2019
    },
    {
      "authors": [
        "Josua Krause",
        "Aritra Dasgupta",
        "Jordan Swartz",
        "Yindalon Aphinyanaphongs",
        "Enrico Bertini"
      ],
      "title": "A workflow for visual diagnostics of binary classifiers using instance-level explanations",
      "venue": "IEEE Conference on Visual Analytics Science and Technology (VAST)",
      "year": 2017
    },
    {
      "authors": [
        "Josua Krause",
        "Adam Perer",
        "Enrico Bertini"
      ],
      "title": "INFUSE: interactive feature selection for predictive modeling of high dimensional data",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 20,",
      "year": 2014
    },
    {
      "authors": [
        "Josua Krause",
        "Adam Perer",
        "Kenney Ng"
      ],
      "title": "Interacting with predictions: Visual inspection of black-box machine learning models",
      "venue": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems",
      "year": 2016
    },
    {
      "authors": [
        "Todd Kulesza",
        "Margaret Burnett",
        "Weng-Keen Wong",
        "Simone Stumpf"
      ],
      "title": "Principles of explanatory debugging to personalize interactive machine learning",
      "venue": "In Proceedings of the 20th International Conference on Intelligent User Interfaces",
      "year": 2015
    },
    {
      "authors": [
        "Todd Kulesza",
        "Simone Stumpf",
        "Margaret Burnett",
        "Irwin Kwan"
      ],
      "title": "Tell Me More?: The Effects of Mental Model Soundness on Personalizing an Intelligent Agent",
      "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI \u201912)",
      "year": 2012
    },
    {
      "authors": [
        "Todd Kulesza",
        "Simone Stumpf",
        "Margaret Burnett",
        "Weng-Keen Wong",
        "Yann Riche",
        "Travis Moore",
        "Ian Oberst",
        "Amber Shinsel",
        "Kevin McIntosh"
      ],
      "title": "Explanatory debugging: Supporting end-user debugging of machine-learned programs",
      "venue": "In Visual Languages and Human-Centric Computing (VL/HCC),",
      "year": 2010
    },
    {
      "authors": [
        "Todd Kulesza",
        "Simone Stumpf",
        "Margaret Burnett",
        "Sherry Yang",
        "Irwin Kwan",
        "Weng-Keen Wong"
      ],
      "title": "Too much, too little, or just right? Ways explanations impact end users\u2019 mental models",
      "venue": "In Visual Languages and Human-Centric Computing (VL/HCC),",
      "year": 2013
    },
    {
      "authors": [
        "Isaac Lage",
        "Emily Chen",
        "Jeffrey He",
        "Menaka Narayanan",
        "Been Kim",
        "Samuel J Gershman",
        "Finale Doshi-Velez"
      ],
      "title": "Human evaluation of models built for interpretability",
      "venue": "In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,",
      "year": 2019
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Stephen H Bach",
        "Jure Leskovec"
      ],
      "title": "Interpretable decision sets: A joint framework for description and prediction",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2016
    },
    {
      "authors": [
        "Ellen J Langer",
        "Arthur Blank",
        "Benzion Chanowitz"
      ],
      "title": "The mindlessness of ostensibly thoughtful action: The role of \u201cplacebic\u201d information in interpersonal interaction",
      "venue": "Journal of Personality and Social Psychology 36,",
      "year": 1978
    },
    {
      "authors": [
        "Min Kyung Lee",
        "Anuraag Jain",
        "Hea Jin Cha",
        "Shashank Ojha",
        "Daniel Kusbit"
      ],
      "title": "Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediation",
      "venue": "Proceedings of the ACM on Human-Computer Interaction",
      "year": 2019
    },
    {
      "authors": [
        "Min Kyung Lee",
        "Daniel Kusbit",
        "Evan Metsky",
        "Laura Dabbish"
      ],
      "title": "Working with machines: The impact of algorithmic and data-driven management on human workers",
      "venue": "In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems",
      "year": 2015
    },
    {
      "authors": [
        "Bruno Lepri",
        "Nuria Oliver",
        "Emmanuel Letouz\u00e9",
        "Alex Pentland",
        "Patrick Vinck"
      ],
      "title": "Fair, Transparent, and Accountable Algorithmic Decision-making Processes",
      "venue": "Philosophy & Technology",
      "year": 2017
    },
    {
      "authors": [
        "Piyawat Lertvittayakumjorn",
        "Francesca Toni"
      ],
      "title": "Human-grounded Evaluations of Explanation Methods for Text Classification",
      "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "year": 2019
    },
    {
      "authors": [
        "Benjamin Letham",
        "Cynthia Rudin",
        "Tyler H McCormick",
        "David Madigan"
      ],
      "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model",
      "venue": "The Annals of Applied Statistics 9,",
      "year": 2015
    },
    {
      "authors": [
        "Alexander Lex",
        "Marc Streit",
        "H-J Schulz",
        "Christian Partl",
        "Dieter Schmalstieg",
        "Peter J Park",
        "Nils Gehlenborg"
      ],
      "title": "StratomeX: Visual Analysis of Large-Scale Heterogeneous Genomics Data for Cancer Subtype Characterization",
      "venue": "In Computer Graphics Forum,",
      "year": 2012
    },
    {
      "authors": [
        "Kunpeng Li",
        "Ziyan Wu",
        "Kuan-Chuan Peng",
        "Jan Ernst",
        "Yun Fu"
      ],
      "title": "Tell me where to look: Guided attention inference network",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "year": 2018
    },
    {
      "authors": [
        "Kunpeng Li",
        "Yulun Zhang",
        "Kai Li",
        "Yuanyuan Li",
        "Yun Fu"
      ],
      "title": "Attention bridging network for knowledge transfer",
      "venue": "In Proceedings of the IEEE International Conference on Computer",
      "year": 2019
    },
    {
      "authors": [
        "Brian Lim"
      ],
      "title": "Improving Understanding, Trust, and Control with Intelligibility in Context-Aware Applications. Human-Computer Interaction",
      "year": 2011
    },
    {
      "authors": [
        "Brian Y Lim",
        "Anind K Dey"
      ],
      "title": "Assessing demand for intelligibility in context-aware applications",
      "venue": "In Proceedings of the 11th International Conference on Ubiquitous Computing",
      "year": 2009
    },
    {
      "authors": [
        "Brian Y Lim",
        "Anind K Dey",
        "Daniel Avrahami"
      ],
      "title": "Why and why not explanations improve the intelligibility of context-aware intelligent systems",
      "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "year": 2009
    },
    {
      "authors": [
        "Brian Y Lim",
        "Qian Yang",
        "Ashraf M Abdul",
        "Danding Wang"
      ],
      "title": "Why these explanations? Selecting intelligibility types for explanation Goals",
      "venue": "In IUI Workshops",
      "year": 2019
    },
    {
      "authors": [
        "Zachary C Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "arXiv preprint arXiv:1606.03490",
      "year": 2016
    },
    {
      "authors": [
        "Mengchen Liu",
        "Shixia Liu",
        "Xizhou Zhu",
        "Qinying Liao",
        "FuruWei",
        "Shimei Pan"
      ],
      "title": "An uncertainty-aware approach for exploratory microblog retrieval",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 22,",
      "year": 2016
    },
    {
      "authors": [
        "Mengchen Liu",
        "Jiaxin Shi",
        "Kelei Cao",
        "Jun Zhu",
        "Shixia Liu"
      ],
      "title": "Analyzing the training processes of deep generative models",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 24,",
      "year": 2018
    },
    {
      "authors": [
        "Mengchen Liu",
        "Jiaxin Shi",
        "Zhen Li",
        "Chongxuan Li",
        "Jun Zhu",
        "Shixia Liu"
      ],
      "title": "Towards better analysis of deep convolutional neural networks",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "year": 2017
    },
    {
      "authors": [
        "Shixia Liu",
        "Xiting Wang",
        "Jianfei Chen",
        "Jim Zhu",
        "Baining Guo"
      ],
      "title": "TopicPanorama: A full picture of relevant topics",
      "venue": "In Visual Analytics Science and Technology (VAST),",
      "year": 2014
    },
    {
      "authors": [
        "Tania Lombrozo"
      ],
      "title": "The structure and function of explanations",
      "venue": "Trends in Cognitive Sciences 10,",
      "year": 2006
    },
    {
      "authors": [
        "Tania Lombrozo"
      ],
      "title": "Explanation and categorization: How \u201cwhy?\u201d informs \u201cwhat?",
      "venue": "Cognition 110,",
      "year": 2009
    },
    {
      "authors": [
        "Scott M Lundberg",
        "Su-In Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2017
    },
    {
      "authors": [
        "Laurens van der Maaten",
        "Geoffrey Hinton"
      ],
      "title": "Visualizing data using t-SNE",
      "venue": "Journal of Machine Learning Research 9,",
      "year": 2008
    },
    {
      "authors": [
        "Maria Madsen",
        "Shirley Gregor"
      ],
      "title": "Measuring human-computer trust",
      "venue": "In 11th Australasian Conference on Information Systems,",
      "year": 2000
    },
    {
      "authors": [
        "Ninareh Mehrabi",
        "Fred Morstatter",
        "Nripsuta Saxena",
        "Kristina Lerman",
        "Aram Galstyan"
      ],
      "title": "A survey on bias and fairness in machine learning",
      "year": 2019
    },
    {
      "authors": [
        "Sarah Mennicken",
        "Jo Vermeulen",
        "Elaine M Huang"
      ],
      "title": "From today\u2019s augmented houses to tomorrow\u2019s smart homes: new directions for home automation research",
      "venue": "In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing",
      "year": 2014
    },
    {
      "authors": [
        "Stephanie M Merritt",
        "Heather Heimbaugh",
        "Jennifer LaChapell",
        "Deborah Lee"
      ],
      "title": "I trust it, but I don\u2019t know why: Effects of implicit attitudes toward automation on trust in an automated system",
      "venue": "Human Factors 55,",
      "year": 2013
    },
    {
      "authors": [
        "Miriah Meyer",
        "Michael Sedlmair",
        "P Samuel Quinan",
        "Tamara Munzner"
      ],
      "title": "The nested blocks and guidelines model",
      "venue": "Information Visualization 14,",
      "year": 2015
    },
    {
      "authors": [
        "Tim Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "Artificial Intelligence",
      "year": 2019
    },
    {
      "authors": [
        "Yao Ming",
        "Shaozu Cao",
        "Ruixiang Zhang",
        "Zhen Li",
        "Yuanzhe Chen",
        "Yangqiu Song",
        "Huamin Qu"
      ],
      "title": "Understanding hidden memories of recurrent neural networks",
      "venue": "IEEE Conference on Visual Analytics Science and Technology (VAST)",
      "year": 2017
    },
    {
      "authors": [
        "Yao Ming",
        "Huamin Qu",
        "Enrico Bertini"
      ],
      "title": "Rulematrix: Visualizing and understanding classifiers with rules",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 25,",
      "year": 2018
    },
    {
      "authors": [
        "Brent Mittelstadt"
      ],
      "title": "2016. Automation, algorithms, and politics: Auditing for transparency in content personalization systems",
      "venue": "International Journal of Communication",
      "year": 2016
    },
    {
      "authors": [
        "Sina Mohseni",
        "Akshay Jagadeesh",
        "Zhangyang Wang"
      ],
      "title": "Predicting model failure using saliency maps in autonomous driving systems. ICML Workshop on Uncertainty & Robustness in Deep Learning (2019)",
      "venue": "ACM Trans. Interact. Intell. Syst.,",
      "year": 2019
    },
    {
      "authors": [
        "Sina Mohseni",
        "Mandar Pitale",
        "Vasu Singh",
        "Zhangyang Wang"
      ],
      "title": "Practical solutions for machine learning safety in autonomous vehicles",
      "venue": "In The AAAI Workshop on Artificial Intelligence Safety (Safe AI)",
      "year": 2020
    },
    {
      "authors": [
        "Sina Mohseni",
        "Eric Ragan",
        "Xia Hu"
      ],
      "title": "Open issues in combating fake news: Interpretability as an opportunity",
      "year": 2019
    },
    {
      "authors": [
        "Sina Mohseni",
        "Eric D Ragan"
      ],
      "title": "A human-grounded evaluation benchmark for local explanations of machine learning",
      "venue": "arXiv preprint arXiv:1801.05075",
      "year": 2018
    },
    {
      "authors": [
        "Sina Mohseni",
        "Fan Yang",
        "Shiva Pentyala",
        "Mengnan Du",
        "Yi Liu",
        "Nic Lupfer",
        "Xia Hu",
        "Shuiwang Ji",
        "Eric Ragan"
      ],
      "title": "Trust evolution over time in explainable AI for fake news detection",
      "venue": "Fair & Responsible AI Workshop at CHI",
      "year": 2020
    },
    {
      "authors": [
        "Christoph Molnar"
      ],
      "title": "Interpretable machine learning. Lulu",
      "year": 2019
    },
    {
      "authors": [
        "Gr\u00e9goire Montavon",
        "Wojciech Samek",
        "Klaus-Robert M\u00fcller"
      ],
      "title": "Methods for interpreting and understanding deep neural networks",
      "venue": "Digital Signal Processing",
      "year": 2017
    },
    {
      "authors": [
        "Shane T Mueller",
        "Gary Klein"
      ],
      "title": "Improving users\u2019 mental models of intelligent software tools",
      "venue": "IEEE Intelligent Systems 26,",
      "year": 2011
    },
    {
      "authors": [
        "Bonnie M Muir"
      ],
      "title": "Trust between humans and machines, and the design of decision aids",
      "venue": "International Journal of Man-Machine Studies 27,",
      "year": 1987
    },
    {
      "authors": [
        "Tamara Munzner"
      ],
      "title": "A nested process model for visualization design and validation",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "year": 2009
    },
    {
      "authors": [
        "Brad A Myers",
        "David A Weitzman",
        "Andrew J Ko",
        "Duen H Chau"
      ],
      "title": "Answering why and why not questions in user interfaces",
      "venue": "In Proceedings of the SIGCHI conference on Human Factors in computing systems",
      "year": 2006
    },
    {
      "authors": [
        "Andrew P Norton",
        "Yanjun Qi"
      ],
      "title": "Adversarial-playground: A visualization suite showing how adversarial examples fool deep learning",
      "venue": "In Visualization for Cyber Security (VizSec),",
      "year": 2017
    },
    {
      "authors": [
        "Florian Nothdurft",
        "Felix Richter",
        "Wolfgang Minker"
      ],
      "title": "Probabilistic human-computer trust handling",
      "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)",
      "year": 2014
    },
    {
      "authors": [
        "Mahsan Nourani",
        "Dondald Honeycutt",
        "Jeremy Block",
        "Chiradeep Roy",
        "Tahrima Rahman",
        "Eric D. Ragan",
        "Vibhav Gogate"
      ],
      "title": "Investigating the importance of first impressions and explainable AI with interactive video analysis",
      "venue": "In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. ACM",
      "year": 2020
    },
    {
      "authors": [
        "Mahsan Nourani",
        "Samia Kabir",
        "Sina Mohseni",
        "Eric D Ragan"
      ],
      "title": "The effects of meaningful and meaningless explanations on trust and perceived system accuracy in intelligent systems",
      "venue": "In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,",
      "year": 2019
    },
    {
      "authors": [
        "Besmira Nushi",
        "Ece Kamar",
        "Eric Horvitz"
      ],
      "title": "Towards accountable AI: Hybrid human-machine analyses for characterizing system failure",
      "venue": "In Sixth AAAI Conference on Human Computation and Crowdsourcing",
      "year": 2018
    },
    {
      "authors": [
        "Chris Olah",
        "Arvind Satyanarayan",
        "Ian Johnson",
        "Shan Carter",
        "Ludwig Schubert",
        "Katherine Ye",
        "Alexander Mordvintsev"
      ],
      "title": "The Building Blocks of Interpretability",
      "venue": "Distill (2018)",
      "year": 2018
    },
    {
      "authors": [
        "Cathy O\u2019Neil"
      ],
      "title": "Weapons of math destruction: How big data increases inequality and threatens democracy",
      "year": 2016
    },
    {
      "authors": [
        "Sean Penney",
        "Jonathan Dodge",
        "Claudia Hilderbrand",
        "Andrew Anderson",
        "Logan Simpson",
        "Margaret Burnett"
      ],
      "title": "Toward foraging for understanding of StarCraft agents: An empirical study",
      "venue": "In 23rd International Conference on Intelligent User Interfaces (IUI \u201918)",
      "year": 2018
    },
    {
      "authors": [
        "Nicola Pezzotti",
        "Thomas H\u00f6llt",
        "Jan Van Gemert",
        "Boudewijn PF Lelieveldt",
        "Elmar Eisemann",
        "Anna Vilanova"
      ],
      "title": "DeepEyes: Progressive visual analytics for designing deep neural networks",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 24,",
      "year": 2018
    },
    {
      "authors": [
        "Nina Poerner",
        "Hinrich Sch\u00fctze",
        "Benjamin Roth"
      ],
      "title": "Evaluating neural network explanationmethods using hybrid documents and morphological prediction. In 56th Annual Meeting of the Association for Computational Linguistics (ACL)",
      "year": 2018
    },
    {
      "authors": [
        "Brett Poulin",
        "Roman Eisner",
        "Duane Szafron",
        "Paul Lu",
        "Russell Greiner",
        "David S Wishart",
        "Alona Fyshe",
        "Brandon Pearcy",
        "Cam MacDonell",
        "John Anvik"
      ],
      "title": "Visual explanation of evidence with additive classifiers",
      "venue": "In Proceedings Of The National Conference On Artificial Intelligence,",
      "year": 2006
    },
    {
      "authors": [
        "Forough Poursabzi-Sangdeh",
        "Daniel G Goldstein",
        "Jake M Hofman",
        "Jennifer Wortman Vaughan",
        "Hanna Wallach"
      ],
      "title": "Manipulating and measuring model interpretability",
      "year": 2018
    },
    {
      "authors": [
        "Pearl Pu",
        "Li Chen"
      ],
      "title": "Trust building with explanation interfaces",
      "venue": "In Proceedings of the 11th International Conference on Intelligent User Interfaces",
      "year": 2006
    },
    {
      "authors": [
        "Emilee Rader",
        "Kelley Cotter",
        "Janghee Cho"
      ],
      "title": "Explanations as mechanisms for supporting algorithmic transparency",
      "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM,",
      "year": 2018
    },
    {
      "authors": [
        "Emilee Rader",
        "Rebecca Gray"
      ],
      "title": "Understanding user beliefs about algorithmic curation in the Facebook news feed",
      "venue": "In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems",
      "year": 2015
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i you? Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2016
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Anchors: High-precision model-agnostic explanations",
      "venue": "In AAAI Conference on Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "Caleb Robinson",
        "Fred Hohman",
        "Bistra Dilkina"
      ],
      "title": "A deep learning approach for population estimation from satellite imagery",
      "venue": "In Proceedings of the 1st ACM SIGSPATIAL Workshop on Geospatial Humanities",
      "year": 2017
    },
    {
      "authors": [
        "Marko Robnik-\u0160ikonja",
        "Marko Bohanec"
      ],
      "title": "Perturbation-based explanations of prediction models",
      "venue": "In Human and Machine Learning",
      "year": 2018
    },
    {
      "authors": [
        "Stephanie Rosenthal",
        "Sai P Selvaraj",
        "Manuela Veloso"
      ],
      "title": "Verbalization: narration of autonomous robot experience",
      "venue": "In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence",
      "year": 2016
    },
    {
      "authors": [
        "Andrew Slavin Ross",
        "Finale Doshi-Velez"
      ],
      "title": "Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients",
      "venue": "In Thirty-second AAAI Conference on Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "Andrew Slavin Ross",
        "Michael C. Hughes",
        "Finale Doshi-Velez"
      ],
      "title": "Right for the right reasons: Training differentiable models by constraining their explanations",
      "venue": "In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,",
      "year": 2017
    },
    {
      "authors": [
        "Stephen Rudolph",
        "Anya Savikhin",
        "David S Ebert"
      ],
      "title": "Finvis: Applied visual analytics for personal financial planning",
      "venue": "In Visual Analytics Science and Technology,",
      "year": 2009
    },
    {
      "authors": [
        "Dominik Sacha",
        "Michael Sedlmair",
        "Leishi Zhang",
        "John Aldo Lee",
        "Daniel Weiskopf",
        "Stephen North",
        "Daniel Keim"
      ],
      "title": "Human-centered machine learning through interactive visualization",
      "venue": "In 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning",
      "year": 2016
    },
    {
      "authors": [
        "Dominik Sacha",
        "Hansi Senaratne",
        "Bum Chul Kwon",
        "Geoffrey Ellis",
        "Daniel A Keim"
      ],
      "title": "The role of uncertainty, awareness, and trust in visual analytics",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 22,",
      "year": 2016
    },
    {
      "authors": [
        "Bahador Saket",
        "Arjun Srinivasan",
        "Eric D Ragan",
        "Alex Endert"
      ],
      "title": "Evaluating interactive graphical encodings for data visualization",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 24,",
      "year": 2017
    },
    {
      "authors": [
        "Wojciech Samek",
        "Alexander Binder",
        "Gr\u00e9goire Montavon",
        "Sebastian Lapuschkin",
        "Klaus-Robert M\u00fcller"
      ],
      "title": "Evaluating the visualization of what a deep neural network has learned",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems 28,",
      "year": 2017
    },
    {
      "authors": [
        "Christian Sandvig",
        "Kevin Hamilton",
        "Karrie Karahalios",
        "Cedric Langbort"
      ],
      "title": "Auditing algorithms: Research methods for detecting discrimination on internet platforms. Data and Discrimination: Converting Critical Concerns Into Productive Inquiry",
      "year": 2014
    },
    {
      "authors": [
        "Martin Schaffernicht",
        "Stefan N Groesser"
      ],
      "title": "A comprehensive method for comparing mental models of dynamic systems",
      "venue": "European Journal of Operational Research 210,",
      "year": 2011
    },
    {
      "authors": [
        "Ute Schmid",
        "Christina Zeller",
        "Tarek Besold",
        "Alireza Tamaddoni-Nezhad",
        "Stephen Muggleton"
      ],
      "title": "How does predicate invention affect human comprehensibility",
      "venue": "In International Conference on Inductive Logic Programming",
      "year": 2016
    },
    {
      "authors": [
        "Philipp Schmidt",
        "Felix Biessmann"
      ],
      "title": "Quantifying interpretability and trust in machine learning systems",
      "venue": "arXiv preprint arXiv:1901.08558",
      "year": 2019
    },
    {
      "authors": [
        "Ramprasaath R Selvaraju",
        "Michael Cogswell",
        "Abhishek Das",
        "Ramakrishna Vedantam",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "venue": "In Proceedings of the IEEE international conference on computer vision",
      "year": 2017
    },
    {
      "authors": [
        "Avanti Shrikumar",
        "Peyton Greenside",
        "Anshul Kundaje"
      ],
      "title": "Learning important features through propagating activation differences",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org,",
      "year": 2017
    },
    {
      "authors": [
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "year": 2013
    },
    {
      "authors": [
        "Daniel Smilkov",
        "Shan Carter",
        "D Sculley",
        "Fernanda B Vi\u00e9gas",
        "Martin Wattenberg"
      ],
      "title": "Direct-manipulation visualization of deep networks",
      "year": 2017
    },
    {
      "authors": [
        "Thilo Spinner",
        "Udo Schlegel",
        "Hanna Sch\u00e4fer",
        "andMennatallah El-Assady"
      ],
      "title": "explAIner: A visual analytics framework for interactive and explainable machine learning",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "year": 2019
    },
    {
      "authors": [
        "Hendrik Strobelt",
        "Sebastian Gehrmann",
        "Hanspeter Pfister",
        "Alexander M Rush"
      ],
      "title": "LstmVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks",
      "venue": "IEEE Transactions on Visualization and Computer Graphics 24,",
      "year": 2018
    },
    {
      "authors": [
        "Simone Stumpf",
        "Vidya Rajaram",
        "Lida Li",
        "Weng-Keen Wong",
        "Margaret Burnett",
        "Thomas Dietterich",
        "Erin Sullivan",
        "Jonathan Herlocker"
      ],
      "title": "Interacting meaningfully with machine learning systems: Three experiments",
      "venue": "International Journal of Human-Computer Studies 67,",
      "year": 2009
    },
    {
      "authors": [
        "Simone Stumpf",
        "Simonas Skrebe",
        "Graeme Aymer",
        "Julie Hobson"
      ],
      "title": "Explaining smart heating systems to discourage fiddling with optimized behavior",
      "year": 2018
    },
    {
      "authors": [
        "Latanya Sweeney"
      ],
      "title": "Discrimination in online ad delivery",
      "venue": "Commun. ACM 56,",
      "year": 2013
    },
    {
      "authors": [
        "Jiliang Tang",
        "Huiji Gao",
        "Huan Liu",
        "Atish Das Sarma"
      ],
      "title": "eTrust: Understanding trust evolution in an online world",
      "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2012
    },
    {
      "authors": [
        "Christina Tikkinen-Piri",
        "Anna Rohunen",
        "Jouni Markkula"
      ],
      "title": "EU general data protection regulation: Changes and implications for personal data collecting companies",
      "venue": "Computer Law & Security Review 34,",
      "year": 2018
    },
    {
      "authors": [
        "Nava Tintarev",
        "Judith Masthoff"
      ],
      "title": "Designing and evaluating explanations for recommender systems",
      "venue": "In Recommender Systems Handbook",
      "year": 2011
    },
    {
      "authors": [
        "Richard Tomsett",
        "Dave Braines",
        "Dan Harborne",
        "Alun Preece",
        "Supriyo Chakraborty"
      ],
      "title": "Interpretable to whom? A role-based model for analyzing interpretable machine learning systems",
      "year": 2018
    },
    {
      "authors": [
        "Matteo Turilli",
        "Luciano Floridi"
      ],
      "title": "The ethics of information transparency",
      "venue": "Ethics and Information Technology 11,",
      "year": 2009
    },
    {
      "authors": [
        "Jo Vermeulen",
        "Geert Vanderhulst",
        "Kris Luyten",
        "Karin Coninx"
      ],
      "title": "PervasiveCrystal: Asking and answering why and why not questions about pervasive computing applications",
      "venue": "In Intelligent Environments (IE),",
      "year": 2010
    },
    {
      "authors": [
        "Sandra Wachter",
        "Brent Mittelstadt",
        "Chris Russell"
      ],
      "title": "Counterfactual explanations without opening the black box: Automated decisions and the GDPR",
      "venue": "Harvard Journal of Law & Technology",
      "year": 2017
    },
    {
      "authors": [
        "Danding Wang",
        "Qian Yang",
        "Ashraf Abdul",
        "Brian Y. Lim"
      ],
      "title": "Designing theory-driven user-centric explainable AI",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI \u201919)",
      "year": 2019
    },
    {
      "authors": [
        "Fulton Wang",
        "Cynthia Rudin"
      ],
      "title": "Falling rule lists",
      "venue": "In Artificial Intelligence and Statistics",
      "year": 2015
    },
    {
      "authors": [
        "Qianwen Wang",
        "Jun Yuan",
        "Shuxin Chen",
        "Hang Su",
        "Huamin Qu",
        "Shixia Liu"
      ],
      "title": "Visual genealogy of deep neural networks",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "year": 2019
    },
    {
      "authors": [
        "Daniel S. Weld",
        "Gagan Bansal"
      ],
      "title": "The challenge of crafting intelligible intelligence",
      "venue": "Commun. ACM 62,",
      "year": 2019
    },
    {
      "authors": [
        "Adrian Weller"
      ],
      "title": "Challenges for transparency",
      "venue": "arXiv preprint arXiv:1708.01870",
      "year": 2017
    },
    {
      "authors": [
        "Gesa Wiegand",
        "Matthias Schmidmaier",
        "Thomas Weber",
        "Yuanting Liu",
        "Heinrich Hussmann"
      ],
      "title": "2019. I drive-you trust: Explaining driving behavior of autonomous cars",
      "venue": "In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. ACM,",
      "year": 2019
    },
    {
      "authors": [
        "James A Wise",
        "James J Thomas",
        "Kelly Pennock",
        "David Lantrip",
        "Marc Pottier",
        "Anne Schur",
        "Vern Crow"
      ],
      "title": "Visualizing the non-visual: Spatial analysis and interaction with information from text documents",
      "venue": "In Information Visualization,",
      "year": 1995
    },
    {
      "authors": [
        "Kanit Wongsuphasawat",
        "Daniel Smilkov",
        "James Wexler",
        "Jimbo Wilson",
        "Dandelion Mane",
        "Doug Fritz",
        "Dilip Krishnan",
        "Fernanda B Vi\u00e9gas",
        "Martin Wattenberg"
      ],
      "title": "Visualizing dataflow graphs of deep learning models in tensorflow",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "year": 2017
    },
    {
      "authors": [
        "Samuel C Woolley"
      ],
      "title": "Automating power: Social bot interference in global politics",
      "venue": "First Monday 21,",
      "year": 2016
    },
    {
      "authors": [
        "Mike Wu",
        "Michael C Hughes",
        "Sonali Parbhoo",
        "Maurizio Zazzi",
        "Volker Roth",
        "Finale Doshi-Velez"
      ],
      "title": "Beyond sparsity: Tree regularization of deep models for interpretability",
      "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "Ming Yin",
        "Jennifer Wortman Vaughan",
        "Hanna Wallach"
      ],
      "title": "Understanding the effect of accuracy on trust in machine learning models",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Jason Yosinski",
        "Jeff Clune",
        "Anh Nguyen",
        "Thomas Fuchs",
        "Hod Lipson"
      ],
      "title": "Understanding neural networks through deep visualization",
      "venue": "In ICML Deep Learning Workshop",
      "year": 2015
    },
    {
      "authors": [
        "Rulei Yu",
        "Lei Shi"
      ],
      "title": "A user-based taxonomy for deep learning visualization",
      "venue": "Visual Informatics 2,",
      "year": 2018
    },
    {
      "authors": [
        "Tom Zahavy",
        "Nir Ben-Zrihem",
        "Shie Mannor"
      ],
      "title": "Graying the black box: Understanding DQNs",
      "venue": "In International Conference on Machine Learning",
      "year": 2016
    },
    {
      "authors": [
        "Tal Zarsky"
      ],
      "title": "The trouble with algorithmic decisions: An analytic road map to examine efficiency and fairness in automated and opaque decision making",
      "venue": "Science, Technology, & Human Values 41,",
      "year": 2016
    },
    {
      "authors": [
        "Matthew D Zeiler",
        "Rob Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "In European Conference on Computer",
      "year": 2014
    },
    {
      "authors": [
        "Quanshi Zhang",
        "Wenguan Wang",
        "Song-Chun Zhu"
      ],
      "title": "Examining cnn representations with respect to dataset bias",
      "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "Quan-shi Zhang",
        "Song-Chun Zhu"
      ],
      "title": "Visual interpretability for deep learning: a survey",
      "venue": "Frontiers of Information Technology & Electronic Engineering",
      "year": 2018
    },
    {
      "authors": [
        "Yunfeng Zhang",
        "Q. Vera Liao",
        "Rachel K.E. Bellamy"
      ],
      "title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making",
      "venue": "In Proceedings of the 2020 Conference on Fairness, Accountability,",
      "year": 2020
    },
    {
      "authors": [
        "Zijian Zhang",
        "Jaspreet Singh",
        "Ujwal Gadiraju",
        "Avishek Anand"
      ],
      "title": "Dissonance between human and machine understanding",
      "venue": "Proceedings of the ACM on Human-Computer Interaction",
      "year": 2019
    },
    {
      "authors": [
        "Wen Zhong",
        "Cong Xie",
        "Yuan Zhong",
        "Yang Wang",
        "Wei Xu",
        "Shenghui Cheng",
        "Klaus Mueller"
      ],
      "title": "Evolutionary visual analysis of deep neural networks",
      "venue": "In ICML Workshop on Visualization for Deep Learning",
      "year": 2017
    },
    {
      "authors": [
        "Jichen Zhu",
        "Antonios Liapis",
        "Sebastian Risi",
        "Rafael Bidarra",
        "G Michael Youngblood"
      ],
      "title": "Explainable AI for designers: A human-centered perspective on mixed-initiative co-creation",
      "venue": "IEEE Conference on Computational Intelligence and Games (CIG)",
      "year": 2018
    },
    {
      "authors": [
        "Luisa M Zintgraf",
        "Taco S Cohen",
        "Tameem Adel",
        "Max Welling"
      ],
      "title": "Visualizing deep neural network decisions: Prediction difference analysis",
      "venue": "arXiv preprint arXiv:1702.04595",
      "year": 2017
    }
  ],
  "sections": [
    {
      "text": "1"
    },
    {
      "heading": "A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems",
      "text": "SINA MOHSENI and NILOOFAR ZAREI, Texas A&M University ERIC D. RAGAN, University of Florida\nThe need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable AI systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of Explainable AI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of Explainable AI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of Explainable AI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of Explainable AI design goals and evaluation methods. Our categorization presents the mapping between design goals for different Explainable AI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary Explainable AI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in Explainable AI research.\nAdditional Key Words and Phrases: Explainable artificial intelligence (XAI); human-computer interaction (HCI); machine learning; explanation; transparency;\nACM Reference Format: Sina Mohseni, Niloofar Zarei, and Eric D. Ragan. 2020. A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems. ACM Trans. Interact. Intell. Syst. 1, 1, Article 1 (January 2020), 46 pages. https://doi.org/10.1145/3387166"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Impressive applications of Artificial Intelligence (AI) and machine learning have become prevalent in our time. Tech giants like Google, Facebook, and Amazon have collected and analyzed enough personal data through smartphones, personal assistant devices, and social media that can model individuals better than other people. Recent negative interference of social media bots in political elections [91, 212] were yet another sign of how susceptible our lives are to the misuse of artificial intelligence and big data [163]. In these circumstances, despite tech giants and the thirst for more advanced systems, others suggest holding off on fully unleashing AI for critical applications until they can be better understood by those who will rely on them. The demand for predictable and\nAuthors\u2019 addresses: Sina Mohseni, sina.mohseni@tamu.edu; Niloofar Zarei, n.zarei.3001@tamu.edu, Texas A&M University, College Station, Texas; Eric D. Ragan, eragan@ufl.edu, University of Florida, Gainesville, Florida.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2020 Association for Computing Machinery. 2160-6455/2020/1-ART1 $15.00 https://doi.org/10.1145/3387166\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nar X\niv :1\n81 1.\n11 83\n9v 5\n[ cs\n.H C\n] 5\nA ug\n2 02\n0\naccountable AI grows as tasks with higher sensitivity and social impact are more commonly entrusted to AI services. Hence, algorithm transparency is an essential factor in holding organizations responsible and accountable for their products, services, and communication of information. Explainable Artificial Intelligence (XAI) systems are a possible solution towards accountable AI, making it possible by explaining AI decision-making processes and logic for end users [72]. Specifically, explainable algorithms can enable control and oversight in case of adverse or unwanted effects, such as biased decision-making or social discrimination. An XAI system can be defined as a self-explanatory intelligent system that describes the reasoning behind its decisions and predictions. The AI explanations (either on-demand explanations or in the form of model description) could benefit users in many ways such as improving safety and fairness when relying on AI decisions.\nWhile the increasing impact of advanced black-box machine learning systems in the big-data era has attracted much attention from different communities, interpretability of intelligent systems has also been studied in numerous contexts [69, 167]. The study of personalized agents, recommendation systems, and critical decision-making tasks (e.g., medical analysis, powergrid control) has added to the importance of machine-learning explanation and AI transparency for end-users. For instance, as a step towards this goal, the legal right to explanations has been established in the European Union General Data Protection Regulation (GDPR) commission. While the current state of regulations is mainly focused on user data protection and privacy, it is expected to cover more algorithmic transparency and explanations requirements from AI systems [67]. Clearly, addressing such a broad array of definitions and expectations for XAI requires multidisciplinary research efforts, as existing communities have different requirements and often have drastically different priorities and areas of specialization. For instance, research in the domain of machine learning seeks to design new interpretable models and explain black -box models with ad-hoc explainers. Along the same line but with different approaches, researchers in visual analytics design and study tools and methods for data and domain experts to visualize complex black-box models and study interactions to manipulate machine learning models. In contrast, research in human-computer interaction (HCI) focuses on end-user needs such as user trust and understanding of machine generated explanations. Psychology research also studies the fundamentals of human understanding, interpretability, and the structure of explanations. Looking at the broad spectrum of research on XAI, it is evident that scholars from different disciplines have different goals in mind. Even though different aspects of XAI research are following the general goals of AI interpretability, researchers in each discipline use different measures and metrics to evaluate the XAI goals. For example, numerical analytic methods are employed in machine learning fields to evaluate computational interpretability, while human interpretability and human-subjects evaluations are more commonly the primary goals in HCI and visualization communities. In this regard, although there seems to be a mismatch in specific objectives for designing and evaluating explainability and interpretability, a convergence in goals is beneficial for achieving the full potential of XAI. To this end, this paper presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. To support the diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and HCI, we present a categorization of interpretable machine learning design goals and evaluation methods and show a mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation loops in multidisciplinary teams. Further, we provide summarized ready-to-use evaluation methods for different goals in XAI research. Lastly, we review recommendations for XAI design and evaluation drawn from our literature review.\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020."
    },
    {
      "heading": "2 BACKGROUND",
      "text": "Nowadays, algorithms analyze user data and affect decision-making processes for millions of people on matters like employment, insurance rates, loan rates, and even criminal justice [35]. However, these algorithms that serve critical roles in many industries have their own disadvantages that can result in discrimination [44, 196], and unfair decision-making [163]. For instance, recently, news feed and targeted advertising algorithms in social media have attracted much attention for aggravating the lack of information diversity in social media [23]. A significant part of the trouble could be because algorithmic decision-making systems\u2014unlike recommender systems\u2014do not allow their users to choose between the recommended items, but instead, present the most relevant content or option themselves. To address this, Heer [75] suggests the use of shared representations of tasks that are augmented with both machine learning models and user knowledge to reduce negative effects of immature AI autonomous systems. They present case studies of interactive systems that integrate proactive computational support into interactive systems. Bellotti and Edwards [16] argue that intelligent context-aware systems should not act on our behalf. They suggest user control over the system as a principle to support the accountability of a system and its users. Transparency can provide essential information for decision-making that is hidden to the end-users and prevents blind faith [218]. The key benefits of algorithmic transparency and interpretability include: user awareness [9]; bias and discrimination detection [45, 196]; interpretable behavior of intelligent systems [124]; and accountability for users [46]. Furthermore, considering the growing body of examples of discrimination and other legal aspects of algorithmic decision making, researchers are demanding and investigating transparency and accountability of AI under the law to mitigate adverse effects of algorithmic decision making [49, 145, 201]. In this section, we review research background related to XAI systems from a broad and multidisciplinary perspective. At the end, we relate the summaries and positions derived through our survey to other work in the field."
    },
    {
      "heading": "2.1 Auditing Inexplicable AI",
      "text": "Researchers audit algorithms to study bias and discrimination in algorithmic decision making [184] and study the users\u2019 awareness of the effects of these algorithms [58]. Auditing of algorithms is a mechanism for investigating algorithms\u2019 functionality to detect bias and other unwanted algorithm behaviors without the need to know about its specific design details. Auditing methods focus on problematic effects on the results of algorithmic decision-making systems. To audit an algorithm, researchers feed new inputs to the algorithm and review system output and behavior. Researchers generate new data and user accounts with the help of scripts, bots [44], and crowdsourcing [73] to emulate real data and real users in the auditing process. For bias detection among multiple algorithms, cross-platform auditing can detect if an algorithm behaves differently from another algorithm. A recent example of cross-platform auditing is a work by Eslami et al. [59], in which they analyzed user reviews in three hotel booking websites to study user awareness of bias in online rating algorithms. These examples demonstrate that auditing is a valuable yet time-intensive process that could not be scaled easily to large numbers of algorithms. This calls for new research for more effective solutions toward algorithmic transparency."
    },
    {
      "heading": "2.2 Explainable AI",
      "text": "Along with the methods mentioned above for supporting transparency, machine learning explanations have also become a common approach to achieve transparency in many applications such as social media, e-commerce, and data-driven management of human workers [116, 197, 199]. The XAI system, as illustrated in Figure 1, is able to generate explanations and describe the reasoning\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nbehind machine-learning decisions and predictions. Machine-learning explanations enable users to understand how the data is processed. They aim to bring awareness to possible bias and system malfunctions. For example, to measure user perception of justice in intelligent decision making, Binns et al. [20] studied explanations in systems for everyday tasks such as determining car insurance rates and loan application approvals. Their results highlight the importance of machine learning explanations in users\u2019 comprehension and trust in algorithmic decision-making systems. In a similar work studying knowledge of social media algorithms, Radar et al. [170] ran a crowdsourced study to see how different types of explanations affect users\u2019 beliefs on news feed algorithmic transparency in a social media platform. In their study, they measured users\u2019 awareness, correctness, and accountability to evaluate algorithmic transparency. They found that all explanations caused users to become more aware of the system\u2019s behavior. Stumpf et al. [194] designed experiments to investigate meaningful explanations and interactions to hold users accountable by machine learning algorithms. They show explanations as a potential method for supporting richer human-computer collaboration to share intelligence.\nThe recent advancements and trends for explainable AI research demand a wide range of goals for algorithmic transparency which calls for research across varied application areas. To this end, our review encourages a cross-discipline perspective of intelligibility and transparency goals."
    },
    {
      "heading": "2.3 Related Surveys and Guidelines",
      "text": "In recent years, there have been surveys and position papers suggesting research directions and highlighting challenges in interpretable machine learning research [48, 78, 127]. Although our review is limited to computer science literature, here we summarize several of the most relevant peer-reviewed surveys related to the topic of XAI across active disciplines including social science. While all surveys, models, and guidelines in this section add value to the XAI research, to the best of our knowledge, there is no existing comprehensive survey and framework for evaluation methods of explainable machine learning systems.\n2.3.1 Social Science Surveys. Research in the social sciences is particularly important for XAI systems to understand how people generate, communicate, and understand explanations by taking into account each others\u2019 thinking, cognitive biases, and social expectations in the process of explaining. Hoffman, Mueller, and Klein reviewed the key concepts of explanations for intelligent systems in a series of essays to identify how people formulate and accept explanations, ways to generate self-explanations, and identified purposes and patterns for causal reasoning [83, 84, 102]. They lastly focus on deep neural networks (DNN) to examine their theoretical and empirical findings on a machine learning algorithm [79]. In other work, they presented a conceptual model of the process of explaining in the XAI context [85]. Their framework includes specific steps and\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nmeasures for the goodness of explanations, user satisfaction and understanding of explanations, users\u2019 trust and reliance on XAI systems, effects of curiosity on the search for explanations, and human-XAI system performance.\nMiller [142] suggests a close collaboration between machine learning researchers in the space of XAI with social science would further refine the explainability of AI for people. He discusses how understanding and replicating how people generate, select, and present explanations could improve human-XAI interactions. For instance, Miller reviews how people generate and select explanations that are involvedwith cognitive biases and social expectations. Other papers reviewing social science aspects of XAI systems include studies on the role of algorithmic transparency and explanation in lawful AI [49] and of fair and accountable algorithmic decision-making processes [117].\n2.3.2 Human Computer Interactions Surveys. Many HCI surveys discuss the limitations and challenges in AI transparency [208] and interactive machine learning [6]. Others suggest a set of theoretical and design principles to support intelligibility of intelligent system and accountability of human users (e.g., [16, 90]). In a recent survey, Abdul et al. [1] presented a thorough literature analysis to find XAI-related topics and relationships among these topics. They used visualization of keywords, topic models, and citation networks to present a holistic view of research efforts in a wide range of XAI related domains; from privacy and fairness to intelligent agents and context-aware systems. In another work, Wang et al. [204] explored theoretical underpinnings of human decisionmaking and proposed a conceptual framework for building human-centered decision-theory-driven XAI systems. Their framework helps to choose better explanations to present, backed by reasoning theories, and human cognitive biases. Focused on XAI interface design, Eiband et al. [56] present a stage-based participatory process for integration of transparency in existing intelligent systems using explanations. Another design framework is XAID from Zhu et al. [225], which presents a human-centered approach for facilitating game designers to co-create with machine learning techniques. Their study investigates the usability of XAI algorithms in terms of how well they support game designers.\n2.3.3 Visual Analytics Surveys. XAI-related surveys in the visualization domain follow visual analytics goals such as understanding and interacting with machine learning systems in different visual analytics applications [57, 180]. Choo and Liu [34] reviewed challenges and opportunities for Visual Analytics for explainable deep learning design. In a recent paper, Hohman et al. [88] provide an excellent review and categorization of visual analytics tools for deep learning applications. They cover various data and visualization techniques that are being used in deep visual analytics applications. Also, Spinner et al. [192] proposed a XAI pipeline which maps the XAI process to an iterative workflow in three stages: model understanding, diagnosis, and refinement. To operationalize their framework, they designed explAIner, a visual analytics system for interactive and interpretable machine learning that instantiates all steps of their pipeline.\n2.3.4 Machine Learning Surveys. In the machine learning area, Guidotti et al. [71] present a comprehensive review and classification of machine learning interpretability techniques. Also, Montavon et al. [152] focus on interpretability techniques for DNN models. On Convolutional Neural Network (CNN), Zhang et al. [221] reviews research on interpretability techniques in six directions including visualization of CNN representations, diagnosing techniques for CNNs, approaches for transforming CNN representations into interpretable graphs, building explainable models, and semantic-level learning based on model interpretability. In another work, Gilpin et al. [64] reviews interpretability techniques in machine learning algorithms and categorizes evaluation approaches to bridge the gap between machine learning and HCI communities.\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nIn complementing the existing work, our survey provides a multidisciplinary categorization of design goals and evaluation methods for XAI systems. As a result of surveyed papers, we propose a framework that provides a step-by-step design and evaluation plan for a multidisciplinary team of designers for building real-world XAI systems. Unlike Eiband et al. [56], we do not make the assumption of adding transparency to an existing intelligent interface and do not limit the evaluation of XAI systems to the users\u2019 mental model. We instead characterize both design goals and evaluation methods and compile all in a unified framework for multidisciplinary teamwork. Our design framework has similarities to Wang et al\u2019s [204] theoretical framework which supports our design goals (see Section 9.6). Our multidisciplinary work extends their conceptual framework by 1) including the design of interpretability algorithms as part of the framework and 2) pairing evaluation methods with each design step to close the iterative design and evaluation loops."
    },
    {
      "heading": "3 SURVEY METHOD",
      "text": "We conducted a survey of the existing research literature to capture and organize the breadth of designs and goals for XAI evaluation. We used a structured and iterative methodology to find XAIrelevant research and categorize the evaluation methods presented in research articles (summarized in Figure 2). In our iterative paper selection process, we started by selecting existing work from top computer science conferences and journals across the fields of HCI, visualization, and machine learning. However, since XAI is a quite fast growing topic, we also wanted to include arXiv preprints and useful discussions in workshop papers. We started with 40 papers related to XAI topics across three research fields including but not limited to research on interpretable machine learning techniques, deep learning visualization, interactive model visualization, machine explanations in intelligent agents and context-aware systems, explainable user interfaces, explanatory debugging, and algorithmic transparency and fairness.\nWe then used selective coding to identify 10 main research attributes in those papers. The main attributes we identified include: research discipline (social science, HCI, visualization, or machine learning), paper type (interface design, algorithm design, or evaluation paper), application domain (machine learning interpretability, algorithmic fairness, recommendation systems, transparency of intelligent systems, intelligent interactive systems and agents, explainable intelligent systems and agents, human explanations, or human trust), machine learning model (e.g., deep learning, decision trees, SVM), data modality (image, text, tabular data), explanation type (e.g., graphical, textual, data visualization), design goal (e.g., model debugging, user reliance, bias mitigation), evaluation type (e.g., qualitative, computational, quantitative with human-subjects), targeted user (AI novices, data experts, AI experts), and evaluation measure (e.g., user trust, task performance, user mental model).\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nIn the second round of collecting XAI literature, we conducted an upward and downward literature investigation using the Google Scholar search engine to add 80 more papers to our reference table. We narrowed down our search by XAI related topics and keywords including but not limited to: interpretability, explainability, intelligibility, transparency, algorithmic decisionmaking, fairness, trust, mental model, and debugging in machine learning and intelligent systems. With this information, we performed axial coding to organize the literature and started discussions on our proposed design and evaluation categorization. Finally, to maintain reasonable literature coverage and balance the number of papers for each of our categories of design goals and evaluation measures, we added another 80 papers to our reference table. The conferences from which we selected XAI related paper were including but not limited to: CHI, IUI, HCOMP, SIGDIAL, UbiComp, AIES, VIS, ICWSM, IJCAI, KDD, AAAI, CVPR, and NeurIPS conferences. The journals included: Trends in cognitive science, Transactions on Cognitive and Developmental Systems, Cognition Journal, Transactions on Interactive Intelligent Systems, International Journal of Human-Computer Studies, Transactions on Visualization and Computer Graphics, and Transactions on Neural Networks and Learning Systems.\nFollowing a review of 226 papers, our categorization of XAI design goals and evaluation methods is supported by references from papers performing design or evaluation of XAI systems. Our reference table1 is available online to the research community to provide further insight beyond our discussions in this document. Table 2 shows a digest of our surveyed papers that contains 42 papers with both design and evaluation of XAI system. Later in the Section 7, we provide a series of tables to organize different evaluation methods from research papers with example references for each, documenting our in-depth analysis of 69 papers in total."
    },
    {
      "heading": "4 XAI TERMINOLOGY",
      "text": "To familiarize the readers with common XAI concepts and terminologies that are repeatedly referenced in this review, the following four subsections summarize high-level characterizations of model explanations. Many related surveys (e.g., [2, 207]) and reports (e.g., [38, 200]) also provide useful compilations of terminology and concepts in comprehensive reports. For instance, Abdul et al. [1] present a citation graph from diverse domains related to explanations, including intelligible intelligent systems, context-aware systems, and software learnability. Later, Arrieta et al. [11] present a thorough review of XAI concepts and taxonomies and arrives at the concept of Responsible AI as a manifold of multiple AI principles including model fairness, explainability, and privacy. Similarly, the concept of Safe AI has been reviewed by Amodei et al. [8], which is an interest in safety-critical intelligent applications such as autonomous vehicles [147]. Table 1 presents descriptions for 14 common terms related to this survey\u2019s topic and organizes their relation to Intelligible Systems and Transparent AI topics. We consider Transparent AI systems as the AI-based class of Intelligible Systems. Therefore, properties and goals previously established for Intelligible Systems are ideally transferable to Transparent AI systems. However, challenges and limitations for achieving transparency in complex machine learning algorithms raise issues (e.g., ensuring the fairness of an algorithm) that were not necessarily problematic in intelligible rule-based systems but now require closer attention from research communities.\nThe descriptions presented in Table 1 are meant to be an introduction to these terms, though exact definitions and interpretations can depend on usage context and research discipline. Consequently, researchers from different disciplines often use these terms interchangeably, disregarding differences in meaning [2]. Perhaps the two generic terms of Black-box Model and Transparent Model are in the center of XAI terminology ambiguity. The black-box term refers to complex machine learning\n1https://github.com/SinaMohseni/Awesome-XAI-Evaluation\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nmodels that are not human-interpretable [127] as opposed to transparent models which are simple enough to be human-interpretable [11]. We find it more accurate and consistent to separate the transparency of an XAI system (as described in Figure 1) from the interpretability of its internal machine learning models. Specifically, Table 1 shows that Transparent AI could be achieved by either Interpretable AI or Explainable AI approaches. Other examples of terminology ambiguity include the terms Interpretability and Explainability that are often used as synonyms in the field of machine learning. For example the phrase \u201cinterpretable machine learning technique\u201d often refers to ad-hoc techniques for generating explanations for non-interpretable models such as DNNs [151]. Another example is the occasional case of using the terms Transparent System and Explainable System interchangeably in HCI research [56], while others clarify that explainability is not equivalent to transparency because it does not require knowing the flow of the bits in the AI decision-making process [49]."
    },
    {
      "heading": "4.1 Global and Local Explanations",
      "text": "One way to classify explanations is by their interpretation scale. For instance, an explanation could be as thorough as describing the entire machine learning model. Alternatively, it could only\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\npartially explain the model, or it could be limited to explaining an individual input instance. Global Explanation (or Model Explanation) is an explanation type that describes how the overall machine learning model works. Model visualization [130, 131] and decision rules [113] are examples of explanations falling in this category. In other cases, interpretable approximations of complex models serve as the model explanation. Tree regularization [213] is a recent example of regularized complex model to learn tree-like decision boundaries. Model complexity and explanation design are the main factors used to choose between different types of global explanations.\nIn contrast, Local Explanations (or Instance Explanations) aim to explain the relationship between specific input-output pairs or the reasoning behind the results for an individual user query. This type of explanation is thought to be less overwhelming for novices, and it can be suited for investigating edge cases for the model or debugging data. Local explanations often make use of saliency methods [14, 219] or local approximation of the main model [172, 173]. Saliency methods (also as known as attribution maps or sensitivity maps) use different approaches (e.g., perturbationbased methods, gradient-based methods) to show what features in the input strongly influence the model\u2019s prediction. Local approximation of the model, on the other hand, trains an interpretable model (learned from the main model) to locally represent the complex model\u2019s behavior."
    },
    {
      "heading": "4.2 Interpretable Models vs. Ad-hoc Explainers",
      "text": "The human interpretability of a machine learning model is inversely proportional to the model\u2019s size and complexity. Complex models (e.g., deep neural networks) with high performance and robustness in real-world applications are not interpretable by human users due to their large variable space. Linear regression models or decision trees offer better interpretability but have limited performance on high-dimensional data, whereas a random forest model (ensemble of hundreds of decision trees) can have much higher performance but is less understandable. This trade-off between model interpretability and performance led researchers to design ad-hoc methods to explain any black-box machine learning algorithm such as deep neural networks. Ad-hoc explainers (e.g., [134, 172]) are independent algorithms that can describe model predictions by explaining \u201cwhy\u201d a certain decision has been made instead of describing the whole model. However, there are limitations in explaining black-box models with ad-hoc explainers, such as the uncertainty of the fidelity of the explainer itself. We will discuss more about the fidelity of explanations in Section 7.5. Furthermore, although ad-hoc explainers generally describe \u201cwhy\u201d a prediction is made, these methods lack in explaining \u201chow\u201d the decision is made."
    },
    {
      "heading": "4.3 What to Explain",
      "text": "When users face a complex intelligent system, they may demand different types of explanatory information and each explanation type may require its own design. Here we review six common types of explanations used in XAI system designs.\nHow Explanations demonstrate a holistic representation of the machine learning algorithm to explain How the model works. For visual representations, model graphs [113] and decision boundaries [135] are common design examples for How explanations. However, research shows users may also be able to develop a mental model of the algorithm based on a collection of explanations from multiple individual instances [133]. Why Explanations describeWhy a prediction is made for a particular input. Such explanations aim to communicate what features in the input data [172] or what logic in the model [113, 173] has led to a given prediction by the algorithm. This type of explanation can have either model agnostic [134, 172] or model dependent [188] solutions.\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nWhy-not Explanations help users to understand the reasons why a specific output was not in the output of the system [202].Why-not explanations (also called Contrastive Explanations) characterize the reasons for differences between a model prediction and the user\u2019s expected outcome. Feature importance (or feature attribution) is commonly used as an interpretability technique for Why and Why-not explanations. What-If Explanations involve demonstration of how different algorithmic and data changes affect model output given new inputs [29], manipulation of inputs [125], or changing model parameters [103]. Different what-if scenarios may be automatically recommended by the system or can be chosen for exploration through interactive user control. Domains with high-dimensional data (e.g., image and text) and complex machine learning models (e.g., DNNs) have fewer parameters for users to directly tune and examine trained model compared to simpler data (e.g., low-dimensional tabular data) and models. How-to Explanations spell out hypothetical adjustments to the input ormodel that would result in a different output [125, 126], such as a user-specified output of interest. Techniques to generateHowto (or Counterfactual) explanations are ad-hoc and model-agnostic considering that model structure and internal values are not a part of the explanation [203]. Such methods can work interactively with the user\u2019s curiosity and partial conception of the system to allow an evolving mental model of the system through iterative testing. What-else Explanations present users with similar instances of input that generate the same or similar outputs from the model. Also called Explanation by Example, What-else explanations pick samples from the model\u2019s training dataset that are similar to the original input in the model representation space [30]. Although very popular and easy to achieve, research shows examplebased explanations could be misleading when training datasets lack uniform distribution of the data [98]."
    },
    {
      "heading": "4.4 How to Explain",
      "text": "In all types of machine learning explanations, the goal is to reveal new information about the underlying system. In this survey, we mainly focus on human-understandable explanations, though we note that research on interpretable machine learning has also studied other purposes such as knowledge transfer, object localization, and error detection [61, 162]. Explanations can be designed using a variety of formats for different user groups [216]. Visual Explanations use visual elements to describe the reasoning behind the machine learning models. Attention maps and visual saliency in the form of saliency heatmaps [190, 219] are examples of visual explanations that are widely used in machine learning literature. Verbal Explanations describe the machine\u2019s model or reasoning with words, phrases, or natural language. Verbal explanations are popular in applications like question-answering explanations and decision lists [113]. This form of explanation has also been implemented in recommendation systems [17, 77] and robotics [176]. Explainable interfaces commonly make use of multiple modalities (e.g., visual, verbal, and numerical elements) for explanations to support user understanding [156]. Analytic Explanation is another approach to view and explore the data and the machine learning models representations [88]. Analytic explanations commonly rely on numerical metrics and data visualizations. Visual analytics tools also allow researchers to review model structures, relations, and their parameters in complex deep models. Heatmap visualizations [193], graphs and networks [66], and hierarchical (decision trees) visualizations are commonly used to visualize analytic explanations for interpretable algorithms. Recently, Hohman et al. [87] implemented a combination of visualization and verbalization to communicate or summarize key aspects of a model. From a different perspective, Chromik et al. [36] extends the idea of \u201cdark patterns\u201d from interactive user interface design [68] into machine learning explanations. They review possible\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nways that phrasing of explanations and their implementation in the interface could deceive users for the benefit of other parties. They review negative effects such as lack of user attention to explanations, formation of an incorrect mental model, and even algorithmic anxiety [93] could be among the consequences of such deceptive presentations and interactions of machine learning explanations."
    },
    {
      "heading": "5 CATEGORIZATION OF XAI DESIGN GOALS AND EVALUATION METHODS",
      "text": "While an ideal XAI system should be able to answer all user queries and meet all XAI concept goals [72], individual research efforts focus on designing and studying XAI systems with respect to specific interpretability goals and specific users. Similarly, evaluating the explanations can demonstrate and verify the effectiveness of the explainable systems for their intended goals.\nAfter careful review and analysis of XAI goals and their evaluation methods in the literature, we recognized the following two attributes to be most significant for our purposes of interdisciplinary organization of XAI design and evaluation methods:\n\u2022 Design Goals. The first attribute in our categorization is the design goal for interpretable algorithms and explainable interfaces in XAI research. We obtain XAI design goals from multiple research disciplines: machine learning, data visualization, and HCI. To better understand the differences between various goals for XAI, we organize XAI design goals with their three user groups: AI novices (i.e., general AI product end-user), data experts (experts in data analytics and domain experts), and AI experts (machine learning model designers). \u2022 EvaluationMeasures. We review evaluation methods and discuss measures used to evaluate machine learning explanations. The measures include user mental model, user trust and reliance, explanation usefulness and satisfaction, human-machine task performance, and computational measures. In our review, we will pay more attention to evaluation measures of XAI as the authors believe that this category is relatively less explored.\nFigure 3 presents the pairing between XAI design goals and their evaluation measures. Note that user groups is used as an auxiliary dimension to emphasize on the importance of end users for\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nsystem goals. The overlap between XAI user groups shows similarities in the design and evaluation methods between different targeted user groups. However, the similar XAI goals in different user groups require different research objectives, design methods, and implementation paths. To help summarize our characterization along with example literature, Table 2 presents a cross-reference table of XAI evaluation literature to emphasize the importance of design goals, evaluation measures, and user types. We first review details of research focusing on XAI design goals in Section 6 including eight goals organized by their user groups. We then review evaluation measures and methods in Section 7 including six main measures and their methods collected from the surveyed literature."
    },
    {
      "heading": "6 XAI DESIGN GOALS",
      "text": "Research efforts have explored many goals for XAI systems. Doshi-Velez and Kim [48] reviewed multiple high-level priorities for XAI systems with examples including safety, ethics, user reliance, and scientific understanding. Later, Arrieta et al. [11] presented a thorough review of XAI opportunities in different application domains. Accordingly, different design choices such as explanation type, scope, and level of detail will be affected by the application domain, design goal, and user type. For example, while machine learning experts might prefer highly-detailed visualizations of deep models to help them optimize and diagnose algorithms, end-users of daily-used AI products do not expect fully detailed explanations for every query from a personalized agent. Therefore, XAI systems are expected to provide the right type of explanations for the right group of users, meaning it will be most efficient to design an XAI system according to the user\u2019s needs and levels of expertise. To this end, we distinguish XAI design goals based on the designated end-user and evaluation subjects, which we categorize into three general groups of AI experts, data experts, and AI novices. We emphasize that this separation of groups is presented primarily for organizational convenience, as goals are not mutually exclusive across groups, and specific priorities are case dependent for any particular project. The XAI design goals also extend to the broader goal of Responsible AI by improving transparency and explainability of intelligent systems. Note that although there are overlaps in the methods used to achieve these goals, the research objectives and design approaches are substantially different among distinct research fields and their user groups. For instance, even though leveraging interpretable models to reduce machine learning model bias is a research goal for AI experts, bias mitigation is also a design goal for AI novices to avoid adverse effects of algorithmic decision-making in their respective domain settings. However, interpretability techniques for AI experts and bias mitigation tools for AI novice require different design methods and elements. In the following subsections, we review eight design goals for XAI systems organized by their user groups."
    },
    {
      "heading": "6.1 AI Novices",
      "text": "AI novices refer to end-users who use AI products in daily life but have no (or very little) expertise on machine learning systems. These include end-users of intelligent applications like personalized agents (e.g., home assistant devices), social media, and e-commerce websites. In most smart systems, machine learning algorithms serve as internal functions and APIs to enable specific features embedded in intelligent and context-aware interfaces. Previous research shows intuitive interface and interaction design can enhance users\u2019 experience with the system through improving endusers\u2019 comprehension and reliance on the intelligent systems [154]. In this regard, creating humanunderstandable and yet accurate representations of complicated machine learning explanations for novice end-users is a challenging design trade-off in XAI systems. Note that although there are\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\noverlaps among goals for AI Novices and AI experts who build interpretable algorithms, each user group requires a different set of design methods and objectives that are being studied in different research communities.\nThe main design goals for AI novice end-users of XAI system can be itemized as the following:\nG1: Algorithmic Transparency: An immediate goal for a XAI system \u2013 in comparison to an inexplicable intelligent system \u2013 is to help end-users understand how the intelligent system works. Machine learning explanations improve users\u2019 mental model of the underlying intelligent algorithms by providing comprehensible transparency for the complex intelligent algorithms [208]. Further, transparency of a XAI system can improve user experience through better user understanding of model output [123], thus improving user interactions with the system [108].\nG2: User Trust and Reliance: XAI system can improve end-users trust in the intelligent algorithm by providing explanations. A XAI system lets users assess system reliability and calibrate their perception of system accuracy. As a result, users\u2019 trust in the algorithm leads to their reliance on the system. Example applications where XAI aims to improve user reliance through its transparent design include recommendation systems [17], autonomous systems [209], and critical decision making systems [26] .\nG3: Bias Mitigation: Unfair and biased algorithmic decision-making is a critical side effect of intelligent systems. Bias in machine learning has many sources, including biased training data and feature learning that could result in discrimination in algorithmic decision-making [137]. Machine learning explanations can help end-users to inspect if the intelligent systems are biased in their decision-making. Examples of cases in which XAI is used for bias mitigation and fairness assessment are criminal risk assessment [20, 115] and loan and insurance rate prediction [32]. It is worth mentioning that there is overlap between the biased decision-making mitigation goal for AI novices and the goal of dataset bias for AI experts (Section 6.2), which results in shared implementation techniques. However, the two distinct user groups require their own sets of XAI design goals and processes.\nG4: Privacy Awareness: Another goal in designing XAI systems is to provide a means for endusers to assess their data privacy. Machine learning explanations can disclose to end-users what user data is being used in algorithmic decision-making. Examples of AI application examples in which privacy awareness is primarily important include personalized advertisements using users\u2019 online advertisement [44] and personalized news feed in social media [58, 170].\nIn addition to the major XAI goals, interactive visualization tools have also been developed to help AI novices to learn machine learning concepts and models by interacting with simplified data and model representations. Examples of these educative tools include TensorFlow PlayGround [191] for teaching elementary neural networks concepts and Adversarial Playground [157] for learning concept of adversarial examples in DNNs. These minor goals cover XAI system objectives that have limited extent compared to main goals."
    },
    {
      "heading": "6.2 Data Experts",
      "text": "Data experts include data scientists and domain experts who use machine learning for analysis, decision-making, or research. Visual analysis tools can support interpretable machine learning in many ways, such as visualizing the network architecture of a trained model and training process\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nof machine learning models. Researchers have implemented various visualization designs and interaction techniques to understand better and improve machine learning models. Data experts analyze data in specialized forms and domains, such as cybersecurity [18, 66], medicine [31, 107], text [128, 131], and satellite image analysis [174]. These users might be experts of certain domain areas or experts in general areas of data science, but in our categorization, we consider users in the data experts category to generally lack expertise in the technical specifics of the machine learning algorithms. Instead, this group of users often utilize intelligent data analysis tools or visual analytics systems to obtain insights from the data. Notice that there are overlaps between XAI goals in different disciplines and visual analytics tools designed for Data Experts could be used by both model designers and data analysts. However, design needs and approaches for these XAI systems may be different across research communities. The main design goals for data experts users of a XAI system are as follows:\nG5: Model Visualization and Inspection: Similar to AI novices, data experts also benefit from machine learning interpretability to inspect model uncertainty and trustworthiness [181]. For instance, machine-learning explanations help data experts to visualize models [86] and inspect for problems like bias [4]. Another important aspect of model visualization and inspection for domain experts is to identify and analyze failure cases of machine learning models and systems [144]. Therefore, the main challenge for data-analysis and decision-support systems is to improve model transparency via visualization and interaction techniques for domain experts [216].\nG6: Model Tuning and Selection: Visual analytics approaches can help data experts to tune machine learning parameters for their specific data in an interactive visual fashion [131]. The interpretability element in XAI visual analytic systems increase data experts\u2019 ability to compare multiple models [5] and select the right model for the targeted data. As an example, Du et al. [51] present EventAction, an event sequence recommendation approach that allows the users to interactively select records that share their desired attribute values. In the case of tuning DNN networks, visual analytics tools enhance designers\u2019 ability to modify networks [165], improve training [129], and to compare different networks [211]."
    },
    {
      "heading": "6.3 AI Experts",
      "text": "In our categorization, AI experts are machine learning scientists and engineers who design machine learning algorithms and interpretability techniques for XAI systems. Machine learning interpretability techniques either provide model interpretation or instance explanations. Examples of model interpretation techniques include inherently interpretable models [205], deep model simplification [213], and visualization of model internals [215]. Instance explanations techniques, however, present feature importance for individual instances such as saliency map in image data and attention in textual data [43]. AI engineers also benefit from visualization and visual analytics tools to interactively inspect model internal variables [129] to detect architecture and training flaws or monitor and control the training process [95], which indicates possible overlaps among design goals. We list main design goals for AI Experts into two following items:\nG7: Model Interpretability: Model interpretability is often a primary XAI Goal for AI experts. Model interpretability allows getting new insights into how deep models learn patterns from data [162]. In this regard, various interpretability techniques for different domains have been proposed to satisfy the need for explanation [99, 188]. For example, Yosinski et al. [215] created an interactive toolbox to explore CNN\u2019s activation layers in real-time that gives an intuition about\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nG8: Model Debugging: AI researchers use interpretability techniques in different ways to improve model architecture and training process. For example, Zeiler and Fergus [219] present a use case in which visualization of filters and feature maps in CNN leads to revising training hyper-parameters and, therefore, model performance improvement. In another work, Ribeiro et al. [172] used model instance explanations and human review of explanations to improve model performance through feature engineering.\nOther than main XAI goals for AI experts, machine learning explanations are used for other purposes including detecting dataset bias [220], adversarial attack detection [61], and model failure prediction [146]. Also, visual saliency maps and attention mechanisms have been used as weakly supervised object localization [190], multiple object recognition [12], and knowledge transfer [122] techniques."
    },
    {
      "heading": "7 XAI EVALUATION MEASURES",
      "text": "Evaluation measures for XAI systems is another important factor in the design process of XAI systems. Explanations are designed to answer different interpretability goals, and hence different measures are needed to verify explanation validity for the intended purpose. For example, experimental design with human-subject studies is a common approach to perform evaluations with AI novice end-users. Various controlled in-lab and online crowdsourced studies have been used for XAI evaluation. Also, case studies aim to collect domain expert users\u2019 feedback while performing high-level cognitive tasks with analytics tools. By contrast, computational measures are designed to evaluate the accuracy and completeness of explanations from interpretable algorithms. In this section, we review and categorize the main evaluation measures for XAI systems and algorithms. Table 2 shows a list of five evaluation measures associated with their design goals. Additionally, we provide summarized and ready-to-use XAI evaluation measures and methods extracted from the literature in Tables 3-7."
    },
    {
      "heading": "7.1 M1: Mental Model",
      "text": "Following cognitive psychology theories, a mental model is a representation of how users understands a system. Researchers in HCI study users\u2019 mental models to determine their understanding of intelligent systems in various applications. For example, Costanza et al. [40] studied how users understand a smart grid system, and Kay et al. [96] studied how users understand and adapt to uncertainty in machine learning prediction of bus arrival times. In the context of XAI, explanations help users to create a mental model of how the AI works. Machine learning explanation is a way to help the users in building a more accurate mental model. Studying users\u2019 mental models of XAI systems can help verify explanation effectiveness\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nin describing an algorithm\u2019s decision-making process. Table 3 summarizes different evaluation methods used to measure users\u2019 mental model of machine learning models.\nPsychology research in human-AI interactions has also explored structure, types, and functions of explanations to find essential ingredients of ideal explanation for better user understanding and more accurate mental models [97, 132]. For instance, Lombrozo [133] studied how different types of explanations can help structure conceptual representation. In order to find out how an intelligent system should explain its behavior for non-experts, research on machine learning explanations has studied how users interpret intelligent agents [47, 164] and algorithms [171] to find out what users expect from machine explanations. Related to this, Lim and Dey [124] elicit types of explanations that users might expect in four real-world applications. They specifically study what types of explanations users demand in different scenarios such as system recommendation, critical events, and unexpected system behavior. In measuring user mental model through model failure prediction, Bansal et al. [15] designed a game in which participants receive monetary incentives based on their final performance score. Although experiments were done on a simple three-dimensional task, their results indicate a decrease in users\u2019 ability to predict model failure as data and model get more complicated. A useful way of studying users comprehension of intelligent systems is to directly ask them about the intelligent system\u2019s decision-making process. Analyzing users\u2019 interviews, think-alouds, and self-explanations provides valuable information about the users\u2019 thought processes and mental models [110]. On studying user comprehension, Kulesza et al. [111] studied the impact of explanation soundness and completeness on fidelity of end-users mental model in a music recommendation interface. Their results found that explanation completeness (broadness) had a more significant effect on user understanding of the agent compared to explanation soundness. In another example, Binns et al. [20] studied the relation between machine explanations and users\u2019 perception of justice in algorithmic decision-making with different sets of explanation styles. User attention and expectations may also be considered during the interpretable interface design cycles for intelligent systems [195].\nInterest in developing and evaluating human-understandable explanations has also led to interpretable models and ad-hoc explainers to measure mental models. For example, Ribeiro et al. [172] evaluated users\u2019 understanding of the machine learning algorithm with visual explanations. They showed how explanations mitigate human overestimation of the accuracy of an image classifier and help users choose a better classifier based on the explanations. In a follow-up work, they compared the global explanations of a classifier model with the instance explanations of the same model and found global explanations were more effective solutions for finding the model weaknesses [173]. In another paper, Kim et al. [99] conducted a crowdsourced study to evaluate feature-based explanation understandability for end-users. Addressing understanding of model representations, Lakkaraju et al. [113] presented interpretable decision sets, an interpretable classification model, and measured users\u2019 mental models with different metrics such as user accuracy on predicting machine output and length of users\u2019 self-explanations."
    },
    {
      "heading": "7.2 M2: Explanation Usefulness and Satisfaction",
      "text": "End-user satisfaction and usefulness of machine explanation are also of importance when evaluating explanations in intelligent systems [19]. Researchers use different subjective and objective measures for understandability, usefulness, and sufficiency of details to assess explanatory value for users [142]. Although there are implicit methods to measure user satisfaction [80], a considerable part of the literature follows qualitative evaluation of satisfaction in explanations, such as questionnaires and interviews. For example, Gedikli et al. [62] evaluated ten different explanation types with user ratings of explanation satisfaction and transparency. Their results showed a strong\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nrelationship between user satisfaction and perceived transparency. Similarly, Lim et al. [125] explore explanation usefulness and efficiency in their interpretable context-aware system by presenting different types of explanations such as \u201cwhy\u201d, \u201cwhy not\u201d and \u201cwhat if\u201d explanation types and measuring users response time.\nAnother line of research studies whether intelligible systems are always appreciated by the users or it has a conditional value. An early work from Lim and Dey [124] studied user understanding and satisfaction of different explanation types in four real-world context-aware applications. Their findings show that, when considering scenarios involved with criticality, users want more information explaining the decision making process and experience higher levels of satisfaction after receiving these explanations. Similarly, Bunt et al. [25] considered whether explanations are always necessary for users in every intelligent system. Their results show that, in some cases, the cost of viewing explanations in diary entries like Amazon and YouTube recommendations could outweigh their benefits. To study the impact of explanation complexity on users\u2019 comprehension, Lage et al. [112] studied how explanation length and complexity affect users\u2019 response time, accuracy, and subjective satisfaction. They also observed that increasing explanation complexity resulted in lowered subjective user satisfaction. In a recent study, Coppers et al. [39] also show that adding intelligibility does not necessarily improve user experience in a study with expert translators. Their experiment suggests that an intelligible system is preferred by experts when the additional explanations are not part of the translators readily available knowledge. In another work, Curran et al. [41] measured users\u2019 understanding and preference of explanations in an image recognition task by ranking and coding user transcripts. They provide three types of instance explanations for participants and show that although all explanations were coming from the same model, participants had different levels of trust in explanations\u2019 correctness, according to explanations clarity and understandability. Table 4 summarizes the study methods used to measure user satisfaction and usefulness of machine learning explanations. Note that the primary goal of XAI system evaluations for domain and AI experts is through direct evaluation of user satisfaction of explanation design during the design cycle. For example, case studies and participatory design are common approaches for directly including expert users as part of the system design and evaluation processes."
    },
    {
      "heading": "7.3 M3: User Trust and Reliance",
      "text": "User trust in an intelligent system is an affective and cognitive factor that influences positive or negative perceptions of a system [82, 136]. Initial user trust and the development of trust over time have been studied and presented with different terms such as swift trust [141], default trust [139] and suspicious trust [21]. Prior knowledge and beliefs are important in shaping the initial state of trust; however, trust and confidence can change in response to exploring and challenging the\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nsystem with edge cases [81]. Therefore, the user may have different feelings of trust and mistrust during different stages of experience with any given system.\nResearchers define and measure trust in different ways. User knowledge, technical competence, familiarity, confidence, beliefs, faith, emotions, and personal attachments are common terms used to analyze and investigate trust [94, 136]. For these outcomes, user trust and reliance can be measured by explicitly asking about user opinions during and after working with a system, which can be done through interviews and questionnaires. For example, Ming et al. [214] studied the importance of model accuracy on user trust. Their findings show that user trust in the system was affected by both the system\u2019s stated accuracy and users\u2019 perceived accuracy over time. Similarly, Nourani et al. [160] explored how explanation inclusion and level of meaningfulness would affect the user\u2019s perception of accuracy. Their controlled experiment results show that whether explanations are human-meaningful can significantly affect perception of system accuracy independent of the actual accuracy observed from system usage. Additionally, trust assessment scales could be specific to the systems application context and XAI design purposes. For instance, multiple scales would assess user opinion on systems reliability, predictability, and safety separately. Related to this, a detailed trust measurement setup is presentation in the paper by Cahour and Forzy [28], which measures user trust with multiple trust scales (constructs of trust), video recording, and self-confrontation interviews to evaluate three modes of system presentation. Also, to better understand factors that influence trust in adaptive agents, Glass et al. [65] studied which types of questions users would like to be able to ask an adaptive assistant. Others have looked at changes to user awareness over time by displaying system confidence and uncertainty of the machine learning outputs in applications with different degrees of criticality [10, 96].\nMultiple efforts have studied the impact of XAI on developing justified trust in users in different domains. For instance, Pu and Chen [169] proposed an organizational framework for generating explanations and measured perceived competence and user\u2019s intention to return as the measures for user trust. Another example compared user trust with explanations for different goals like transparency and justification explanation [158]. They considered perceived understandability to measure user trust and show that transparent explanations can help reduce the negative effects of trust loss in unexpected situations.\nStudying user trust in real-world applications, Berkovsky et al. [17] evaluated trust with various recommendation interfaces and content selection strategies. They measured user reliance on a movie recommender system with six distinct constructs of trust. Also on recommender algorithms, Eiband et al. [55] repeats the Langer et al.\u2019s experiment [114] on the role of \u201cplacebic\u201d explanations (i.e., explanations that convey no information) in mindlessness of user behavior. They studied if providing placebic explanations would increase user reliance on the recommender system. Their results suggest that future work on explanations for intelligent systems may consider using placebic\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nexplanations as a baseline for comparison with machine learning generated explanations. Also concerned with expert user\u2019s trust, Bussone et al. [26] measured trust by Likert-scale and thinkalouds and found that explanations of facts lead to higher user trust and reliance in a clinical decision-support system. Table 5 summarizes a list of subjective and objective evaluation methods to measure user trust in the machine learning systems and their explanations. Many studies evaluate user trust as a static property. However, it is essential to take user\u2019s experience and learning over time into account when working with complex AI systems. Collecting repeated measures over time can help in understanding and analyzing the trend of users\u2019 developing trust with the progression of experience. For instance, in their study, Holliday et al. [89] evaluated trust and reliance in multiple stages of working with an explainable text-mining system. They showed that the level of user trust in the system varied over time as the user gained more experience and familiarity with the system. We note that although our literature review did not find a direct measurement of trust to be commonly prioritized in analysis tools for data and machine learning experts, users\u2019 reliance on tools and the tendency to continue using tools are often considered as a part of the evaluation pipeline during case studies. In other words, our summarization is not meant to claim that data experts do not consider trust, but rather we did not find it to be a core outcome explicitly measured in the literature for this user group."
    },
    {
      "heading": "7.4 M4: Human-AI Task Performance",
      "text": "A key goal of XAI is to help end-users to be more successful in their tasks involving machine learning systems [90]. Thus, human-AI task performance is a measure relevant to all three groups of user types. For example, Lim et al. [125] measured users\u2019 performance in terms of success rate and task completion time to evaluate the impact of different types of explanations. They use a generic interface that can be applied to various types of sensor-based context-aware systems, such as weather prediction. Further, explanations can assist users in adjusting the intelligent system to their needs. Kulesza et al. [109] study of explanations for a music recommender agent found a positive effect of explanations on users\u2019 satisfaction with the agent\u2019s output, as well as on users\u2019 confidence in the system and their overall experience. Another use case for machine learning explanations is to help users judge the correctness of system output [70, 105, 194]. Explanations also assist users in debugging interactive machine learning programs for their needs [108, 110]. In a study of end-users interacting with an email classifier system, Kulesza et al. [108] measured classifier performance to show that explanatory debugging benefits user and machine performance. Similarly, Ribeiro et al. [172] found users could detect and remove wrong explanations in text classification, resulting in training better classifiers with higher performance and explanations quality. To support these goals, Myers et al. [156] designed a framework that users can ask why and why not questions and expect explanations from the intelligent interfaces. Table 6 summarizes a list of evaluation methods to measure task performance in human-AI collaboration and model tuning scenarios. Visual analytics tools also help domain experts to better perform their tasks by providing model interpretations. Visualizing model structure, details, and uncertainty in machine outputs can allow domain experts to diagnose models and adjust hyper-parameters to their specific data for better analysis. Visual analytics research has explored the need for model interpretation in text [92, 128, 210] and multimedia [24, 33] analysis tasks. This body of work demonstrates the importance of integrating user feedback to improve model results. An example of a visual analytics tool for text analysis is TopicPanaroma [131], which models a textual corpus as a topic graph and incorporates machine learning and feature selection to allow users to modify the graph interactively. In their evaluation procedure, they ran case studies with two domain experts: a public\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nrelations manager used the tool to find a set of tech-related patterns in news media, and a professor analyzed the impact of news media on the public during a health crisis. In analysis of streaming data, automated approaches are error-prone and require expert users to review model details and uncertainty for better decision making [18, 179]. For example, Goodall et al. [66] presented Situ, a visual analytics system for discovering suspicious behavior in cyber network data. The goal was to make anomaly detection results understandable for analysts, so they performed multiple case studies with cybersecurity experts to evaluate how the system could help users to improve their task performance. Ahn and Lin [4] present a framework and visual analytic design to aid fair data-driven decision making. They proposed FairSight, a visual analytic system to achieve different notions of fairness in ranking decisions through visualizing, measuring, diagnosing, and mitigating biases.\nOther than domain experts using visual analytics tools, machine learning experts also use visual analytics to find shortcomings in the model architecture or training flaws in DNNs to improve the classification and prediction performance [130, 165]. For instance, Kahng et al. [95] designed a system to visualize instance-level and subset-level of neuron activation in a long-term investigation and development with machine learning engineers. In their case studies, they interviewed three machine learning engineers and data scientists who used the tool and reported the key observations. Similarly, Hohman et al. [86] present an interactive system that scalably summarizes and visualizes what features a DNN model has learned and how those features interact in instance predictions. Their visual analytic system presents activation aggregation to discover important neurons and neuron-influence aggregation to identify interactions between important neurons. In the case of recurrent neural networks (RNN), LSTMVis [193] and RNNVis [143] are tools to interpret RNN models for natural language processing tasks. In another recent paper, Wang et al. [206] presented DNN Genealogy, an interactive visualization tool that offers a visual summary of DNN representations.\nAnother critical role of visual analytics for machine learning experts is to visualize model training processes [224]. An example of a visual analytics tool for diagnosing the training process of a deep generative model is DGMTracker [129], which helps experts understand the training process by visually representing training dynamics. An evaluation of DGMTracker was conducted in two case studies with experts to validate efficiency of the tool in supporting understanding of the training process and diagnosing a failed training process."
    },
    {
      "heading": "7.5 M5: Computational Measures",
      "text": "Computational measures are common in the field of machine learning to evaluate interpretability techniques\u2019 correctness and completeness in terms of explaining what the model has learned. Herman [78] notes that reliance on human evaluation of explanations may lead to persuasive explanations rather than transparent systems due to user preference for simplified explanations.\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nTherefore, this problem leads to the argument that explanations\u2019 fidelity to the black-box model should be evaluated by computational methods instead of human subject studies. Fidelity of an adhoc explainer refers to the correctness of the ad-hoc technique in generating the true explanations (e.g., correctness of a saliency map) for model predictions. This leads to a series of computational methods to evaluate correctness of generated explanations, consistency of explanation results, and fidelity of ad-hoc interpretability techniques to the original black-box model [175]. In many cases, machine learning researchers often consider consistency in explanation results, computational interpretability, and qualitative self-interpretation of results as evidence for explanation correctness [162, 215, 217, 226]. For example, Zeiler and Fergus [219] discuss fidelity of the visualization for CNN network by its validity in finding model weaknesses resulted in improved prediction results. In other cases, comparing a new explanation technique with existing state-of-the-art explanation techniques is is used to verify explanation quality [37, 134, 189]. For instance, Ross et al. [178] designed a set of empirical evaluations and compared their explanations\u2019 consistency and computational cost with the LIME technique [172]. In a comprehensive setup, Samek et al. [183] proposed a framework for evaluating saliency explanations for image data that quantify the feature importance with respect to the classifier prediction. They compared three different saliency explanation techniques for image data (sensitivity-based [190], deconvolution [219], and layer-wise relevance propagation [13]) and investigated the correlation between saliency map quality and network performance on different image datasets under input perturbation. On the contrary, Kindermans et al. [101] show interpretability techniques have inconsistencies on simple image transformations, hence their saliency maps can be misleading. They define an input invariance property for reliability of explanations from saliency methods. To extend a similar idea, Adebayo et al. [3] propose three tests to measure adequacy of interpretability techniques for tasks that are sensitive to either data or the model itself. Other evaluation methods include assessing explanation\u2019s fidelity in comparison to inherently interpretable models (e.g., linear regression and decision trees). For example, Ribeiro et al. [172] compared explanations generated by the LIME ad-hoc explainer to explanations from an interpretable model. They created gold standard explanations directly from the interpretable models (sparse logistic regression and decision trees) and used these for comparisons in their study. A downside of this approach is that the evaluation is limited to generating a gold standard by an interpretable model. User simulated evaluation is another method to perform computational evaluation of model explanations. Ribeiro et al. [172] simulated user trust in explanations and models by defining \u201cuntrustworthy\u201d explanations and models. They tested a hypothesis on how real users\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nwould prefer more reliable explanations and choose better models. The authors later repeated similar user simulated evaluations in the Anchors explanation approach [173] to report simulated users\u2019 precision and coverage in finding the better classifier by only looking at explanations. A different approach in quantifying explanations quality with human intuition has been taken by Schmidt and Biessmann [187] by defining an explanation quality metric based on user task completion time and agreement of predictions. Another example is the work of Lundberg and Lee [134], who compared the SHAP ad-hoc explainer model with LIME and DeepLIFT [189] with the assumption that good model explanations should be consistent with the explanations from humans who understand the model. Lertvittayakumjorn and Toni [118] also present three user tasks to evaluate local explanation techniques for text classification through revealing model behavior to human users, justifying the predictions, and helping humans investigate uncertain predictions. A similar idea has been implemented in [149] by feature-wise comparison of a ground-truth and model explanation. They provide a user annotated benchmark to evaluate machine learning instance explanations. Later, Poerner et al. [166] use this benchmark as human annotated ground truth in comparison to small-context (word level) and large-context (sentence level) explanation evaluation. User annotated benchmarks can be valuable when considering human meaningfulness of explanations, though the discussion by Das et al. [43] implies that machine learning models (visual question answering attention models in their case) do not seem to look at the same regions as humans. They introduce a human-attention dataset [42] (collection of mouse-tracking data) and evaluate attention maps generated by state-of-the-art models against human.\nInterpretability techniques also enable quantitative measures for evaluating model trustworthiness (e.g., model fairness, reliability, and safety) through its explanations. Trustworthiness of a model represents a set of domain specific goals such as fairness (by fair feature learning), reliability, and safety (by robust feature learning). For example, Zhang et al. [220] present a case of using machine learning explanations to find representation learning flaws caused by potential biases in the training dataset. Their technique mines the relationships between pairs of attributes according to their inference patterns. Further, Kim et al. [99] presented quantitative testing of machine learning models by their explanations. In their concept activation vector technique, the model can be tested for specific concepts (e.g., image patterns) and a vector score shows if the model is biased toward that concept. They later extended their concept-based global explanation of model representation learning for systematic discovery of concepts that are human meaningful and important for the model prediction [63]. They used human subject experiments to evaluate learned concepts. Table 7 summarizes a list of evaluation methods to measure fidelity of interpretability techniques and model trustworthiness with computational techniques."
    },
    {
      "heading": "8 XAI DESIGN AND EVALUATION FRAMEWORK",
      "text": "The variety of different XAI design goals (Section 6) and evaluation methods (Section 7) from our review suggests the need for diverse sets of techniques to build end-to-end XAI systems. However, it is generally insufficient to take design practices and evaluation methods separately. A holistic and more actionable vantage will require consideration of dependencies between design goals and evaluation methods and will inform when to choose between them during the design cycles. Previously, various models and guidelines for the design and evaluation of AI-infused interactive user interfaces [7, 54] and visual analytics systems [155] have been proposed to help designers through the design process. However, challenges in generating useful machine learning explanations and presenting them through an appropriate interface that aligns with target outcomes call for a multidisciplinary workflow framework. Thus, based on our analysis of prior work, we propose a design and evaluation framework for XAI systems. The impetus for this framework is the desire to organize and relate the diverse set of\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nexisting design goals and evaluation methods in a unified model. The framework is intended to give guidance on what evaluation measures are appropriate to use at which design stage of the XAI system. Figure 4 summarizes the framework as a nested model for end-to-end XAI system design and evaluation. The formulation of the model as layers relates to the core design goals and evaluation interests from the different research communities (as identified from the literature review) to help promote interdisciplinary progress in XAI research. The model is structured to support system design steps by starting from the outer layer (XAI System Goals), then addressing end-user needs in the middle layer (Explainable Interface), and finally focusing on underlying interpretable algorithms in the innermost layer (Interpretable Algorithms). The nested model is organized with a Design Pole focusing on design goals and choices, and an Evaluation Pole presenting appropriate evaluation methods and measures for each layer. Our framework suggests iterative cycles of design and evaluation to cover both algorithmic and human-related aspects of XAI systems. In this section, we elaborate on details of the nested framework and provide guidelines on using it for multidisciplinary XAI system design.\nCase Study Example: To showcase a practical example of using the framework, we also include a case study of a collaborative design and development effort for an XAI system. In the scenario of the case study, a multidisciplinary team of researchers designed a XAI system for fake news\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\ndetection for non-expert (not AI experts or news analysts) daily newsreaders. The design team planned to add a XAI Assistant feature to a news reading and sharing website to perform fake news detection. The system design consisted of a news reading interface equipped with the XAI news assistant (news assistant) to help the user identify fake news while reviewing news stories and articles. The presented case is the result of an ongoing research done over a one-year period by a team of eight university researchers with HCI, Visualization, and AI backgrounds. During the following subsections, each framework guideline is followed by an example of its application in our case study."
    },
    {
      "heading": "8.1 XAI System Goals Layer",
      "text": "As designers in a multidisciplinary team have different roles and priorities in building a XAI system, we suggest beginning the system design cycle from the XAI Goal layer (the outer layer of Figure 4) to characterize design goal and system expectations. Specifically, this step involves identifying the purpose for explanation and choosing what to explain for the targeted end-user and dedicated application. The iterative refinements between XAI goal (top pole) and system evaluation (bottom pole) present how the paired evaluation measures help to improve system design. We organize the following guidelines for the XAI goal layer.\nAt the beginning of the system design process, the teamwill need to specify explainability requirements for each framework layer based on the evaluation metrics. The explainability requirements are intended to satisfy overall system goals defined by user (or customer) needs, and sometimes regulations, laws, and safety standards. Later, the evaluation step in each design cycle will have the team revisit the initial XAI system requirements. The sufficiency of the evaluation results in comparison to the initial design requirements serves as a key indicator of whether to stop or continue design iteration. However, since many subjective measures are used in the process, it is important to choose an appropriate evaluation baseline (see Section 9.4) to track progress during design cycles.\nGuideline 1: Determine XAI System Goals: Identifying and establishing clear goals and expectations from XAI system is the first step in the design process. XAI Design goals could be driven by many motivations like improving user experience on an existing system, advancing scientific findings [107, 120], or adhering to new regulations [198]. In Section 6 we reviewed eight main goals (G1-G8) for XAI systems. Also, ordering the priority of goals in cases with multiple design goals can be beneficial in the next steps of the process (see Guideline 2). Given the fact that different XAI user types and applications are interested in various design goals, it is important to establish these goals early in the design process to identify and align with appropriate design principles. A pitfall in this stage is to pick XAI goals without considering the end-user group, algorithmic limitations, and user preferences in the context of the application. Overshooting XAI goals could hurt evaluation results moving forward in the design process.\nApplication in Case Study: In the first step of our case study with a news curation application, the team started with identifying the main goals and expectations for the XAI news assistant. The design focused on novice end-users without any particular expertise. The XAI design goal was to improve user reliance and mental model of news predictions through explainable design. The team hypothesized that end-users would trust and rely on the fake news detection assistant, given that the new XAI is capable of providing explanations for each news story. Also, the team hoped that users would be able to use the explanations to learn model weaknesses and strengths to provide feedback to the developer team.\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nGuideline 2: Decide What to Explain: The second step in the XAI system design is to identify \u201cwhat to explain\u201d to the user in order to achieve the initial XAI goals (see Guideline 1) of the system. We reviewed multiple machine learning interpretability techniques and explanation types in Sections 4.1, 4.2, and 4.3 which can provide different types of information to the user. Although theory-driven design frameworks discuss explanation mechanisms driven by human reasoning semantics [126], user-centered methods to identify useful explanations such as online surveys, interviews, and user observations (e.g., [26, 138]) to understandwhen andwhat needs to be explained for the users to understand better and trust intelligent systems. Preliminary experiments are valuable in the early steps of the design cycle to identify and narrow down explanation options for the user in order to satisfy design goals. A typical approach for evaluating the effectiveness and usefulness of explanation choice in user-centric experiments is to compare the user\u2019s mental model of the system with and without explanation components. On this subject, Lim and Dey [124] conducted experiments to discover what type of information users are interested in different real-world context-aware application scenarios. Stumpf et al. [195] also performed end-user interviews to identify user perceptions and expectations from an interpretable interface as well as finding main decision points where users may need explanations. In another work, Haynes et al. [74] provide a review and studies incorporating different explanations (operational, ontological, mechanistic, and design rationale explanations) in intelligent systems. Similarly, visual analytics design involves expert interviews and focus groups in the design path to identify design goals [155].\nThe design process in this step involves algorithmic implementation constraints like \u201cwhat can be explained\u201d to the user. For example, global explanations from a DNN may not feasible and comprehensible due to the large number of variables in the graph. Additionally, research shows instance explanations from a DNN lack completeness and may fail to present salient features in cases [3]. Such constraints and decision points could be solved through focused groups, brainstorming, and interviews between model designers and interface designers in the team. Therefore, a design pitfall for explanation choices is not to take limitations of interpretability techniques into account.\nApplication in Case Study: In our scenario, efficient news curation required fake news detection with the help of our XAI assistant. In the analysis of what the system should explain, the design team decided to identify candidate useful and impactful explanation options. We started with reviewing machine learning research on false information (e.g., rumor, hoax, fake news, clickbait) detection as well as HCI research on news feeds and news search systems to identify key attributes for news veracity checking [148]. Given the non-expert target end-users, explanatory information needed to limit technical details. Next, the user interface designers and machine learning designers in the team discussed candidate explanation choices and algorithmic constraints in interpretability techniques. That is, some options for what to explainmay not be entirely possible given the interpretability of existing models, and the team needed to consider whether alternative learning techniques could provide better explanations or if the design team would need to figure out meaningful ways to explain the information that was available from the model.\nGuideline 3: Evaluate System Outcomes: Evaluation of XAI system outcomes is the final step in the evaluation process. Figure 4 shows how the final system outcome evaluation is paired with the initial design goals in the outer layer of our framework. The main goal of this stage is to quantitatively and qualitatively assess the effectiveness of the XAI system for the initially established system-level XAI goals. Clearly, evaluation of final system outcomes could be influenced by the design of the explainable user interface (intermediate layer) and the design of interpretable\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nalgorithms (innermost layer). For example, evaluating a newborn interpretable machine learning algorithm\u2019s output using human subjects through a weak in-lab or crowdsourced user study may not be meaningful or productive for XAI system outcomes if core computational changes are still in progress and could ultimately change the entire model interpretability and explanation format later. Also, changes in the targeted user could affect evaluation results at this stage. For example, a system designed for novices may not satisfy the needs of an expert user and hence would not improve performance as expected. Evaluation measures in this layer depend on the design goals, application domain, and targeted users. Example evaluation measures for final system outcomes include user trust [169] and reliance on the system [17], human-machine task performance [15], user awareness [96], and user understanding of their personal data [170]. An effective process for evaluation of high-level XAI outcomes is to break down the evaluation goal into multiple well-defined measures and metrics. This way, the team can perform evaluation studies on different steps using valid methods in controlled setup. For example, in the evaluation of XAI systems for trustworthiness, several factors of human trust could be measured during and after a period of user experience with the XAI system. In addition, computational measures (Section 7.5) are used to examine the fidelity of interpretability methods and trustworthiness of the model with objective metrics. A possible pitfall in evaluation of the XAI system outcomes is performing the evaluation without considering the model trustworthiness and explanations\u2019 correctness from the interpretable model layer (see Guideline 7) and explanation understandability and usefulness from the user interface layer (see Guideline 5).\nApplication in Case Study: In our case study with news review and curation, we needed to evaluate our XAI news assistant with non-expert users who would gather news stories while flagging fake news articles. In the evaluation step, the team ran multiple large-scale human-subject studies with novice participants recruited through Amazon Mechanical Turk to work with our news reading system. Note that both the explainable interface and interpretable algorithm passed multiple design and testing iterations before this evaluation step. Major decisions for this evaluation was how to structure the duration and complexity of the user task while appropriately testing the system\u2019s full range of functionality. The task was designed with questions built in to help collect subjective data in addition to the objective user performance data. Multiple evaluation measures were chosen for system outcomes, including: subjective user trust in the news assistant, user agreement rate with the news assistant, veracity of user-shared news stories, and user accuracy in guessing the news assistant output. Both qualitative and quantitative analysis of user feedback and interaction data were valuable to the evaluation of system outcomes. The results and analysis from these evaluations helped the team to understand the effectiveness of the XAI elements (in both the algorithm and the interface) for the initial system goals."
    },
    {
      "heading": "8.2 User Interface Design Layer",
      "text": "The middle layer of our framework is concerned with designing and evaluating an explainable interface or visualization for the user to interact with XAI system. Interface design for explanations consists of presenting model explanations from interpretable algorithms to end-users in terms of their explanation format and interaction design. The importance of this layer is to satisfy design requirements and needs to be determined in the XAI system design layer (see Guideline 2). An elegant translation of machine-generated explanations (e.g., verbal, numeric, or visual explanation)\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nneeds carefully designed human-understandable and satisfying explanations in the user interface. In Section 4.4, we reviewed multiple types of explanation formats for integrating XAI elements into the user interface. The iterative movement between Design pole and Evaluation pole in this layer presents design refinement in pursuit a desired goal state.\nGuideline 4: Decide How to Explain: Identifying candidate explanation formats for the targeted system and user group is the first step to deliver machine learning explanations to end-users. The design process can account for different levels of complexity, length, presentation state (e.g., permanent or on-demand), and interactivity options depending on the application and user type. The explanations format in the interface is particularly important to improve user understanding of underlying algorithms. Studies show that while detailed and complex interactive representations may aim to communicate the explanations to the expert users, AI-novice users of XAI system prefer more simplified explanation and representation interfaces [112]. User satisfaction of interface design is also another critical factor in user engagement of the interface components [154]. Additionally, interaction design for explainable interfaces can allow a user to communicate with the system to adjust explanations and could better support user inspection of the system [110]. Research of intelligent interface design presents multiple design methods such as wireframing and low-fidelity prototyping (e.g., [26, 138]) that could also be adapted to the explainable interface design. Also, existing design guidelines and best-practice knowledge for AI-infused interfaces (e.g., [7]) and visualizations (e.g., [140]) could be used in this stage to leverage similar systems for explainable interface design. Aside from model explanations, providing prediction uncertainty also has been identified as an important factor for both general end-users and data expert users [181]. For example, Kay et al. [96] presented the full design cycle for an uncertainty visualization interface in a bus arrival time application. Their design process included surveying to identify usage requirements, developing alternative layouts, running user testing, and final evaluation of user understanding of machine learning output.\nApplication in Case Study: To determine how to explain news classification results to nonexpert end users, the user interface design team started the process by reviewing the initial system goals and explanation types. The team then continued with multiple interface sketches that matched the intended application and user tasks. During the initial design steps, the team tried to keep a balance between interface complexity and explanation usefulness by choosing among available explanation types from our interpretable machine learning algorithms. Next, mock-ups from the top three designs were implemented for testing with a small number of participants. Each mock-up had a different arrangement of data, user task flow, and explanation format for the news assistant interface. Our human-subject experiments in this stage were based on user observations and postusage interviews to collect qualitative feedback regarding participant understanding and subjective satisfaction of explanation components and interface arrangements. Interviews resulted in the selection of the most comprehensible and conclusive design among the available options to continue with (see Guidelines 5).\nGuideline 5: Evaluate Explanation Usefulness: This mid-layer evaluation step can be used along with various measures to help assess user understanding of the XAI underlying intelligent algorithms. A series of user-centered evaluations of explainable interface with multiple goals and granularity levels could be performed to measure: (1) User understanding of explanation.\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n(2) User satisfaction of explanation. (3) User mental model of the intelligent system. Evaluations in the middle layer are particularly important due to the impact on XAI system outcomes (outer layer) and being affected by interpretable model outputs (inner-most layer). Specifically, evaluation measures in this stage can inform how well users understand the interpretable system, however, the design validity at this step also may be reflected by higher-level XAI outcomes (i.e., outer-layer evaluation) such as user trust and task performance. Note that user understanding of an XAI system could be limited to parts of the system rather than the entire system; similarly, understanding may be limited to a subspace of scenarios rather than all possible scenarios. The three evaluation measures introduced for this step could be used on multiple iterative cycles to improve overall explainable interface design. For example, Saket et al. [182] studies users understanding of visualization encoding and effectiveness of interactive graphical encoding for end-user. On the other hand, user satisfaction of explanation type and format depends on factors such as targeted application criticality and user-preferred cognitive load [48]. Evaluating user mental model is also an effective way to measure usefulness of explainable interfaces. Tables 3 and 4 present a list of measures for evaluating explainable interfaces in this step. The choice of baseline is another important factor in evaluating explainable interfaces. Typically, a combination of qualitative and quantitative analysis are used to measure effects of explanation components (in comparison to non-explainable system) or to compare multiple explanations types. However, the choice of placebic explanations has been proposed as the evaluation baseline for more accurate measurement of explanation content [55]. In the case of expert review, evaluation of a domain expert\u2019s mental model commonly involves comparison with the AI expert\u2019s mental model and description of \u201chow the model works\u201d. Section 9.4 reviews common choices of ground-truth baselines in XAI evaluation studies. With all approaches, updates in explanation components of the interface require assessment of their impact on user experience and understandability. However, the metrics and depth of evaluation vary during the evaluation cycles as the team narrows down specific needs. Finally, a possible evaluation pitfall for explainable interfaces is going after broad measures of XAI outcomes (See Guideline 3) rather than focusing on a narrower scope of explanation components and interactions.\nApplication in Case Study: In our case study, interface designers started evaluation of candidate explanation components by a series of small studies with a repeated-measures design so that the same study participant could experience different explanation designs in one session. Next, we analyzed quantitative and qualitative data collected from the end-users to choose candidate designs and routes to further improve the interface for explainable components. Discussions with the machine learning team also helped to find sources of limitations in the interpretability technique that could possibly affect user satisfaction. After the initial cycles of revision, we collected a round of external and internal expert reviews to update the study methodology and data collection details according to project progress."
    },
    {
      "heading": "8.3 Interpretable Algorithm Design Layer",
      "text": "The innermost layer of our framework involves designing interpretable algorithms that are able to generate explanations for the users. The last design step in our XAI system framework is the choice of interpretability technique (design pole) to generate the outlined explanation types. However, evaluating the generated explanation (evaluation pole) is the first evaluation step before\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nhuman-subject evaluations in the explainable interface. Ideally, the interpretability techniques should generate explanations in accordance with the requirements in the explainable interface design step (see Guideline 4); however, the choice of interpretability technique depends on domain and carries implementation limitations. For example, while shallow models are desired for their high interpretability, these models typically do not perform well in cases of complex and high dimensional data like image and text. On the other hand, highly accurate predictions in black-box models (e.g., deep neural networks and random forest models) require post-processing and ad-hoc algorithms to generate explanations. The ad-hoc approach also has limitations on both choice of explanation type and need for completeness [3] and fidelity [172] validation compared to the original model. This shows not only machine learning designers should consider the trade-off between model interpretability and performance but also should consider the fidelity of the ad-hoc explainer to black-box model. We suggest two following design and evaluation guidelines for this layer:\nGuideline 6: Design Interpretability Technique: Designing interpretable decision-making algorithms starts with the choice of machine learning model. Shallow machine learning models (e.g., linear models and decision trees) have intrinsic interpretability due to low number of variables and model simplicity. For more complex models (e.g., random forest and DNN), ad-hoc explainer technique (see Section 4.2) are needed to generate explanations. However, the choice of machine learning model (i.e., shallow vs. deep) is bounded by model\u2019s performance on data domain. Secondly, ad-hoc explainer techniques have certain limitations in their explanation type. The importance of choosing the right combination of model and explainer is in their impact on providing useful (See Guideline 4) and trustworthy explanations for end-users.\nMachine learning research has proposed various ad-hoc explainers to generate \u201cWhy\u201d explanations (e.g., feature attribution [99, 134]), \u201cHow\u201d explanations (e.g., rules list [119, 205]), \u201cWhat else\u201d explanation (e.g., similar training instance [98, 135]), and \u201cWhat if\u201d (e.g., sensitivity analysis [219]) explanation types. However, despite substantial research in interpretable machine learning techniques, a core issue in model explanations is the difference between machine learning model\u2019s decision-making logic and human sense-making as the receiver [100, 223].\nApplication in Case Study: In our fake news detection case study, the explainable interface design team had previously discussed candidate explanation choices with the machine learning design team (see Guidelines 2 and 4). Therefore, a final review of model-generated explanations and an assessment of implementation limitations were performed in this step. For example, removing noise-like features from saliency maps, normalizing attributions scores, and resolving contradicting explanations between an ensemble of models were primary implementation bottlenecks that were resolved in this step. Specifically, as a decision point for trade-offs between clarity and faithfulness of explanations, the team decided on using heuristic filters to eliminate features with a very low attribution score for the sake of presentation simplicity.\nGuideline 7: Evaluate Model Trustworthiness: Evaluating the interpretable machine learning is the first evaluation step in our framework due to its impact on outer layer evaluation measure. The high significance of this evaluation step stems from the possibility that any unreliability of interpretability at this inner layer will propagate to all other outer layers. Such unintended error propagation may lead to problematic outer-layer design decisions as well as misleading evaluation results. We discuss two main evaluation goals for the innermost layer:\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n(1) Evaluating model trustworthiness. (2) Evaluating ad-hoc explainer fidelity. The first evaluation goal aims to utilize interpretability techniques as a debugging tool to analyze the model\u2019s trustworthiness on learning concepts beyond general performance measures [99]. Examples of model trustworthiness validation include evaluating model reliability in financial risk assessment [60], model fairness in social influencing applications [220], and model safety for its intended functionality [22]. Researchers have also proposed various regularization techniques for enhancing trustworthy feature learning in machine learning models [76, 178]. Next, the second evaluation goal targets fidelity of ad-hoc explainer techniques to the black-box model. Research shows that different ad-hoc interpretability techniques have inconsistencies and can be misleading [3]. Evaluating explanation trustworthiness can verify explainer fidelity in terms of how well it represents the black-box model (see Section 7.5).\nApplication in Case Study: In our case study, we paid careful attention to qualitative reviewing of the model explanations after each design iteration. Our initial qualitative review of model explanations led to dataset cleaning through a heuristic search aimed at the removal of mislabeled examples and unrelated news articles. An improvement to model performance was achieved after dataset cleaning. Then, after the first round of human-subject evaluation of the explainable interface (see Guideline 5), the team identified negative effects of keyword explanations with low attention scores from endusers. The team decided on using a lower threshold for visualizing attention maps to reduce clutter and \u201cnoisy explanations\u201d for end-users. Finally, after one round of XAI outcome evaluation (see Guideline 3), analysis of users\u2019 mental models revealed that a dataset imbalance between the \u201cfake news\u201d and \u201ctrue news\u201d was causing a bias for the model in that the model was usually more confident in predicting fake news over true news."
    },
    {
      "heading": "9 DISCUSSION",
      "text": "In our review, we discussed multiple XAI design goals and evaluation measures appropriate for various targeted user types. Table 2 presents the categorization of selected existing design and evaluation methods that organizes literature along three perspectives: design goals, evaluation methods, and the targeted users of the XAI system. Our categorization revealed the necessity of an interdisciplinary effort for designing and evaluating XAI systems. To address these issues, we proposed a design and evaluation framework that connects design goals and evaluation methods for end-to-end XAI systems design, as presented through a model (Figure 4) and guidelines. In this section, we discuss further considerations for XAI designers to benefit from the body of knowledge of XAI system design and evaluation. The following recommendations support and promote different layers of the proposed design and evaluation framework as well."
    },
    {
      "heading": "9.1 Pairing Design Goals with Evaluation Methods",
      "text": "It is essential to use appropriate measures for evaluating the effectiveness of design elements. A common pitfall in choosing evaluation measures in XAI systems is that the same evaluation measure is sometimes used for multiple design goals. A simple solution to address this issue is to distinguish between measurements by using multiple scales to capture different attributes in each evaluation target. For example, the concept of user trust consists of multiple constructs [28] that could be measured with separate scales in questionnaires and interviews (see Section 7.3). User satisfaction measurements could also be designed for various attributes such as understandability of\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nexplanations, usefulness of explanations, and sufficiency of details [85] to target specific explanation qualities (see Section 7.2).\nAn efficient way to pair design goals with appropriate evaluation measures is to balance different design methods and evaluation types in iterative cycles of design. Managing the trade-offs between qualitative and quantitative methods in the design process can allow designers to take advantage of different approaches, as needed. For example, while focus groups and interviews provide more detailed and in-depth feedback on the users\u2019 mental model [109], remote measurements are highly valuable due to the scalability of the collected data even though they provide less detail for drawing conclusions [112]. Thus, one successful approach could be to start with multiple small-scale prototyping and formative studies collecting qualitative measures at the earlier stages of the design (e.g., for XAI system goals layer in the framework) and continue with larger-scale studies and quantitative measures in the later stages (e.g., for interpretable model and interface evaluations in the framework)."
    },
    {
      "heading": "9.2 Overlap Among Design Goals",
      "text": "In our categorization of XAI systems, we chose two main dimensions to organize XAI systems by their Design Goals and Evaluation Measures in Section 5. The XAI design goals (G1\u2013G8) were based on the goals extracted from the surveyed papers, and since the XAI design goals are primarily derived from their targeted user groups, we note that overlaps among goals do exist across disciplines. For instance, there is overlap of the goals of G1: Algorithmic Transparency for novice users in HCI research, G5: Model Visualization for data experts in visual analytics, and G7: Interpretability Techniques for AI experts in machine learning research. While overlapping, these similar goals are studied with different objectives across the three research disciplines leading to diverse sets of design requirements and implementation paths. For example, designing XAI systems for AI novices requires processes and steps to build human-centered explainable interfaces to communicate model explanations to the end-users, whereas designing new interpretability techniques for AI experts has a different set of computational requirements. Another example of overlap in XAI goals is between the goal for G6: Model Visualization and Inspection for data experts and G8: Model Debugging for AI experts, in which different sets of tools and requirements are used to address different research objectives. To address the overlap between XAI goal among research disciplines, we used the XAI User Groups as an auxiliary dimension to organize XAI goals in this cross-disciplinary topic (Section 6) and emphasize the diversity of diverse research objectives. The three user groups were chosen to organize research objectives and efforts into HCI (for AI novices), visual analytics (for data experts), and machine learning (for AI experts) research fields. Additionally, as described in the framework, the three user groups prioritize design objectives in the design process for the XAI system rather than absolute separation of design goals. For example, the objectives and priorities in XAI system design for algorithmic bias mitigation for domain experts in a law firm are certainly different from those of model training and tuning tools for AI experts. However, by following the multidisciplinary design framework, a design team can translate XAI system goals into design objectives for explainable interface and machine learning techniques to improve the design process in different layers. Therefore, in the above example, the design team can focus on diverse interface design and interpretability technique objectives to achieve the primary XAI goal of bias mitigation for the domain experts. Note that the specifics of any particular system will determine the priorities of different objectives.\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020."
    },
    {
      "heading": "9.3 System Evaluation Over Time",
      "text": "An important aspect in evaluating complex AI and XAI systems is to take the user learning into account. Learnability is even more critical when measuring mental models and user trust in the system. A user learns and gets more familiar with the system over time with continued interaction with the system. This brings the importance of repeated temporal data capture (in contrast to static measurements) for XAI evaluations. Holliday et al. [89] present an example of multiple trust assessments during the user study. They measured user trust at regular intervals during the study to capture changes in user trust as the user interacts more with the system. Their results indicates an XAI system outperformed a non-XAI counterpart in maintaining user trust over time. Time-based measurements, also referred to as dynamic measurements, allows designers to monitor explanation usability and effectiveness in various contexts and situations [50, 185]. For instance, Zhang et al. [222] explore the effect of intelligent system explanations in user trust calibration. In their experiments, they observe significant effect on calibration of trust when model prediction confidence score was shown to participants. In another example, a study by Nourani et al.[159] controlled whether users\u2019 early experiences with an explainable activity recognition system had better or worse model outputs, and the first impressions significantly affected both task performance and user confidence in understanding how the system works. In a study with a news review task, Mohseni et al. [150] identified different user profiles for changes in trust over time (trust dynamics) while working with the assistance of an explainable fake news detector. Their analysis of results revealed a significant effect of machine learning explanations on user trust dynamics.\nLong-term evaluation of XAI systems can also allow designers to estimate valuable user experience factors such as over-trust and under-trust on the system. User-perceived system accuracy [110] and transparency [171] are examples of long-term measures for explanation usability that depend on building user trust in the system\u2019s interpretability. As more information is provided by explanations over time, reasoning and mental strategies may change as users create new hypotheses about system functionality. Therefore, it is essential to also consider users\u2019 mental models and trust in extended studies to evaluate all aspects of the XAI system. Another use case of long-term measurements is to evaluate the effects of intelligent system\u2019s non-uniform behaviors in real-world scenarios. This means, although in a controlled study setup, a balanced set of input examples will present the system to the user, in real-world scenarios, users may face alterations in system performance in long-term interaction with the system. Long-term measurements will identify user\u2019s unjust trust in the system due to a limited or biased set of interactions with the system. For example, in the context of autonomous vehicles, Kraus et al. [104] presented a model of trust calibration and presented studies on trust dynamics in the early phases of user interaction with the system. Their results indicate the effects of error-free automation in steady increase of user trust as well as the effects of user a priori information in eliminating the decrease of trust in case of system malfunction."
    },
    {
      "heading": "9.4 Evaluation Ground Truth",
      "text": "Research on XAI systems study various goals with different measures across multiple domains. The breadth of XAI research makes it challenging to interpret and transfer findings from one task and domain to another. Knowing key factors for interpreting implications of evaluation results is essential to aggregate findings across domains and disciplines. An important factor in understanding XAI evaluation results and comparing results among multiple studies is the choice of ground truth. In the following, we review common choices of ground truth for both human-subject and computational evaluation methods.\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nHuman-subject experiments often take the form of controlled studies to examine the effects of machine learning explanations on a control group in comparison to a baseline group. In these setups, the choice of the baseline could affect results implications and significance. Our review of papers in the space of XAI evaluation shows the majority of study designs use a no explanation condition as the baseline condition to measure the effectiveness of model explanations in an explanation group. Examples for the baseline include approaches that remove model explanations related components and features form the interface in the baseline condition [103, 160]. In other work, Poursabzi et al. [168] also included a no AI baseline to measure participants\u2019 performance without the help of model predictions. Another way is to compare the effects of explanation type or complexity between study conditions without the no explanation baseline. For instance, Lage et al. [112] present a study to evaluate the effects of explanation complexity on participants\u2019 comprehension and performance. They used linear and logistic regression to estimate the effects of explanation complexity on participants\u2019 normalized response time, response accuracy, and subjective task difficulty rating.\nThough the above mentioned studies are controlled experiments, there may still be unaccounted human behavioral implications due to differences in the complex process of explaining worthy of consideration. Langer et al. [114] present an experiment on \u201cplacebic\u201d explanations that shows people\u2019s mindless behavior when facing explanations for actions. In a simple setup, their study showed that when asking a request, inclusion of explanations and justifications increased user\u2019s willingness to comply even if the explanations convey no meaningful information. Recently, Eiband et al. [55] proposed using placebic explanations instead of a no explanation condition as the baseline for XAI human subject studies. Therefore, using non-informative or even randomly generated explanations as the baseline condition could potentially counteract a participant\u2019s positive tendency toward explanations and improve study results.\nConsidering other approaches, a commonly accepted computational technique for quantitatively evaluating instance explanations is to create a ground truth based on the input features that semantically contribute to the target class. For example, image segmentation maps (annotations of objects in images) are used to evaluate model generated saliency maps in weakly supervised object localization tasks [121]. Mohseni et al. [149] proposed a multi-layer Human-Attention baseline for feature-level evaluation of machine learning explanations. Their Human-Attention baseline provides a human-grounded feature attribution map with a higher level of granularity compared to object segmentation maps. Similarly, feature-level annotations have been used as the explanation ground truth in the text classification domain [53]. Other less accurate means of feature attribution like bounding box in images datasets have been used for quantitative evaluation of saliency maps. For instance, Du et al. [52] evaluated saliency maps generated from a CNN model by calculating pixel-wise IOU (intersection over union) of model-explanation bounding boxes and ground truth bounding boxes."
    },
    {
      "heading": "9.5 Role of User Interactions in XAI",
      "text": "Another important consideration in designing XAI systems is how to leverage user interactions to better support system understandability. The benefits of interactive system design have been previously explored in the topic of interactive machine learning [6, 7] for novice end-users. AI and data experts also benefit from interactive visual tools to improve model and task performance [57]. In this section, we discuss multiple examples of interaction design that support user understanding of the underlying black-box model.\nFocusing on interactive design for AI-based systems for AI novices, Amershi et al. [6] reviewed multiple case studies that demonstrate the effectiveness of interactivity with a tight coupling between the algorithm and the user. They emphasize how interactive machine learning processes\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nallow the users to instantly examine the impact of their actions and adapt their next queries to improve outcomes. Such interactions allow users to test various inputs and learn about the model by creating What-If explanations [204]. Particularly, user-led cyles of trial and error help novices to understand how the machine learning model works and how to steer the model to improve results. In the context of XAI, Jongejan and Holbrook [29] present a study in which users draw images to see whether an image recognition algorithm can correctly recognize the intended sketch. Their system and study allows for interactive trial-and-error to explore how the algorithm works. In addition, their system provides example-based explanations in cases where the algorithm fails to correctly classify drawings. Another approach is to allow users to control or tune algorithmic parameters to achieve better results. For example, Kocielnik et al. [103] present a study in which users were able to freely control detection sensitivity in an AI assistant. Their results showed a significant effect on user perception of control and acceptance.\nVisual analytics tools also support model understanding for expert users through interaction with algorithms. Examples including allowing data scientists and model experts to interactively explore model representations [86], analyze model training processes [129], and detect learning biases [27]. Also, embedded interaction techniques can support the exploration of very large deep learning networks. For instance, Hohman et al. [86] present multiple interactive features to select and filter of neurons and zoom and pan in feature representations to support AI experts in interpreting and reviewing trained models."
    },
    {
      "heading": "9.6 Generalization and Extension of the Framework",
      "text": "Our framework is extendable and compatible with existing AI-infused interface and interaction design guidelines. For example, Amershi et al. [7] propose 18 design guidelines for human-AI interaction design. Their guidelines are based on a review of a large number of AI-related design recommendation sources. They systematically validated guidelines through multiple rounds of evaluations with 49 design practitioners in 20 AI-infused products. Their design guidelines provide further details within the user interface design layer of our framework (Section 8.2) to guide the development of appropriate user interactions with model output and interactions. In other work, Dudley and Kristensson [54] present a review and characterization of user interface design principles for interactive machine learning systems. They propose a structural breakdown of interactive machine learning systems and present six principles to support system design. This work also benefits our framework by contributing practices of interactive machine learning design to the XAI system goals layer (Section 8.1) and the user interface design layer (Section 8.2) From the standpoint of evaluation methods, Mueller and Klein [153] discuss how common usability tests cannot address intelligent tools where software replicates human intelligence. They suggest new solutions are needed to allow the users to experience an AI-based tool\u2019s strengths and weaknesses. Likewise, our nested framework points out the potential for error propagation from the inner layers (e.g., interpretable algorithms design) to the outer layers (e.g., system outcomes) in the XAI system evaluation pole. The iterative back-and-forth between layers in the nested model encourages expert review of system outcomes, user-centered evaluation of the explainable interface, and computational evaluation of machine learning algorithms."
    },
    {
      "heading": "9.7 Limitations of the Framework",
      "text": "Our framework provides a basis for XAI system design in interdisciplinary teamwork and we have described our case study example to validate and improve the framework. The presented case study serves as a practical example of using our framework in a multidisciplinary collaborative XAI design and development effort. Our use case is the result of a year-long (and ongoing) research done by a team of eight university researchers with diverse backgrounds. The lessons learned and\nACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.\npitfalls in our end-to-end implementation case study are incorporated in the presented design guidelines. However, no framework is perfect or entirely comprehensive. We acknowledge that the validity and usefulness of a framework are to be proven in practice with further case studies. In our future work, we plan to run multiple validation case studies to examine practicality and usefulness of this framework.\nMoreover, this framework has a common limitation of many multidisciplinary design frameworks of being light on specific details at each step. Rather than contributing detailed guidelines for each framework layer, the framework is intended to pave the path for efficient collaboration among and within different teams, which is essential for XAI system design given the inherently interdisciplinary nature of this field. This higher level of freedom allows for extendability with other design guidelines (see the discussion in Section 9.6) to integrate with more tailored approaches for specific domains. Additionally, the diversity of design goals and evaluation methods at each layer can help maintain the balance of attention from the design team to different aspects of XAI system."
    },
    {
      "heading": "10 CONCLUSION",
      "text": "We reviewed XAI-related research to organize multiple XAI design goals and evaluation measures. Table 2 presents our categorization of selected existing design and evaluationmethods that organizes literature along three perspectives: design goals, evaluation methods, and the targeted users of the XAI system. We provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research. Our categorization revealed the necessity of an interdisciplinary effort for designing and evaluating XAI systems. We want to draw attention to related resources in the social sciences that can facilitate the extent of social and cognitive aspects of explanations. To address these issues, we proposed a design and evaluation framework that connects design goals and evaluation methods for end-to-end XAI systems design, as presented through a model and a series of guidelines. We hope our framework drives further discussion about the interplay between design and evaluation of explainable artificial intelligent systems. Although the presented framework is organized to provide a high-level guideline for a multidisciplinary effort to build XAI systems, it is not meant to offer all aspects of interface and interaction design and development of interpretable machine learning techniques. Lastly, we briefly discussed additional considerations for XAI designers to benefit from the body of knowledge of XAI system design and evaluation."
    },
    {
      "heading": "11 ACKNOWLEDGMENTS",
      "text": "The authors would like to thank anonymous reviewers for their helpful comments on earlier versions of this manuscript. The work in this paper is supported by the DARPA XAI program under N66001-17-2-4031 and by NSF award 1900767. The views and conclusions in this paper are those of the authors and should not be interpreted as representing any funding agencies."
    }
  ],
  "title": "A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems",
  "year": 2020
}
