{
  "abstractText": "Recent advances in interpretable Machine Learning (iML) and eXplainable AI (XAI) construct explanations based on the importance of features in classification tasks. However, in a highdimensional feature space this approach may become unfeasible without restraining the set of important features. We propose to utilize the human tendency to ask questions like \u201cWhy this output (the fact) instead of that output (the foil)?\u201d to reduce the number of features to those that play a main role in the asked contrast. Our proposed method utilizes locally trained one-versusall decision trees to identify the disjoint set of rules that causes the tree to classify data points as the foil and not as the fact. In this study we illustrate this approach on three benchmark classification tasks.",
  "authors": [
    {
      "affiliations": [],
      "name": "Jasper van der Waa"
    },
    {
      "affiliations": [],
      "name": "Marcel Robeer"
    },
    {
      "affiliations": [],
      "name": "Jurriaan van Diggelen"
    },
    {
      "affiliations": [],
      "name": "Matthieu Brinkhuis"
    },
    {
      "affiliations": [],
      "name": "Mark Neerincx"
    }
  ],
  "id": "SP:1d5923618a0698946aed9d7a001a3ad15b3d0e07",
  "references": [
    {
      "authors": [
        "S. Barocas",
        "A.D. Selbst"
      ],
      "title": "Big Data\u2019s Disparate Impact",
      "venue": "Cal. L. Rev.,",
      "year": 2016
    },
    {
      "authors": [
        "A. Caliskan",
        "J.J. Bryson",
        "A. Narayanan"
      ],
      "title": "Semantics Derived Automatically from Language Corpora Contain Human-Like Biases",
      "year": 2017
    },
    {
      "authors": [
        "C. Coglianese",
        "D. Lehr"
      ],
      "title": "Regulating by Robot: Administrative Decision Making in the Machine-Learning Era",
      "venue": "Geo. LJ,",
      "year": 2016
    },
    {
      "authors": [
        "M.W. Craven",
        "J.W. Shavlik"
      ],
      "title": "Rule Extraction: Where Do We Go from Here",
      "venue": "Technical report,",
      "year": 1999
    },
    {
      "authors": [
        "K. Crawford"
      ],
      "title": "Artificial Intelligence\u2019s White Guy Problem",
      "year": 2016
    },
    {
      "authors": [
        "A. Datta",
        "S. Sen",
        "Y. Zick"
      ],
      "title": "Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems",
      "venue": "In Proc. 2016 IEEE Symp. Secur. Priv. (SP",
      "year": 2016
    },
    {
      "authors": [
        "Dhurandhar",
        "Amit",
        "Chen",
        "Pin-Yu",
        "Luss",
        "Ronny",
        "Tu",
        "ChunChen",
        "Ting",
        "Paishun",
        "Shanmugam",
        "Karthikeyan",
        "Das",
        "Payel"
      ],
      "title": "Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives",
      "venue": "arXiv preprint arXiv:1802.07623,",
      "year": 2018
    },
    {
      "authors": [
        "F Doshi-Velez",
        "B. Kim"
      ],
      "title": "Towards A Rigorous Science of Interpretable Machine Learning",
      "venue": "arXiv preprint arXiv:1702.08608,",
      "year": 2017
    },
    {
      "authors": [
        "U. Ehsan",
        "B. Harrison",
        "L. Chan",
        "M.O. Riedl"
      ],
      "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations",
      "venue": "arXiv preprint arXiv:1702.07826,",
      "year": 2017
    },
    {
      "authors": [
        "E.R. Elenberg",
        "A.G. Dimakis",
        "M. Feldman",
        "A. Karbasi"
      ],
      "title": "Streaming Weak Submodularity: Interpreting Neural Networks on the Fly",
      "venue": "arXiv preprint arXiv:1703.02647,",
      "year": 2017
    },
    {
      "authors": [
        "S.A. Friedler",
        "C. Scheidegger",
        "S. Venkatasubramanian",
        "S. Choudhary",
        "E.P. Hamilton",
        "D. Roth"
      ],
      "title": "A Comparative Study of Fairness-Enhancing Interventions in Machine Learning",
      "venue": "arXiv preprint arXiv:1802.04422,",
      "year": 2018
    },
    {
      "authors": [
        "D Hein",
        "S Udluft",
        "T.A. Runkler"
      ],
      "title": "Interpretable Policies for Reinforcement Learning by Genetic Programming",
      "venue": "arXiv preprint arXiv:1712.04170,",
      "year": 2017
    },
    {
      "authors": [
        "L.A. Hendricks",
        "Z. Akata",
        "M. Rohrbach",
        "J. Donahue",
        "B. Schiele",
        "T. Darrell"
      ],
      "title": "Generating Visual Explanations",
      "venue": "In Eur. Conf. Comput. Vis.,",
      "year": 1946
    },
    {
      "authors": [
        "B. Herman"
      ],
      "title": "The Promise and Peril of Human Evaluation for Model Interpretability",
      "venue": "In Conf. Neural Inf. Process. Syst.,",
      "year": 2017
    },
    {
      "authors": [
        "J. Huysmans",
        "K. Dejaeger",
        "C. Mues",
        "J. Vanthienen",
        "B. Baesens"
      ],
      "title": "An Empirical Evaluation of the Comprehensibility of Decision Table, Tree and Rule Based Predictive Models",
      "venue": "Decis. Support Syst.,",
      "year": 2011
    },
    {
      "authors": [
        "R. Krishnan",
        "G. Sivakumar",
        "P. Bhattacharya"
      ],
      "title": "Extracting Decision Trees",
      "venue": "From Trained Neural Networks. Pattern Recognit.,",
      "year": 1999
    },
    {
      "authors": [
        "T. Kulesza",
        "S. Stumpf",
        "Wong",
        "W.-K",
        "M.M. Burnett",
        "S. Perona",
        "A. Ko",
        "I. Oberst"
      ],
      "title": "Why-Oriented End-User Debugging of Naive Bayes Text Classification",
      "venue": "ACM Trans. Interact. Intell. Syst. (TiiS),",
      "year": 2011
    },
    {
      "authors": [
        "T. Kulesza",
        "M. Burnett",
        "Wong",
        "W.-K",
        "S. Stumpf"
      ],
      "title": "Principles of Explanatory Debugging to Personalize Interactive Machine Learning",
      "venue": "In Proc. 20th Intl. Conf. on Intell. User Interfaces,",
      "year": 2015
    },
    {
      "authors": [
        "T. Lei",
        "R. Barzilay",
        "T. Jaakkola"
      ],
      "title": "Rationalizing Neural Predictions",
      "venue": "arXiv preprint arXiv:1606.04155,",
      "year": 2016
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The Mythos of Model Interpretability",
      "venue": "In 2016 ICML Work. Hum. Interpret. Mach. Learn.,",
      "year": 2016
    },
    {
      "authors": [
        "S. Lundberg",
        "Lee",
        "S.-I"
      ],
      "title": "An Unexpected Unity Among Methods for Interpreting Model Predictions",
      "venue": "In 29th Conf. Neural Inf. Process. Syst. (NIPS",
      "year": 2016
    },
    {
      "authors": [
        "D.M. Malioutov",
        "K.R. Varshney",
        "A. Emad",
        "S. Dash"
      ],
      "title": "Learning Interpretable Classification Rules with Boolean Compressed Sensing",
      "venue": "Transparent Data Min. Big Small Data. Stud. Big Data,",
      "year": 2017
    },
    {
      "authors": [
        "T. Miller",
        "P. Howe",
        "L. Sonenberg"
      ],
      "title": "Explainable AI: Beware of Inmates Running the Asylum",
      "venue": "In Proc. Int. Jt. Conf. Artif. Intell. (IJCAI),",
      "year": 2017
    },
    {
      "authors": [
        "G. Montavon",
        "S. Lapuschkin",
        "A. Binder",
        "W. Samek",
        "K.R. M\u00fcller"
      ],
      "title": "Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition",
      "venue": "Pattern Recognit.,",
      "year": 2017
    },
    {
      "authors": [
        "A. Nguyen",
        "A. Dosovitskiy",
        "J. Yosinski",
        "T. Brox",
        "J. Clune"
      ],
      "title": "Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks",
      "venue": "Adv. Neural Inf. Process. Syst.,",
      "year": 2016
    },
    {
      "authors": [
        "M. Pacer",
        "T. Lombrozo"
      ],
      "title": "Ockham\u2019s Razor Cuts to the Root: Simplicity in Causal Explanation",
      "venue": "J. Exp. Psychol. Gen.,",
      "year": 2017
    },
    {
      "authors": [
        "N. Puri",
        "P. Gupta",
        "P. Agarwal",
        "S. Verma",
        "B. Krishnamurthy"
      ],
      "title": "MAGIX: Model Agnostic Globally Interpretable Explanations",
      "venue": "arXiv preprint arXiv:1702.07160,",
      "year": 2017
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier",
      "venue": "In Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min",
      "year": 2016
    },
    {
      "authors": [
        "R.R. Selvaraju",
        "M. Cogswell",
        "A. Das",
        "R. Vedantam",
        "D. Parikh",
        "D. Batra"
      ],
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "venue": "In NIPS 2016 Work. Interpret. Mach. Learn. Complex Syst.,",
      "year": 2016
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Taly",
        "Q. Yan"
      ],
      "title": "Axiomatic Attribution for Deep Networks",
      "venue": "In Proc. 34th Int. Conf. Mach. Learn. (ICML),",
      "year": 2017
    },
    {
      "authors": [
        "J.J. Thiagarajan",
        "B. Kailkhura",
        "P. Sattigeri",
        "K.N. Ramamurthy"
      ],
      "title": "TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning",
      "venue": "In NIPS 2016 Work. Interpret. Mach. Learn. Complex Syst.,",
      "year": 2016
    },
    {
      "authors": [
        "T. Wang",
        "C. Rudin",
        "F. Velez-Doshi",
        "Y. Liu",
        "E. Klampfl",
        "P. Macneille"
      ],
      "title": "Bayesian Rule Sets for Interpretable Classification",
      "venue": "In Proc. IEEE Int. Conf. Data Min. (ICDM),",
      "year": 2017
    },
    {
      "authors": [
        "J. Zhang",
        "S.A. Bargal",
        "Z. Lin",
        "J. Brandt",
        "X. Shen",
        "S. Sclaroff"
      ],
      "title": "Top-Down Neural Attention by Excitation Backprop",
      "venue": "Int. J. Comput. Vis.,",
      "year": 1405
    },
    {
      "authors": [
        "Y. Zhou",
        "G. Hooker"
      ],
      "title": "Interpreting Models via Single Tree Approximation",
      "venue": "arXiv preprint arXiv:1610.09036,",
      "year": 2016
    }
  ],
  "sections": [
    {
      "heading": "1. Introduction",
      "text": "The research field of making Machine Learning (ML) models more interpretable is receiving much attention. One of the main reasons for this is the advance in such ML models and their applications to high-risk domains. Interpretability in ML can be applied for the following purposes: (i) transparency in the model to facilitate understanding by users (Herman, 2017); (ii) the detection of biased views in a model (Crawford, 2016; Caliskan et al., 2017); (iii) the identification of situations in which the model works adequately and safely (Barocas & Selbst, 2016; Coglianese & Lehr, 2016; Friedler et al., 2018); (iv) the construction of accurate explanations that explain the underlying causal phenomena (Lipton, 2016); and (v) to build tools that allow\n*Equal contribution 1Perceptual and Cognitive Systems, Dutch Research Organization for Applied Research (TNO), Soesterberg, The Netherlands 2Interactive Intelligence group, Technical University of Delft, Delft, The Netherlands 3Department of Information and Computing Sciences, Utrecht University, Utrecht, The Netherlands. Correspondence to: Jasper van der Waa <jasper.vanderwaa@tno.nl>.\n2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden. Copyright by the author(s).\nmodel engineers to build better models and debug existing models (Kulesza et al., 2011; 2015).\nThe existing methods in iML focus on different approaches of how the information for an explanation can be obtained and how the explanation itself can be constructed. See for example for an overview the review papers of Guidotti et al. (2018) and Chakraborty et al. (2017). A number of examples of common methods are: ordering the feature\u2019s contribution to an output (Datta et al., 2016; Lei et al., 2016; Ribeiro et al., 2016), attention maps and saliency of the features (Selvaraju et al., 2016; Montavon et al., 2017; Sundararajan et al., 2017; Zhang et al., 2017), prototype selection, construction and presentation (Nguyen et al., 2016), word annotations (Hendricks et al., 2016; Ehsan et al., 2017), and summaries with decision trees (Krishnan et al., 1999; Thiagarajan et al., 2016; Zhou & Hooker, 2016) and decision rules (Hein et al., 2017; Malioutov et al., 2017; Puri et al., 2017; Wang et al., 2017). In this study we focus on feature-based explanations. Such explanations tend to be long when based on all features or use an arbitrary cutoff point. We propose a model-agnostic method to limit the explanation length with the help of contrastive explanations. The method also adds information of how that feature contributes to the output in the form of decision rules.\nThroughout this paper, the main reason for explanations is to offer transparency in the model\u2019s given output based on which features play a role and what that role is. A few methods that offer similar explanations are LIME (Ribeiro et al., 2016), QII (Datta et al., 2016), STREAK (Elenberg et al., 2017) and SHAP (Lundberg & Lee, 2016). Each of these approaches answers the question \u201cWhy this output?\u201d in some way by providing a subset of features or an ordered list of all features, either visualized or structured in a text template. However, when humans answer such questions to each other they tend to limit their explanations to a few vital points (Pacer & Lombrozo, 2017). This human tendency for simplicity also shows in iML: when multiple explanations hold we should pick the simplest explanation that is consistent with the data (Huysmans et al., 2011). The mentioned approaches do this by either thresholding the contribution parameter to a fixed value, presenting the entire ordered list or by applying it only to low-dimensional data.\nar X\niv :1\n80 6.\n07 47\n0v 1\n[ st\nat .M\nL ]\n1 9\nJu n\n20 18\nThis study offers a more human-like way of limiting the list of contributing features by setting a contrast between two outputs. The proposed contrastive explanations present only the information that causes some data point to be classified as some class instead of another (Miller et al., 2017). Recently, Dhurandhar et al. (2018) have proposed constructing explanations by finding contrastive perturbations\u2014minimal changes required to change the current classification to any arbitrary other class. Instead, our approach creates contrastive targeted explanations by first defining the output of interest. In other words, our contrastive explanations answer the question \u201cWhy this output instead of that output?\u201d. The contrast is made between the fact, the given output, and the foil, the output of interest.\nA relative straightforward way to construct contrastive explanations given a foil based on feature contributions, is to compare the two ordered feature lists and see how much some feature differs in their ranking. However, a feature may have the same rank in both ordered lists but can be used in entirely different ways for the fact and foil classes. To mitigate this problem we propose a more meaningful comparison based on how a feature is used to distinct the foil from the fact. We train an arbitrary model to distinguish between fact and foil that is more accessible. From that model we distill two sets of rules; one used to identify data points as a fact and the other to identify data points as a foil. Given these two sets, we subtract the factual rule set from the foil rule set. This relative complement of the fact rules in the foil rules is used to construct our contrastive explanation. See Figure 1 for an illustration.\nThe method we propose in this study obtains this complement by training a one-versus-all decision tree to recognize the foil class. We refer to this decision tree as the Foil Tree. Next, we identify the fact-leaf\u2014the leaf in which the current questioned data point resides. Followed by identifying the foil-leaf, which is obtained by searching the tree with some strategy. Currently our strategy is simply to choose the closest leaf to the fact-leaf that classifies data points as\nthe foil class. The complement is then the set of decision nodes (representing rules) that are a parent of the foil-leaf but not of the fact-leaf. Rules that overlap are merged to obtain a minimum coverage rule set. The rules are then used to construct our explanation. The method is discussed in more detail in section 2. An example of its usage is discussed in section 3 on three benchmark classification tasks. The validation on these three tasks shows that the proposed method constructs shorter explanations than the fully feature list, provide more information of how these features contribute and that this contribution matches the underlying model closely."
    },
    {
      "heading": "2. Foil Trees; a way for obtaining contrastive explanations",
      "text": "The method we propose learns a decision tree centred around any questioned data point. The decision tree is trained to locally distinguish the foil-class from any other class, including the fact class. Its training occurs on data points that can either be generated or sampled from an existing data set, each labeled with predictions from the model it aims to explain. As such, our method is modelagnostic. Similar to LIME (Ribeiro et al., 2016), the sample weights of each generated or sampled data point depend on its similarity to the data point in question. Samples in the vicinity of the questioned data point receive higher weights in training the tree, ensuring its local faithfulness.\nGiven this tree, the \u2018foil-tree\u2019, we search for the leaf in which the data point in question resides, the so called \u2018factleaf\u2019. This gives us the set of rules that defines that data point as the not-foil class according to the foil-tree. These rules respect the decision boundary of the underlying ML model as it is trained to mirror the foil class outputs. Next, we use an arbitrary strategy to locate the \u2018foil-leaf\u2019\u2014for example the leaf that classifies data point as the foil class with the lowest number of nodes between itself and the fact-leaf. This results in two rule sets, whose relative complement define how the data point in question differs from the foil data points as classified by the foil-leaf. This explanation of the difference is done in terms of the input features themselves.\nIn summary, the proposed method goes through the following steps to obtain a contrastive explanation for an arbitrary ML model, the questioned data point and its output according to that ML model:\n1. Retrieve the fact; the output class.\n2. Identify the foil; explicitly given in the question or derived (e.g. second most likely class).\n3. Generate or sample a local data set; either randomly sampled from an existing data set, generated\naccording to a normal distribution, generated based on marginal distributions of feature values or more complex methods.\n4. Train a decision tree; with sample weights depending on the training point\u2019s proximity or similarity to the data point in question.\n5. Locate the \u2018fact-leaf\u2019; the leaf in which the data point in question resides.\n6. Locate a \u2018foil-leaf\u2019; we select the leaf that classifies data points as part of the foil class with the lowest number of decision nodes between it and the fact-leaf.\n7. Compute differences; to obtain the two set of rules that define the difference between fact- and foil-leaf, all common parent decision nodes are removed from each rule sets. From the decision nodes that remain, those that regard the same feature are combined to form a single literal.\n8. Construct explanation; the actual presentation of the differences between the fact-leaf and foil-leaf.\nFigure 2 illustrates the aforementioned steps. The search for the appropriate foil-leaf in step 6 can vary. In Section 2.1 we discuss this more in detail. Finally, note that the method is not symmetrical. There will be a different answer on the question \u201cWhy class A and not B?\u201d then on \u201cWhy class B and not A?\u201d as the foil-tree is trained in the first case to identify class B and in the second case to identify class A. This is because we treat the foil as the expected class or the class of interest to which we compare everything else. In addition, even if the trees are similar, the relative complements of their rule sets are reversed"
    },
    {
      "heading": "2.1. Foil-leaf strategies",
      "text": "Up to now we mentioned one strategy to find a foil-leaf, however multiple strategies are possible\u2014although not all strategies may result in a satisfactory explanation according to the user. The strategy used in this study is simply the first leaf that is closest to the fact-leaf in terms of number decision nodes, resulting in a minimal length explanation.\nA disadvantage of this strategy is its ignorance towards the value of the foil-leaf compared to the rest of the tree. The nearest foil-leaf may be a leaf that classifies only a relatively few data points or classifies them with a relatively high error rate. To mitigate such issues the foil-leaf selection mechanism can be generalized to a graph-search from a specific (fact) vertex to a different (foil) vertex while minimizing edge weights. The foil-tree is treated as a graph whose decision node and leaf properties influence some weight function. This generalization allows for a number of strategies, and each may result in a different foil-leaf.\nThe strategy used in this preliminary study simply reduces to each edge having a weight of one, resulting in the nearest foil-leaf when minimizing the total weights.\nAs an example, an improved strategy may be where the edge weights are based on the relative accuracy of a node (based on its leaves) or leaf. Where a higher accuracy results in a lower weight, allowing the strategy to find more distant, but more accurate, foil-leaves. This may result in relatively more complex and longer explanations, which nonetheless hold in more general cases. For example the nearest foil-leaf may only classify a few data points accurately, whereas a slightly more distant leaf classifies significantly more data points accurately. Given the fact that an explanation should be both accurate and fairly general, this proposed strategy may be more beneficial (Craven & Shavlik, 1999).\nNote that the proposed method assumes the knowledge of the used foil. In all cases we take the second most likely class as our foil. Although this may be an interesting foil it may not be the contrast the user actually wants to make. Either the user makes its foil explicit or we introduce a feedback loop in the interaction that allows our approach to learn which foil is asked for in which situations. We leave this for future work."
    },
    {
      "heading": "3. Validation",
      "text": "The proposed method is validated on three benchmark classification tasks from the UCI Machine Learning Repository (Dua & Karra Taniskidou, 2017); the Iris data set, the PIMA Indians Diabetes data set and the Cleveland Heart Disease data set. The first data set is a well-known classification task of plants based on four flower leaf characteristics with a size of 150 data points and three classes. The second data set is a binary classification task whose task is to correctly diagnose diabetes and contains 769 data points and has nine features. The third data set is aims at classifying the risk of heart disease from no presence (0) to presence (1\u20134), consisting of 297 instances with 13 features.\nTo show the model-agnostic nature of our proposed method we applied four distinct classification models to each data set: a random forest, logistic regression, support vector machine (SVM) and a neural network. Table 1 shows for each data set and classifier the F1 score of the trained model. We validated our approach on four measures; explanation length, accuracy, fidelity and time. These measures for evaluating iML decision rules are adapted from Craven & Shavlik (1999), where the mean length serves as a proxy measure demonstrating the relative explanation comprehensibility (Doshi-Velez & Kim, 2017). The fidelity allows us to state how well the tree explains the underlying model,\nand the accuracy tells us how well its explanations generalize to unseen data points. Below we describe each in detail:\n1. Mean length; average length of the explanation in terms of decision nodes. The ideal value is in the range [1.0, Nr. features), since a length of 0 means that no explanation is found and a length near the number of features offers little gain compared to showing the entire ordered feature contribution list as in other iML methods.\n2. Accuracy; F1 score of the foil-tree for its binary classification task on the test set compared to the true labels. This measure indicates how general the explanations generated from the Foil Tree are on an unseen test set.\n3. Fidelity; F1 score of the foil-tree on the test set compared to the model output. This measure provides a quantitative value of how well the Foil Tree agrees with the underlying classification model it tries to explain.\n4. Time; number of seconds needed on average to explain a test data point.\nEach measure is cross-validated three times to account for randomness in foil-tree construction. These results are shown in their respective columns in Table 1. They show that on average the Foil Tree is able to provide concise explanations, with a mean length 1.33, while accurately mimicking the decision boundaries used by the model with a\nmean fidelity of 0.93 and generalizes well to unseen data with a mean accuracy of 0.92. The foil-tree performs similar to the underlying ML model in terms of accuracy. Note that for the random forest, logistic regression and SVM models on the diabetes data set rules of length zero were found\u2014i.e. no explanatory differences were found between facts and foils in a number of cases\u2014, resulting in a mean length of less than one. For all other models our method was able to find a difference for every questioned data point.\nTo further illustrate the proposed method, below we present a single explanation of two classes of the Iris data set in a dialogue setting;\n\u2022 System: The flowertype is \u2018Setosa\u2019. \u2022 User: Why \u2018Setosa\u2019 and not \u2018Versicolor\u2019? \u2022 System: Because for it to be \u2018Versicolor\u2019 the\n\u2018petal width (cm)\u2019 should be smaller and the \u2018sepal width (cm)\u2019 should be larger.\n\u2022 User: How much smaller and larger? \u2022 System: The \u2018petal width (cm)\u2019 should be\nsmaller than or equal to 0.8 and the \u2018sepal width (cm)\u2019 should be larger than 3.3.\nThe fact is the \u2018Setosa\u2019 class, the foil is the \u2018Versicolor\u2019 class and the total length of the explanation contains two decision nodes or literals. The generation of this small dialogue is based on text templates and fixed interactions for the user."
    },
    {
      "heading": "4. Conclusion",
      "text": "Current developments in Interpretable Machine Learning (iML) created new methods to answer \u201cWhy output A?\u201d for Machine Learning (ML) models. A large set of such methods use the contributions of each feature used to classify A and then provides either a subset of feature whose contribution is above a threshold, the entire ordered feature list or simply apply it only to low-dimensional data.\nThis study proposes a novel method to reduce the number of contributing features for a class by answering a contrasting question of the form \u201cWhy output A (fact) instead of output B (foil)?\u201d for an arbitrary data point. This allows us to construct an explanation in which only those features play a role that distinguish A from B. Our approach finds the contrastive explanation by taking the complement set of decision rules that cause the classification of A in the rule set of B. In this study we implemented this idea by training a decision tree to distinguish between B and not-B (oneversus-all approach). A fact-leaf is found in which the data point in question resides. Also, a foil-leaf is selected according to a strategy where all data points are classified as the foil (output B). We then form the contrasting rules by extracting the decision nodes in the sub-tree from the lowest common ancestor between the fact-leaf and foil-leaf, that hold for the foil-leaf but not for the fact-leaf. Overlapping rules are merged and eventually used to construct an explanation.\nWe introduced a simple and naive strategy of finding an appropriate foil-leaf. We also provided an idea to extend this method with more complex and accurate strategies, which is part of our future work. We plan a user validation of our explanations with non-experts in Machine Learning to test the satisfaction of our explanations. In this study we tested if the proposed method is viable on three different benchmark tasks as well as to test its fidelity on different\nunderlying ML models to show its model-agnostic capacity.\nThe results showed that for different classifiers our method is able to offer concise explanations that accurately describe the decision boundaries of the model it explains.\nAs mentioned, our future work will consist out of extending this preliminary method with more foil-leaf search strategies as well as applying the method to more complex tasks and validating its explanations with users. Furthermore, we plan to extend the method with an adaptive foil-leaf search to adapt explanations towards a specific user based on user feedback."
    }
  ],
  "title": "Contrastive Explanations with Local Foil Trees",
  "year": 2018
}
