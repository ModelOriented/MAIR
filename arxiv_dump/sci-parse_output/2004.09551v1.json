{
  "abstractText": "As Machine Learning (ML) systems becomes more ubiquitous, ensuring the fair and equitable application of their underlying algorithms is of paramount importance. We argue that one way to achieve this is to proactively cultivate public pressure for ML developers to design and develop fairer algorithms \u2014 and that one way to cultivate public pressure while simultaneously serving the interests and objectives of algorithm developers is through gameplay. We propose a new class of games \u2014 \u201cgames for fairness and interpretability\u201d \u2014 as one example of an incentive-aligned approach for producing fairer and more equitable algorithms. Games for fairness and interpretability are carefully-designed games with mass appeal. They are inherently engaging, provide insights into how machine learning models work, and ultimately produce data that helps researchers and developers improve their algorithms. We highlight several possible examples of games, their implications for fairness and interpretability, how their proliferation could creative positive public pressure by narrowing the gap between algorithm developers and the general public, and why the machine learning community could benefit from them.",
  "authors": [
    {
      "affiliations": [],
      "name": "Eric Chu"
    },
    {
      "affiliations": [],
      "name": "Nabeel Gillani"
    },
    {
      "affiliations": [],
      "name": "Sneha Priscilla Makini"
    }
  ],
  "id": "SP:e2954e5fe02231f4a00c5511c541fabc7aae84cd",
  "references": [
    {
      "authors": [
        "David Autor"
      ],
      "title": "Why Are There Still So Many Jobs? The History and Future of Workplace Automation",
      "venue": "Journal of Economic Perspectives 29,",
      "year": 2015
    },
    {
      "authors": [
        "Luke Barrington",
        "Douglas Turnbull",
        "Gert Lanckriet"
      ],
      "title": "Game-powered machine learning",
      "venue": "Proceedings of the National Academy of Sciences 109,",
      "year": 2012
    },
    {
      "authors": [
        "David Bau",
        "Bolei Zhou",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "title": "Network dissection: Quantifying interpretability of deep visual representations",
      "venue": "In Computer Vision and Pattern Recognition (CVPR),",
      "year": 2017
    },
    {
      "authors": [
        "Tolga Bolukbasi",
        "Kai-Wei Chang",
        "James Y Zou",
        "Venkatesh Saligrama",
        "Adam T Kalai"
      ],
      "title": "2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "venue": "InAdvances in Neural Information Processing Systems",
      "year": 2016
    },
    {
      "authors": [
        "Joy Buolamwini",
        "Timnit Gebru"
      ],
      "title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification",
      "venue": "In Proceedings of Machine Learning Research, Conference on Fairness,",
      "year": 2018
    },
    {
      "authors": [
        "Aylin Caliskan",
        "Joanna J Bryson",
        "Arvind Narayanan"
      ],
      "title": "Semantics derived automatically from language corpora contain human-like biases",
      "venue": "Science 356,",
      "year": 2017
    },
    {
      "authors": [
        "Chaofan Chen",
        "Oscar Li",
        "Daniel Tao",
        "Alina Barnett",
        "Cynthia Rudin",
        "Jonathan K Su"
      ],
      "title": "2019. This looks like that: deep learning for interpretable image recognition",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Xi Chen",
        "Yan Duan",
        "Rein Houthooft",
        "John Schulman",
        "Ilya Sutskever",
        "Pieter Abbeel"
      ],
      "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2016
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Been Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "year": 2017
    },
    {
      "authors": [
        "Cynthia Dwork",
        "Moritz Hardt",
        "Toniann Pitassi",
        "Omer Reingold",
        "Richard Zemel"
      ],
      "title": "Fairness through awareness",
      "venue": "In Proceedings of the 3rd innovations in theoretical computer science conference",
      "year": 2012
    },
    {
      "authors": [
        "Ziv Epstein",
        "Blakely H. Payne",
        "Judy Hanwen Shen",
        "Abhimanyu Dubey",
        "Bjarke Felbo",
        "Matthew Groh",
        "Nick Obradovich",
        "Manuel Cebrian",
        "Iyad Rahwan"
      ],
      "title": "Closing the AI Knowledge Gap",
      "year": 2018
    },
    {
      "authors": [
        "Virginia Eubanks"
      ],
      "title": "Automating Inequality: how high-tech tools profile, police, and punish the poor",
      "year": 2018
    },
    {
      "authors": [
        "Kevin Eykholt",
        "Ivan Evtimov",
        "Earlence Fernandes",
        "Bo Li",
        "Amir Rahmati",
        "Chaowei Xiao",
        "Atul Prakash",
        "Tadayoshi Kohno",
        "Dawn Song"
      ],
      "title": "Robust physicalworld attacks on deep learning visual classification",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "year": 2018
    },
    {
      "authors": [
        "Michael Feldman",
        "Sorelle A Friedler",
        "JohnMoeller",
        "Carlos Scheidegger",
        "Suresh Venkatasubramanian"
      ],
      "title": "Certifying and removing disparate impact",
      "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2015
    },
    {
      "authors": [
        "Nicholas Frosst",
        "Geoffrey Hinton"
      ],
      "title": "Distilling a Neural Network Into a Soft Decision Tree",
      "venue": "arXiv preprint arXiv:1711.09784",
      "year": 2017
    },
    {
      "authors": [
        "Ian J Goodfellow",
        "Jonathon Shlens",
        "Christian Szegedy"
      ],
      "title": "Explaining and harnessing adversarial examples",
      "venue": "arXiv preprint arXiv:1412.6572",
      "year": 2014
    },
    {
      "authors": [
        "Moritz Hardt",
        "Eric Price",
        "Nati Srebro"
      ],
      "title": "Equality of opportunity in supervised learning",
      "venue": "In Advances in neural information processing systems",
      "year": 2016
    },
    {
      "authors": [
        "Lisa Anne Hendricks",
        "Zeynep Akata",
        "Marcus Rohrbach",
        "Jeff Donahue",
        "Bernt Schiele",
        "Trevor Darrell"
      ],
      "title": "Generating visual explanations",
      "venue": "In European Conference on Computer",
      "year": 2016
    },
    {
      "authors": [
        "Irina Higgins",
        "Loic Matthey",
        "Arka Pal",
        "Christopher Burgess",
        "Xavier Glorot",
        "Matthew Botvinick",
        "Shakir Mohamed",
        "Alexander Lerchner"
      ],
      "title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
      "year": 2016
    },
    {
      "authors": [
        "Sarthak Jain",
        "Byron C Wallace"
      ],
      "title": "Attention is not Explanation",
      "year": 2019
    },
    {
      "authors": [
        "Faisal Kamiran",
        "Toon Calders"
      ],
      "title": "Data preprocessing techniques for classification without discrimination",
      "venue": "Knowledge and Information Systems 33,",
      "year": 2012
    },
    {
      "authors": [
        "Firas Khatib",
        "Frank DiMaio",
        "Seth Cooper",
        "Maciej Kazmierczyk",
        "Miroslaw Gilski",
        "Szymon Krzywda",
        "Helena Zabranska",
        "Iva Pichova",
        "James Thompson",
        "Zoran Popovi\u0107"
      ],
      "title": "Crystal structure of a monomeric retroviral protease solved by protein folding game players",
      "venue": "Nature Structural and Molecular Biology 18,",
      "year": 2011
    },
    {
      "authors": [
        "Pieter-Jan Kindermans",
        "Sara Hooker",
        "Julius Adebayo",
        "Maximilian Alber",
        "Kristof T Sch\u00fctt",
        "Sven D\u00e4hne",
        "Dumitru Erhan",
        "Been Kim"
      ],
      "title": "The (un) reliability of saliency methods. In Explainable AI: Interpreting",
      "year": 2019
    },
    {
      "authors": [
        "Tejas D Kulkarni",
        "William FWhitney",
        "Pushmeet Kohli",
        "Josh Tenenbaum"
      ],
      "title": "Deep convolutional inverse graphics network",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2015
    },
    {
      "authors": [
        "Alexey Kurakin",
        "Ian Goodfellow",
        "Samy Bengio"
      ],
      "title": "Adversarial examples in the physical world",
      "venue": "arXiv preprint arXiv:1607.02533",
      "year": 2016
    },
    {
      "authors": [
        "Edith Law",
        "Luis Von Ahn"
      ],
      "title": "Input-agreement: a new mechanism for collecting data using human computation games",
      "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "year": 2009
    },
    {
      "authors": [
        "Jeehyung Lee",
        "Wipapat Kladwang",
        "Minjae Lee",
        "Daniel Cantu",
        "Martin Azizyan",
        "Hanjoo Kim",
        "Alex Limpaecher",
        "Snehal Gaikwad",
        "Sungroh Yoon",
        "Adrien Treuille"
      ],
      "title": "RNA design rules from a massive open laboratory",
      "venue": "Proceedings of the National Academy of Sciences 111,",
      "year": 2014
    },
    {
      "authors": [
        "Tao Lei",
        "Regina Barzilay",
        "Tommi Jaakkola"
      ],
      "title": "Rationalizing neural predictions",
      "venue": "arXiv preprint arXiv:1606.04155",
      "year": 2016
    },
    {
      "authors": [
        "Zachary C Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "arXiv preprint arXiv:1606.03490",
      "year": 2016
    },
    {
      "authors": [
        "Aravindh Mahendran",
        "Andrea Vedaldi"
      ],
      "title": "Understanding deep image representations by inverting them",
      "year": 2015
    },
    {
      "authors": [
        "Alexander Mordvintsev",
        "Christopher Olah",
        "Mike Tyka"
      ],
      "title": "Inceptionism: Going deeper into neural networks",
      "venue": "Google Research Blog. Retrieved June 20,",
      "year": 2015
    },
    {
      "authors": [
        "Safiya Umoja Noble"
      ],
      "title": "Algorithms of oppression : how search engines reinforce racism",
      "year": 2018
    },
    {
      "authors": [
        "Cathy O\u2019Neil"
      ],
      "title": "Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy",
      "year": 2016
    },
    {
      "authors": [
        "Vitali Petsiuk",
        "Abir Das",
        "Kate Saenko"
      ],
      "title": "Rise: Randomized input sampling for explanation of black-box models",
      "venue": "arXiv preprint arXiv:1806.07421",
      "year": 2018
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2016
    },
    {
      "authors": [
        "Amir Rosenfeld",
        "Richard Zemel",
        "John K Tsotsos"
      ],
      "title": "The elephant in the room",
      "venue": "arXiv preprint arXiv:1808.03305",
      "year": 2018
    },
    {
      "authors": [
        "Daniel Smilkov",
        "Nikhil Thorat",
        "Been Kim",
        "Fernanda Vi\u00e9gas",
        "Martin Wattenberg"
      ],
      "title": "Smoothgrad: removing noise by adding noise",
      "year": 2017
    },
    {
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org,",
      "year": 2017
    },
    {
      "authors": [
        "Joaquin Vanschoren",
        "Jan N Van Rijn",
        "Bernd Bischl",
        "Luis Torgo"
      ],
      "title": "OpenML: networked science in machine learning",
      "venue": "ACM SIGKDD Explorations Newsletter 15,",
      "year": 2014
    },
    {
      "authors": [
        "Luis Von Ahn",
        "Laura Dabbish"
      ],
      "title": "Labeling images with a computer game",
      "venue": "In Proceedings of the SIGCHI conference on Human factors in computing systems",
      "year": 2004
    },
    {
      "authors": [
        "Luis Von Ahn",
        "Laura Dabbish"
      ],
      "title": "Designing games with a purpose",
      "venue": "Commun. ACM 51,",
      "year": 2008
    },
    {
      "authors": [
        "Sandra Wachter",
        "Brent Mittelstadt",
        "Chris Russell"
      ],
      "title": "Counterfactual explanations without opening the black box: Automated decisions and the GDPR",
      "venue": "Harv. JL & Tech",
      "year": 2017
    },
    {
      "authors": [
        "Rich Zemel",
        "Yu Wu",
        "Kevin Swersky",
        "Toni Pitassi",
        "Cynthia Dwork"
      ],
      "title": "Learning fair representations",
      "venue": "In International Conference on Machine Learning",
      "year": 2013
    },
    {
      "authors": [
        "Zhengli Zhao",
        "Dheeru Dua",
        "Sameer Singh"
      ],
      "title": "Generating natural adversarial examples",
      "venue": "arXiv preprint arXiv:1710.11342",
      "year": 2017
    }
  ],
  "sections": [
    {
      "text": "CCS Concepts \u2022 Human-centered computing; Keywords machine learning, interpretability, fairness, games, crowdsourcing ACM Reference Format: Eric Chu, Nabeel Gillani, and Sneha Priscilla Makini. 2020. Games for Fairness and Interpretability. In Proceedings of The Web Conference 2020 (WWW \u201920), April 20\u201324, 2020, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/1122445.XXXXXXX"
    },
    {
      "heading": "1 Introduction",
      "text": "As ML increasingly permeates virtually all aspects of life \u2014 and unequally serves, or fails to serve, certain subsegments of the population [4\u20136] \u2014 there is a need for a deeper exploration of how ML algorithms can be made fairer and more interpretable. To achieve this, we believe effective public pressure will be one lever to better models. There are several examples from history of how public pressure has spurred changes to technology policies. The creation of dynamite; America\u2019s use of the atomic bomb during the second world war; and the eugenics movement from the early 20th century \u2217Both authors contributed equally.\nThis paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW \u201920, April 20\u201324, 2020, Taipei, Taiwan \u00a9 2020 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License. ACM ISBN 978-1-4503-7023-3/20/04. https://doi.org/10.1145/1122445.XXXXXXX\nare all examples of ethically dubious endeavors that were at least somewhat abated by a critical public response1.\nHowever, recent stories about Facebook and Cambridge Analytica, driverless cars going rogue2, and even machine-powered labor displacement [1] have hinted at the dangers of simply letting history unfold. In all of these instances, there were certainly changes to the underlying technological methods \u2014 but it is hard to deny the importance of collective public pressure in catalyzing dialogue to envision a new set of policies and practices surrounding these powertools. It is unlikely that methodological changes alone would have been sufficient. Public pressure is often reactive and arises in the wake of crises. To counter this, we ask: how can public pressure operate proactively in order to ensure ML can effectively ground itself in \u2014 and respond to \u2014 calls for fairness and interpretability?\nTo that end, some authors have recently sparked public conversation around the ethical pitfalls of machine learning [12, 32, 33]. Furthermore, initiatives like Turingbox [11] and OpenML [39] are actively seeking to create platforms and marketplaces where members of the scientific community and general public can audit ML algorithms to promote more fairness, transparency, and accountability. These efforts are important first steps towards generating proactive public pressure. However, they fail to directly align incentives between those who design and deploy algorithms and those who are affected by them. Why should an algorithm developer care about how a niche group of individuals rates the fairness or interpretability of his or her algorithms? Why should members of the general public spend their time trying to understand, let alone evaluate, these algorithms? It is unclear how sustainable current efforts to generate proactive public pressure will be without incentive alignment.\nTo align incentives between ML developers and the general public in a quest for more interpretable \u2014 and as a result, in due course, fairer \u2014 ML, we propose \u201cgames for fairness and interpretability\u201d: networked games that as a byproduct of the game\u2019s objectives, engage the general public in auditing algorithms while simultaneously generating valuable training sets for ML developers."
    },
    {
      "heading": "2 ML Powered Games",
      "text": "Inspired by Luis von Ahn\u2019s Games with a Purpose (GWAP) framework [40, 42], we propose using ML-powered games to enhance model interpretability \u2014 which we view as an important step towards developing fairer ML.\n1https://www.bostonglobe.com/ideas/2018/03/22/computerscience-faces-ethics-crisis-the-cambridge-analytica-scandalproves/IzaXxl2BsYBtwM4nxezgcP/story.html 2https://www.nytimes.com/2018/03/23/technology/uber-self-driving-carsarizona.html\nar X\niv :2\n00 4.\n09 55\n1v 1\n[ cs\n.C Y\n] 2\n0 A\npr 2\n02 0"
    },
    {
      "heading": "2.1 Games with a Purpose",
      "text": "Described as \u201chuman computation\", the GWAP framework was designed for problems solvable by humans but beyond the capabilities of machines. Instead of relying on financial incentives or altruism, GWAPs simply rely on people\u2019s desire for fun and entertainment. A successful GWAP can produce not only novel and creative solutions to difficult problems, but also provide large amounts of labeled data for training machine learning models. Since its inception, GWAPs have attracted hundreds of thousands of players in order to tackle problems ranging from protein folding [22] and RNA folding [27] to examining the human perception of correlation in scatter plots3. The framework has also since been extended to machine learning, such as using active learning to select examples during gameplay [2].\nThe GWAP framework includes several different templates of games [42]. Output-agreement games has two players attempt to produce the same output when shown the same input. In the ESP game, shown in Figure 1, the players are shown an image and asked to guess what words the other player would use to describe the image. A variation of the game includes taboo words for each image, thus requiring users to guess more uncommonwords, in turn producing more interesting labeled data [41]. In input-agreement games, two players are each provided an input which may or may not be different; the players are asked to output descriptions of the inputs and then finally guess whether they were shown the same input. For instance, players in the Tagatune game are given song clips and asked to output tags, before finally guessing whether they had the same clip [26]."
    },
    {
      "heading": "2.2 Designing Games for Fairness and Interpretability",
      "text": "While reputation-based incentives can create social pressure and motivate ML developers, we believe a well-designed game aligns 3http://guessthecorrelation.com/\nincentives between ML developers and the consumers of ML (i.e. the general public). Due to the importance of labeled data for deep neural networks, we believe ML researchers will have strong incentives to upload their models if the games that leverage them can produce valuable training data or adversarial examples.\nOn the consumer side, GWAPs have shown that such games can reach large audiences. Furthermore, a larger audience is often a broader audience, thus allowing more diverse probing of the model. We believe that there is an appetite for ML games, due both to increasing media attention on ML and the growing capabilities of new models. Recent examples of games that engage a general audience in exploring ML include the text auto-complete \u201cTalk to Transformer\u201d4, a Pictionary-like game Quick, Draw!5, word embedding-powered word association games6, and an endless textadventure game built using a generative text model7.\nWe define \u201cgames for fairness and interpretability\u201d asML-powered games in which the output and / or interaction with human players is produced by a machine learning model. These games can also be networked to enable human-human interaction and competition. Games should be fun and engaging, provide insight into how the underlying machine learning models work, and produce data that helps models improve \u2014 in particular, so that the models are betterequipped to more equitably serve a diverse range of individuals and scenarios.\nOne might imagine a platform for such games, where once a game has been designed and open-sourced, its backend model could be swapped for any model with similar inputs and outputs. The platform could also serve as a public forum for widespread participation in, and discussion about, the evaluation of new ML models. This unique forum \u2014 one where both ML developers and members of the public are present \u2014 could serve as an important vehicle for a) enhancing broader familiarity with and awareness of ML and its applications, and perhaps eventually, b) creating proactive public pressure that motivates algorithm developers to build more interpretable and fairer ML."
    },
    {
      "heading": "2.3 Proposed Categories of Games",
      "text": "In the spirit of GWAPs, we describe possible categories of games in the following sections."
    },
    {
      "heading": "2.3.1 Humans vs. AI",
      "text": "Setup. Player 1 provides an input, and Player 2 competes against an AI to guess the correct answer. Example game 1\u2014GuessWho? Player 1 describes themselves, their interests, job, and other attributes through freeform short text. Player 2 and the AI attempt to guess the age, sex, and location of Player 1.\nExample game 2 \u2013 Codenames. Inspired by the popular Codenames board game [44] , the players are presented with a 5x5 grid of words. Player 1 is a \u201cspymaster\u201d who is also allowed to see the placement of bombs on the grid. The spymaster\u2019s role is to give a one word clue, plus the number of words that matches the clue. Player 2\u2019s goal is to guess the correct words; however, if he or\n4https://talktotransformer.com/ 5https://quickdraw.withgoogle.com/ 6http://robotmindmeld.com/ 7https://www.aidungeon.io/\n(a) Example of aHumans vs. AI game. Player 1 (the \u201cspymaster\u201d) provides an input, while Player 2 competes against an AI to produce the correct answer. Here, since the human and the AI both guessed \u201cEngland\u201d, only \u201cYard\u201d would count as a correct answer.\n(b) Example of a Break the Bot game. Player 1 and Player 2 compete against each other in producing adversarial attacks that will reduce the accuracy of the model\u2019s predictions. Players should be incentivized tomake small edits that nevertheless produce large decreases in accuracy. In this example, players are provided tools to change the lighting and color, or add and remove common objects.\nFigure 2: Examples of possible Games for Fairness and Interpretability. Both types of games are designed to surface model biases and deficiencies, while also producing more robust and diverse training data.\nshe guesses a bomb, the game is over. The game is won if all the non-bomb words are guessed correctly. The goal is to finish the game in fewer rounds; saying a larger number allows the team to win more quickly, but it is also more difficult to come up with clues.\nIn our ML-powered variant, the AI also attempts to guess the words; if the AI\u2019s guesses matches Player 2\u2019s guesses, those guesses are invalid. Figure 2a shows an example round.\nData produced and insight into interpretability. Player 1 will have to produce inputs that are recognizable by another human but undetectable or incorrectly classified by the AI. This requires a player to intuit the space of inputs that a model understands and in which cases it might fail. For instance, Player 1 may find that cultural references are harder for a MLmodel. Natural language processing models that can incorporate common sense reasoning and knowledge also remains an open area of research. The successful inputs and clues can be used as more robust training data.\nIn addition, baseline models for the AI could be based on word embeddings, which have been shown to reflect implicit human biases around gender, race, occupation, etc. [6]. These biases may be surfaced if the AI incorrectly relies on them to make predictions."
    },
    {
      "heading": "2.3.2 Break the Bot",
      "text": "Setup. Each player is shown an input and the model\u2019s output (e.g. a prediction). Each player is asked to make a small modification to the input. Whoever can cause the largest change in the model output, while using the smallest modification, receives more points.\nExample game 1\u2014Vandalize it!The brittleness of deep neural networks has been illustrated in several computer vision systems. For example, graffiti on signs can significantly lower object recognition accuracy [13], while Rosenfeld et al. showed that adding an object to a scene could drastically change the ability to recognize all\nother objects [36]. These deficiencies can have catastrophic effects on real-world systems.\nIn this self-driving car inspired game, players are shown street images overlaid with bounding boxes of detected objects. For example, a stop sign may be detected by the model with probability 0.85. The players\u2019 goal is to change that prediction by making small edits to the sign and its surroundings. The game will give players tools to alter the angle, lighting, hue of the image, as well as add and subtract other objects and artificats. (The game will have to measure the \u2018size\u2019 of modifications in order to assign scores). Figure 2b shows an example of how the game might look.\nExample game 2\u2014Beat the Banker.ML has begun to be used in higher-stakes situations, ranging from recidivism prediction to loan default rate prediction. Unfortunately, these systems have also been shown to be susceptible to demographic features and unfairness [17]. In this game, the players are bankers. The input is a hypothetical set of demographic features of an individual, and the output is the predicted probability of that individual\u2019s loan repayment. Faced with a loan rejection, the goal is to find seemingly innocuous changes that can make the loan approved.\nData produced and insight into interpretability.These games provide adversarial examples and sensitivity analysis on model inputs. This is important as the field of adversarial examples is becoming increasingly important [16], especially as ML models become deployed in the real world [25], and obtaining those examples can often be difficult [46]. ML researchers can also gain a greater understanding of how inputs may be modified in semantically meaningful ways, as well as if the observed model behavior is desirable (e.g. fair)."
    },
    {
      "heading": "3 Games and Current Research Directions in Machine Learning",
      "text": "The previous section illustrates how thoughtfully-designed games might help align incentives between ML developers and the general public, cultivating public pressure and awareness \u2014 along with the new, more representative datasets \u2014 to promote fairer, more inclusive ML systems. We believe the time to develop games for fairness and interpretability is now, largely because they align with several current directions in ML research. We highlight some of these directions below and explore howmembers of these respective research communities may benefit from games for fairness and interpretability."
    },
    {
      "heading": "3.1 Fairness",
      "text": "AsMLmodels become more pervasive, there has been an increasing call for models that can prevent discrimination along sensitive attributes such as race and gender. Part of the problem is detecting that biases in models even exist in the first place. To that end, recent research has shown how word embeddings encode biases as measured by standard tests such as the Implicit Association Test [6], with relationships between word embeddings reflecting negative stereotypes about gender [4]. Other work highlights deficiencies in datasets used for facial recognition, resulting in models that fail more frequently for women and people with darker skin tones [5].\nHow can models handle these sensitive attributes? A naive approach of removing sensitive attributes may not prevent discrimination if the sensitive attributes are correlated with other attributes left in the dataset. Enforcing demographic parity, in which the outcome is uncorrelated with the sensitive attribute, is also problematic because it does not guarantee fairness, and the sensitive attribute may actually be important for prediction, making removal of all correlation unrealistic. Thus far, various approaches to formalize and operationalize fairness include using the 80% rule of \u201cdisparate impact\u201d outlined by the US Equal Employment Opportunity Commission as a definition of discrimination [14], treating similar individuals similarly by enforcing a Lipschitz condition on similar individuals and the classifier predictions for those individuals [10], preprocessing the dataset through methods such as weighting and sampling [21], and allowing use of the sensitive attribute but aiming for \u201cequality of opportunity\u201d through the notion of equalized odds [17]. Certain frameworks also provide the ability for people to select the tradeoff between model performance and fairness. Other work has centered on learning transferable fair representations that can be reused across tasks [45].\nWe believe ML fairness researchers would find value in the datasets produced by games for fairness and interpretability. For example, machine predictions from \u201cHuman vs. AI\u201d games would provide clear insights into which kinds of biases certain algorithms harbor; \u201cBreak the Bot\u201d games might shed light on how robust or brittle algorithms are to changes in the datasets they operate on."
    },
    {
      "heading": "3.2 Interpretability",
      "text": "While deep neural networks have found great success as powerful function approximators, they have also developed a reputation as black boxes. Interpretability may be a case of \u201cyou know it when you see it\u201d, but recent work has attempted to make the problem\nmore tractable by defining interpretability, explaining why it is important, and explaining when it is necessary [9, 29].\nThere has also been a wide range of methods focused on introspection and visualization, including (but not limited to) \u201cinverting\u201d intermediate representations to generate images [30, 31], producing input feature attributions and saliency maps [3, 34, 35, 37, 38] vs. producing counterfactual explanations [43] vs. pointing to protoypical examples [7], local per-example explanations [35] vs. global explanations based on feature representations across the entire dataset [3], clear-box approaches with access to model gradients [37, 38] vs. black box approaches [34, 35]. These methods often highlight what parts of the input (e.g. a segment of the image, or a span of the text), were most important to the model\u2019s decision.\nWhile there is also work worth mentioning on (1) generating human readable explanations in natural langauge [18, 28], (2) distilling neural networks into more interpretable models such as decision trees [15], and (3) disentangling factors of variation for generative models [8, 19, 24], a significant portion of the field has focused on the aforementioned introspection and visualization methods. At the core, many of the methods attempt to relate input or internal representations to the model outputs. However, there are questions around the reliability and intuitiveness of these explanations ([20, 23]). The games\u2019 data can be analyzed through these methods, perhaps providing insight into how well current explanations match human intuitions. The \u201cBreak the Bot\u201c games would also produce valuable counterfactual data; analyzing changes in the outputs of their underlying models as a function of changes to inputs could provide a deeper understanding of how, exactly, these models are conducting their computations."
    },
    {
      "heading": "4 Conclusion",
      "text": "As ML-powered technologies continue to proliferate, the threat of biased and opaque decision-making looms large. We believe public pressure is a powerful mechanism for inspiring changes in how algorithms are developed. Games for fairness and interpretability provide one means for engaging the public in probes of ML systems while simultaneously producing hard-to-source data that serves the interests of ML developers. We believe games are unique in their ability to engage different audiences and are thus a promising avenue in which to pursue complicated, multi-stakeholder challenges like building fairer ML systems.\nLooking ahead, there are several open questions: who should be responsible for designing and developing games for fairness and interpretability? How will the games be deployed and marketed so as to recruit a diverse range of players? What new risks or threats might these games introduce? These are important questions that will require continuous exploration and reflection. We hope this paper serves as an initial stepping stone and inspires individuals bothwithin and beyond theML community to consider the potential power of games."
    }
  ],
  "title": "Games for Fairness and Interpretability",
  "year": 2020
}
