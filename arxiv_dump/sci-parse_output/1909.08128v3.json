{
  "abstractText": "A number of techniques have been proposed to explain a machine learning model\u2019s prediction by attributing it to the corresponding input features. Popular among these are techniques that apply the Shapley value method from cooperative game theory. While existing papers focus on the axiomatic motivation of Shapley values, and efficient techniques for computing them, they offer little justification for the game formulations used, and do not address the uncertainty implicit in their methods\u2019 outputs. For instance, the popular SHAP algorithm\u2019s formulation may give substantial attributions to features that play no role in the model. In this work, we illustrate how subtle differences in the underlying game formulations of existing methods can cause large differences in the attributions for a prediction. We then present a general game formulation that unifies existing methods, and enables straightforward confidence intervals on their attributions. Furthermore, it allows us to interpret the attributions as contrastive explanations of an input relative to a distribution of reference inputs. We tie this idea to classic research in cognitive psychology on contrastive explanations, and propose a conceptual framework for generating and interpreting explanations for ML models, called formulate, approximate, explain (FAE). We apply this framework to explain black-box models trained on two UCI datasets and a Lending Club dataset.",
  "authors": [
    {
      "affiliations": [],
      "name": "Luke Merrick"
    },
    {
      "affiliations": [],
      "name": "Ankur Taly"
    }
  ],
  "id": "SP:e6d5392afe06d03db356dfc287304edad50cbebd",
  "references": [
    {
      "authors": [
        "K. Aas",
        "M. Jullum",
        "A. L\u00f8land"
      ],
      "title": "Explaining individual predictions when features are dependent: More accurate approximations to shapley values",
      "venue": "arXiv preprint arXiv:1903.10464",
      "year": 2019
    },
    {
      "authors": [
        "M. Ancona",
        "E. Ceolini",
        "C. ztireli",
        "M. Gross"
      ],
      "title": "Towards better understanding of gradientbased attribution methods for deep neural networks",
      "venue": "International Conference on Learning Representations",
      "year": 2018
    },
    {
      "authors": [
        "M. Ancona",
        "C. Oztireli",
        "M. Gross"
      ],
      "title": "Explaining deep neural networks with a polynomial time algorithm for shapley value approximation",
      "venue": "Proceedings of the 36th International Conference on Machine Learning",
      "year": 2019
    },
    {
      "authors": [
        "J. Chen",
        "L. Song",
        "M.J. Wainwright",
        "M.I. Jordan"
      ],
      "title": "L-shapley and c-shapley: Efficient model interpretation for structured data",
      "venue": "arXiv preprint arXiv:1808.02610",
      "year": 2018
    },
    {
      "authors": [
        "S. Cohen",
        "E. Ruppin",
        "G. Dror"
      ],
      "title": "Feature selection based on the shapley value",
      "venue": "In other words 1, 98Eqr",
      "year": 2005
    },
    {
      "authors": [
        "A. Datta",
        "S. Sen",
        "Y. Zick"
      ],
      "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
      "venue": "2016 IEEE symposium on security and privacy (SP). pp. 598\u2013617. IEEE",
      "year": 2016
    },
    {
      "authors": [
        "A. Dhurandhar",
        "P. Chen",
        "R. Luss",
        "C. Tu",
        "P. Ting",
        "K. Shanmugam",
        "P. Das"
      ],
      "title": "Explanations based on the missing: Towards contrastive explanations with pertinent negatives",
      "year": 2018
    },
    {
      "authors": [
        "B. Efron",
        "R. Tibshirani"
      ],
      "title": "Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy",
      "venue": "Statistal Science pp. 54\u201375",
      "year": 1986
    },
    {
      "authors": [
        "A. Ghorbani",
        "J. Zou"
      ],
      "title": "Data shapley: Equitable valuation of data for machine learning",
      "venue": "Proceedings of the 36th International Conference on Machine Learning",
      "year": 2019
    },
    {
      "authors": [
        "G. Hesslow"
      ],
      "title": "The problem of causal selection",
      "venue": "Hilton, D.J. (ed.) Contemporary Science and Natural Explanation: Commonsense Conceptions of Causality. New York University Press",
      "year": 1988
    },
    {
      "authors": [
        "C. Hitchcock",
        "J. Knobecaus"
      ],
      "title": "Cause and norm",
      "venue": "Journal of Philosophy 106(11), 587\u2013612",
      "year": 2009
    },
    {
      "authors": [
        "A. Holzinger",
        "M. Kickmeier-Rust",
        "H. Mller"
      ],
      "title": "KANDINSKY Patterns as IQ-Test for Machine Learning",
      "venue": "Holzinger, A., Kieseberg, P., Tjoa, A.M., Weippl, E. (eds.) Machine Learning and Knowledge Extraction. pp. 1\u201314. Lecture Notes in Computer Science, Springer International Publishing, Cham",
      "year": 2019
    },
    {
      "authors": [
        "X.J. Hunt",
        "R. Abbey",
        "R. Tharrington",
        "J. Huiskens",
        "N. Wesdorp"
      ],
      "title": "An ai-augmented lesion detection framework for liver metastases with model interpretability",
      "venue": "arXiv preprint arXiv:1907.07713",
      "year": 2019
    },
    {
      "authors": [
        "D. Janzing",
        "L. Minorics",
        "P. Blbaum"
      ],
      "title": "Feature relevance quantification in explainable ai: A causal problem",
      "venue": "arXiv preprint arXiv:1910.13413",
      "year": 2019
    },
    {
      "authors": [
        "D. Kahneman",
        "D.T. Miller"
      ],
      "title": "Norm theory: Comparing reality to its alternatives",
      "venue": "Psychological review 93(2), 136",
      "year": 1986
    },
    {
      "authors": [
        "G. Ke",
        "Q. Meng",
        "T. Finley",
        "T. Wang",
        "W. Chen",
        "W. Ma",
        "Q. Ye",
        "T.Y. Liu"
      ],
      "title": "Lightgbm: A highly efficient gradient boosting decision tree",
      "venue": "Advances in Neural Information Processing Systems. pp. 3146\u20133154",
      "year": 2017
    },
    {
      "authors": [
        "P. Lipton"
      ],
      "title": "Contrastive explanation",
      "venue": "Royal Institute of Philosophy Supplement 27, 247266",
      "year": 1990
    },
    {
      "authors": [
        "S.M. Lundberg",
        "G.G. Erion",
        "S.I. Lee"
      ],
      "title": "Consistent individualized feature attribution for tree ensembles",
      "venue": "arXiv preprint arXiv:1802.03888",
      "year": 2018
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "Advances in Neural Information Processing Systems. pp. 4765\u20134774",
      "year": 2017
    },
    {
      "authors": [
        "S. Maleki",
        "L. Tran-Thanh",
        "G. Hines",
        "T. Rahwan",
        "A. Rogers"
      ],
      "title": "Bounding the estimation error of sampling-based shapley value approximation",
      "venue": "arXiv preprint arXiv:1306.4265",
      "year": 2013
    },
    {
      "authors": [
        "T. Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "arXiv preprint arXiv:1706.07269",
      "year": 2017
    },
    {
      "authors": [
        "B. Mittelstadt",
        "C. Russell",
        "S. Wachter"
      ],
      "title": "Explaining explanations in ai",
      "venue": "Proceedings of the conference on fairness, accountability, and transparency. pp. 279\u2013288. ACM",
      "year": 2019
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "SIGKDD international conference on knowledge discovery and data mining. pp. 1135\u20131144",
      "year": 2016
    },
    {
      "authors": [
        "L.S. Shapley"
      ],
      "title": "A value for n-person games",
      "venue": "Contributions to the Theory of Games 2(28), 307\u2013317",
      "year": 1953
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Kundaje"
      ],
      "title": "Learning important features through propagating activation differences",
      "venue": "34th International Conference on Machine Learning-Volume 70. pp. 3145\u20133153",
      "year": 2017
    },
    {
      "authors": [
        "E. \u0160trumbelj",
        "I. Kononenko"
      ],
      "title": "An efficient explanation of individual classifications using game theory",
      "venue": "Journal of Machine Learning Research 11, 1\u201318",
      "year": 2010
    },
    {
      "authors": [
        "E. \u0160trumbelj",
        "I. Kononenko"
      ],
      "title": "Explaining prediction models and individual predictions with feature contributions",
      "venue": "Knowledge and information systems 41(3), 647\u2013665",
      "year": 2014
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Najmi"
      ],
      "title": "The many shapley values for model explanation",
      "venue": "arXiv preprint arXiv:1908.08474",
      "year": 2019
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Taly",
        "Q. Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume 70. pp. 3319\u20133328. JMLR. org",
      "year": 2017
    },
    {
      "authors": [
        "H.P. Young"
      ],
      "title": "Monotonic solutions of cooperative games",
      "venue": "International Journal of Game Theory 14, 65\u201372",
      "year": 1985
    }
  ],
  "sections": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Complex machine learning models are rapidly spreading to high stakes tasks such as credit scoring, underwriting, medical diagnosis, and crime prediction. Consequently, it is becoming increasingly important to interpret and explain individual model predictions to decision-makers, end-users, and regulators. A common form of model explanations are based on feature attributions, wherein a score (attribution) is ascribed to each feature in proportion to the feature\u2019s contribution to the prediction. Over the last few years there has been a surge in feature attribution methods, with methods based on Shapley values from cooperative game theory being prominent among them [27,6,19,1,18,3,4].\nShapley values [24] provide a mathematically fair and unique method to attribute the payoff of a cooperative game to the players of the game. Recently, there have been a number of Shapley-value-based methods for attributing an ML model\u2019s prediction to input features. Prominent among them are SHAP and KernelSHAP [19], TreeSHAP [18], QII [6], and IME [26]. In applying the Shapley values method to ML models, the key step is to setup a cooperative game whose players are the input features and whose payoff is the model prediction. Due to its strong axiomatic guarantees, the Shapley values\nar X\niv :1\n90 9.\n08 12\n8v 3\n[ cs\n.L G\n] 2\n5 Ju\nn 20\nmethod is emerging as the de facto approach to feature attribution, and some researchers even speculate that it may be the only method compliant with legal regulation such as the General Data Protection Regulation\u2019s \u201cright to an explanation\u201d [1].\nIn this work, we study several Shapley-value-based explanation techniques. Paradoxically, while all techniques lay claim to the axiomatic uniqueness of Shapley values, we discover that they yield significantly different attributions for the same input even when evaluated exactly (without approximation). In some cases, we find the attributions to be completely counter-intuitive. For instance, in Section 2, we show a simple model for which the popular SHAP method gives substantial attribution to a feature that is irrelevant to the model function. We trace this shortcoming and the differences across existing methods to the varying cooperative games formulated by the methods.1 We refer to such games as explanation games. Unfortunately, while existing methods focus on the axiomatic motivations of Shapley values, they offer little justification for the design choices made in their explanation game formulations. The goal of this work is to shed light on these design choices, and their implications on the resulting attributions.\nOur main technical result shows that various existing techniques can be unified under a common game formulation parameteric on a reference distribution. The Shapley values of this unified game formulation can be decomposed into the Shapley values of single-reference games that model a feature\u2019s absence by replacing its value with the corresponding value from a specific reference input.\nThis decomposition is beneficial in two ways. First, it allows us to efficiently compute confidence intervals and other supplementary information about attributions, a notable advancement over existing methods (which lack confidence intervals even though they approximate metrics of random variables using finite samples). Second, it offers conceptual clarity. It unlocks the interpretation that attributions explain the prediction at an input in contrast to other reference inputs. The attributions vary across existing methods as each method chooses a different reference distribution to contrast with. We tie the idea to classic research in cognitive psychology, and propose a conceptual formulate, approximate, explain (FAE) framework to create Shapley-value-based contrastive feature attributions. The goal of the framework is to produce attributions that are not only axiomatically justified, but also relevant to the underlying explanation question.\nWe illustrate our ideas via case studies on models trained on two UCI datasets (Bike Sharing and Adult Income) and a Lending Club dataset. We find that in these real-world situations, explanations generated using our FAE framework uncover important patterns that previous attribution methods cannot identify. In summary, we make the following key contributions:\n\u2013 We highlight several shortcomings of existing Shapley-value-based feature attribution methods (Sections 2), and analyze the root cause of these issues (Section 4.1). \u2013 We present a novel game formulation that unifies existing methods (Section 4.2), and helps characterize their uncertainty with confidence intervals (Section 4.3). \u2013 We offer a novel framework for creating and interpreting attributions (Section 4.4), and demonstrate its use through case studies (Section 5).\n1 We note that this shortcoming, and the multiplicity of game formulations has also been noted in parallel work [28,14]"
    },
    {
      "heading": "2 A motivating example",
      "text": "To probe existing Shapley-value-based model explanation methods, we evaluate them on two toy models for which it is easy to intuit correct attributions. We leverage a modified version of the example provided in [6]: a system that recommends whether a moving company should hire a mover applicant. The input vector to both models comprises two binary features \u201cis male\u201d and \u201cis good lifter\u201d (denoted by x = (xmale, xlift)), and output a recommendation score between 0 (\u201cno hire\u201d) and 1 (\u201chire\u201d). We define two models \u2014 fmale(x) ::= xmale (only hire males), and fboth(x) ::= xmale \u2227 xlift (only hire males who are good lifters). Table 1 specifies a probability distribution over the input space, along with the predictions from the two models.\nConsider the input x = (1, 1) (i.e. a male who is a good lifter), for which both models output a recommendation score of 1. Table 2 lists the attributions from several existing methods. Focusing on the relative attribution between xmale and xlift, we make the following surprising observations. First, even though xlift is irrelevant to fmale, the SHAP algorithm2 results in equal attribution to both features. This contradicts our intuition around the \u201cDummy\u201d axiom of Shapley values, which states that attribution to a player (feature) that never contributes to the payoff (prediction) must be zero.\nAdditionally, the SHAP attributions present a misleading picture from a fairness perspective: fmale relies solely on xmale, yet the attributions do not reflect this bias and instead claim that the model uses both features equally. Second, although fboth treats its features symmetrically, and x has identical values in both its features, many of the methods considered do not provide symmetrical attributions. This again is intuitively at odds with the \u201cSymmetry\u201d axiom of Shapley values, which states that players (features) that always contribute equally to the payoff (prediction) must receive equal\n2 As defined by Equation 9 in [19].\nattribution. These unintuitive behaviors surfaced by the above observations demand an in-depth study of the internal design choices of these methods. We carry out this study in Section 4."
    },
    {
      "heading": "3 PRELIMINARIES",
      "text": ""
    },
    {
      "heading": "3.1 Additive feature attributions",
      "text": "Additive feature attributions [19] are attributions that sum to the difference between the explained model output f(x) and a reference output value \u03c60. In practice, \u03c60 is typically an average model output or model output for a domain-specific \u201cbaseline\u201d input (e.g. an empty string for text sentiment classification).\nDefinition 1 (Additive feature attributions). Suppose f : X \u2192 R is a model mapping an M -dimensional feature space X to real-valued predictions. Additive feature attributions for f(x) at input x = (x1, . . . , xM ) \u2208 X comprise of a reference (or baseline) attribution \u03c60 and feature attributions \u03c6 = (\u03c61, \u03c62, . . . , \u03c6M ) corresponding to the M features such that f(x) = \u03c60 + \u2211M i=1 \u03c6i.\nThere currently exist a number of competing methodologies for computing these attributions (see [2]). Given the difficulty of empirically evaluating attributions, several methods offer an axiomatic justification, often through the Shapley values method."
    },
    {
      "heading": "3.2 Shapley values",
      "text": "The Shapley values method is a classic technique from game theory that fairly attributes the total payoff from a cooperative game to the game\u2019s players [24]. Recently, this method has found numerous applications in explaining ML models (e.g. [5,19,9]).\nFormally, a cooperative game is played by a set of playersM = {1, . . . ,M} termed the grand coalition. The game is characterized by a set function v : 2M \u2192 R such that v(S) is the payoff for any coalition of players S \u2286 M, and v(\u2205) = 0. Shapley values are built by examining the marginal contribution of a player to an existing coalition S, i.e., v(S \u222a {i}) \u2212 v(S). The Shapley value of a player i, denoted \u03c6i(v), is a certain weighted aggregation of its marginal contribution to all possible coalitions of players.\n\u03c6i(v) = 1\nM \u2211 S\u2286M\\{i} ( M \u2212 1 |S| )\u22121 (v(S \u222a {i})\u2212 v(S)) (1)\nThe Shapley value method is the unique method satisfying four desirable axioms: Dummy, Symmetry, Efficiency, and Linearity. We informally describe the axioms in Appendix A, and refer the reader to [30] for formal definitions and proofs.\nApproximating Shapley values Computing Shapley values involves evaluating the game payoff for every possible coalition of players. This makes the computation exponential in the number of players. For games with few players, it is possible to exactly compute the Shapley values, but for games with many players, the Shapley values can\nonly be approximated. Recently there has been much progress towards the efficient approximation of Shapley values. In this work we focus on a simple sampling approximation, presenting two more popular techniques in the Appendix B. We refer the reader to [20,4,1,13,3] for a fuller picture of recent advances in Shapley value approximation.\nA simple sampling approximation (used by [9], among other works) relies on the fact that the Shapley value can be expressed as the expected marginal contribution a player has when players are added to a coalition in a random order. Let \u03c0(M) be the ordered set of permutations ofM , andO be an ordering randomly sampled from \u03c0(M). Let prei(O) be the set of players that precede player i inO. The Shapley value of player i is the expected marginal contribution of the player under all possible orderings of players.\n\u03c6i(v) = E O\u223c\u03c0(M) [v(prei(O) \u222a {i})\u2212 v(prei(O))] (2)\nBy sampling a number of permutations and averaging the marginal contributions of each player, we can estimate this expected value for each player and approximate each player\u2019s Shapley value."
    },
    {
      "heading": "4 EXPLANATION GAMES",
      "text": "In order to explain a model prediction with the Shapley values method, it is necessary to formulate a cooperative game with players that correspond to the features and a payoff that corresponds to the prediction. In this section, we analyze the methods examined in Section 2, and show that their surprising attributions are an artifact of their game formulations. We then discuss a unified game formulation and its decomposition to single-reference games, enabling conceptual clarity about the meanings of existing methods\u2019 attributions.\nNotation LetDinp be the input distribution, which characterizes the process that generates model inputs. We denote the input of an explained prediction as x = (x1, . . . , xM ) and use r to denote another \u201creference\u201d input. We use boldface to indicate when a variable or function is vector-valued, and capital letters for random variable inputs (although S continues to represent the set of contributing players/features). Thus, xi is a scalar input, x is an input vector, and X is a random input vector. We use xS = {xi : i \u2208 S} to represent a sub-vector of features indexed by S. This notation is also extended to random input vectorsX . Lastly, we introduce the composite input z(x,r, S), which agrees with the input x on all features in S and with r on all features not in S. Note that z(x,r, \u2205) = r, and z(x,r,M) = x.\nz(x,r, S) = (z1, z2, ..., zM ), where zi = { xi i \u2208 S ri i /\u2208 S\n(3)"
    },
    {
      "heading": "4.1 Existing game formulations",
      "text": "The explanation game payoff function vx must be defined for every feature subset S such that vx(S) captures the contribution of xS to the model\u2019s prediction. This allows\nus to compute each feature\u2019s possible marginal contributions to the prediction and derive its Shapley value (see Section 3.2).\nBy the definition of additive feature attributions (Definition 1) and the Shapley values\u2019 Efficiency axiom, we must define vx(M) ::= f(x) \u2212 \u03c60 (i.e. the payoff of the full coalition must be the difference between the explained model prediction and a baseline prediction). Although this definition is fixed, it leaves us the challenge of coming up with the payoff when some features do not contribute (that is, when they are absent).\nWe find that all existing approaches handle this feature-absent payoff by randomly sampling absent features according to a particular reference distribution and then computing the expected value of the prediction. The resulting game formulations differ from one another only in the reference distribution they use. Additionally, we note that in practice small samples are used to approximate the expected value present in these payoff functions. This introduces a significant source of attribution uncertainty not clearly quantified by existing work.\nConditional distribution The game formulation of SHAP [19], TreeSHAP [18], and [1] simulates feature absence by sampling absent features from the conditional distribution based on the values of the present (or contributing) features:\nvcondx (S) = E R\u223cDinp [f(z(x,R, S)) |RS = xS ]\u2212 E R\u223cDinp [f(R)] (4)\nUnfortunately, this is not a proper simulation of feature absence as it does not break correlations between features [14]. This could lead to unintuitive attributions. For instance, in the fmale example from Section 2, it causes the the irrelevant feature xlift to receive a nonzero attribution. Specifically, since the event xmale = 1 is correlated3 with xlift = 1, once xlift = 1 is given, the expected prediction becomes 1. This causes the xlift feature to have a non-zero marginal contribution (relative to when both features are absent), and therefore a nonzero Shapley value. More generally, whenever a feature is correlated with a model\u2019s prediction on inputs drawn from Dinp, this game formulation results in non-zero attribution to the feature regardless of whether the feature directly impacts the prediction.\nInput distribution Another option for simulating feature absence, which is used by KernelSHAP, is to sample absent features from the corresponding marginal distribution in Dinp:\nvinpx (S) = E R\u223cDinp [f(z(x,R, S))]\u2212 E R\u223cDinp [f(R)] (5)\nSince this formulation breaks correlation with the contributing features, it ensures irrelevant features receive no attribution (e.g. no attribution to xlift when explaining fmale(1, 1) = 1). We formally describe this property via the Insentivity axiom in Section 4.2.\n3 In this context, correlation refers to general statistical dependence, not just a nonzero Pearson correlation coefficient.\nWe note that this formulation is still subject to artifacts of the input distribution, as evident from the asymmetrical attributions when explaining the prediction fboth(1, 1) = 1 (see Table 2). The features receive different attributions because they have different marginal distributions in Dinp, not because they impact the model differently.\nJoint-marginal distribution QII [6] simulates feature absence by sampling absent features one at a time from their own univariate marginal distributions. In addition to breaking correlation with the contributing features, this breaks correlation between absent features as well. Formally, the QII formulation uses a distribution we term the \u201cjoint-marginal\u201d distribution (DJ.M.), where:\nPr X \u223c DJ.M. [X = (x1, . . . , xM )] = M\u220f i=1 Pr X\u223cDinp [Xi = xi]\nThe joint-marginal formulation vJ.M.x is similar to v inp x , except that the reference distribution is DJ.M. instead of Dinp:\nvJ.M.x (S) = E R\u223cDJ.M. [f(z(x,R, S))]\u2212 E R\u223cDJ.M. [f(R)] (6)\nUnfortunately, like vinpx , this game formulation is also tied to the input distribution and under-attributes features that take on common values in the background data. This is evident from the attributions for the fboth model shown in Table 2.\nUniform distribution The last formulation we study from the prior art simulates feature absence by drawing values from a uniform distribution U over the entire input space, as in IME [26].4 Completely ignoring the input distribution, this payoff vunifx considers all possible feature values (edge-cases and common cases) with equal weighting.\nvunifx (S) = E R\u223cU [f(z(x,R, S))]\u2212 E R\u223cU [f(R)] (7)\nIn Table 2, we see that this formulation yields intuitively correct attributions for fmale and fboth. However, the uniform distribution can sample so heavily from irrelevant outlier regions of X that relevant patterns of model behavior become masked (we study the importance of relevant references both theoretically in Section 4.4 and empirically in Section 5)."
    },
    {
      "heading": "4.2 A unified formulation",
      "text": "We observe that the existing game formulations vinpx , vJ.M.x , and v unif x can be unified as a single game formulation vx,Dref that is parameterized by a reference distribution Dref .\nvx,Dref (S) = E R\u223cDref [f(z(x,R, S))]\u2212 E R\u223cDref [f(R)] (8)\n4 It is somewhat unclear whether IME proposes U or Dinp, as [26] assumes Dinp = U , while [27] calls for values to be sampled from X \u201cat random.\u201d\nFor instance, the formulation for KernelSHAP is recovered when Dref = Dinp, and QII is recovered when Dref = DJ.M.. In the rest of this section, we discuss several properties of this general formulation that help us better understand its attributions. Notably, the formulation vcondx cannot be expressed in this framework; we discuss the reason for this later in this section.\nA decomposition in terms of single-reference games We now introduce single-reference games, a conceptual building block that helps us interpret the Shapley values of the vx,Dref game. A single-reference game vx,r simulates feature absence by replacing the feature value with the value from a specific reference input r:\nvx,r(S) = f(z(x,r, S))\u2212 f(r) (9)\nThe attributions from a single-reference game explain the difference between the prediction for the input and the prediction for the reference (i.e. \u2211 i \u03c6i(vx,r) = vx,r(M) = f(x)\u2212f(r), and \u03c60 = f(r)). Computing attributions relative to a single reference point (also referred to as a \u201cbaseline\u201d) is common to several others methods [29,25,7,3]. However, while those works seek a neutral \u201cinformationless\u201d reference (e.g. an all-black image for image models), we find it beneficial to consider arbitrary references and interpret the resulting attributions relative to the reference. We develop this idea further in our FAE framework (see Section 4.4).\nWe now state Proposition 1, which shows how the Shapley values of vx,Dref can be expressed as the expected Shapley values of a (randomized) single-reference game vx,R , where R \u223c D. The proof (provided in Appendix C) follows from the Shapley values\u2019 Linearity axiom and the linearity of expectation.\nProposition 1. \u03c6(vx,Dref ) = ER\u223cDref [\u03c6(vx,R)] Proposition 1 brings conceptual clarity and practical improvements (confidence intervals and supplementary metrics) to existing methods. It shows that the attributions from existing games (vinpx , vJ.M.x , and v unif x ) are in fact differently weighted aggregations of attributions from a space of single-reference games. For instance, vunifx weighs attributions relative to all reference points equally, while vinpx weighs them using the input distribution Dinp.\nInsensitivity axiom We show that attributions from the game vx,Dref satisfy the Insensitivity axiom from [29], which states that a feature that is mathematically irrelevant to the model must receive zero attribution. Formally, a feature i is irrelevant to a model f if for any input, changing the feature does not change the model output. That is, \u2200x,r \u2208 X : xM\\{i} = rM\\{i} =\u21d2 f(x) = f(r). Proposition 2. If a feature i is irrelevant to a model f then \u03c6i(vx,Dref ) = 0 for all distributions Dref . Notably, the vcondx formulation does not obey the Insensitivity axiom (a counter-example being the fmale attributions from Section 2). Accordingly, our general formulation (Equation 7) cannot express this formulation. In the rest of the paper, we focus on game formulations that satisfy the Insensitivity axiom. We refer to [28] for a comprehensive analysis of the axiomatic guarantees of various game formulations."
    },
    {
      "heading": "4.3 Confidence intervals on attributions",
      "text": "Existing game formulations involve computing an expected value (over a reference distribution) in every invocation of the payoff function. In practice, this expectation is approximated via sampling, which introduces uncertainty. The original formulations of these games do not lend themselves well to quantify such uncertainty. We show that by leveraging our unified game formulation, one can efficiently quantify the uncertainty using confidence intervals (CIs).\nOur decomposition in Proposition 1 shows that the attributions themselves can be expressed as an expectation over (deterministic) Shapley value attributions from a distribution of single-reference games. Consequently, we can quantify attribution uncertainty by estimating the standard error of the mean (SEM) across a sample of Shapley values from single-reference games. In terms of the sample standard deviation (SSD), 95% CIs on the mean attribution (\u03c6\u0304) from a sample of size N are given by\n\u03c6\u0304 \u00b1 1.96\u00d7 SSD({\u03c6(vx,ri)} N i=1)\u221a\nN (10)\nWe note that while one could use bootstrap [8] to obtain CIs, the SEM approach is more efficient as it requires no additional Shapley value computations.\nA unified CI As discussed in Section 3.2, often the large number of features (players) in an explanation game necessitates the approximation of Shapley values. The approximation may involve random sampling, which incurs its own uncertainty. In what follows, we derive a general SEM-based CI that quantifies the combined uncertainty from sampling-based approximations of Shapley values and the sampling of references.\nLet us consider a generic estimator \u03c6\u0302(G)i (vx,r) parameterized by some random sampleG. An example of such an approach is the feature ordering based approximation of Equation 2, for which G = (Oj)kj=1 represents a random sample of feature orderings, and:\n\u03c6\u0302 (G) i (vx,r) =\n1\nk k\u2211 j=1 v(prei(Oj) \u222a {i})\u2212 v(prei(Oj))\nAs long as the generic \u03c6\u0302(G)i is an unbiased estimator (like the feature ordering estimator of Equation 2), andG andR \u223c Dref are sampled independently from one another, we can derive a unified CI using the SEM. By the estimator\u2019s unbiasedness and Proposition 1, the Shapley value attributions can be expressed as:\n\u03c6i(vx,Dref ) = E R E G\n[ \u03c6\u0302 (G) i (vx,R) ] (11)\nSince G is independent of R, this expectation can be Monte Carlo estimated using the sample mean of the sequence ( \u03c6\u0302 (gj) i (vx,rj ) )N j=1 (where (gj , rj) N j=1 is a joint sample of (G,R)). As the attribution recovered by this estimation is simply the mean of a sample from a random variable, its uncertainty can be quantified by estimating the SEM. In\nterms of the sample standard deviation, 95% CIs on the mean attribution (\u03c6\u0304) from a sample of size N are given by:\n\u03c6\u0304 \u00b1 1.96\u00d7 SSD\n(( \u03c6\u0302 (gj) i (vx,rj ) )N j=1 ) \u221a N\n(12)"
    },
    {
      "heading": "4.4 Formulate, Approximate, Explain",
      "text": "So far we studied the explanation game formulations used by existing methods, and noted how the formulations impact the resulting Shapley value attributions. We show that the attributions explain a prediction in contrast to a distribution of references; see Proposition 1. Existing methods differ in the attribution they produce because each of them picks a different reference distribution to contrast with. We also proposed a mechanism to quantify the approximation uncertainty incurred in computing attributions.\nWe now put these ideas together in a single conceptual framework formulate, approximate, explain (FAE). Our key insight is that rather than viewing the reference distribution as an implementation detail of the explanation method, it must by made a first-class argument to the framework. That is, the references must be consciously chosen by the explainee to obtain a specific contrastive explanation.\nOur emphasis on treating attributions as contrastive explanations stems from cognitive psychology. Several works in cognitive psychology argue that humans frame explanations of surprising outcomes by contrasting them with to one or more normal outcomes [15,21,22,10,11,17,12]. In our setting, the normal outcomes are the reference predictions that the input prediction is contrasted with. The attributions essentially explain what drives the prediction at hand away from the reference predictions. The choice of references may depend on the context of the question, and may vary across explainers and explainees [15]. Moreover, it is important for the references to be relevant to the input at hand [11]. For instance, if we are explaining why an auto-grading software assigns a B+ to a student\u2019s submission, it would be proper to contrast with the submissions that were graded as A- (next higher grade after B+), instead of contrasting with the entire pool of submissions.\nFormulate The mandate of the Formulate step is to generate a contrastive question that specifies one or more relevant references. The question pins down the distributionDref of the chosen references. For instance, in the grading example above, the references would be all submissions obtaining an A- grade.\nApproximate Once a meaningful contrastive question and its corresponding reference distribution Dref has been formulated, we consider the distribution of single-reference games whose references are drawn from Dref , and approximate the Shapley values of these games. Formally, we approximate the distribution of the random-valued attribution vector \u03a6x,R = \u03c6(vx,R), whereR \u223c Dref . This involves two steps: (1) sampling a sequence of references (ri) N i=1 from R \u223c Dref , and (2) approximating the Shapley value of the single-reference games relative each to reference in (ri) N i=1. This yields a\nsequence of approximated Shapley values. It is important to account for the uncertainty resulting from sampling in steps (1) and (2), and quantify it in the Explain step.\nExplain In the final step, we must summarize the sampled Shapley value vectors (drawn from \u03a6x,R) obtained from the Approximate step. One simple summarization would be the presentation of a few representative samples, in the style of the SP-LIME algorithm [23]. Another simple summarization is the sample mean, which approximates E [\u03a6x,R ], and is equivalent to the attributions from the unified explanation game vx,Dref . This is the summarization used by existing Shapley-value-based explanation methods. When using the sample mean, the framework of Section 4.3 can be used to quantify the uncertainty from sampling. In addition, one must be careful that the mean does not hide important information. For instance, a feature\u2019s attributions may have opposite signs relative to different references. Averaging these attributions will cause them to cancel each other out, yielding a small mean that incorrectly suggests that the feature is unimportant. We discuss a concrete example of this in Section 5.1. At the very least, we recommend confirming through visualization and summary statistics like variance and interquartile range that the mean is a good summarization, before relying upon it. We discuss a clustering based summarization method in Section 5 while leaving further research on faithful summarization methods to future work."
    },
    {
      "heading": "5 CASE STUDIES",
      "text": "In this section we apply the FAE framework to LightGBM [16] Gradient Boosted Decision Trees (GBDT) models trained on real data: the UCI Bike Sharing and Adult Income datasets, and a Lending Club dataset.5 For parsimony, we analyze models that use only five features; complete model details are provided in Appendix D. For the Bike Sharing model, we explain a randomly selected prediction of 210 rentals for a certain hour. For the Adult Income model, we explain a counter-intuitively low prediction for an individual with high education-num. For the Lending Club model, we explain a counter-intuitive rejection (assuming a threshold that accepts 15% of loan applications) for a high-income borrower. In the rest of this section, we present a selection of the results across all three models, while the full set of results are provided in Appendix E."
    },
    {
      "heading": "5.1 Shortcomings of existing methods",
      "text": "Recall from Section 4.2 that the attributions from existing methods amount to computing the mean attribution for a distribution of single-reference games vx,R , where the reference R is sampled from a certain distribution. The choice of distribution varies across the methods, which in turns leads to very different attribution. This is illustrated in Table 3 for the Bike sharing model.\n5 In Bike Sharing we model hourly bike rentals from temporal and weather features, in Adult Income we model whether an adult earns more than $50,000 annually, and in Lending Club we model whether a borrower will default on a loan.\nMisleading means In Section 4.4, we discussed that the mean attribution can potentially be a misleading summarization. Here, we illustrate this using the attributions from the KernelSHAP game vinpx for the Bike Sharing example; see Table 3. The mean attribution to the feature hr is tiny, suggesting that the feature has little impact. However, the distribution of single-reference game attributions (Figure 1) reveals a large spread centered close to zero. In fact, we find that by absolute value hr receives the largest attribution in over 60% of the single-reference games. Consequently, only examining the mean of the distribution may be misleading.\nUnquantified uncertainty Lack of uncertainty quantification in existing techniques can result in misleading attributions. For instance, taking the mean attribution of 100 randomly-sampled Bike Sharing single-reference games6 gives hr an attribution of - 10 and workingday an attribution of 8. Without any sense of uncertainty, we do not know how accurate these (estimated) attributions are. The 95% confidence intervals\n6 The official implementation of KernelSHAP [19] raises a warning if over 100 references are used.\n(estimated using the method described in Section 5.2) show that they are uncertain indeed: the CIs span both positive and negative values.\nIrrelevant references In Section 4.4, we noted the importance of relevant references (or norms), and how the IME game vunifx based on the uniform distribution U can focus on irrelevant references. We illustrate this on the Adult Income example; see the third row of Table 4. We find that almost all attribution from the vunifx game falls on the capitalgain feature. This is surprising as capitalgain is zero for the example being explained, and for over 90% of examples in the Adult Income dataset. The attributions are an artifact of uniformly sampling reference feature values, which causes nearly all references to have non-zero capital gain (as the probability of sampling an exactly zero capital gain is infinitesimal)."
    },
    {
      "heading": "5.2 Applying the FAE framework",
      "text": "We now consider how the FAE framework enables more faithful explanations for the three models we study.\nFormulating contrastive questions A key benefit of FAE is that it enables explaining predictions relative to a selected group of references. For instance, in the Lending\nClub model, rather than asking \u201cWhy did our rejected example receive a score of 0.28?\u201d we ask the contrastive question \u201cWhy did our rejected example receive a score of 0.28 relative to the examples that were accepted?\u201d This is a more apt question, as it explicitly discards irrelevant comparisons to other rejected applications. In terms of game formulation, the contrastive approach amounts to considering single-reference games where the reference is drawn from the distribution of accepted applications (denoted by Daccept ) rather than all applications. The attributions for each of these questions (shown in Table 5) turn out to be quite different. For instance, although number of recently-opened accounts (acc) is still the highest-attributed feature, we find that credit score (fico), income (inc), and debt-to-income ratio (dti) receive significantly higher attribution in the contrastive formulation. Without formulating the contrastive question, we would be misled into believing that these features are unimportant for the rejection.\nQuantifying uncertainty When summarizing the attribution distribution with the mean, confidence intervals can be computed using the standard error of the mean (see Section 4.3). Returning to our Bike Sharing example, with 100 samples, the 95% confidence intervals for hr and workingday are -36 to 15, and -1 to 12, respectively. The large CIs caution us that 100 samples are perhaps too few. When using the full test set, the 95% CIs drop to 0.0 to 5.1 for hr, and 0.6 to 2.0 for workingday.\nSummarizing attribution distributions To obtain a more faithful summarization of the single-reference game attributions, we explore a clustering based approach. We compute attributions for single reference games relative to points sampled from the the input distribution (Dinp), and then apply k-means clustering to these attributions. The resulting clusters effectively group references that yield similar (contrastive) attributions for the prediction at the explained point. Consequently,the attribution distribution within each cluster has a small spread, and can be summarized via the mean.\nWe applied this approach to all three models and obtained promising results, wherein, clustering helps mine distinct attribution patterns. Table 4 (bottom) shows the results for the Adult Income model; results for other models are provided in Appendix E. Notice that clustering identifies a large group of irrelevant references (cluster 2) which are similar to the explained point, demonstrating low attributions and predictions. Cluster 3 discovers the same pattern that the vunifx formulation did: high capitalgain causes extremely high scores. Since over 90% of points in the dataset have zero capitalgain, this pattern is \u201cwashed out in the average\u201d relative to the entire data distribution Dinp (as in KernelSHAP); see the first row of Table 4. On the other hand, the IME formulation identifies nothing but this pattern. Our clustering also helps identify other patterns. Clusters 1 and 5 show that when compared to references that obtain a high-but-notextreme score, marital-status, relationship, and education-num are the primary factors accounting for the lower prediction score for the example at hand."
    },
    {
      "heading": "6 CONCLUSION",
      "text": "We perform an in-depth study of various Shapley-value-based model explanation methods. We find cases where existing methods yield counter-intuitive attributions, and we\ntrace these misleading attributions to the cooperative games formulated by these methods. We propose a generalizing formulation that unifies attribution methods, offers clarity for interpreting each method\u2019s attributions, and admits straightforward confidence intervals for attributions.\nWe propose a conceptual framework for model explanations, called formulate, approximate, explain (FAE), which is built on principles from cognitive psychology. We advise practitioners to formulate contrastive explanation questions that specify the references relative to which a prediction should be explained, for example \u201cWhy did this rejected loan application receive a score of 0.28 in contrast to the applications that were accepted?\u201d By approximating the Shapley values of games formulated relative to the chosen references, and explaining the distribution of approximated Shapley values, we provide a more relevant answer to the explanation question at hand.\nFinally, we conclude that axiomatic guarantees do not inherently guarantee relevant explanations, and that game formulations must be constructed carefully. In summarizing attribution distributions, we caution practitioners to avoid coarse-grained summaries, and to quantify any uncertainty resulting from any approximations used."
    },
    {
      "heading": "A Shapley Value Axioms",
      "text": "We briefly summarize the four Shapley value axioms.\n\u2013 The Dummy axiom requires that if player i has no possible contribution (i.e. v(S \u222a {i}) = v(S) for all S \u2286M), then that player receives zero attribution. \u2013 The Symmetry axiom requires that two players that always have the same contribution receive equal attribution, Formally, if v(S \u222a {i}) = v(S \u222a {j}) for all S not containing i or j then \u03c6i(v) = \u03c6j(v). \u2013 The Efficiency axiom requires that the attributions to all players sum to the total payoff of all players. Formally, \u2211 i \u03c6i(v) = v(M)). \u2013 The Linearity axiom states that for any payoff function v that is a linear combination of two other payoff functions u and w (i.e. v(S) = \u03b1u(S) + \u03b2w(S)), the Shapley values of v equal the corresponding linear combination of the Shapley values of u and w (i.e. \u03c6i(v) = \u03b1\u03c6i(u) + \u03b2\u03c6i(w))."
    },
    {
      "heading": "B Additional Shapley value approximations",
      "text": "Marginal contribution sampling We can express the Shapley value of a player as the expected value of the weighted marginal contribution to a random coalition S sampled uniformly from all possible coalitions excluding that player, rather than an exhaustive weighted sum. A sampling estimator of this expectation is by nature unbiased, so this can be used as an alternative to the permutation estimator in approximating attributions with confidence intervals.\n\u03c6i(v) = E S\n[ 2M\u22121\nM ( M \u2212 1 |S| )\u22121 (v(S \u222a {i})\u2212 v(S)) ] (13)\nEquation 13 can be approximated with a Monte Carlo estimate, i.e. by sampling from the random S and averaging the quantity within the expectation.\nWeighted least squares The Shapley values are the solution to a certain weighted least squares optimization problem which was popularized through its use in the KernelSHAP algorithm. For a full explanation, see https://arxiv.org/abs/1903. 10464.\n\u03c6 = arg min \u03c6 \u2211 S\u2286M M \u2212 1( M |S| ) |S|(M \u2212 |S|)\n( v(S)\u2212\nM\u2211 i=1 \u03c6i\n)2 (14)\nThe fraction in the left of Equation 14 is often referred to as the Shapley kernel . In practice, an approximate objective function is minimized. The approximate objective is defined as a summation over squared error on a sample of coalitions rather than over squared error on all possible coalitions. Additionally, the \u201cKernelSHAP trick\u201d may be employed, wherein sampling is performed according to the Shapley kernel (rather than\nuniformly), and the least-squares optimization is solved with uniform weights (rather than Shapley kernel weights) to account for the adjusted sampling.\nTo the best of our knowledge, there exists no proof that the solution to a subsampled objective function of the form in Equation 14 is an estimator (unbiased or otherwise) of the Shapley values. In practice, it does appear that subsampling down to even a small fraction of the total number of possible coalitions (weighted by the Shapley kernel or uniformly) does a good job of estimating the Shapley values for explanation games. Furthermore, approximation errors in such experiments do not yield signs of bias. However, we do note that using the weighted least squares approximation with our confidence interval equation does inherently imply an unproved assumption that it is an unbiased estimator."
    },
    {
      "heading": "C Proofs",
      "text": "In what follows, we prove the lemmas from the main paper. The proofs refer to equations and definition from the main paper.\nC.1 Proof of Proposition 1\nFrom the definitions of vx,Dref (Equation 8) and vx,r (Equation 9), it follows that vx,Dref (S) = ER\u223cDref [vx,R(S)]. Thus, the game vx,Dref is a linear combination of games {vx,r | r \u2208 X} with weights defined by the distribution Dref . From the Linearity axiom of Shapley values, it follows that the Shapley values of the game vx,Dref must be a corresponding linear combination of the Shapley values of the games {vx,r | r \u2208 X} (with weights defined by the distribution Dref ). Thus, \u03c6(vx,Dref ) = ER\u223cDref [\u03c6(vx,R)]. ut\nC.2 Proof of Proposition 2\nFrom Proposition 1, we have \u03c6i(vx,Dref ) = ER\u223cDref [\u03c6i(vx,R)]. Thus, to prove this lemma, it suffices to show that for any irrelevant feature i, the Shapley value from the game vx,r is zero for all references r \u2208 X . That is,\n\u2200r \u2208 X \u03c6i(vx,r) = 0 (15)\nFrom the definition of Shapley values (Equation 1), we have:\n\u03c6i(vx,r) = 1\nM \u2211 S\u2286M\\{i} ( M \u2212 1 |S| )\u22121 (vx,r(S \u222a {i})\u2212 vx,r(S)) (16)\nThus, to prove Equation 15 it suffices to show the marginal contribution (vx,r(S\u222a{i})\u2212 vx,r(S)) of an irrelevant feature i to any subset of features S \u2286M\\{i} is always zero. From the definition of the game vx,r , we have:\nvx,r(S \u222a {i})\u2212 vx,r(S) = f(z(x,r, S \u222a {i}))\u2212 f(z(x,r, S)) (17)\nFrom the definition of composite inputs z (Equation 3), it follows that the inputs z(x,r, S\u222a {i}) and z(x,r, S) agree on all features except i. Thus, if feature i is irrelevant, f(z(x,r, S\u222a {i})) = f(z(x,r, S)), and consequently by Equation 16, vx,r(S \u222a {i})\u2212 vx,r(S) = 0 for all subsets S \u2286 M \\ {i}. Combining this with the definition of Shapley values (Equation 1) proves Equation 15. ut"
    },
    {
      "heading": "D Reproducibility",
      "text": "For brevity, we omitted from the main paper many of the mundane choices in the design of our toy examples and case studies. To further transparency and reproducibility, we include them here.\nD.1 Fitting models\nFor both case studies, we used the LightGBM package configured with default parameters to fit a Gradient Boosted Decision Trees (GBDT) model. For the Bike Sharing dataset, we fit on all examples from 2011 while holding out the 2012 examples for testing. We omitted the atemp feature, as it is highly correlated to temp (r = 0.98), and the instant feature because the tree-based GBDT model cannot capture its time-series trend. For parsimony, we refitted the model to the top five most important features by cumulative gain (hr, temp, workingday, hum, and season). This lowered test-set r2 from 0.64 to 0.63. For the Adult Income dataset, we used the pre-defined train/test split. Again, we refitted the model to the top five features by cumulative gain feature importance (relationship, capitalgain, education-num, marital-status, and age). This increased test-set misclassification error from 14.73% to 10.97%.\nD.2 Selection of points to explain\nFor the Bike Share case study, we sampled ten points at random from the test set. We selected one whose prediction was close to the middle of the range observed over the entire test set (predictions ranged approximately from 0 to 600). Specifically, we selected instant 11729 (2012-05-08, 9pm). We examined other points from the same sample of ten to suggest a random but meaningful comparative question. We found another point with comparable workingday, hum, and season: instant 11362. This point caught our eye because it differed only in hr (2pm rather than 9pm), and temp (0.36 rather than 0.64) but had a much lower prediction.\nFor the Adult Income case study, we wanted to explain why a point was scored as likely to have low income, a task roughly analogous to that of explaining why an application for credit is rejected by a creditworthiness model in a lending setting. We sampled points at random with scores between 0.01 and 0.1, and chose the 9880th point in the test set due to its strikingly high education-num (most of the low-scoring points sampled had lower education-num).\nFor the Lending Club data, we chose an open-source subset of the dataset that has been pre-cleaned to a predictive task on 3-year loans. For the five-feature model, we selected the top five features by cumulative gain feature importance from a model fit to the full set of features.\nD.3 K-means clustering\nWe choose k = 5 arbitrarily, having observed a general tradeoff of conciseness for precision as k increases. In the extremes, k = 1 maintains the overall attribution distribution, while k = N examines each single-reference game separately."
    },
    {
      "heading": "E Case Study Supplemental Material",
      "text": ""
    }
  ],
  "title": "The Explanation Game: Explaining Machine Learning Models Using Shapley Values",
  "year": 2020
}
