{"abstractText": "Explainable Artificial Intelligence (or xAI) has become an important research topic in the fields of Machine Learning and Deep Learning. In this paper, we propose a Genetic Programming (GP) based approach, named Genetic Programming Explainer (GPX), to the problem of explaining decisions computed by AI systems. The method generates a noise set located in the neighborhood of the point of interest, whose prediction should be explained, and fits a local explanation model for the analyzed sample. The tree structure generated by GPX provides a comprehensible analytical, possibly nonlinear, symbolic expression which reflects the local behavior of the complex model. We considered three machine learning techniques that can be recognized as complex black-box models: Random Forest, Deep Neural Network and Support Vector Machine in twenty data sets for regression and classifications problems. Our results indicate that the GPX is able to produce more accurate understanding of complex models than the state of the art. The results validate the proposed approach as a novel way to deploy GP to improve interpretability.", "authors": [{"affiliations": [], "name": "Leonardo Augusto Ferreira"}, {"affiliations": [], "name": "Frederico Gadelha Guimar\u00e3es"}, {"affiliations": [], "name": "Rodrigo Silva"}], "id": "SP:5e1cebf8719c9439a8d86bb65abae5454963a86a", "references": [{"authors": ["Z.-H. Zhou", "Y. Jiang", "Y.-B. Yang", "S.-F. Chen"], "title": "Lung cancer cell identification based on artificial neural network ensembles", "venue": "Artificial Intelligence in Medicine, vol. 24, no. 1, pp. 25 \u2013 36, 2002. [Online]. Available: http://www.sciencedirect.com/science/article/ pii/S093336570100094X", "year": 2002}, {"authors": ["S. Chakraborty", "R. Tomsett", "R. Raghavendra", "D. Harborne", "M. Alzantot", "F. Cerutti", "M. Srivastava", "A. Preece", "S. Julier", "R.M. Rao", "T.D. Kelley", "D. Braines", "M. Sensoy", "C.J. Willis", "P. Gurram"], "title": "Interpretability of deep learning models: A survey of results", "venue": "pp. 1\u20136, Aug 2017.", "year": 2017}, {"authors": ["J. Zhu", "A. Liapis", "S. Risi", "R. Bidarra", "G.M. Youngblood"], "title": "Explainable ai for designers: A human-centered perspective on mixed-initiative co-creation", "venue": "2018 IEEE Conference on Computational Intelligence and Games (CIG), 2018, pp. 1\u20138.", "year": 2018}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier", "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201916. New York, NY, USA: Association for Computing Machinery, 2016, p. 1135\u20131144. [Online]. Available: https://doi.org/10.1145/2939672.2939778", "year": 2016}, {"authors": ["E. Tjoa", "C. Guan"], "title": "A survey on explainable artificial intelligence (xai): Towards medical xai", "venue": "10 2019.", "year": 2019}, {"authors": ["P. Hall", "N. Gill"], "title": "An Introduction to Machine Learning Interpretability: An Applied Perspective on Fairness, Accountability, Transparency, and Explainable AI", "venue": "O\u2019Reilly Media,", "year": 2018}, {"authors": ["C. Molnar"], "title": "Interpretable Machine Learning. A Guide for Making Black Box Models Explainable, 2019, https://christophm.github.io/ interpretable-ml-book", "year": 2019}, {"authors": ["C. Tankard"], "title": "What the gdpr means for businesses", "venue": "Network Security, vol. 2016, no. 6, pp. 5 \u2013 8, 2016. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S1353485816300563", "year": 2016}, {"authors": ["J.P. Albrecht"], "title": "How the gdpr will change the world", "venue": "European Data Protection Law Review, vol. 2, no. 3, 2016. [Online]. Available: https://doi.org/10.21552/EDPL/2016/3/4", "year": 2016}, {"authors": ["B.P. Evans", "B. Xue", "M. Zhang"], "title": "What\u2019s inside the blackbox? a genetic programming method for interpreting complex machine learning models", "venue": "p. 1012\u20131020, 2019. [Online]. Available: https://doi.org/10.1145/3321707.3321726", "year": 2019}, {"authors": ["B. Goodman", "S. Flaxman"], "title": "European union regulations on algorithmic decision-making and a \u201dright to explanation", "venue": "AI Magazine, vol. 38, pp. 50\u201357, 2017.", "year": 2017}, {"authors": ["Z. Che", "S. Purushotham", "R.G. Khemani", "Y. Liu"], "title": "Interpretable deep models for icu outcome prediction", "venue": "AMIA ... Annual Symposium proceedings. AMIA Symposium, vol. 2016, pp. 371\u2013380, 2016.", "year": 2016}, {"authors": ["S.M. Lundberg", "S.-I. Lee"], "title": "A unified approach to interpreting model predictions", "venue": "pp. 4765\u2013 4774, 2017. [Online]. Available: http://papers.nips.cc/paper/ 7062-a-unified-approach-to-interpreting-model-predictions.pdf", "year": 2017}, {"authors": ["A.B. Arrieta]", "N. D\u0131\u0301az-Rodr\u0131\u0301guez", "J.D. Ser]", "A. Bennetot", "S. Tabik", "A. Barbado", "S. Garcia", "S. Gil-Lopez", "D. Molina", "R. Benjamins", "R. Chatila", "F. Herrera"], "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai", "venue": "Information Fusion, vol. 58, pp. 82 \u2013 115, 2020. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S1566253519308103", "year": 2020}, {"authors": ["F. Livingston"], "title": "Implementation of breiman\u2019s random forest machine learning algorithm", "venue": "ECE591Q Machine Learning Journal Paper, pp. 1\u201313, 2005.", "year": 2005}, {"authors": ["S. Haykin"], "title": "Neural Networks: A Comprehensive Foundation, 2nd ed", "year": 1998}, {"authors": ["J.R. Koza"], "title": "Genetic Programming: On the Programming of Computers by Means of Natural Selection", "year": 1992}, {"authors": ["A. Gaspar-Cunha", "R. Takahashi", "C. Antunes"], "title": "Manual de computa\u00e7\u00e3o evolutiva e metaheur\u0131\u0301stica, ser. Ensino", "venue": "Imprensa da Universidade de Coimbra / Coimbra University Press,", "year": 2012}, {"authors": ["J.W. Smith", "J. Everhart", "W. Dickson", "W. Knowler", "R. Johannes"], "title": "Using the adap learning algorithm to forecast the onset of diabetes mellitus", "venue": "Proceedings. Symposium on Computer Applications in Medical Care, p. 261\u2014265, November 1988. [Online]. Available: https://europepmc.org/articles/PMC2245318", "year": 1988}, {"authors": ["M. Buscema", "S. Terzi", "W. Tastle"], "title": "A new meta-classifier", "venue": "2010 Annual Meeting of the North American Fuzzy Information Processing Society, 2010, pp. 1\u20137.", "year": 2010}, {"authors": ["S.B. Thrun", "J. Bala", "E. Bloedorn", "I. Bratko", "B. Cestnik", "J. Cheng", "K.D. Jong", "S. Dzeroski", "S.E. Fahlman", "D. Fisher", "R. Hamann", "K. Kaufman", "S. Keller", "I. Kononenko", "J. Kreuziger", "R. Michalski", "T. Mitchell", "P. Pachowicz", "Y. Reich", "H. Vafaie", "W.V.D. Welde", "W. Wenzel", "J. Wnek", "J. Zhang"], "title": "The monk\u2019s problems a performance comparison of different learning algorithms", "venue": "Tech. Rep., 1991.", "year": 1991}, {"authors": ["J.-L. Voz", "M. Verleysen", "P. Thissen", "J.-D. Legat"], "title": "A practical view of suboptimal bayesian classification with radial gaussian kernels", "venue": "Proceedings of the International Workshop on Artificial Neural Networks: From Natural to Artificial Neural Computation, ser. IWANN \u201996. Berlin, Heidelberg: Springer-Verlag, 1995, p. 404\u2013411.", "year": 1995}, {"authors": ["I.-C. Yeh", "K.-J. Yang", "T.-M. Ting"], "title": "Knowledge discovery on rfm model using bernoulli sequence", "venue": "Expert Systems with Applications, vol. 36, no. 3, Part 2, pp. 5866 \u2013 5871, 2009. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0957417408004508", "year": 2009}, {"authors": ["K. Zhang", "W. Fan"], "title": "Forecasting skewed biased stochastic ozone days: Analyses, solutions and beyond", "venue": "Knowl. Inf. Syst., vol. 14, no. 3, p. 299\u2013326, Mar. 2008. [Online]. Available: https://doi.org/10.1007/s10115-007-0095-1", "year": 2008}, {"authors": ["D. Dua", "C. Graff"], "title": "UCI machine learning repository", "venue": "2017. [Online]. Available: http://archive.ics.uci.edu/ml", "year": 2017}, {"authors": ["B.V. Ramana", "M.S.P. Babu", "N.B. Venkateswarlu"], "title": "A critical comparative study of liver patients from usa and india: An exploratory analysis", "venue": "2012.", "year": 2012}, {"authors": ["L. Buitinck", "G. Louppe", "M. Blondel", "F. Pedregosa", "A. Mueller", "O. Grisel", "V. Niculae", "P. Prettenhofer", "A. Gramfort", "J. Grobler", "R. Layton", "J. VanderPlas", "A. Joly", "B. Holt", "G. Varoquaux"], "title": "API design for machine learning software: experiences from the scikit-learn project", "venue": "ECML PKDD Workshop: Languages for Data Mining and Machine Learning, 2013, pp. 108\u2013122.", "year": 2013}, {"authors": ["P. Cortez", "A. Cerdeira", "F. Almeida", "T. Matos", "J. Reis"], "title": "Modeling wine preferences by data mining from physicochemical properties.", "venue": "Decis. Support Syst.,", "year": 2009}, {"authors": ["R. Ballester-Ripoll", "E.G. Paredes", "R. Pajarola"], "title": "Sobol tensor trains for global sensitivity analysis", "venue": "Reliability Engineering & System Safety, vol. 183, pp. 311\u2013322, 2019.", "year": 2019}, {"authors": ["L. Breiman"], "title": "Random forests", "venue": "Machine learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "year": 2001}, {"authors": ["I.G. Goodfellow", "Y. Bengio", "A.C. Courville"], "title": "Deep learning", "venue": "Nature, vol. 521, pp. 436\u2013444, 2015.", "year": 2015}, {"authors": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"], "title": "A training algorithm for optimal margin classifiers", "venue": "Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pp. 144\u2013152."}, {"authors": ["H. Drucker", "C.J. Burges", "L. Kaufman", "A.J. Smola", "V. Vapnik"], "title": "Support vector regression machines", "venue": "Advances in neural information processing systems, 1997, pp. 155\u2013161.", "year": 1997}, {"authors": ["A.N. Tikhonov"], "title": "Solution of incorrectly formulated problems and the regularization method", "venue": "Soviet Math. Dokl., vol. 4, pp. 1035\u20131038, 1963.", "year": 1963}], "sections": [{"text": "Index Terms\u2014Interpretability, Machine Learning, Genetic Programming, Explainability\nI. INTRODUCTION\nAdvances in Machine Learning (ML) and Deep Learning (DL) have had a profound impact in science and technology. These techniques have had many recent successes, achieving unprecedented performance in tasks such as image classification, machine translation and speech recognition, to cite a few. The remarkable performance of Artificial Intelligence (AI) methods and the growing investment on AI worldwide will lead to an ever-increasing utilization of AI systems, having a significant impact on society and everyday life decisions. However, depending on the model used, understanding why it makes a certain prediction can be difficult. This is particularly the case with the high performing DL models and ML models in general. The more complex the model, the more opaque its decisions are to human understanding. Black-box ML models are increasingly used in critical applications, leading to an\nThis work has been supported by the Brazilian agencies (i) National Council for Scientific and Technological Development (CNPq); (ii) Coordination for the Improvement of Higher Education (CAPES) and (iii) Foundation for Research of the State of Minas Gerais (FAPEMIG, in Portuguese).\nMINDS Laboratory \u2013 https://minds.eng.ufmg.br/\nurgent need for understanding and justifying their decisions. This difficulty is an important impediment for the adoption of ML, particularly DL, in domains such as healthcare [1], criminal justice and finance [2]. The term Explainable AI (or XAI) has been adopted by the community to refer to techniques that help the understanding of decisions or results of AI artifacts by a given audience, which can be domain experts, regulatory agencies, managers, decision-makers, policy-makers or users affected by these decisions.\nThe interpretability problem can be understood as the technical challenge of explaining AI decisions, especially when the underlying AI technology is perceived as a black-box model. Humans tend to be less willing to accept decisions that are not directly interpretable and trustworthy [3]. Therefore, users should be able to understand the models outputs in order to trust them. There are two types of trusting, as described in [4]: one about the model and another about the prediction. Though similar, they are not the same thing. The first one is related to whether someone will choose a model or not, whereas the second relates to whether someone will make a decision relying on that prediction.\nInterpretability, as pointed out in [5], is associated with a human perception i.e. the ability to classify something or someone according to their main characteristics. This idea applied to ML models would be to highlight the main features that contributed to a prediction. Other works, such as [6] and [7], define interpretability as \u201cthe ability to explain or to present in understandable terms to a human\u201d. In other words, a model can be defined as explainable whether its decisions are easier for a human to understand.\nAnother issue involving interpretability goes beyond trusting some model or prediction. The European Union has recently deployed the General Data Protection Regulation (GDPR) as pointed out in [8] and [9]. GDPR directly deals with subjects related to European citizens\u2019 data, for example: it prohibits judgments based solely on automated decisions [10]. European Union\u2019s new GDPR has a major impact in deploying machine learning algorithms and AI-based systems. It restricts automated individual decision-making which \u201csignificantly affects\u201d users. The law also creates a \u201cright to explanation,\u201d\nar X\niv :2\n00 5.\n09 51\n2v 1\n[ cs\n.L G\n] 1\n8 M\nay 2\nwhereby someone has a right to be given an explanation for an output of the algorithm [11].\nThere are many important results of interpretability and explainability in the literature. For instance, interpretable mimic learning [12] is an approach in which the behavior of a slow and complex model is approximated by a faster and simpler model (also more transparent) with comparable performance. The idea is that by mimicking the performance of other complex models, one is able to compress the learning into a more interpretable model and derive rules or logical relationships. In this regard, it is possible to cite Lime (Local Interpretable Model-Agnostic Explanations) [4] and SHAP (SHapley Additive exPlanations) [13], which have been widely used for interpretability. Despite their success, both Lime and SHAP assume that a linear model is a good local representation of the original one. This simplification may cause, in some circumstances, a significant loss in the mimicking model accuracy, which may spoil the final interpretation.\nIn this paper we present an approach to interpretability based on Genetic Programming (GP), named Genetic Programming Explainer (GPX). GP has the ability to produce linear and nonlinear models increasing the flexibility of the aforementioned methods. The evolution process of the GP algorithm ensures the selection of the best features for the analyzed sample. Moreover, the tree structure representation naturally provides an analytical expression that reflects a local explanation about the model\u2019s prediction. The main goal is to produce an accurate mimicking model which preserves the advantages of having a closed mathematical expression such as readability and the ability of computing partial derivatives to assess the sensitivity of the output with respect to the input parameters. According to the taxonomy recently advocated by [14], the proposed approach can be categorized as a model agnostic technique for post-hoc explainability, able to provide a local explanation for a specific prediction output given by a complex black-box model.\nIn this work we have considered the following pre-trained complex ML algorithms that can be recognized as blackbox models: Random Forest [15], Supppot Vector Machines (SVM) and a Deep Neural Network (DNN) [16] in a number of different data sets (10 regression and 10 binary classification problems available in public repositories). These methods have great performance in most ML problems, however, their explainability is low. For each pre-trained complex model, we compared (GPX) against other methods that could be used for generating local explanations: Lime and Decision Tree/Regression. The statistical analysis shows that GPX was able to better approximate the complex model, providing an interpretable explanation in terms of a symbolic expression. Genetic Programming brought us a new approach for interpretability with a local explanation for black-box models predictions. In addition, we present two case studies to illustrate the proposed methodology and serve as a guide for future use. The first one is about predicting home prices in Boston area and the other one measures the progression of Diabetes over the years.\nIn summary, the proposed approach aims to contribute to improving interpretability of black-box ML by using automatic generation of model-agnostic explanations that are able to fit the input-output decisions performed by the more complex model, which would be otherwise impossible to be derived by usual analytical methods. The GP algorithm is applied locally and provides an analytical and visual explanation in terms of a human readable expression represented as a tree. These evolved explanations can be easily interpreted and help understanding these complex decisions.\nThis paper is organized as follows. Section III-A reviews the main ideas of GP Algorithm and discusses how it will be applied for the purpose to provide interpretability. Section II introduces some concepts about interpretability and describes our approach according to these. Section III presents our methodology and the main idea of our solution for approaching the interpretability problem. Section V discusses the results of this article compared with the state of the art."}, {"heading": "II. CONCEPTS OF INTERPRETABILITY", "text": "Humans are capable of making predictions about a subject and build a logical explanation to justify it. When a prediction is based on understandable choices, it gives the decision maker more confidence on the model [2]. On the other hand, Machine Learning and Deep Learning models are not able to provide the same level confidence. Their complexity and exorbitant number of parameters make them unintelligible for a human being and for most of the purposes they are seen as blackboxes. Humans tend to be resistant to techniques that are not well understood or that cannot be directly interpreted [5].\nMore recently, several strategies have been applied to understand how black-box models work. These strategies aim to decrease the opacity of artificial intelligence systems and to make these models more user friendly. In order to address the opacity of some machine learning models, we first must introduce some concepts of interpretability [7], [2]:\n\u2022 Comprehensibility: refers to the ability of a learning algorithm to represent its learned knowledge in a human understandable fashion, in such a way that the knowledge representation can be comprehended. \u2022 Interpretability: It is defined as the ability to explain or to provide the meaning in understandable terms to a human. \u2022 Explainability: is associated with the notion of explanation as an interface between humans and a decision maker that is, at the same time, both an accurate proxy of the decision maker and comprehensible to humans. \u2022 Transparency: A model is considered to be transparent if by itself it is understandable. For instance, a decision tree is a transparent model. The model must allow the reproduction of every calculation step, it needs to have a clear explanation of parameters and hyper-parameters and to provide an explanation about the learning process. Basically complex models can be considered as black-box models because they lack transparency.\n\u2022 Functionality: the model must provide an understandable output, it needs to have visualization tools and to ensure a local explanation [4], [13]. The outputs must be presented in a user-friendly way.\nPost hoc explainability techniques employ other methods after training step to analyze a given model. Different techniques can be used to enhance interpretability of a blackbox model, such as text explanations, visual explanations, local explanations, explanations by example, explanations by simplification and feature relevance explanations [5].\nMore specifically, post hoc explainability by means of local explanations is an approach applied for a prediction or set of predictions of pre-trained black-box models, for example: Lime [4] and SHAP [13] fall in this category. Lime generates a local explanation model while SHAP identifies feature relevance explanations.\nThe approach presented in this paper can be considered as a post hoc technique since the GP algorithm is applied to a pre-trained model. The goal of the GP is to produce a local symbolic model to provide a visual explanation in terms of a human readable expression represented as a tree. Thus, in this sense, the proposed method generates a knowledge representation that can be comprehended, a local explanation model that is interpretable, transparent and functional.\nExplainability and interpretability are extremely relevant issues nowadays. They are fundamental pieces for dealing with several philosophical and ethical aspects of the interaction between humans and AI artifacts."}, {"heading": "III. EVOLVING EXPLANATIONS", "text": ""}, {"heading": "A. Genetic Programming", "text": "Genetic Programming (GP) was developed by Koza [17] to evolve functional structures such as algebraic expressions, computer programs, and logical expressions [18]. GP works with a population of programs, usually initialized at random. In a GP there is a problem to be solved and the fitness of the individuals is related to how well they solve this problem. Thus, the GP algorithm evolves this population until it finds the best solution for the problem. Figure 1 presents all steps of the GP algorithm.\nThe GP algorithm is relatively simple to describe, as observed in Figure 1. The first step is to generate a random population with several individuals. The next step is to evaluate the fitness of each individual to measure how well an individual solves the problem at hand. If some individual satisfies the stopping criterion, the algorithm ends. Otherwise, it goes to the selection step in which the algorithm will favor the better individuals based on fitness evaluation. After the selection step a genetic operator is chosen randomly to generate new individuals [17], [18] .\nIn this work, genetic programming is used to evolve nonlinear symbolic expressions for regression and classification problems. These expressions are represented in the program as a tree over which the genetic operations of mutation, crossover and reproduction are defined. Figure 2 illustrates the tree representation of a mathematical expression.\nThe gplearn1 Python library was used as the GP search engine. This library extends from scikit-learn, a widely known Python library for ML."}, {"heading": "B. GP approach to local explanation", "text": "In this section, the proposed approach to the interpretability problem is described. The example discussed here helps understanding the steps needed to achieve our local interpretation.\nLet x \u2208 Rn be an input fed to a complex pre-trained machine learning model. The first step in our method is to generate m sample points around the input x. This set of samples, called noise set, \u03b7, is created by sampling from a multivariate Gaussian distribution centered at x with covariance matrix, \u03a3 = In\u00d7\u03c3 where, In is the n\u00d7n identity matrix and \u03c3 is measured on training data.\nThe goal of GPX is to find the function, f\u2217 : Rn \u2192 R which is easy to interpret and, at the same time, mimics the behavior of original complex model, g : Rn \u2192 R, over the sample set, \u03b7. Formally, the problem to be solved by GPX can be defined as follows:\n1https://gplearn.readthedocs.io/\nf\u2217 = argmin f\u2208F,si\u2208\u03b7 d([f(s1), . . . , f(sm)]\u2212 [g(s1), . . . , g(sm)])\n(1) where: \u2022 F : set of all admissible functions f : Rd \u2192 R; \u2022 {s1, s2, ..., sm}: samples form the noise set, \u03b7; \u2022 g(si): is the prediction given by the complex ML model; \u2022 f(si) is the prediction generated by a given individual in\nthe population; and \u2022 d(\u00b7): some distance metric, usually the l2-norm or the\nroot-mean-square error (RMSE).\nAs an example, Figure 3 illustrates some training data Xk\u00d7n, where k = 1, 500 samples and n = 2. Assume that a model, g(\u00b7), has been previously obtained from this data set.\nAfter defining \u03c3, it is possible to generate the noise set, \u03b7, located in the neighborhood of the point of interest x whose prediction should be explained. Figure 4 shows x and the noise set, \u03b7, generated randomly around x.\nThe next step is to apply GP to try to find f\u2217 as defined in Equation (1).\nFigure 5 illustrates the best representation found for f\u2217 by the GP. For didactic reasons, this example was built with two dimensions only and the produced representation for f\u2217 contains all the input features. It is important to highlight, however, that the GP is not obliged to use all the features. Thus, the produced model already indicates which variables are important. Besides, the returned closed mathematical expression allows the user to compute partial derivatives to assess the local importance of each feature."}, {"heading": "IV. EXPERIMENTAL METHODOLOGY", "text": ""}, {"heading": "A. Data Sets", "text": "In this work twenty data sets were used. Ten for classification and ten regression. All data sets were extracted from well-known repositories such as UCI Machine Learning Repository2, OpenML3 and Kaggle4. Our selection was based on features variability between data sets and number of downloads. Table I lists the chosen data sets for the classification problems and the corresponding number of used features.\nTable II presents the data sets chosen for the regression problems and the corresponding number of used features.\n2https://archive.ics.uci.edu// 3https://www.openml.org 4https://www.kaggle.com/"}, {"heading": "B. Complex, black-box, models", "text": "There are several machine learning models which can be considered black-box. In this work we have selected three of the most popular to serve as \u201ccomplex models\u201d. That is, model for which explanations have to be produced. These models are the Random Forest, Deep Neural Networks and Suport Vector Machines. They are briefly introduced below.\nRandom Forest is a type of tree ensemble which have shown improvements for the generalization of a single decision tree and has achieved remarkable results in the literature [15], [30]. Our experiments set Random Forest with up two thousand trees. The more trees are used in the architecture the more complexity is brought to the understanding of the model.\nDeep neural networks (DNNs) [16] are artificial neural networks (ANNs) with multiple layers between the input and output layers. The multi-layer architecture is inspired in the human brain structure and has shown to be quite effective in many difficult classification and regression problems [31]. This architecture, however, represents compositions of nonlinear functions which are not easy to interpret.\nFinally, Support Vector Machines (SVMs) [32] are classifiers which find a separating hyperplane, such that the distance on either side of that hyperplane to the next-closest data points is maximized. In other words, given labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples. Support Vector Regresssion (SVR) is the extension of SVMs for regression problems [33]. The SVR tries to fit the error within a certain threshold and can also be considered opaque since it is hard to interpret its decisions, specially when nonlinear kernel functions are used."}, {"heading": "C. Competing Explainers", "text": "In this section we present the competing explainers. Just like our method, the idea is to sample a noise set, \u03b7, around the point of interest, x. The set \u03b7 is then used to build a local comprehensible model which will be used as an explainer. We say that an explainer, f , \u201cunderstands\u201d the complex model, g, if its error on the noise set, \u03b7, relative to the complex model predictions is small. For regression problems, this concept may be translated into the Root Mean Squared Error between the model and the explainer as follows:\nur(f) = 1 |\u03b7| \u2211 si\u2208\u03b7 (f(si)\u2212 g(si))2 (2)\nFor classification problems, it may be formalized as the accuracy of the explainer with respect to the complex models predictions.\nuc(f) = 1 |\u03b7| \u2211 si\u2208\u03b7 h(si) (3)\nwhere,\nh(si) = { 1 if f(si) = g(si) 0 if f(si) 6= g(si)\n(4)\nThe first method, named Lime [4], generates an explainer based on a linear least squares method with l2-norm regularization [34]. This linear model is used into Lime in order to measure locally the feature importance.\nAnother good candidate for explainer is the Decision Tree (DT). A DT is, as the name implies, a tree where each node represents a feature, each branch represents a decision, and each leaf represents a prediction. By making a path from the root to the prediction leaf node, the user can obtain an explanation for that prediction.\nThe performance of Lime, DT and GPX (explained in section III-A) as explainers of Support Vector Machines, Neural Networks and Random Forests are discussed next."}, {"heading": "V. TESTING EXPLAINERS ACCURACY", "text": ""}, {"heading": "A. Experimental Setup", "text": "In this section we test the ability of Lime, GPX, DTs to understand the different complex models discussed in section IV-B. The experimental steps can be described as follows:\n1) Divide all the data sets described in section IV-A into training and test data. We used 80% for training and 20% for test. 2) Train the complex models (described in section IV-B) with training data. 3) Using the trained complex models, predict the value of 100 random samples selected from the test set. At this point, we have six thousand predictions for the regression and classification problems together (20 data sets \u00d7 100 predictions \u00d7 3 complex models). 4) Build the three explainers (Lime, GPX, DT) for each of these six thousand predictions in order to measure which interpreter can better understand the black-box prediction. At this time we have 18,000 data in order to apply a statistical analysis.\nThe GPX hyper-parameters were set as follows: \u2022 Population size: 100 individuals. \u2022 Probability of crossover: 70%. \u2022 Probability of hoist mutation: 5%. This mutation is called\nhoist mutation because the method chooses randomly a subtree and hoist it into the tree.\n\u2022 Probability of point mutation: 10%. This mutation selects a random node to be replaced. \u2022 Search interval: [\u2212100, 100]. The other hyper-parameters are set to the library default values. The noise set, \u03b7 (see section III-B), has the same one thousand samples for each explainer. Thus, the explainers always access same data. In order to measure how well the explainers understand the complex models (2) and (3) are used for the regression and classification problems, respectively."}, {"heading": "B. Results", "text": "Table III presents the average and the standard deviation of the error, computed with (2), for each explainer across the regression problems.\nA Permutation Pairwise Test5 was performed in order to test the hypothesis that the difference between the error means is zero. The results are shown in Table IV where P.adjust represents the P-Value adjusted with the Bonferroni Method. Considering a confidence level of 95%, it is possible to say that there is no difference in the mean error presented by the DT and the GPX. On the other hand, the results indicate that both DT and GPX, understand the complex model better than Lime.\nTable V presents the average and the standard deviation of the accuracy, computed with (3), for each explainer across the classification problems.\nA Permutation Pairwise Test was performed again in order to test the hypothesis that the difference between the accuracy means is zero. The results are shown in Table VI where P.adjust represents the P-Value adjusted with the Bonferroni Method. Considering a confidence level of 95%, the results\n5See https://rdrr.io/cran/rcompanion/man/pairwisePermutationTest.html\nindicate that, in average, GPX was better than both DT and Lime in understanding the complex models. The DT, in turn, was better than Lime.\nThe complete table of results as well as the analysis scripts are available at https://github.com/leauferreira/GpX.\nOverall, these results show that, at least for the scenarios presented here, the Lime assumption that the complex model can be locally approximated by a linear model, is not always adequate. The DT and the GPX specially were superior methods in understanding the complex model.\nHaving shown that the proposed methodology can indeed produce accurate local models, in the next section, a demonstration of the use of GPX in practice is presented."}, {"heading": "VI. CASE STUDY ON INTERPRETABILITY FOR RANDOM FOREST REGRESSOR", "text": "In this case study we selected the Boston and the Diabetes data sets, see Table II, and the Random Forest Regressor with 2,000 estimators (trees), from scikit-learn. Both data sets (Diabetes and Boston) can be found in scikit-learn\nThe Boston data set has 13 features, 506 samples and the target consists of home prices in Boston. The data set was randomly split into training and test sets with 80% of the samples for training and the remainder for testing. Apart from the number of trees the other hyper-parameters were set to the default values in the library. It is hard to analyze the joint decision performed by 2,000 trees, therefore explainability of Random Forest is low, although its performance is high.\nAfter the training step we take the first sample in the test set, x1, and apply the process described in Section III-B in order to create a noise around x1. Then, the GP algorithm is trained with the noise set, \u03b7 = {s1, ..., sn} where the targets are given by g(si), \u2200si \u2208 \u03b7, and g is the Random Forest Regressor model.\nThe Boston data set consists of d = 13 features. However, as observed in Figure 6, after the evolution process only two features were chosen by the GP. They were: \u2022 PTRATIO : pupil-teacher ratio by town \u2022 NOX: nitric oxides concentration (parts per 10 million) In (5) we changed PTRATIO to xptratio and NOX to xnox. The tree structure in Figure 6 represents the following equation:\nf\u2217(s) = x2ptratioxnox\n28.390 (5)\nFigure 7 presents the result of the same process but in a different area, by considering sample x2. It is possible to observe that different features were chosen during the\nevolutionary process. The features chosen for the solution shown in Figure 7 were:\n\u2022 PTRATIO: pupil-teacher ratio by town \u2022 INDUS proportion of non-retail business acres per town\ncentres \u2022 LSTAT lower status of the population\nThe expression represented in Figure 7 can be defined as:\nf\u2217(s) = xindus xlstat + xptratio (6)\nBased on the equations (5) and (6) we can understand the behavior around x1 and x2. First of all, it is possible to observe that the feature PTRATIO (pupil-teacher ratio by town) is relevant to define home prices in the neighborhood of both instances (x1, x2). However, in the neighborhood of x2, PTRATIO has more influence.\nThese results can help a decision-maker to understand which features contribute the most to the increase in the price in a neighborhood. Moreover, it is possible to know which features changed in order to increase or decrease home prices. In this regard, it is useful to compute the gradient of the model output with respect to the selected input parameters. Lets take equation (5) as an example. Its gradient is given by:\n\u2207f\u2217(s) =\n[ xptratioxnox\n14.195 , x2ptratio 28.390\n]T (7)\nBy analyzing equations (5) and (7) it is easy to understand how and by how much each feature affects housing prices.\nThe Random Forest Regressor has also been applied to the Diabetes data set. This data set consists of 402 samples and 10 features. In this case, the target is a quantitative measure of disease progression. The GPX was set with the same hyperparameters used for the Boston data set. Figure 8 presents the result for the Diabetes data set.\nThe evolutionary process chose a final tree with two features: \u2022 bmi: Body mass index \u2022 S6: blood serum measurements The tree structure in Figure 8 represents the following expression:\n\u2212 67.934xS6 + xbmi + 158.525 (8)\nwhere xbmi is bmi and xS6 is S6. This expression shows for instance that decreasing body mass index is a relevant feature to decrease diabetes progression for that specific patient."}, {"heading": "VII. CONCLUSION", "text": "This paper presented an approach to the interpretability problem based on Genetic Programming. We discussed several concepts of interpretability and why this subject is relevant nowadays.\nThe GP algorithm used was able to produce a non-linear algebraic expression as output, which in turn provides many opportunities for the interpretability of the more sophisticated ML algorithms. It naturally selects the most important features\nto build a local explanation around a given sample. Besides, the produced analytic expression allows for easy differentiation, which gives the sensitivity of the output with respect to each feature.\nExamples using the classic Boston and Diabetes data sets show that the proposed approach can be seen as a source for interpretability and help the decision maker to understand better how the complex model is making a decision.\nWe submit the explainers GPX, Lime and Decision Tree to a stress test in order to measure which one can, locally, better understand the black-box model. The statistical analysis showed us that GPX, besides bringing a new approach to interpretability, presented better or at least similar results when compared with the state of the art."}], "title": "Applying Genetic Programming to Improve Interpretability in Machine Learning Models", "year": 2020}