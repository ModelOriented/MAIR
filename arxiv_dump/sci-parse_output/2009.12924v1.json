{
  "abstractText": "Research into the explanation of machine learning models, i.e., explainable AI (XAI), has seen a commensurate exponential growth alongside deep artificial neural networks throughout the past decade. For historical reasons, explanation and trust have been intertwined. However, the focus on trust is too narrow, and has led the research community astray from tried and true empirical methods that produced more defensible scientific knowledge about people and explanations. To address this, we contribute a practical path forward for researchers in the XAI field. We recommend researchers focus on the utility of machine learning explanations instead of trust. We outline five broad use cases where explanations are useful and, for each, we describe pseudo-experiments that rely on objective empirical measurements and falsifiable hypotheses. We believe that this experimental rigor is necessary to contribute to scientific knowledge in the field of XAI.",
  "authors": [
    {
      "affiliations": [],
      "name": "Brittany Davis"
    },
    {
      "affiliations": [],
      "name": "Maria Glenski"
    },
    {
      "affiliations": [],
      "name": "William Sealy"
    },
    {
      "affiliations": [],
      "name": "Dustin Arendt"
    }
  ],
  "id": "SP:440e39fe61ffdf94f692728bf6fe863e638f5dd2",
  "references": [
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)",
      "venue": "IEEE Access, 6:52138\u2013 52160",
      "year": 2018
    },
    {
      "authors": [
        "A. Alqaraawi",
        "M. Schuessler",
        "P. Wei\u00df",
        "E. Costanza",
        "N. Berthouze"
      ],
      "title": "Evaluating saliency map explanations for convolutional neural networks: A user study",
      "venue": "In Proceedings of the 25th International Conference on Intelligent User Interfaces,",
      "year": 2020
    },
    {
      "authors": [
        "A. Anderson",
        "J. Dodge",
        "A. Sadarangani",
        "Z. Juozapaitis",
        "E. Newman",
        "J. Irvine",
        "S. Chattopadhyay",
        "A. Fern",
        "M. Burnett"
      ],
      "title": "Explaining reinforcement learning to mere mortals: An empirical study",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)",
      "year": 2019
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation",
      "venue": "PLOS ONE,",
      "year": 2015
    },
    {
      "authors": [
        "G. Bansal",
        "B. Nushi",
        "E. Kamar",
        "W.S. Lasecki",
        "D.S. Weld",
        "E. Horvitz"
      ],
      "title": "Beyond accuracy: The role of mental models in human-ai team performance",
      "venue": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, vol. 7, pp. 2\u201311",
      "year": 2019
    },
    {
      "authors": [
        "S. Berkovsky",
        "R. Taib",
        "D. Conway"
      ],
      "title": "How to Recommend?: User Trust Factors in Movie Recommender Systems",
      "venue": "In Proceedings of the 22nd International Conference on Intelligent User Interfaces,",
      "year": 2017
    },
    {
      "authors": [
        "M. Bilgic",
        "R. Mooney"
      ],
      "title": "Explaining Recommendations: Satisfaction vs",
      "venue": "Promotion. Proceedings of Beyond Personalization 2005: A Workshop on the Next Stage of Recommender Systems Research at the 2005 International Conference on Intelligent User Interfaces, 01",
      "year": 2005
    },
    {
      "authors": [
        "A. Bussone",
        "S. Stumpf",
        "D. O\u2019Sullivan"
      ],
      "title": "The Role of Explanations on Trust and Reliance in Clinical Decision Support Systems",
      "venue": "In 2015 International Conference on Healthcare Informatics,",
      "year": 2015
    },
    {
      "authors": [
        "D. Castelvecchi"
      ],
      "title": "Can we open the black box of ai? Nature",
      "venue": "538(7623):2023",
      "year": 2016
    },
    {
      "authors": [
        "J. Chang",
        "J. Boyd-Graber",
        "S. Gerrish",
        "C. Wang",
        "D. Blei"
      ],
      "title": "Reading tea leaves: How humans interpret topic models",
      "year": 2009
    },
    {
      "authors": [
        "A. Chatzimparmpas",
        "R.M. Martins",
        "I. Jusufi",
        "K. Kucher",
        "F. Rossi",
        "A. Kerren"
      ],
      "title": "The state of the art in enhancing trust in machine learning models with the use of visualizations",
      "venue": "Computer graphics forum (Print)",
      "year": 2020
    },
    {
      "authors": [
        "A. Chouldechova"
      ],
      "title": "Fair prediction with disparate impact: a study of bias in recidivism prediction instruments",
      "venue": "Big data, 5(2):153\u2013163",
      "year": 2017
    },
    {
      "authors": [
        "T. Davidson",
        "D. Bhattacharya",
        "I. Weber"
      ],
      "title": "Racial bias in hate speech and abusive language detection datasets",
      "venue": "Proceedings of the Third Workshop on Abusive Language Online, pp. 25\u201335",
      "year": 2019
    },
    {
      "authors": [
        "W.K. Diprose",
        "N. Buist",
        "N. Hua",
        "Q. Thurier",
        "G. Shand",
        "R. Robinson"
      ],
      "title": "Physician understanding, explainability, and trust in a hypothetical machine learning risk calculator",
      "venue": "Journal of the American Medical Informatics Association,",
      "year": 2020
    },
    {
      "authors": [
        "J. Dressel",
        "H. Farid"
      ],
      "title": "The accuracy",
      "venue": "fairness, and limits of predicting recidivism. Science advances, 4(1):eaao5580",
      "year": 2018
    },
    {
      "authors": [
        "M.R. Endsley"
      ],
      "title": "Measurement of situation awareness in dynamic systems",
      "venue": "Human Factors: The Journal of the Human Factors and Ergonomics Society, 37(1):6584",
      "year": 1995
    },
    {
      "authors": [
        "D. Ensign",
        "S.A. Friedler",
        "S. Neville",
        "C. Scheidegger",
        "S. Venkatasubramanian"
      ],
      "title": "Runaway feedback loops in predictive policing",
      "venue": "Conference on Fairness, Accountability and Transparency, pp. 160\u2013 171",
      "year": 2018
    },
    {
      "authors": [
        "B. Goodman",
        "S. Flaxman"
      ],
      "title": "European union regulations on algorithmic decision-making and a right to explanation",
      "venue": "AI magazine, 38(3):50\u201357",
      "year": 2017
    },
    {
      "authors": [
        "D. Gunning"
      ],
      "title": "Broad Agency Announcement: Explainable Artificial Intelligence (XAI), 2016",
      "year": 2016
    },
    {
      "authors": [
        "S.R. Haynes",
        "M.A. Cohen",
        "F.E. Ritter"
      ],
      "title": "Designs for Explaining Intelligent Agents",
      "venue": "International Journal of Human-Computer Studies,",
      "year": 2009
    },
    {
      "authors": [
        "J.L. Herlocker",
        "J.A. Konstan",
        "J. Riedl"
      ],
      "title": "Explaining collaborative filtering recommendations",
      "venue": "Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work - CSCW \u201900, pp. 241\u2013250. ACM Press, Philadelphia, Pennsylvania, United States",
      "year": 2000
    },
    {
      "authors": [
        "R.R. Hoffman",
        "S.T. Mueller",
        "G. Klein",
        "J. Litman"
      ],
      "title": "Metrics for explainable ai: Challenges and prospects",
      "venue": "arXiv preprint arXiv:1812.04608",
      "year": 2018
    },
    {
      "authors": [
        "F. Hohman",
        "M. Kahng",
        "R. Pienta",
        "D.H. Chau"
      ],
      "title": "Visual analytics in deep learning: An interrogative survey for the next frontiers",
      "venue": "IEEE transactions on visualization and computer graphics, 25(8):2674\u20132693",
      "year": 2018
    },
    {
      "authors": [
        "D. Holliday",
        "S. Wilson",
        "S. Stumpf"
      ],
      "title": "User Trust in Intelligent Systems: A Journey Over Time",
      "venue": "Proceedings of the 21st International Conference on Intelligent User Interfaces - IUI \u201916, pp. 164\u2013168. ACM Press, Sonoma, California, USA",
      "year": 2016
    },
    {
      "authors": [
        "A. Holzinger",
        "G. Langs",
        "H. Denk",
        "K. Zatloukal",
        "H. M\u00fcller"
      ],
      "title": "Causability and Explainability of Artificial Intelligence in Medicine",
      "venue": "WIREs Data Mining and Knowledge Discovery,",
      "year": 2019
    },
    {
      "authors": [
        "K. Holzinger",
        "K. Mak",
        "P. Kieseberg",
        "A. Holzinger"
      ],
      "title": "Can we trust Machine Learning Results? Artificial Intelligence in Safety-Critical decision Support",
      "venue": "Research and Innovation, pp. 112\u2013113",
      "year": 2018
    },
    {
      "authors": [
        "H. Jiang",
        "B. Kim",
        "M. Guan",
        "M. Gupta"
      ],
      "title": "To Trust Or Not To Trust A Classifier",
      "venue": "32nd Conference on Neural Information Processing Systems ",
      "year": 2018
    },
    {
      "authors": [
        "A. Krizhevsky",
        "I. Sutskever",
        "G.E. Hinton"
      ],
      "title": "ImageNet classification with deep convolutional neural networks",
      "venue": "Communications of the ACM,",
      "year": 2017
    },
    {
      "authors": [
        "T. Kulesza",
        "S. Stumpf",
        "M. Burnett",
        "S. Yang",
        "I. Kwan",
        "W.-K. Wong"
      ],
      "title": "Too much, too little, or just right? Ways explanations impact end users\u2019 mental models",
      "venue": "IEEE Symposium on Visual Languages and Human Centric Computing,",
      "year": 2013
    },
    {
      "authors": [
        "S. Lapuschkin",
        "S. Wldchen",
        "A. Binder",
        "G. Montavon",
        "W. Samek",
        "K.-R. Mller"
      ],
      "title": "Unmasking clever hans predictors and assessing what machines really learn",
      "venue": "Nature Communications,",
      "year": 2019
    },
    {
      "authors": [
        "B. Lim",
        "A. Dey",
        "D. Avrahami"
      ],
      "title": "Why and why not explanations improve the intelligibility of context-aware intelligent systems",
      "venue": "Conference on Human Factors in Computing Systems - Proceedings,",
      "year": 2009
    },
    {
      "authors": [
        "S. Mohseni",
        "N. Zarei",
        "E.D. Ragan"
      ],
      "title": "A survey of evaluation methods and measures for interpretable machine learning",
      "venue": "arXiv preprint arXiv:1811.11839",
      "year": 2018
    },
    {
      "authors": [
        "A. Nguyen",
        "A. Dosovitskiy",
        "J. Yosinski",
        "T. Brox",
        "J. Clune"
      ],
      "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
      "venue": "[cs],",
      "year": 2016
    },
    {
      "authors": [
        "F. Nothdurft",
        "F. Richter",
        "W. Minker"
      ],
      "title": "Probabilistic Human- Computer Trust Handling",
      "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pp. 51\u201359. Association for Computational Linguistics, Philadelphia, PA, U.S.A.",
      "year": 2014
    },
    {
      "authors": [
        "V. Pallotta",
        "P. Bruegger",
        "B. Hirsbrunner"
      ],
      "title": "Smart heating systems: Optimizing heating systems by kinetic-awareness",
      "venue": "Third International Conference on Digital Information Management,",
      "year": 2008
    },
    {
      "authors": [
        "J.H. Park",
        "J. Shin",
        "P. Fung"
      ],
      "title": "Reducing gender bias in abusive language detection",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2799\u20132804",
      "year": 2018
    },
    {
      "authors": [
        "P. Pu",
        "L. Chen"
      ],
      "title": "Trust building with explanation interfaces",
      "venue": "Proceedings of the 11th International Conference on Intelligent User Interfaces - IUI \u201906, p. 93. ACM Press, Sydney, Australia",
      "year": 2006
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": " why should i trust you?\u201d explaining the predictions of any classifier",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135\u20131144",
      "year": 2016
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Anchors: High-precision model-agnostic explanations",
      "venue": "AAAI",
      "year": 2018
    },
    {
      "authors": [
        "M. Sap",
        "D. Card",
        "S. Gabriel",
        "Y. Choi",
        "N.A. Smith"
      ],
      "title": "The risk of racial bias in hate speech detection",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1668\u20131678",
      "year": 2019
    },
    {
      "authors": [
        "K.E. Schaefer",
        "E.R. Straub",
        "J.Y. Chen",
        "J. Putney",
        "A. Evans"
      ],
      "title": "Communicating intent to develop shared situation awareness and engender trust in human-agent teams",
      "venue": "Cognitive Systems Research,",
      "year": 2017
    },
    {
      "authors": [
        "R.R. Selvaraju",
        "M. Cogswell",
        "A. Das",
        "R. Vedantam",
        "D. Parikh",
        "D. Batra"
      ],
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization",
      "venue": "IEEE International Conference on Computer Vision (ICCV),",
      "year": 2017
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "venue": "[cs],",
      "year": 2014
    },
    {
      "authors": [
        "K. Sokol",
        "P.A. Flach"
      ],
      "title": "Glass-box: Explaining ai decisions with counterfactual statements through conversation with a voice-enabled virtual assistant",
      "venue": "IJCAI, pp. 5868\u20135870",
      "year": 2018
    },
    {
      "authors": [
        "A. Toon"
      ],
      "title": "Where is the understanding? Synthese",
      "venue": "192",
      "year": 2015
    },
    {
      "authors": [
        "B. Ustun",
        "A. Spangher",
        "Y. Liu"
      ],
      "title": "Actionable recourse in linear classification",
      "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency, pp. 10\u201319",
      "year": 2019
    },
    {
      "authors": [
        "J. Wexler",
        "M. Pushkarna",
        "T. Bolukbasi",
        "M. Wattenberg",
        "F. Vi\u00e9gas",
        "J. Wilson"
      ],
      "title": "The what-if tool: Interactive probing of machine learning models",
      "venue": "IEEE transactions on visualization and computer graphics, 26(1):56\u201365",
      "year": 2019
    },
    {
      "authors": [
        "F. Yang",
        "Z. Huang",
        "J. Scholtz",
        "D.L. Arendt"
      ],
      "title": "How do visual explanations foster end users\u2019 appropriate trust in machine learning? In Proceedings of the 25th International Conference on Intelligent User Interfaces",
      "venue": "pp. 189\u2013201",
      "year": 2020
    },
    {
      "authors": [
        "M. Yin",
        "J. Wortman Vaughan",
        "H. Wallach"
      ],
      "title": "Understanding the Effect of Accuracy on Trust in Machine Learning Models",
      "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI \u201919, pp. 1\u201312. ACM Press, Glasgow, Scotland Uk",
      "year": 2019
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and Understanding Convolutional Networks",
      "venue": "European Conference on Computer Vision (ECCV), 8689(01):818\u2013833",
      "year": 2013
    }
  ],
  "sections": [
    {
      "text": "Research into the explanation of machine learning models, i.e., explainable AI (XAI), has seen a commensurate exponential growth alongside deep artificial neural networks throughout the past decade. For historical reasons, explanation and trust have been intertwined. However, the focus on trust is too narrow, and has led the research community astray from tried and true empirical methods that produced more defensible scientific knowledge about people and explanations. To address this, we contribute a practical path forward for researchers in the XAI field. We recommend researchers focus on the utility of machine learning explanations instead of trust. We outline five broad use cases where explanations are useful and, for each, we describe pseudo-experiments that rely on objective empirical measurements and falsifiable hypotheses. We believe that this experimental rigor is necessary to contribute to scientific knowledge in the field of XAI.\nIndex Terms: Human-centered computing\u2014Human computer interaction (HCI)\u2014HCI design and evaluation methods\u2014User studies; Computing methodologies\u2014Machine learning\u2014Machine learning approaches\u2014Neural networks"
    },
    {
      "heading": "1 POSITION: MEASURE UTILITY, GAIN TRUST",
      "text": "Many AI, HCI, and Visualization researchers may have assumed the purpose of a machine learning explanation is to enhance, increase, or calibrate users\u2019 trust in the model [11]. In contrast with this viewpoint and the large body of existing research, we argue that trust is an insufficient metric for evaluating explanations (or models). We recommend that researchers objectively measure the utility of the explanation instead of subjectively measuring trust. Trust should manifest through experience \u2014 as users use a system containing a model and an explanation, their trust in the system will grow if that system is reliable and provides a benefit. We also suspect that designing explanations to optimize trust may short-circuit the natural process of trust-building and also mislead users. This could occur if the design of the explanation obfuscates the actual utility or capability of the system. Optimizing for trust could be considered a form of \u201cmetric hacking\u201d, leading to artificially inflated levels of trust compared to what would build naturally through experience.\nIf users\u2019 increased trust in the model is not necessarily the most appropriate way to measure the \u201cgoodness\u201d of an explanation, then what is? Do explanations have an intrinsic value? We believe the answer to the latter question is simply no. Unlike machine learning models, which are built directly upon ground truth data, a ground\n*e-mail: brittany.f.davis@wsu.edu \u2020e-mail: maria.glenski@pnnl.gov \u2021e-mail: william.sealy@gatech.edu \u00a7e-mail: dustin.arendt@pnnl.gov\ntruth \u201ccorrect\u201d explanation of a machine learning model is not generally available. Thus, we are not likely to find a way to measure the error between a given explanation and the correct one, except in very specific use cases, e.g., image captioning. Much like a data visualization, the purpose of an explanation is to communicate useful information to a human \u2014 the visualization community has no methods for directly measuring the correctness of a visualization. We do however have many methods to measure the utility of visualization, which all require considering what tasks users perform with that visualization. In fact, many explanations are visualisations [23], so the full range of techniques the visualization community has employed for evaluating visualizations remain relevant for evaluating machine learning explanations.\nThere are many use cases where explanations are helpful as part of a larger workflow. Hohman et al. [23] and Mohseni et al. [32] both describe in depth the various users and uses of explanations. For the purposes of our argument in this paper, we focus on the following use cases: debugging, validating, and selecting a model; understanding a model; teaming with a model; and challenging a model. Each of these use cases allow us to imagine \u201cdownstream tasks\u201d where we can quantitatively measure the users\u2019 performance with or without the explanation. Resulting differences in performance provide indirect but compelling evidence of the utility of the explanation. There are many potential ways of measuring \u201cexplanation goodness\u201d, which interested readers can find surveyed by Mohseni et al. [32]. Hoffman et al. [22] also discuss several methods for evaluating explanations including those that go beyond trust.\nWe have taken the position that trust is a flawed metric for measuring the \u201cgoodness\u201d of a machine learning explanation. The next section of this paper will provide historical context for this argument. We found that trust is a metric advocated by people who want the models they build to be adopted (thus, a good explanation is one that builds trust and increases the likelihood of adoption). The final section of this paper provides practical guidance for how to measure the utility of an explanation across the use cases mentioned above."
    },
    {
      "heading": "2 THE PAST: HISTORICAL REASONS FOR THE COUPLING",
      "text": "OF TRUST AND EXPLANATIONS\nWhile the success of deep neural networks in various machine learning tasks is a recent phenomenon, machine learning as a general practice has existed for much longer. Almost since the beginning, developers of machine learning algorithms have sought to increase the acceptance and uptake of those algorithms. To do so, researchers had to prove to others what they felt they knew to be true \u2014 that these algorithms could be trusted to work correctly and had benefit. Thus, explanations of classical machine learning were initially developed for the purpose of increasing users\u2019 trust. As the field changed over time, the coupling of explanations to trust-building became a habit more than best-practice. Today, explanations can and should be used more widely beyond increasing trust \u2014 measuring their effectiveness should account for this broader applicability.\nar X\niv :2\n00 9.\n12 92\n4v 1\n[ cs\n.H C\n] 2\n7 Se\np 20\n20"
    },
    {
      "heading": "2.1 \u201cClassical\u201d Machine Learning",
      "text": "In 2000, AlexNet had not yet won an image classification competition and the field of artificial intelligence (AI) was focused on things like \u201csymbolic systems\u201d, which were built on pieces of modular knowledge, gleaned from experts or large databases and stored into a searchable structure such as a decision tree. Herlocker et al. [21] wrote that \u201ccurrent recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation\u201d and presented an experiment that measured which explanations increased consumers acceptance of recommendations generated by an intelligent agent and which \u201cnegatively contributing to the acceptance of the recommendation.\u201d These systems may appear to be mysterious to end users but they were just data structures to the researchers who built and deployed them. Researchers knew how to construct and debug these structures, because it was the same way that they constructed and debugged code.\nHowever, these researchers needed a way to get the skeptical public including corporations to accept these expert systems. As Bilgic and Mooney [7] put it in 2005, \u201cin order for users to benefit, they must trust the systems recommendations and accept them. A systems ability to explain its recommendations in a way that makes its reasoning more transparent can contribute significantly to users acceptance of its suggestions.\u201d This was the apparent goal of trust with pre-neural-network-AI: to help purchasers and users to understand the limits of the system so that an inevitable wrong answer would not scare them away. By 2006, Pu and Chen [37] referred to this pursuit as \u201cinvestigating design issues for trust-inducing interfaces,\u201d and in 2009, Haynes et al. [20] observed that \u201c[a]s intelligent agents become more pervasive in our day-to-day computing environment, and as their role becomes more consequential with respect to human purposes, they will be increasingly called upon to communicate in a way that engenders trust\u201d"
    },
    {
      "heading": "2.2 Deep Learning and the Image Domain",
      "text": "In 2012, AlexNet achieved a top-5 error rate of 15.3% in the ImageNet Large Scale Visual Recognition Challenge [28] and by 2013 researchers were trying to peer inside the Convolutional Neural Networks which made up AlexNet. Why? Because, as Zeiler and Fergus [51] noted less than a year later, \u201cthere is no clear understanding of why they perform so well, or how they might be improved.\u201d Or, as Holzinger et al. [25] put it,\u201cTechnically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful but increasingly opaque.\u201d Researchers were suddenly in the same position as those suspicious consumers: they weren\u2019t sure why or how neural networks worked, or what tweaks to their construction might improve or deteriorate performance. In response, researchers began trying to find ways to prove to themselves that these new \u2018expert systems were trustworthy.\nFrom figuring out which layers were detecting edges versus texture [43], to generating generalized images of what specific neuron clusters were capturing [33], or understanding what changes could produce wrong answers in adversarial attacks, researchers in AI had to go back to basics when neural networks exploded onto the scene. By 2016, Zeiler and Ferguss [4] method of pushing convolutions backwards in a network had inspired the development of Layerwise Relevance Propagation. That same year, Ribiero et al. [38] published their methodology, called LIME, for generating local or global counter-explanations for image classifiers. In 2017, a methodology called Grad-CAM was developed, which used gradients to highlight parts of the input to an image classifier which contributed most to the outcome [42]. All of these methods were used by the research community to try and better understand the inner workings\nof deep neural networks. Most work on end-user-facing trust in this period was still analyzing trust in symbolic systems and other more established machine learning techniques. These experiments still largely aimed to increase users trust in the systems, by understanding what modulated user trust up or down. Researchers tested explanations of simplified models of bagged decision trees, and found that \u201cwhen soundness was very low, participants experienced more mental demand and lost trust in the explanations\u201d [29]. Another experiment tested explanations of a system which added a probabilistic Markov Decision Process to a task planning application based on a finite state machine, and found \u201cthat transparency explanations can help to reduce the negative effects of trust loss [34].\u201d Some experiments used the \u201cWizard of Oz\u201d approach to test user trust in automated systems, where \u201cthe behavior of the software is controlled by the researcher unbeknown to the participant [8].\u201d Another experiment conducted in 2016 used an Auto-Encoder developed in 2010 to find \u201cthat perceived system ability was more important in determining trust amongst with-explanation participants and perceived transparency was a greater influence on the trust of participants who did not receive explanations [24].\u201d Berkovsky et al. [6] ran an experiment \u201cto investigate the dependencies between various aspects of recommendation interfaces and user-system trust.\u201d\n2.3 The Shift to Appropriate Trust\nIn August of 2016, DARPA announced its Explainable Artificial Intelligence program. The announcement brought into common use among researchers the phrase \u201cappropriate trust and the idea of explainable AI among the general population [19]. Around the same time, the European Union (EU) had announced that the General Data Protection Regulation (GDPR) would take effect in 2018 [48]. This regulation specified that companies could no longer use systems to make certain decisions about consumers who lived in the EU, unless the company could explain the decision. These two events seem to have spurred researchers to tentatively turn their efforts at explaining neural networks outside of the research community, engaging end users and sometimes everyday people.\nResearchers began to comb over the progress made on peering inside of neural networks and trying to find ways to use these tools to increase appropriate trust among people outside the AI community. For example, Schaefer et al. [41] found that \u201cby understanding the transparency elements that increase effective bi-directional communication [in human-computer teams], we can engender appropriate trust and reliance in the system.\u201d\nIn addition, much debate ensued over defining, justifying, and measuring explainable and trust in this new context. Jiang et al. [27] presented the concept of a trust score, to provide \u201cinformation about the relative positions of the data points, which may be lost in common approaches such as the model confidence when the model is trained using stochastic gradient descent.\u201d A plethora of taxonomies for trust measurements and explanations appeared. Adadi and Berrada [1] claimed explanations are needed for four reasons: to justify, to control, to improve, and to discover. Pallotta et al. [35] argue that \u201cexplanations need to be carefully crafted to fit with their desired aim, and described a methodology which would increase user trust enough to prevent users from interfering with home heating systems. Holzinger et al. [26] argued that increasing trust in deep learning systems necessarily included mechanisms for users to change the systems outcomes.\nOut of this debate emerged a growing consensus that the emphasis should be on the word \u2018appropriate in the phrase appropriate trust. Yin et al. [50] measured users trust in a model and found that both the actual capabilities of the model and the specific instances seen by the user influence a users trust in the model. That is to say, if a model is not accurate, and this is evident to users, they dont trust it and thats a good thing. However, another experiment found that\nusers can rely too heavily on a poor model, reporting that \u201cin 67.3% of all cases, participants predicted that the system would be correct, whereas it was only correct in 42.9% of the cases [2].\u201d\nNeural networks and deep learning shifted the paradigm in such a way that engendering trust, even appropriate trust, is no longer sufficient. The idea of trust is already ambiguous, and applying it to deep neural networks when even those who build them do not understand the intricacies of the model logic or determinations creates too many layers of abstraction to produce meaningful science. Instead, researchers should re-evaluate the revelations of 2013, and be humbled by the fact that we still do not know how to debug or improve these networks with much certainty. To that end, creating falsifiable and provable hypotheses should replace the concept of increasing or calibrating trust. Although appropriate trust is a more objective measurement of trust, we ought to be measuring these systems with more relevant metrics that can be clearly measured, tested, and replicated."
    },
    {
      "heading": "3 A POSSIBLE FUTURE: DOWNSTREAM TASKS AND FALSIFIABLE HYPOTHESES",
      "text": "As previously argued, the utility of an explanation must be tied to its purpose \u2014 why it was created and the context in which it is intended to be used. Trust should not be a metric we maximize through design, but should be a benefit gained after a user interacts with a useful system over time. Thus we believe that studies evaluating explanations should not measure trust unless they are longitudinal, \u201cin the wild\u201d, and consider the entire system. Instead, we argue that research studies that seek to measure the benefit of novel explanations should focus on utility over trust. We have identified five broad use cases where researchers could design experiments to measure the utility of explanations.\n1. Model Debugging and Validation: Is the model working as designed? Why is the model making mistakes?\n2. Model Selection: Potentially going beyond simple performance metrics like F-score, which model is best?\n3. Mental Model and Model Understanding: How does the model function or behave? Can I learn something interesting from the model?\n4. Human Machine Teaming: Can I do a task with the model better than on my own (and better than the model on its own)?\n5. Model Feedback, Challenging, and Prescription: When I am affected by a model\u2019s decision, how do I challenge that decision or correct the model when it\u2019s wrong about me? What should I change about me to get a better outcome in the future?\nThe utility of a model and associated explanations can be measured from several viewpoints. We focus on three: the model developer, the end user of the model, and \u201cimposed users\u201d (individuals or groups who are affected by the model\u2019s decisions, outcomes, or recommendations). The model developer may consider each of the first three contexts \u2014 Model Debugging and Validation; Model Selection; and Mental Model and Model Understanding \u2014 as they iterate in development. In contrast, end users will typically participate in Model Selection, Mental Model and Model Understanding, or Human Machine Teaming. Model Feedback, Challenging, and Prescription is of greatest interest to imposed users. They may also be interested in the Mental Model and Model Understanding use case. Figure 1 illustrates the overlap of relevant use cases across the three types of users. Our taxonomy is similar to that of Mohseni et al [32], although we distinguish imposed users from end users and do not distinguish end users from \u201cdata experts\u201d.\nIn addition to considering the above users and use cases, i.e., the context in which an explanation is used, we also need to design controlled experiments with falsifiable hypotheses. We believe the\n\u201cgold standard\u201d XAI evaluation experiment should be one where all participants perform the same task, but a randomly assigned group of participants performs the task without the assistance of the explanation being evaluated. By comparing the performance of groups with and without the explanation, we can make claims about the benefit of that explanation for the task. The hypothesis (that the explanation is useful) is falsified when there is no significant difference between these groups.\nNot all experiments in this research domain should follow this rigid experimental design, but we suggest following some basic guidelines. Researchers should first consider the three key components of their system: the human, the machine, and the explanation. Next, researchers should determine what can be measured (or is meaningful to measure) using different combinations of these components. Below are examples of four combinations of these components and how they provide us different information:\n\u2022 Human only: baseline performance of human at the task (the fully manual scenario)\n\u2022 Machine only: baseline performance of machine at the task (the fully automated scenario)\n\u2022 Human + Machine: baseline performance of the system when the user can rely on the machine learning output\n\u2022 Human + Machine + Explanation: the performance of the system when the explanation and machine learning output are available to the user\nThus, one should compare the performance of Human + Machine + Explanation group against Human + Machine group. The Human only and Machine only provide additional context for this comparison. For example, there may be a significant benefit of the explanation, but perhaps the Machine only performance is greatest, indicating that the system should be fully automated.\nOf course, researchers should use their best judgement about what combinations are meaningful or practical for their specific applications. For example, Yang et al. [49] designed an experiment in-line with this framework. However, the researchers decided not to measure the Human only condition because there was no reasonable expectation that unaided participants had the expertise to perform the task (identifying the species of a tree given a picture of a single leaf). The researchers decided to measure Human + Machine performance as a baseline instead, which they found closely aligned with Machine only performance, indicative of overtrust. In that study, the Human + Machine + Explanation performance was greater than both of the baselines, providing strong evidence of the benefit of the explanation.\nThe remainder of this section is organized around the use cases previously discussed and hypothetical downstream tasks for evaluation purposes. For each downstream task we describe a pseudoexperiment that is intended to provide inspiration for researchers who wish to conduct the scientifically rigorous research we have argued for in our position statement."
    },
    {
      "heading": "3.1 Model Debugging and Validation",
      "text": "Model Debugging and Validation is a developer-focused use case that leverages explanations as a means of improving the model. Here, explanations are used to provide insight into the mechanisms of complex \u201cblack box\u201d models, e.g., neural networks or other deep learning models [9], to identify flaws or biases in the algorithm or the training data that can be addressed in development. For example, a developer may use explanations of model decisions of varying confidence to probe whether the model relies strongly on non-actionable or biased features and identify constraints that are necessary to implement within the model (as such models should not be used in most, if not all, settings [46]).\nDownstream Task: Given an incorrect model decision and corresponding explanation, determine the reason the model made a mistake. A set of model mistakes are coded by the research team in order to establish a ground truth. In the case of a classification task, these coded mistakes may be an annotation of the image qualities that misled the model, e.g., occlusion of the subject, pixellation, and artifacts in the image as well as qualities of the model behavior, e.g., misplaced model attention, lack of training examples. Inter-rater reliability is established on the coded mistakes and the images are presented to users to determine if explanation methods are useful in helping users determining why a model made an error. Users\u2019 ability to correctly describe the reason for the model\u2019s mistakes are measured with and without the explanation.\nDownstream Task: Identify if the model will improve from additional training. Given three datasets, a machine learning model is trained on one and the user is presented with data in all three sets and the performance of the model, e.g. F-score on validation (from train) and the two test sets. The user is asked whether the performance on the test set would significantly improve if the model can include the second test set in its training. Both train and test examples are available to the user, and in the Human + Machine + Explanation condition, the user can view explanations of model decisions on the original train and test data. The ground truth is measurable because the difference in model performance on the smaller and larger training set are available. A variant of this experiment would ask the user to estimate the difference in performance. A challenge of this experimental design is the creation of datasets with suitable differences in performance.\nDownstream Task: Given a model that exploits artifacts or loopholes of the data, describe the model\u2019s behavior. Model \u201cintelligence\u201d has been reasonably challenged in recent years by the discovery of \u2018Clever Hans\u2019 behaviors. This reveals the model\u2019s reliance on features of the data that humans would consider unintuitive (such as source tags in images) and are threats to generalizability [30]. In this task, users explore the model explanation to describe how decisions are being made about classes within the data. Performance is measured by determining whether users are able to discover undesirable behaviors in the model\u2019s decisionmaking, such as identification of trains by spotting rails, boats just by identifying water, or wolves by focusing on snow [38]. A control condition, i.e. no explanation, is possible, but would require showing the user many correct and incorrect classifications to give the user an opportunity to understand the model based solely on behavior."
    },
    {
      "heading": "3.2 Model Selection",
      "text": "Our second use case, Model Selection, is typically the focus of trust and explanation analyses when considered together because it seeks to answer the intuitive question of \u201cdo I trust this model enough to use it?\u201d or \u201cdo I trust this model more than another, and thus should use it instead?\u201d Although we argue this is a narrow application of trust and explanations together, it is a common (and important) use case to consider. We note that the model selection use case is more complicated (and potentially problematic) when a second predictive or generative model is required to generate the explanations rather than using artifacts of or features extracted directly from the model under assessment (e.g. captions generated from image inputs and model decisions to explain model decisions).\nDownstream Task: Determine which model is better suited for a given task. In this task, users consider two models and either the decisions alone or the decisions alongside their accompanying explanations. Users\u2019 performance identifying which model performs better on an unseen test set across the two groups can be used to quantify the quality of the accompanying explanations \u2014 e.g., by measuring whether using the explanations to identify whether the model decision was right (or wrong) for the right (or wrong) reasons enabled users to better distinguish which model is best suited for the task.\nAs a variant, users may be asked to determine which of the models would best extend to a specific out-of-domain task, e.g., classifying foods after seeing classification examples of animals or classifying posts on Twitter after seeing classification examples using Reddit data. These tasks can intuitively be extended to an experiment ranking multiple models, all of which can be presented and paired with or without an explanation method and outcomes."
    },
    {
      "heading": "3.3 Mental Model and Model Understanding",
      "text": "The Mental Model and Model Understanding use case relates to whether a user builds an accurate mental model, that is, a mental model which functionally mirrors the overall behavior and decision making of the machine learning model. Visualizations are a popular type of explanation for building and eliciting mental models. Mental models are important for developers to understand their own models. They are also critical for end users who use machine learning to gain insight about a new domain or to complete a task. Accurate mental models can also be helpful for imposed users who are affected by model decisions, for example, if they are denied services because of the model\u2019s classification of their profile or history. These users may want to understand how the model makes decisions in order to set expectations of how the model will impact the user.\nThis use case differs from Model Selection or Model Debugging and Validation in that the focus shifts from explaining specific decisions to comprehending the relationships between the model input and output, or understanding the inner workings of a model\u2019s decision-making. This use case is challenging because there are no ways to directly observe a user\u2019s mental model, and appropriate metrics for measuring understanding are still widely debated [45].\nDownstream Task: Given examples of past behaviors, extrapolate what a model will do given unseen inputs. In this task, users are given a series of inputs, and the corresponding model outputs. Then, users are presented with a series of previously unseen inputs. A randomly assigned subset, representing the Human + Machine + Explanation condition, are given the corresponding explanations. The control group, representing the Human + Machine condition, is not provided the explanations. Users are then asked to either select from a list, or describe in their own words, their expectation of model output. More accurate user predictions of the model output for the altered input provides evidence of the quality of the mental model built with the support of the explanation.\nSome variations of this task include using different types of input for the user to base their extrapolations on. For instance, the set of inputs may consist of all new inputs, none ever seen in the initial series of inputs with associated outputs, or a mix of inputs seen before and new inputs. Ribiero et al. [39] used all new inputs when testing a tool which presents a visual summary of why the model made a specific classification. In their user study, users were asked to predict a model\u2019s output first without an explanation, then presented with a set of decisions with explanation and finally asked to perform one more round of predicting the model\u2019s outputs.\nAnother variation could be in the temporal dimension of extrapolation. For a model such as an image classifier, any input and output can happen in any order. However, with a model such as a reinforcement learning agent, the task could consist of predicting the agent\u2019s immediate next move, or any number of time steps in the future, as explored by Anderson et al. [3]. This type of variation would explore if a user\u2019s mental model is accurate enough to predict the model\u2019s future choices, and how far into the future that mental model accurately extends.\nDownstream Task: Given information about a model\u2019s past performance, match the model to a novel output. When the context of decisions belonging only to a specified model or models is explicitly set, as in the above described task, users may overestimate how well they understand a model. In this task, user groups have that narrowed context removed, and are asked to differentiate between multiple models. User groups are given a series of input-output pairs for a set of models during a training phase. It is specified which model created each output to help users build mental models. After the training phase, users are presented with a new, previously unseen set of inputs and corresponding model outputs. Users are asked to match the models to the new decisions, or to identify if none of the previously presented models would have produced the given decision. This task is repeated including explanations for the decisions made by the model.\nThe Human + Machine condition would be represented by a user group receiving no additional explanation of the model\u2019s outputs during the process of building a mental model. The Human + Machine + Explanation condition would be represented by a group of users who get an explanation of each model\u2019s outputs in the mental model building phase. Any improvement in ability to correctly correspond models to the new decisions would signal that explanations help users to better understand the model.\nOne variant of this experiment would be to test users\u2019 understanding of a single model by purposely altering the output of the model and testing if users can identify if the output is incorrect, and what part of the output is incorrect. Chang et al. [10] use this variation in a user study of a model which sorted documents into topics, which are defined by a set of words. Users were tested to see if they could detect manipulation by researchers of both the words describing a topic, and the topic assigned to a document.\nAnother variant of this task would be to change the training process, so that instead of using the input-output pairs, the users would be given a global explanation or description of each model. For example, users may be told that an image classifier was trained on a specific data set of birds, and has been observed to rely heavily on beak shape and size in its classifications.\nDownstream Task: Given a model\u2019s past behavior, identify if explanations speed up user\u2019s creation of a mental model. This task consists of selecting the expected model output given a previously unseen input (evaluation phase) after having studied examples of inputs and the corresponding models outputs (learning phase). Users in the Human + Machine + Explanation group are also provided an explanation corresponding to each model input-output pair. Users are informed that they will be timed and can proceed\nthrough as many sets of inputs and outputs as they want. Users can switch to the evaluation phase at anytime where real-time feedback is given on whether the user chose the right output. The user is free to move back to the learning phase to study more sets of known inputs and outputs before returning again to the evaluation phase. The total time spent, number of cycles, or accuracy in predicting the model behavior can be compared across groups to determine the benefit of the explanation. In a variation used by Lim et al. [31], a set number of examples are presented in a learning phase and users are timed according to how long they spend in the learning phase. Once users move on to the evaluation phase, they cannot return to the learning phase, and the time taken to answer each question in the task phase is also recorded. Lim et al. compared groups of users receiving explanations during the learning phase to groups getting no explanations. Alternatively, this task can be used to compare the efficacy of different explanations, which could be imperative in safety-critical environments."
    },
    {
      "heading": "3.4 Human Machine Teaming",
      "text": "Human Machine Teaming distinguishes models as teammates, beyond simple tools. Here, high-quality explanations can elevate models to act as teammates by providing more insightful and actionable recommendations. Essentially, good explanations can serve as the models response to \u201cexplain your work\u201d or \u201cwhy?\u201d queries and assist users in complicated tasks or alleviate the cognitive load of human teammates. As human machine teaming necessarily generates more specific use cases than the tasks enumerated in previous sections, this section utilizes more specific use cases, that can of course be generalized to other domains.\nDownstream Task: Identify whether users should accept or reject model decisions. Given a series of inputs, users are tasked with agreeing or disagreeing with a model\u2019s output, accompanied by an explanation of interest for a subset of user groups. Performance in this case is measured based on the accuracy of final decisions selected by the user, which is either the model decision when users agree with the decision or the user-selected decision when users disagree with the model. This task effectively calibrates the users\u2019 appropriate trust in the model. Performance of the Human + Machine + Explanation group is compared directly to the Human + Machine group. This experiment requires that the Human only performance be less than the Machine only performance to show a benefit, such as classifying tree leaves in images [49] (instead of everyday objects) or performing quality control in an assembly line scenario [5].\nAn extension to this task can consider reduction of the cognitive load for users as the desired outcome and user performance can also be measured using proxies for cognitive load such as the number of correct judgements made by the user in a limited time period or the time needed to complete a specified number of correct judgements.\nDownstream Task: Determine whether properly abstracted explanations improve human experience and performance in an autonomous driving scenario. One of the challenges when designing model explanations lies in understanding which end user the explanation is being designed for. For example, levels of abstraction change drastically if explanations are designed to target the software engineer responsible for autonomous navigation and collision avoidance rather than the driver sitting behind the wheel. In this task, user groups driving simulated autonomous vehicles would be provided with simplistic explanations of the car\u2019s behavior as a driving test takes place such as warnings about poor object detection in fog or reduced traction in sharp curves. These explanations would be given when environmental dangers are encountered during the simulation. A control group would receive no explanations. Performance can then be measured based on the\nusers\u2019 situational awareness (SAGAT, etc. [16]), attention switching from the road to the explanations, trust questionnaires, and other physiological indicators (e.g., heart rate, eye motion).\nAn extension to this task can present the explanations of the car\u2019s behavior in a \u201cuser manual\u201d style before the driving test begins and measure how often the drivers accurately respond to hazardous conditions with no real-time input.\nDownstream Task: Determine whether explanations increase the efficiency of a human machine team. Healthcare has been a focus of recent work in human machine teaming, and serves as an exemplary application domain. Studies have shown the effectiveness of explanations on both trust and interpretability in ML models focused on medical diagnosis [14]. As the availability and complexity of medical technology increases, physicians may find themselves in need of machine agents who can help them narrow in on useful treatments and diagnoses. In this task, the user works with an automated physician\u2019s assistant who makes recommendations for data collection, testing, and diagnoses during a physician-patient interaction.\nThe model presents choices to the physician with accompanying explanations of these choices in the Human + Machine + Explanation case, such as visuals of specific patient data and its risk contribution for certain diagnoses. This can present the issue of branching, where presented choices may generate a longer path to the end goal with the potential of detours that do not offer viable paths to the solution. Two control groups exist for this task, that being Human + Machine groups with no generated explanations of proffered choices and Human only groups which receive no model assistance. How long it takes user groups to arrive at the correct diagnosis, the cost of that treatment, and how many incorrect branches were explored are all viable performance metrics to establish whether the explanations of model benefited users. As this task requires trained experts, i.e. physicians, the Human only baseline is meaningful.\nA wide array of extended tasks can be generated from this initial example. Time limits can be imposed on the entire task to consider cognitive load on the users, and to examine whether or not user groups are able to finish the task at all. The use case can also be extended into the work allocation domain, wherein the model recommends actions and gives explanations (or does not) that have cascading consequences on the subsequent work tasks, creating a dynamic environment that is shaped by the human-machine team and which can end up in any number of end states with measurable utilities. Additional physiological measures, post-experiment questionnaires, and human factors metrics (e.g. situational awareness or perceived cognitive load) can then be applied to understand the usefulness of these model explanations along different dimensions."
    },
    {
      "heading": "3.5 Model Feedback, Challenging, and Prescription",
      "text": "The Model Feedback, Challenging, and Prescription use case effectively considers trust in the system, i.e., the model in context of the impact on imposed users of its decisions and subsequent recommendations or actions taken. The need for effective explanation of model decisions for recourse is a natural response to the continued widespread application of artificial intelligence or machine learning models to supplement or automate tasks in domains where incorrect or biased recommendations can have significant human impacts. These domains include predictive policing [17], recidivism prediction [12, 15], and hate speech and abusive language identification online [13, 36, 40]. As an example of the recognized necessity of clear explanations for this use case, the European Union\u2019s GDPR directly addresses the \u201cright of citizens to receive an explanation for algorithmic decisions\u201d [18].\nDownstream Task: Given a model\u2019s decision, identify how to get a better outcome.\nIn this task, a user may be given an input and a decision from a model and asked to identify what has to be changed or updated in the input to get a better decision. User groups are either given an accompanying explanation for the decision of the unaltered input as well as or only the model output. This task touches on counterfactual explanations and the prescriptive use of algorithmic decision making systems. The \u201cWhat if tool\u201d [47] supports this type of counterfactual reasoning. However, the tool is oriented towards end users and developers \u2014 there is still a research opportunity to design explanations that support imposed users for this use case.\nGiven the model is trustworthy, providing the right outcome given the right data, but the user desires a different outcome, does the model explanation provide enough information for a user to identify the personal changes needed to obtain the desired outcome? GlassBox [44] is an example of such a downstream task in practice \u2013 users, given a pre-established persona, probe a loan application system that provides contrastive, counterfactual explanations to understand and challenge the model\u2019s automated decisions.\nDownstream Task: Given a model\u2019s decision, determine if the decision was based on incorrect data, biased data, or bad inference. User groups are provided an overview of the training examples for a given model and a series of input-output pairs where the model output was incorrect. Users are then asked to identify why the model provided an incorrect response or what methods might be employed to correct the decision (e.g., more training examples for edge-case inputs, removal of misleading or biased training examples, or removal or preprocessing of flawed inputs) and whether the change required is a reasonable expectation (reduce the level of outstanding credit to receive a new load) or an indication of bias (change your gender/race to receive a new loan). A control group (Human + Machine) will not receive model explanations, but would have access to the training dataset in order to contrast performance with the Human + Machine + Explanation group to examine the benefits of the explanations. More accurate user predictions of the model output for the altered input provides a quantitative measure of the quality of the explanation to identify biases in the model \u2013 whether the model is fair across the affected population \u2013 and also as a means to identify challenge-worthy individual decisions."
    },
    {
      "heading": "4 CONCLUSIONS",
      "text": "In this paper we have argued that trust in a machine learning model is a benefit of a useful and reliable system that employs that model. However, trust develops slowly over time, and to rely on trust as a metric for evaluating the value of an explanation is problematic and could lead to artificially inflated levels of trust to the users\u2019 detriment. We believe trust should only be measured in a longitudinal and empirical study considering the full system.\nInstead, researchers should design for and measure utility. Utilityoriented evaluation encourages researchers to consider the broader context of the explanation, i.e., how it is intended to be used. It also encourages researchers to employ scientific methodologies to evaluate explanations, leveraging falsifiable hypotheses and objectively measurable quantities as evidence. Towards this end, we have suggested many pseudo-experimental designs involving \u201cdownstream tasks\u201d that can be used to evaluate explanations in this manner. We hope the impact of this work will be to inspire many new experiments that solidify the scientific foundation relating humans, machines, and explanations."
    },
    {
      "heading": "ACKNOWLEDGMENTS",
      "text": "The authors wish to thank Matthew Taylor for helpful discussions around downstream tasks in reinforcement learning."
    }
  ],
  "title": "Measure Utility, Gain Trust: Practical Advice for XAI Researchers",
  "year": 2020
}
