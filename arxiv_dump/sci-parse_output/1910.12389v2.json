{
  "abstractText": "Variable selection is of significant importance for classification and regression tasks in machine learning and statistical applications where both predictability and explainability are needed. In this paper, a Copula Entropy (CE) based method for variable selection which use CE based ranks to select variables is proposed. The method is both model-free and tuning-free. Comparison experiments between the proposed method and traditional variable selection methods, such as Distance Correlation, Hilbert-Schmidt Independence Criterion, Stepwise Selection, regularized generalized linear models and Adaptive LASSO, were conducted on the UCI heart disease data. Experimental results show that CE based method can select the \u2018right\u2019 variables out more effectively and derive better interpretable results than traditional methods do without sacrificing accuracy performance. It is believed that CE based variable selection can help to build more explainable models.",
  "authors": [
    {
      "affiliations": [],
      "name": "Jian MA"
    }
  ],
  "id": "SP:b156d2b962261fabacd1124209e77a8c1e7e930f",
  "references": [
    {
      "authors": [
        "I Guyon",
        "A. Elisseeff"
      ],
      "title": "An introduction to variable and feature selection [J",
      "venue": "Journal of Machine Learning Research,",
      "year": 2003
    },
    {
      "authors": [
        "I. George E"
      ],
      "title": "The variable selection problem [J",
      "venue": "Journal of the American Statistical Association,",
      "year": 2000
    },
    {
      "authors": [
        "M Cover T",
        "A. Thomas J"
      ],
      "title": "Elements of information theory [M",
      "year": 2012
    },
    {
      "authors": [
        "Sz\u00e9kely",
        "G\u00e1bor J",
        "Maria L. Rizzo",
        "Nail K. Bakirov"
      ],
      "title": "Measuring and testing dependence by correlation of distances [J",
      "venue": "The Annals of Statistics,",
      "year": 2007
    },
    {
      "authors": [
        "Sz\u00e9kely",
        "G\u00e1bor J",
        "Maria L. Rizzo"
      ],
      "title": "Brownian distance covariance [J",
      "venue": "The Annals of Applied Statistics,",
      "year": 2009
    },
    {
      "authors": [
        "Li",
        "Runze",
        "Wei Zhong",
        "Liping Zhu"
      ],
      "title": "Feature screening via distance correlation learning [J",
      "venue": "Journal of the American Statistical Association,",
      "year": 2012
    },
    {
      "authors": [
        "Gretton",
        "Arthur"
      ],
      "title": "A Kernel Statistical Test of Independence",
      "venue": "Advances in Neural Information Processing Systems,",
      "year": 2007
    },
    {
      "authors": [
        "Pfister",
        "Niklas"
      ],
      "title": "Kernelbased tests for joint independence [J",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "year": 2018
    },
    {
      "authors": [
        "Song",
        "Le"
      ],
      "title": "Feature selection via dependence maximization [J",
      "venue": "Journal of Machine Learning Research,",
      "year": 2012
    },
    {
      "authors": [
        "H. Akaike"
      ],
      "title": "A new look at the statistical model identification [J",
      "venue": "IEEE Transactions on Automatic Control,",
      "year": 1974
    },
    {
      "authors": [
        "G. Schwarz"
      ],
      "title": "Estimating the dimension of a model [J",
      "venue": "The Annals of Statistics,",
      "year": 1978
    },
    {
      "authors": [
        "E Hoerl A",
        "W. Kennard R"
      ],
      "title": "Ridge regression: Biased estimation for nonorthogonal problems",
      "venue": "[J]. Technometrics,",
      "year": 1970
    },
    {
      "authors": [
        "R. Tibshirani"
      ],
      "title": "Regression shrinkage and selection via the lasso [J",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "year": 1996
    },
    {
      "authors": [
        "H Zou",
        "T. Hastie"
      ],
      "title": "Regularization and variable selection via the elastic net [J",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "year": 2005
    },
    {
      "authors": [
        "J Fan",
        "J. Lv"
      ],
      "title": "A selective overview of variable selection in high dimensional feature space [J",
      "venue": "Statistica Sinica,",
      "year": 2010
    },
    {
      "authors": [
        "C Wu",
        "S. Ma"
      ],
      "title": "A selective review of robust variable selection with applications in bioinformatics [J",
      "venue": "Briefings in bioinformatics,",
      "year": 2014
    },
    {
      "authors": [
        "J Fan",
        "R. Li"
      ],
      "title": "Variable selection via nonconcave penalized likelihood and its oracle properties [J",
      "venue": "Journal of the American Statistical Association,",
      "year": 2001
    },
    {
      "authors": [
        "H. Zou"
      ],
      "title": "The adaptive lasso and its oracle properties [J",
      "venue": "Journal of the American Statistical Association,",
      "year": 2006
    },
    {
      "authors": [
        "J Dziak J",
        "L Coffman D",
        "T Lanza S"
      ],
      "title": "Sensitivity and specificity of information criteria [J",
      "venue": "PeerJ PrePrints,",
      "year": 2017
    },
    {
      "authors": [
        "B\u00f6ttcher"
      ],
      "title": "Copula versions of distance multivariance and dHSIC via the distributional transform \u2013 a general approach to construct invariant dependence measures",
      "year": 2020
    },
    {
      "authors": [
        "J Ma",
        "Z. Sun"
      ],
      "title": "Mutual information is copula entropy [J",
      "venue": "Tsinghua Science & Technology,",
      "year": 2011
    },
    {
      "authors": [
        "B. Nelsen R"
      ],
      "title": "An introduction to copulas [M",
      "venue": "New York: Springer,",
      "year": 2007
    },
    {
      "authors": [
        "H. Joe"
      ],
      "title": "Dependence modeling with copulas [M",
      "venue": "London: Chapman and Hall/CRC,",
      "year": 2014
    },
    {
      "authors": [
        "M. Sklar"
      ],
      "title": "Fonctions de repartition an dimensions et leurs marges [J",
      "venue": "Publications de l\u2019Institut de statistique de l\u2019Universite\u0301 de Paris,",
      "year": 1959
    },
    {
      "authors": [
        "A Kraskov",
        "H St\u00f6gbauer",
        "P. Grassberger"
      ],
      "title": "Estimating mutual information [J",
      "venue": "Physical Review E,",
      "year": 2004
    },
    {
      "authors": [
        "J Nahar",
        "T Imam",
        "S Tickle K"
      ],
      "title": "Computational intelligence for heart disease diagnosis: A medical knowledge driven approach [J",
      "venue": "Expert Systems with Applications,",
      "year": 2013
    },
    {
      "authors": [
        "J Smola A",
        "B. Sch\u00f6lkopf"
      ],
      "title": "A tutorial on support vector regression [J",
      "venue": "Statistics and Computing,",
      "year": 2004
    },
    {
      "authors": [
        "Chang C.-C",
        "Lin C.-J"
      ],
      "title": "LIBSVM: A library for support vector machines [J",
      "venue": "ACM Transactions on Intelligent Systems and Technology,",
      "year": 2011
    },
    {
      "authors": [
        "J Friedman",
        "T Hastie",
        "R. Tibshirani"
      ],
      "title": "Regularization paths for generalized linear models via coordinate descent [J",
      "venue": "Journal of Statistical Software,",
      "year": 2010
    },
    {
      "authors": [
        "N Kraemer",
        "J Schaefer",
        "L. Boulesteix A"
      ],
      "title": "Regularized estimation of largescale gene regulatory networks using gaussian graphical models [J",
      "venue": "BMC Bioinformatics,",
      "year": 2009
    },
    {
      "authors": [
        "J. Ma"
      ],
      "title": "Discovering Association with Copula Entropy",
      "venue": "arXiv preprint arXiv:1907.12268,",
      "year": 1907
    }
  ],
  "sections": [
    {
      "text": "Keywords: Copula Entropy; Variable Selection; Distance Correlation; HilbertSchmidt Independence Criterion; LASSO; Ridge Regression; Elastic Net; Adaptive LASSO; AIC; BIC; Explainability\n\u2217Email: majian@hitachi.cn\nar X\niv :1\n91 0.\n12 38\n9v 2\n[ cs\n.L G"
    },
    {
      "heading": "1 Introduction",
      "text": ""
    },
    {
      "heading": "1.1 Variable Selection Problem",
      "text": "Variable selection is one of the old and widely studied model selection problems in statistics and machine learning [1, 2]. The problem arises when one wants to model the relationship between a variable of interest (response) and a (large) amount of potential explainable variables (predictors) but only a subset of latter explainable variables may be relevant to the former variable. The aim of the problem is to select a subset of variables under certain criteria. The criteria of selection are variables ability of prediction and interpretation."
    },
    {
      "heading": "1.2 Existing Methods",
      "text": ""
    },
    {
      "heading": "1.2.1 Measure based Variable Selection",
      "text": "The most natural way of variable selection is based on statistical association measures between response and individual predictor. Due to its simplicity and interpretability, measure based selection enjoys widely adoption and empirical successes in practice. The most common traditional measure is Pearson Correlation Coefficient (CC) for linear models. However, the application of CC assumes Gassianity, which is unrealistic for most non-linear and non-Gaussian cases. For nonlinear dependence measure, some may considered Mutual Information (MI) in information theory [3], but only a few applications are reported due to notorious difficulty of estimating MI. Besides MI, several other nonlinear dependence measures were proposed.\nDistance Correlation (dCor) is a nonlinear generalization of traditional correlation concept proposed by Sze\u0301kely, et al [4, 5]. It generalizes bivariate second-order correlation to multivariate nonlinear cases via distance covariance. dCor between random vectors X and Y is defined as\ndCor(X, Y ) = \u03bd2(X, Y )\u221a \u03bd2(X)\u03bd2(Y ) , (1)\nwhere \u03bd2(X, Y ) is distance covariance defined with characteristic function f as\n\u03bd2(X, Y ;w) = \u2016fX,Y (t, s)\u2212 fX(t)fY (s)\u20162w. (2)\nHere, \u2016 \u00b7 \u2016w is the norm in the weighted L2 function space defined with positive weight function w(\u00b7, \u00b7) [4, 5]. dCor characterizes independence:\ndCor(X, Y ) \u2265 0, and dCor(X, Y ) = 0 if and only if X, Y are independent. dCor has been proposed as a tool for variable selection [6].\nHilbert-Schmidt Independence Criterion (HSIC) is another widely studied independence measure [7] and it has multivariate version \u2013 d-variable HSIC (dHSIC) [8]. dHSIC defines a nonlinear dependence measure in Reproducing Kernel Hilbert Spaces (RKHS) with kernel function, as follows:\ndHSIC(P (X1, \u00b7 \u00b7 \u00b7 , Xd)) = \u2016\u03a0(P (X1)\u2297, \u00b7 \u00b7 \u00b7 ,\u2297P (Xd))\u2212\u03a0(P (X1, \u00b7 \u00b7 \u00b7 , Xd))\u2016, (3) where \u03a0 is kernel mean embedding function, and \u2297 is tensor products of kernels. dHSIC can be considered as the distance in RKHS between the embeddings of joint distribution and margins. dHSIC also characterizes independence: dHSIC(P (X1, . . . , Xd)) = 0 if and only if X1, . . . , Xd are independent so it is natural to apply it to variable selection problem [9]."
    },
    {
      "heading": "1.2.2 Stepwise Selection with Information Criteria",
      "text": "Stepwise Selection is a standard approach for variable selection, usually on linear regression models, which sequentially selecting or eliminating the predictors once at a time based on certain criteria. Two of the main criteria are Akaike Information Criterion (AIC) [10] and Bayesian Information Criterion (BIC) [11]. Let l denotes the log maximum likelihood of the model, p denotes the number of free parameters of the model and N denotes the number of observation, and then AIC is defined as\nAIC = \u22122l + 2p, (4)\nand BIC is defined as BIC = \u22122l + p logN. (5)\nIt can be learned from the definitions that both criteria are defined as penalized likelihood criteria that try to achieve a balance between goodness of fit (likelihood) and penalty on overfitting (number of free parameters) for model selection problem."
    },
    {
      "heading": "1.2.3 Regularized Generalized Linear Models",
      "text": "Linear Regression (LR) is the commonly used model in most of the researches that study the relationships under the assumption of linearity. Generalized\nLinear Models (GLMs) are a group of variants of LR, such as Logistic Regression and Poisson Regression, which introduce nonlinear response by means of link function. Due to their poor ability of variable selection, LR or GLMs are not applied directed to many cases for model selection, especially to high dimensional problems. To tackle this issue, regularization techniques are introduced to formalize a new learning problem from GLM problems under the assumption of sparsity. Let y denotes response, X denote predictors, and \u03b2 denote coefficients to be estimated, and then regularized GLMs solve the following problem:\nmin \u03b2 {L(\u03b2; y,X) + \u03bb1\u2016\u03b2\u20161 + \u03bb2\u2016\u03b2\u201622}, (6)\nwhere L(\u00b7) denotes likelihood function, \u2016\u03b2\u2016i denote ith norm of coefficients \u03b2, and \u03bbi denote tuning parameters.\nThree main variants of this regularized problem are defined by tuning the parameters \u03bb1, \u03bb2. The problem for the case where \u03bb1 = 0, \u03bb2 > 0 is called Ridge Regression [12]. The Least Absolute Shrinkage and Selection Operator (LASSO) problem corresponds to the cases where \u03bb1 > 0, \u03bb2 = 0 [13]. The problem for \u03bb1, \u03bb2 > 0 is called Elastic Net [14]. For more variants of regularized GLMs, please refer to [15, 16] and references therein.\nRegularized GLMs select a subset of variable by means of shrinking the non-zero coefficients of variables. Suppose the \u2018true\u2019 model has a group of sparse coefficients, an estimator is said to has oracle property if it can estimate this coefficients effectively and asymptotically [17]. Zou [18] has shown that the LASSO estimator has no oracle property. To address this issue, he proposed the Adaptive LASSO with the adaptive weights technique and demontrated its oracle property under regularity condtions [18]."
    },
    {
      "heading": "1.3 Limitations of the Existing Methods",
      "text": "When using Stepwise Selection with different criteria, one may confuse which criterion is fitful to the given problem since different criteria derive different learning models. Comparison between the definition of AIC (4) and BIC (5) shows that BIC penalize the number of free parameter much more than AIC does. Therefore, it is generally believed that AIC tends to overfitting while BIC tends to underfitting [19].\nThe existing regularized GLMs have their limitations for variable selection as well. As pointed out in [14], Ridge Regression cannot produce parsimonious models. When there are a group of correlated predictors, LASSO tends\nto select only one from the group [14], and to include many false positive variable into models while Elastic Net tends to select the variable group in or out together [15]. The oracle property of Adaptive LASSO is only attached to GLMs under certain restrictive regularity condition.\nThe ultimate evaluation criterion for variable selection methods is whether they can discover the right variables for the response and hence derive an interpretable model for the given problem with good prediction performance. In many domains, the interpretablity of models is a much desired merit than the predictability of models. In this sense, the above existing methods cannot meet the requirement.\nCopula Entropy (CE) is a recently introduced multivariate statistical independence measure [21] (more details in Section 2). It is defined rigorously with copula function and was proved to be equivalent to MI. However, it has not been applied to variable selection problem.\nIn Section 1.2.1, two measures for statistical independence (dCor and dHSIC) are introduced. CE, dCor and dHSIC are all promising tools for variable selection. Recently, Bo\u0308ttcher defined copula versions of dCor and dHSIC [20]. However, no empirical comparison between these three measures has been done yet.\nIn this paper, a variable selection method based on CE [21], which can select the \u2018right\u2019 variables for explainable models without sacrificing prediction ability, is proposed. Due to CE, the proposed method is theoretically sound and computationally efficient. The proposed method is evaluated on a biomedical dataset, and compared with the existing methods on variable selection."
    },
    {
      "heading": "2 Copula Entropy",
      "text": ""
    },
    {
      "heading": "2.1 Theory",
      "text": "Copula theory is about the representation of multivariate dependence with copula function [22, 23]. At the core of copula theory is Sklar theorem [24] which states that multivariate probability density function can be represented as a product of its marginals and copula density function which represents dependence structure among random variables. Such representation seperates dependence structure, i.e., copula function, with the properties of individual variables \u2013 marginals, which make it possible to deal with dependence struc-\nture only regardless of joint distribution and marginal distribution. This section is to define an statistical independence measure with copula. For clarity, please refer to [21] for notations.\nWith copula density, Copula Entropy is define as follows [21]:\nDefinition 1 (Copula Entropy). Let X be random variables with marginal distributions u and copula density c(u). CE of X is defined as\nHc(X) = \u2212 \u222b u c(u) log c(u)du. (7)\nIn information theory, MI and entropy are two different concepts [3]. In [21], Ma and Sun proved that they are essentially same \u2013 MI is also a kind of entropy, negative CE, which is stated as follows:\nTheorem 1. MI of random variables is equivalent to negative CE:\nI(X) = \u2212Hc(X). (8)\nThe proof of Theorem 1 is simple [21]. There is also an instant corollary (Corollary 1) on the relationship between information of joint probability density function, marginal density function and copula density function.\nCorollary 1.\nH(X) = \u2211 i H(Xi) +Hc(X). (9)\nThe above results cast insight into the relationship between entropy, MI, and copula through CE, and therefore build a bridge between information theory and copula theory. CE itself provides a mathematical theory of statistical independence measure."
    },
    {
      "heading": "2.2 Estimation",
      "text": "It has been widely considered that estimating MI is notoriously difficult. Under the blessing of Theorem 1, Ma and Sun [21] proposed a simple and elegant non-parametric method for estimating CE (MI) from data which comprises of only two steps\u2217:\n\u2217The R package copent for estimating CE is available on CRAN and also on GitHub at https://github.com/majianthu/copent.\n1. Estimating Empirical Copula Density (ECD);\n2. Estimating CE.\nFor Step 1, if given data samples {x1, . . . ,xT} i.i.d. generated from random variables X = {x1, . . . , xN}T , one can easily estimate ECD as follows:\nFi(xi) = 1\nT T\u2211 t=1 \u03c7(xit \u2264 xi), (10)\nwhere i = 1, . . . , N and \u03c7 represents for indicator function. Let u = [F1, . . . , FN ], and then one can derives a new samples set {u1, . . . ,uT} as data from ECD c(u). In practice, Step 1 can be easily implemented non-parametrically with rank statistics.\nOnce ECD is estimated, Step 2 is essentially a problem of entropy estimation which has been contributed with many existing methods. Among them, the kNN method [25] was suggested in [21]. With rank statistic and kNN methods, one can derive a non-parametric method of estimating CE, which can be applied to any situation without any assumptions on the underlying system."
    },
    {
      "heading": "3 CE based Variable Selection",
      "text": "In this section, we propose a new variable selection method based on CE. It is a new kind of association measure based method. The idea is simple: the CE between response and predictors are estimated from data, and then predictors are selected according to the value of CE. With the selected variables, a model for the prediction problem is built. In the method, CE is estimated nonparametrically with the method in Section 2.2.\nSince CE has many advantages over traditional association measure CC, it is quite obvious that the new method is superior to CC based method. The new method closely related to previously proposed MI based method but CE has more clear mathematical meaning for all the multivariate cases and is estimated in a new non-parametric way which makes the method efficient, stable and universally applicable. Since CE is defined as a distribution-free measure, the proposed method based on it is therefore model-free.\nAnother advantage of the new method is that the variable such selected can be attached with biological and physical meaning since CE, as a type\nof entropy, measures not only statistical dependence between variable and response, but also information transmission or energy exchange in the underlying systems. If a group of variables are selected by the proposed method, it is supposed to correspond to physical, biological or social meanings in the given system. As contrast, traditional methods have no such merit."
    },
    {
      "heading": "4 Experiments and Results",
      "text": ""
    },
    {
      "heading": "4.1 Data",
      "text": "The heart disease dataset in the UCI machine learning repository [26] is used in our experiments, which contains 4 databases about heart disease diagnosis collected from four locations. The dataset includes 920 samples totally, of which only 899 samples without missing values are used in the experiments. All the dataset have the same instance format with 76 raw attributes, of which the attribute \u2018num\u2019 is the diagnosis of patients disease. In the past research, only 14 attributes are recommended by researchers for clinical use, as listed in Table 1 [27]."
    },
    {
      "heading": "4.2 Evaluation Criteria",
      "text": "To evaluate the variable selection methods, there are two criteria: predictability and interpretability. In our experiments, on one side, we suggest to test the prediction accuracy of the prediction models building on the selected variables. On the other side, we will try to check the explainability of the selected variable with reference to the established domain knowledge."
    },
    {
      "heading": "4.3 Experiments",
      "text": "We conducted 10 experiments on heart disease dataset to compare CE based method with other related methods. The goal of the experiments is to predict the diagnosis from other variables. In all experiments, both training data and test data were the whole dataset since we only investigate the variable selection ability of the methods and do not want to verify the generalizability of the models. The first experiment provides the baseline, in which 13 recommended variables were used and a SVM [28] classifier was trained on these variables. In the following three experiment, a group of variables are selected with the CE, dCor, and dHSIC based method, and then a SVM classifier is trained on such selected variables. The next two experiment is on stepwise selection on GLM with AIC and BIC. Since the response (the \u2018num\u2019 attribute of heart disease dataset) has 5 levels of value, the GLMs in all the experiments are set as Poisson Regression model with the \u2018log\u2019 link function. Stepwise selection is set as a \u2018backward\u2019 one under the guidance of AIC and BIC. The next 3 experiments are on regularized GLMs, including LASSO, Ridge Regression, and Elastic Net with \u03bb1, \u03bb2 = 0.5. For each regularized GLM, the best amount of shrinkage is determined with 10-fold cross validation. The last experiment is on Adaptive LASSO with 10-fold cross validation to test its oracle property. The R packages \u2018copent\u2019 [29], \u2018energy\u2019 [6], \u2018dHSIC\u2019 [9], \u2018e1071\u2019 [30], \u2018glmnet\u2019 [31], and \u2018parcor\u2019 [18, 32] were used for CE, dCor, dHSIC, SVM, regularized GLMs and Adaptive LASSO in the experiments respectively. The default values of the parameters are used for the functions of CE, dCor, and dHSIC in the corresponding packages."
    },
    {
      "heading": "4.4 Results",
      "text": "The prediction accuracy of the models are listed in Table 2. It can be learned that the SVM classifier with variables selected by CE presents the best result (762 out of 899), better (5 more correct prediction, more than 0.56% improvement) than SVM with the recommended variables. The other two dependence measure based method present comparable results. The remaining methods show only moderate prediction accuracy. Clearly, CE base variable selection improves the performance of prediction.\nRegularized GLMs select variables by non-zero coefficents. The coefficients of Regularized GLMs which are used in the prediction task are shown in Figure 1. For multinomial logistic regression, there are 5 groups of coeffi-\ncients for 5 levels of response. We take the mean of 5 groups of coefficients as the overall coefficients. They indicate the relative importance of the variables in each model. The variables corresponding to non-zero coefficients are considered as selected by the GLMs.\nVariables selected by the three dependence measures based method are shown in Figure 2. The dependence strength of \u2018fbs\u2019 (fasting blood sugar, #16) is used as the thresold for selection of all the three methods. The variables selected based on ranks of CE, dCor, and dHSIC, are listed in Table 3.\nTo check the interpretability of the selected variables of different methods, the recommended variables are taken as a golden rule for heart disease diagnosis since they are recommended by perfessional researchers as clinical relevant [27]. The variables selected by different methods are summarized in Table 3. We compare the selected variables in each experiment with the recommended set. It can be learned from Table 3 that CE based method selects 11 out of 13 recommended variables, dCor and dHSIC select 9 and 10 out of 13 variables respectively, and meanwhile CE selects a smaller variable set with fewer false positive variables than the other two measures do which means CE has higher selection accuracy. As contrast, experimental results show that regularized GLMs fail to select the recommended variables out of others and Adaptive LASSO selects 4 out of 13 variables. Meanwhile, Stepwise GLMs with AIC and BIC select 8 and 5 out of 13 variables respectively.\nIt should be mentioned that and the variables (#59-68) corresponding to the properties of vessels are also selected together, all with high CE value, which is meaningful and deserves further investigation in clinical practice. However, stepwise GLM does not select them all out. The results demonstate strong ability of CE based method on selecting meaningful variables against its compititors."
    },
    {
      "heading": "5 Discussion",
      "text": "In the above experiments, the regularized GLMs select the variables based on the coefficients of the models. To achieve the goal, the methods should set up the models manually first, and then one has to tune both penalty parameter \u03bbi and shrinkage amount during model training. This means making assumptions of the model of the underlying system, including sparsity of coefficients and specific nonlinearity, which are usually incorrect. Even though the free parameters has been tuned to optimal, experimental results show that regularized GLM failed to select the recommanded variables out and meanwhile they presented moderate prediction accuracy. Stepwise GLM do better than regularized GLMs on variable selection but presented poor prediction performance.\nCompared with them, CE based method is both model-free and tuning-\nfree. It does nothing on model setting, tunes no parameter, and yet presents good performance on both prediction accuracy and variable selection. This is because that CE is a distribution-free measure of statistical indepedence and that its estimation is done in a non-parametric way. When applied, it makes no assumption on the underlying systems. With CE, variable selection is becoming a science with unversally applicable theory and efficient method, instead of an art like regularized GLMs and other information criteria.\nIn the experiments, dCor and dHSIC also presents comparable results and are also model-free and almost tuning-free. However, CE presents better results on both prediction accuracy and selection efficiency than dCor and dHSIC do. The good performance of CE over dCor and dHSIC can be explained theoretically. Though all the three measures characterize multivariate independence, CE has a much rigorous definition as a type of entropy which has many well-known axiomatic properties for statistical dependence. As contrast, dCor and dHSIC are essentially nonlinear generalizations of traditional correlation concept and do not has the axiomatic properties that CE has, such as invariant to monotonic transformation, equivalent to correlation coefficients in Gaussian cases. As a type of entropy, CE also enjoys intrinsic physical meaning as a measure of information or energy exchange in the underlying systems which makes the variables selected with CE interpretable while the physical meaning of dCor and dHSIC are unclear yet.\nIt can be learned that stepwise GLM selects out less recommended variables than the proposed method does. This is because that the selection criteria of the methods are different. AIC is essentially an approximation of KL divergence [10] between GLM and the underlying distribution. Since it is under the risks of model misspecification and approximation bias, the result is not so good as CE\u2019s. BIC is a criterion which is derived under the Bayesian framework with assumptions on the underlying models and the approximation of bayesian compuation [11]. It is usually unrealistic for BIC to assume that the true model is within the model family under consideration. As contrast to AIC and BIC, the proposed method is guided by the CE, a well defined and estimated measure instead of a divergence or posteriori, which makes no assumption on the underlying models so there are no above risks of model misspecification or unrealistic assumptions and hence the selection is more advantagous theorectially. Meanwhile, the proposed method is computationally effective and efficient while both AIC and BIC are kind of approximations computed under certain assumptions.\nThe Adaptive LASSO which claims to has oracle property selects only a\nfew \u2018oracle\u2019 variable out. It is because that the Adaptive LASSO possesses it oracle property under restrictive model assumptions and regularity conditions. Comparing with it, CE presents very good results because the oracle property of CE is guaranteed by statistical dependence between variables and response measured by CE, which is unconditioned and universally applicable.\nCE also leads to interpretability of models. CE based variable selection is based on the dependence relationships between variables and response of model. It selects variables based on the dependence strength measured by CE, which is independent of variable\u2019s scale and dimension. Such statistical relationships are believed to have real world physical or biological meanings, as has been demonstrated in discovering statistical associations [33] \u2013 a problem closely related to variable selection. The ability of the proposed method to select meaningful variables is also demonstrated in the above experiments where the variables group selected by CE is very similar to the variable group chosen by the perfessionals for clinical use and the additional vessels-related variables which are also meaningful but not considered before are also selected. The models built from the variables such selected can be explained with domain knowledge and applied to the cases where explainability matters."
    },
    {
      "heading": "6 Conclusion",
      "text": "In this paper, we propose a CE based method for variable selection which use CE based ranks to select variables. The proposed method is both model-free and tuning-free. Comparison experiments between the CE based method and traditional variable selection methods, such as dCor, dHSIC, Stepwise Selection with information criteria, Regularized GLMs and Adaptive LASSO, are conducted on the UCI heart disease data. Experimental results show that CE based method can select the \u2018right\u2019 variables out effectively and derive better interpretable results than traditional methods do without sacrificing predictability. It is believed that CE based method makes variable selection becoming a science instead of an art and can help to build more explainable models which can lead to successful applications where interpretability of model matters. In the future, the proposed method will be applied to more real-world dataset to test its effectiveness and efficiency."
    },
    {
      "heading": "Acknowledgement",
      "text": "The author thanks Matsumori Masaki for comments and suggestions."
    }
  ],
  "title": "Variable Selection with Copula Entropy",
  "year": 2020
}
