{"abstractText": "Information fusion is an essential part of numerous engineering systems and biological functions, e.g., human cognition. Fusion occurs at many levels, ranging from the low-level combination of signals to the high-level aggregation of heterogeneous decision-making processes. While the last decade has witnessed an explosion of research in deep learning, fusion in neural networks has not observed the same revolution. Specifically, most neural fusion approaches are ad hoc, are not understood, are distributed versus localized, and/or explainability is low (if present at all). Herein, we prove that the fuzzy Choquet integral (ChI), a powerful nonlinear aggregation function, can be represented as a multi-layer network, referred to hereafter as ChIMP. We also put forth an improved ChIMP (iChIMP) that leads to a stochastic gradient descent-based optimization in light of the exponential number of ChI inequality constraints. An additional benefit of ChIMP/iChIMP is that it enables eXplainable AI (XAI). Synthetic validation experiments are provided and iChIMP is applied to the fusion of a set of heterogeneous architecture deep models in remote sensing. We show an improvement in model accuracy and our previously established XAI indices shed light on the quality of our data, model, and its decisions.", "authors": [{"affiliations": [], "name": "Muhammad Aminul Islam"}, {"affiliations": [], "name": "Derek T. Anderson"}, {"affiliations": [], "name": "Anthony J. Pinar"}, {"affiliations": [], "name": "Timothy C. Havens"}], "id": "SP:379bc98e7b7653430fbc7ccb85cccb1d986f1bb3", "references": [{"authors": ["X. Gu", "P.P. Angelov", "C. Zhang", "P.M. Atkinson"], "title": "A massively parallel deep rule-based ensemble classifier for remote sensing scenes", "venue": "IEEE Geoscience and Remote Sensing Letters, vol. 15, no. 3, pp. 345\u2013349, March 2018.", "year": 2018}, {"authors": ["J.M. Keller", "D.J. Hunt"], "title": "Incorporating fuzzy membership functions into the perceptron algorithm", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, no. 6, pp. 693\u2013699, 1985.", "year": 1985}, {"authors": ["P. Karczmarek", "A. Kiersztyn", "W. Pedrycz"], "title": "On developing sugeno fuzzy measure densities in problems of face recognition", "venue": "International Journal of Machine Intelligence and Sensory Signal Processing, vol. 2, no. 1, pp. 80\u201396, 2017.", "year": 2017}, {"authors": ["R.R. Yager"], "title": "Applications and extensions of owa aggregations", "venue": "International Journal of Man-Machine Studies, vol. 37, no. 1, pp. 103\u2013122, 1992.", "year": 1992}, {"authors": ["\u2014\u2014"], "title": "On ordered weighted averaging aggregation operators in multicriteria decisionmaking", "venue": "IEEE Transactions on Systems, Man, and Cybernetics, vol. 18, no. 1, pp. 183\u2013190, 1988.", "year": 1988}, {"authors": ["C. Sung-Bae"], "title": "Fuzzy aggregation of modular neural networks with ordered weighted averaging operators", "venue": "International Journal of Approximate Reasoning, vol. 13, no. 4, pp. 359\u2013375, 1995.", "year": 1995}, {"authors": ["S.B. Cho", "J.H. Kim"], "title": "Combining multiple neural networks by fuzzy integral for robust classification", "venue": "IEEE Transactions on Systems, Man, and Cybernetics, vol. 25, no. 2, pp. 380\u2013384, 1995.", "year": 1995}, {"authors": ["G.J. Scott", "R.A. Marcum", "C.H. Davis", "T.W. Nivin"], "title": "Fusion of deep convolutional neural networks for land cover classification of high-resolution imagery", "venue": "IEEE Geoscience and Remote Sensing Letters, 2017.", "year": 2017}, {"authors": ["D. Anderson", "G. Scott", "M. Islam", "B. Murray", "R. Marcum"], "title": "Fuzzy choquet integration of deep convolutional neural networks for remote sensing", "venue": "Computational Intelligence for Pattern Recognition, Springer-Verlag, 2018.", "year": 2018}, {"authors": ["M. Sugeno"], "title": "Theory of fuzzy integrals and its applications", "venue": "Ph.D. thesis, Tokyo Institute of Technology, 1974.", "year": 1974}, {"authors": ["G. Choquet"], "title": "Theory of capacities", "venue": "Annales de l\u2019institut Fourier, vol. 5. Institut Fourier, 1954, pp. 131\u2013295. AUTHOR COPY, TO APPEAR IN SPECIAL ISSUE ON DEEP FUZZY MODELS, TRANSACTIONS ON FUZZY SYSTEMS 14", "year": 1954}, {"authors": ["H. Tahani", "J. Keller"], "title": "Information fusion in computer vision using the fuzzy integral", "venue": "IEEE Transactions System, Man, and Cybernetics, vol. 20, pp. 733\u2013741, 1990.", "year": 1990}, {"authors": ["M. Grabisch", "M. Sugeno"], "title": "Multi-attribute classification using fuzzy integral", "venue": "Fuzzy Systems, 1992., IEEE International Conference on. IEEE, 1992, pp. 47\u201354.", "year": 1992}, {"authors": ["M. Grabisch", "J.-M. Nicolas"], "title": "Classification by fuzzy integral: Performance and tests", "venue": "Fuzzy Sets and Systems, vol. 65, no. 2-3, pp. 255\u2013271, 1994.", "year": 1994}, {"authors": ["M. Grabisch"], "title": "The application of fuzzy integrals in multicriteria decision making", "venue": "European Journal of Operational Research, vol. 89, no. 3, pp. 445\u2013456, 1996.", "year": 1996}, {"authors": ["M.A. Islam", "D.T. Anderson", "A.J. Pinar", "T.C. Havens"], "title": "Data-driven compression and efficient learning of the Choquet Integral", "venue": "IEEE Transactions on Fuzzy Systems, vol. PP, no. 99, pp. 1\u20131, 2017.", "year": 2017}, {"authors": ["T.C. Havens", "D.T. Anderson", "C. Wagner"], "title": "Data-informed fuzzy measures for fuzzy integration of intervals and fuzzy numbers", "venue": "IEEE Transactions on Fuzzy Systems, vol. 23, no. 5, pp. 1861\u20131875, Oct 2015.", "year": 1861}, {"authors": ["C. Wagner", "D.T. Anderson"], "title": "Extracting meta-measures from data for fuzzy aggregation of crowd sourced information", "venue": "2012 IEEE International Conference on Fuzzy Systems, June 2012, pp. 1\u20138.", "year": 2012}, {"authors": ["M. Grabisch"], "title": "k-order additive discrete fuzzy measures and their representation", "venue": "Fuzzy Sets and Systems, vol. 92, no. 2, pp. 167 \u2013 189, 1997, fuzzy Measures and Integrals. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S0165011497001681", "year": 1997}, {"authors": ["L. Jin", "M. Kalina", "R. Mesiar", "S. Borkotokey"], "title": "Discrete choquet integrals for riemann integrable inputs with some applications", "venue": "IEEE Transactions on Fuzzy Systems, 2018.", "year": 2018}, {"authors": ["B. Murray", "M.A. Islam", "A.J. Pinar", "T.C. Havens", "D.T. Anderson", "G. Scott"], "title": "Explainable ai for understanding decisions and data-driven optimization of the choquet integral", "venue": "Proceedings of IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2018.", "year": 2018}, {"authors": ["T. Murofushi", "S. Soneda"], "title": "Techniques for reading fuzzy measures (iii): interaction index", "venue": "9th Fuzzy System Symposium. Sapporo,, Japan, 1993, pp. 693\u2013696.", "year": 1993}, {"authors": ["D.T. Anderson", "S.R. Price", "T.C. Havens"], "title": "Regularizationbased learning of the Choquet integral", "venue": "2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), July 2014, pp. 2519\u20132526.", "year": 2014}, {"authors": ["G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten"], "title": "Densely connected convolutional networks", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, vol. 1, no. 2, 2017, p. 3.", "year": 2017}, {"authors": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "title": "Going deeper with convolutions", "venue": "Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "year": 2015}, {"authors": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "title": "Caffe: Convolutional architecture for fast feature embedding", "venue": "Proc. of the 22nd ACM International Conference on Multimedia. ACM, 2014, pp. 675\u2013678.", "year": 2014}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep residual learning for image recognition", "venue": "arXiv preprint arXiv:1512.03385, 2015.", "year": 2015}, {"authors": ["F. Chollet"], "title": "Xception: Deep learning with depthwise separable convolutions", "venue": "arXiv preprint, pp. 1610\u201302 357, 2017.", "year": 2017}, {"authors": ["G.J. Scott", "M.R. England", "W.A. Starms", "R.A. Marcum", "C.H. Davis"], "title": "Training deep convolutional neural networks for land-cover classification of high-resolution imagery", "venue": "IEEE Geoscience and Remote Sensing Letters, vol. 14, no. 4, pp. 549\u2013553, 2017.", "year": 2017}, {"authors": ["D.T. Anderson", "T.C. Havens", "C. Wagner", "J.M. Keller", "M.F. Anderson", "D.J. Wescott"], "title": "Extension of the fuzzy integral for general fuzzy set-valued information", "venue": "vol. 22, no. 6, pp. 1625\u20131639, 2014.", "year": 2014}, {"authors": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "title": "Dropout: a simple way to prevent neural networks from overfitting.", "venue": "Journal of Machine Learning Research,", "year": 2014}], "sections": [{"text": "Index Terms\u2014data fusion, Choquet integral, deep learning, neural network, explainable AI\nI. INTRODUCTION Data are ubiquitous in today\u2019s technological era. This is both a blessing and a curse as we are swimming in sensors but drowning in data. In order to cope with these data, many systems employ\nMuhammad Aminul Islam is with the Department of Electrical and Computer Engineering, Mississippi State University, MS, USA e-mail: (mi160@msstate.edu).\nDerek Anderson, Grant Scott, and James Keller are with the Electrical Engineering and Computer Science Department, University of Missouri, MO, USA. Timothy Havens and Anthony Pinar are with the Electrical and Computer Engineering Department and the Computer Science Department, Michigan Technological University, MI, USA.\nManuscript received May 20, 2018.\ndata/information fusion. For example, you are right now combining multiple sources of data, e.g., taste, smell, touch, vision, hearing, memories, etc. In remote sensing, it is common practice to combine lidar, hyperspectral, visible, radar and/or other variable spectral-spatial-temporal resolution sensors to detect objects, perform earth observations, etc. This is the same story for computer vision, smart cars, Big Data, and numerous other thrusts. While the last decade has seen great strides in topics like deep learning, the reality is that our understanding of fusion in the context of neural networks (NNs) (and therefore deep learning) has not witnessed similar growth. Most approaches to fusion in NNs are ad hoc (specialized for a particular application) and/or they are not well understood nor explainable (i.e., how are the data being combined and why should we trust system outputs).\nLet zi \u2208 <Di be data from source i = 1, . . . , N (sensor, algorithm, human). If fusion is needed, most approaches just concatenate, i.e., z = (z1, ..., zN)\nt, resulting in a higher dimensional input. As such, \u201cfusion\u201d occurs somewhere in the network. Another approach is divide-and-conquer, where individuals NNs are attached to each zi, followed by NN(s) (or other machine learning algorithms like a support vector machine) to combine their output. However, whereas this approach gives rise to a modular design, plug-and-play possibilities, etc., it does so at the expense of likely not exposing low-level data correlations (which are in z). In [1], Xiaowei et al. explored infinite-valued logic on a set of pre-trained convolutional neural networks (CNNs). Pal, Mitra, and others (e.g., Keller and the fuzzy perceptron [2]) explored a variety of topics like fuzzy min-max networks, fuzzy multilayer perceptron (MLP), Sugeno fuzzy measure densities [3], and fuzzy Kohonen networks. In 1992 [4], Yager\nput forth the ordered weighted average (OWA) [5] neuron\u2014which technically is a linear order statistic (LOS) as the weights are real-valued numbers (vs. sets). In 1995, Sung-Bae utilized the OWA for NN aggregation at the decision/output level [6].\nIn 1995, Sung-Bae et al. explored the fuzzy integral, specifically the Sugeno integral, for NN aggregation [7]. They used the Sugeno \u03bb-fuzzy measure (\u03bb-FM) defined on the N singletons versus the full set of 2N subsets and the densities were derived using their respective accuracy rates on training data. In 2017 [8], we used the Sugeno and Choquet integral (ChI) for deep CNN (DCNN) fusion. Specifically, we used data augmentation and transfer learning to adapt GoogLeNet, AlexNet, and ResNet50 from camera imagery to remote sensing imagery. We then applied different aggregations\u2014 fuzzy integral, voting, arrogance, and weighted sum\u2014to these DCNNs. A \u03bb-FM with normalized classifier accuracy densities and also a genetic algorithm was used to learn the densities. In [9], quadratic programming was used to learn the full ChI, relative to pre-trained DCNNs. These are a few NN fusion approaches explored to date.\nHerein, we investigate basic NN fusion questions. The first, Q1, is what fusions\u2014aggregation functions to be precise\u2014are possible relative to existing NN ingredients? Q1 is more-or-less an existence argument. The next, Q2, is can we represent and optimize an aggregation function, such as the ChI, as an NN? As such, Q2 addresses how do we find a solution (versus does one exist). Last, Q3, is can we open the hood on a fusion NN and understand what it has learned?\nThe following contributions are made in this paper. For Q1, we demonstrate that state-of-the-art aggregation operators are achievable using existing NN mathematics. Namely, we show that two NNs can compute the ChI; one based on a selection network and N ! linear convex sums (LCS), the other based on the Mobius transform. We also logically and empirically show that it is a feat to approximate the ChI on limited variety and volume data (which is often the case). For Q2, we present the ChI multi-layer perception (ChIMP) (aka, dedicated fusion network) that can be optimized via stochastic gradient descent (SGD) in light of an exponential number of ChI inequality constraints. For Q3, we use indices for introspection on ChIMP. Whereas most NN fusion solutions to date operate on the\nbasis of implicit and distributed computation, we focus on explicit and centralized computation to promote understandability. ChIMP is used here to fuse a set of heterogeneous architecture deep NNs for remote sensing. Adding ChIMP to the top of a collection of deep NNs results in a deep fuzzy NN.\nThe remainder of this article is organized as such. First, in Section II we introduce the capacity and integral. In Section III different NNs (ChIMPs) for the ChI are put forth; Section IV is an improved ChIMP (iChIMP) (relative to SGD optimization) and Section V presents eXplainable AI (XAI) fusion. The final sections present our experiments, results and, conclusions."}, {"heading": "II. BACKGROUND: MEASURE AND INTEGRAL", "text": "Let X = {x1, x2, ..., xN} be N sources of data/information (e.g., sensor, human, algorithm), let h(xi) be the input from source i, and let h = (h(x1), ..., h(xN))\nt be a vector of inputs. An aggregation operator maps data-to-data, f\u0398(h) = y, which ideally obeys conditions such as idempotency, associativity, continuity, symmetry, etc. Typically, y is not multi-dimensional, but is <-valued. The ChI is a nonlinear aggregation function parameterized by the FM [10], [11]. Whereas the integral has its roots in calculus, Keller et al. were the first to use it for pattern recognition/machine learning [12], [13], [14]. However, the integral has been used in many contexts, e.g., by Grabisch et al. in multi-criteria decision making (MCDM) [15]. Regardless of the application the question remains: where do the parameters come from? Examples include human specification (which becomes quickly intractable; as N grows, there are 2N variables and N(2N\u22121) monotonicity constraints), it can be learned from training data [16], or extrapolated in a crowd sourced fashion [17], [18].\nIn addition, it is important to remark about the complexity of the ChI. For example, for N = 10 there are 1,024 variables and 5,120 constraints. In order to combat the computational complexity, imputation methods like the \u03bb-FM have been put forth, where one specifies the measure of the N individuals and the \u03bb-FM automatically fills in (guesses at) the remaining N(2N\u22121) \u2212 N values. Grabish, Labreuche, and others have explored routes like the k-additive integral to restrict the number of FM variables to at most k tuples [19]. This helps control\nthe complexity of the integral relative to tasks like human decision making and bounded rationality. In [16], we introduced a way to identify data supported and data unsupported ChI variables. Optimization is for data supported variables only, new examples are classified as known or unsupported (i.e., requires unknown variables), and imputation is used to make an intelligent guess in the case of missing variables. The next few subsections are quick reviews of the measure and integral."}, {"heading": "A. Fuzzy Measure", "text": "The FM, g : 2X \u2192 R+, is a function with the following two properties; (i) (boundary condition) g(\u2205) = 0, and (ii) (monotonicity) if A,B \u2286 X , and A \u2286 B, then g(A) \u2264 g(B).1"}, {"heading": "B. Choquet Integral", "text": "The ChI of observation h on X is\u222b h \u25e6 g = Cg(h) =\nN\u2211 j=1 h\u03c0(j)(g(A\u03c0(j))\u2212 g(A\u03c0(j\u22121))),\n(1)\nfor A\u03c0(j) = {x\u03c0(1), . . . , x\u03c0(j)}, g(A\u03c0(0)) = 0, and permutation \u03c0 such that h\u03c0(1) \u2265 h\u03c0(2) \u2265 . . . \u2265 h\u03c0(N).2"}, {"heading": "C. Choquet Integral as N ! Linear Convex Sum Operators", "text": "One way to discuss the ChI is in terms of N ! LCS operators. Relative to a particular sorting of the data (\u03c0i)\u2014of which there are N ! possible sorts\u2014the ChI can be expressed as f\u03c0i = N\u2211 j=1 h\u03c0i(j)(g(A\u03c0i(j))\u2212 g(A\u03c0i(j\u22121))) = ht\u03c0iw\u03c0i ,\n(2)\nwhere w\u03c0i(j) = (g(A\u03c0i(j)) \u2212 g(A\u03c0i(j\u22121))). For the ChI, these N\u00d7N ! weights are tied to the underlying 2N FM variables.\nExample 1. For N = 2, the ChI can be expanded as\nCg(h) = { h1w1 + h2w2 : h1 \u2265 h2 h2w3 + h1w4 : h2 > h1\n(3)\n1Sometimes a normality condition is imposed such that g(X) = 1. 2Shorthand notation hi = h(xi) is used.\nwhere the weights are w1 = g({x1}), w2 = 1 \u2212 g({x1}), w3 = g({x2}), w4 = 1 \u2212 g({x2}). Thus, there are four weights, but just two underlying free FM variables: the densities.\nRemark 1. This difference in weights versus underlying FM variables grows with respect to N . For example, when N = 5, there are 32 FM variables and 600 weights. However, for N = 10 there are 1, 024 FM variables and 36, 288, 000 weights. The ChI can be seen as a form of variable compression."}, {"heading": "D. Restricting the Scope of the FM/ChI", "text": "Since the ChI is a parametric function, once the FM is determined, the ChI turns into a specific operator. For example: if g(A) = 1,\u2200A \u2208 2X \\ \u2205, the ChI becomes the maximum operator; if g(A) = 0,\u2200A \u2208 2X \\ X , we recover the minimum; if g(A) = |A|\nN , we recover the mean; and for g(A) =\ng(B) when |A| = |B|, \u2200A,B \u2286 X , we obtain an LOS. In general, each of these cases can be viewed as constraints or simplifications on the FM, and therefore the ChI. Also, the reader should know that the all-too-familiar operators\u2014mean, max, min, trimmed versions of these operators, etc.\u2014are all subsets of the LOS and therefore of the ChI. As such, the ChI is useful because it can be adapted to suit a wide range of aggregation needs. This is a primary reason for selecting it for use in this article and for sensor fusion, in general."}, {"heading": "III. THE CHOQUET INTEGRAL AS AN NN:", "text": "CHIMP\nIn this section we explore question Q1, can an NN compute the ChI, and therefore a wide set of interesting, useful, and commonly used aggregation operators? To make a long story short, the answer is yes (see Example 2 and Fig. 1). But why is Q1 important? Well, there are many claims about what an NN (e.g., CNN) can do.3 Mathematically, a CNN can encode filters (linear time invariant filters such as a matched filter, low pass filter, or Gabor filter), random projections, and combinations thereof, to name a few. However, limited attention has been placed on understanding aggregation in an NN. Ideally, we would like to know if, and then\n3Keeping in mind the difference between an existence proof versus can we identify a way to achieve (e.g., learn) it.\nwhere, fusion is occurring, understand what aggregation operator was selected (e.g., intersection like, union like, average like, something more exotic), what aggregation functions are possible relative to a network, etc. Understanding if an NN can compute the ChI gives us insight into what is possible, or not possible as it may be.\nExample 2. Consider the case of N = 2 and the NN outlined in Fig. 1. The network output is\no = u(h1\u2212h2)(h1w1+h2w2)+u(h2\u2212h1)(h2w3+h1w4),\nwhere u is a unit/Heaviside step function,4,5 which gives us\no = u(h1 \u2212 h2) [h1g({x1}) + h2(1\u2212 g({x1}))] + u(h2 \u2212 h1)[h2g({x2}) + h1(1\u2212 g({x2}))];\nthus,\no =  h1g({x1}) + h2(1\u2212 g({x1})), h1 > h2, h2g({x2}) + h1(1\u2212 g({x2})), h2 > h1, 0.5(h1g({x1}) + h2(1\u2212 g({x1})))+ 0.5(h2g({x2}) + h1(1\u2212 g({x2}))), h1 = h2.\nWithout loss of generality, this extends to any N .\nAs the reader can see, ChIMP represents the ChI by a set of LCS operators and it uses a selection network to pick one of these results. Technically, our solution can learn and compute the ChI, but it has more functionality (freedom) than a standard ChI as we made the N !\u00d7N weights independent (and in < versus <+), versus reducing them (sharing weights) into the underlying 2N FM variables, which can be done. However, our goal in this section is not to make the simplest possible network, it is to show that an NN can represent the ChI.\nRemark 2. As discussed in the introduction, answers for fusion are in the eye of the beholder;\n4The unit step function is u(x) = 1 if x \u2265 0, otherwise u(x) = 0. In practice, many use a smooth approximation like the logistic function 1\n2 + 1 2 tanh(kx) = 1 1+e\u22122kx , where the larger the k, the\nsharper the transition about x = 0. 5Herein, we consider a modified unit step function that has value 1 N\nif f(x) = 0. In the difference-in-g form of the ChI, what is the rule for the case of equal input values? For example, let h = (0.2, 0.2, 0.1). For h we can choose \u03c0(1) = 1, \u03c0(2) = 2, and \u03c0(3) = 3 or \u03c0(1) = 2, \u03c0(2) = 1, and \u03c0(3) = 3. Depending on the underlying FM, these sorts can yield different ChI outputs. In practice, most sort algorithms use an increasing or decreasing rule (i.e., default mapping to one case). Herein, we augment the unit step function and consider the average function value.\nthat is, context matters. Figure 1 does indeed give us the ChI. For example, we could fuse the soft max outputs i.e., decision-in-decision-out (DIDO) fusion of multiple deep learners (e.g., ResNet and GoogleNet). On the other hand, if ChIMP was pushed back in the network, possibly connected directly to the inputs, it would likely function differently, e.g., signal-in-signal-out (SISO) or SIDO versus DIDO. For example, each LCS neuron could represent a matched filter and the selection network would pick one result. We mention this because it (that is, context) is substantial for XAI.\nRemark 3. Our N ! LCS-based ChIMP is not the only solution. Another example is based on the Mobius transform; see Fig. 2. The Mobius transform of g is\nm(A) = \u2211 B\u2286A (\u22121)|A\\B|g(B),\u2200A \u2286 X, (4)\nwhich is invertable via the Zeta transform, g(A) = \u2211 B\u2286A m(B), \u2200A \u2286 X. (5)\nThe Mobius transform of the ChI is Cg(h) = \u2211 A\u2286X m(A) \u2227 xi\u2208A hi. (6)\nThus, the Mobius ChI can be thought of as a dot product of Mobius terms and a t-norm of the integrand term h. There is no sort in the Mobius\nintegral; the tradeoff is summing across N ! versus N values. The point is, there are many ChIMPs. We presented two, but more of them likely exist.\nRemark 4. The above N ! LCS-based and Mobius transform-based ChIMPs do not scale well with respect to N . For example, the Mobius-based ChIMP has 2N t-norm neurons and one dot product. At higher levels (closer to the output, e.g., DIDO) in a neural network, N might not be large (on the order of 10 classes) so all is well. However, if we consider using ChIMP at lower-levels in a network, a large N could render ChIMP intractable. For example, consider fusing a set of convolutional filters of size 11 \u00d7 11. When unrolled, the 11 \u00d7 11 gives rise to N = 121. As 2121 is a very large number, one could reduce ChIMP network complexity with respect to a method like k-additivity,\nCkg (h) = \u2211\nA\u2286X,|A|\u2264k m(A) \u2227 xi\u2208A hi. (7)\nwhich uses tuples only up to, and including, k. The point is this, as N grows the ChI can be restricted to suit the needs of an application at the expense of loss of some expressability.\nIn summary, our response to Q1 is yes, an NN can represent the ChI and therefore a wide class of useful aggregation operators from the min to max, average, and more exotic variants as well. Furthermore, there are multiple ways (architectures) to achieve this. Technically, there are an infinite number of possibilities, e.g., recursive argument in which each solution is expanded by a single neuron, which could be bypassed or turned off by setting its weights to all zeros. This problem\u2014 existence of multiple ways to encode a solution\u2014\nis well-known in many communities, e.g., bloating in genetic programming, which can be addressed using cost function augmentation with a complexity term. In the next section we explore an iChIMP architecture, which is simple to optimize using SGD and whose weights are explicit, enabling XAI.\nIV. ICHIMP\nIn this section we present iChIMP, an NN with an SGD solution. As such, this addresses Q2. To this end, we explore an alternative way of writing the ChI [20],6\nCg(h) = \u2211 A\u2286X g(A)o(A), (8)\nwhere\no(A) =\n{ max ( 0, \u2227 xi\u2208A hi \u2212 \u2228 xj /\u2208A hj ) , A \u2282 X,\u2227\nxi\u2208A hi, A = X. (9)"}, {"heading": "A. Measure Network", "text": "Our idea is to design an FM NN. This network consists of constants, learnable weights, and existing neural mathematics (dot product, ReLU nonlinearlity, and maximum neurons). Figure 3 illustrates the network for N = 3.\nSpecifically, the densities (FM variables whose set cardinality is 1) are represented as a weight vector and a nonlinearity (e.g., ReLU) enforces\n6Note that the ChI formulation herein differs from article [20] in one respect that Eq. 8 is for R-valued inputs whereas that in [20] is for R+ inputs.\nthe lower boundary condition. Next, each tuple is expressed as\ng(A) = \u2228 B\u2282A g(B) + \u2206g(A), (10)\nwhere \u2206g(A) \u2208 <. Like before, a positive value enforcing nonlinearity is used to ensure the monotonicity property, forcing \u2206g(A) to reside in <+. If g(X) is required to be 1, then all of g can be renormalized by taking the minimum of g and 1. Otherwise, we can ignore the upper boundary condition, since this is not a hard requirement.\nB. Integral and Evaluation Networks\nThe next piece of iChIMP is expanding the 2N\u22121 terms in Eq. (8) as shown in Fig. 4. This MLP has no trainable weights. The network can be achieved using max, min, and a custom f(a, b) = max(0, a\u2212 b) neuron. The final step is a single dot product (see Fig. 5) of the expanded integrand terms (see Fig. 4) and the FM variable values (see Fig. 3)."}, {"heading": "C. iChIMP Optimization", "text": "For readability, the derivation of iChIMP SGD optimization is presented in the Appendix.\nV. XAI FOR THE CHOQUET INTEGRAL\nIn this section, a benefit of designing an explicit neural fusion network is highlighted. In [21], we established ChI indices for XAI. The reader can refer to [21] for full mathematical explanation. Due to manuscript length, we are only able to summarize the indices.\nThe first category of fusion XAI indices explain the quality of the individual sources and their interaction characteristics. The utility of a source, e.g., deep model, can be extracted via the Shapley index, \u03a6g(i) \u2208 [0, 1], where \u2211N i=1 \u03a6g(i) = 1. On the other hand, the Interaction Index [22], Ig(i, j) \u2208 [\u22121, 1], informs us about how two deep models interact with one another\u2014that is, is there an advantage in combining sources. A value of 1 represents the maximum complementary between i and j. On the other hand, a value of \u22121 represents the maximum redundancy between i and j.\nA second category of fusion XAI indices tell us what aggregation was learned. This helps us determine if the data are being combined in a union, intersection, average, or perhaps more unique and worthy of the ChI fashion. In [21], we posed distance formulas to measure how similar a learned g is to the maximum, minimum, mean, and in general, LOS.\nThe third category of fusion XAI indices is data centric. In [21], we determined how often an FM variable is encountered in training data; which helps us find missing FM variables. We also calculated what percentage of FM variables were observed in training data, what percentage of the N ! possible LCSs were observed, and if there is a dominant walk (and therefore lack of training data variety). These indices ultimately inform us about the quality of a learned solution and they highlight what is incomplete with respect to our model/training data. We also postulated a trust index based on what percentage of missing variables are used in a ChI calculation.\nIn summary, in [21] we discussed existing methods and proposed new ways to elicit information about a learned fusion. Since iChIMP is an explicit neural architecture, meaning we know which network elements map to which FM variables, XAI can help us understand, validate, and do iterative development."}, {"heading": "VI. EXPERIMENTS AND RESULTS", "text": "In this subsection, two experiments are performed. The first experiment uses synthetic data. As such, we know the answer and we can control all factors, e.g., noise. We generate familiar operators that range from optimistic union-like to pessimistic intersection-like, average-like, and random operators. The purpose is to show range and variation in the FM and our ability to learn it. Next, we take the validated iChIMP and we use it to fuse a set of heterogeneous architecture DCNNs, to which we note no one knows the solution. The purpose of this experiment is to demonstrate the iChIMP on realworld benchmark data and to compare it to existing work."}, {"heading": "A. Experiment 1: Synthetic Data Set", "text": "The objective of Experiment 1 is to show that we recover the correct ChI and to compare iChIMP to an existing (non-neural) way of solving the ChI, i.e., quadratic programming (QP) [23]. Our data are generated pseudo-randomly from a uniform distribution in a unit interval and consist of M = 300 samples and three inputs (N = 3). We use four FMs with disparate aggregation behavior\u2014FM1 is a soft-max, FM2 is a mean, FM3 is a soft-mean, and FM4 is an arbitrary FM\u2014to generate their labels. Table I shows the four target FMs.\nIn order to investigate the impact of noise on learning, we perturb the true labels with random noise sampled from a Gaussian distribution with variance \u03c32y . We consider six noise levels ranging from no-noise to 50% of the true label standard deviation, i.e., \u03c3n = {0, 0.01\u03c3y, 0.05\u03c3y, 0.1\u03c3y, 0.3\u03c3y, 0.5\u03c3y}.\nThe data are partitioned into two segments, 80% for training and 20% for test. Training parameters for iChIMP are learning rate = 0.001 and number of epochs = 1000. The iChIMP variables are initialized with soft-mean like FM with values randomly\npicked from a uniform distribution in [0.1, 0.2]. For each FM, optimization is performed on the training data to learn the FM variables, which are then used to estimate the label/output of the test data. The mean squared error (MSE) with respect to the true training labels, true test labels, and FM variables are computed and used as a performance metric. The optimization task was repeated 20 times for iChIMP with different initializations, and we report the average of the resultant MSEs. Table II contains the results of Experiment 1.\nTable II tells the following story. The MSEs for individual methods and their differences are quite low (on the order of 10\u22124 \u223c 10\u22125) even at noise levels as high as 0.5\u03c3y. This means that even though iChIMP is optimizing a non-convex network (with its ReLU, max, and min functions), it provides an approximation of the integrals as good as the QP method."}, {"heading": "B. Experiment 2: Real-World Data Set", "text": "Experiment 1 validates iChIMP and Experiment 2 uses it to fuse a set of heterogeneous deep CNNs (DCNN) for remote sensing. An outstanding challenge in deep learning is network architecture. In general, no architecture has been shown to be superior across data sets. This is why we investigate the fusion of different architectures.\nTwo benchmark remote sensing data sets are investigated herein for land cover classification and object detection. The aerial image data set (AID) has 30 classes, it has approximately 330 images per class, and the ground sampling distance (GSD) varies between 0.5 to 8 meters. The remote sensing imagery scene classification-45 (R45) was specifically designed to be challenging for remote sensing image scene classification. It contains 45 classes with 700 images per class and a variable GSD of 0.2 to 30 meters. However, the vast majority of the R45 classes have a GSD < 1 meter.\nHerein, we fuse seven DCNNs that have shown promising results in computer vision: DenseNet [24], GoogLeNet [25], InceptionResNetV2 [26], CaffeNet [27], ResNet-50 [28], Xception [29], and ResNet-101. For both data sets, our DCNNs were trained using the methods in [30], including transfer learning (non-remote sensing weights derived from ImageNet), data augmentation (rotation, noise, scale, and contrast), and 50% dropout. The trained\nDCNNs are then used in a locked state, i.e., no further learning happens during the fusion stage. The training of the DCNNs are done in five-fold, cross validation manner; we have 5 sets of 80% training and 20% testing for both data sets. Per DCNN fold, three-fold cross validation (CV) fusion is used (due to limited volume of data). Table III summarizes the performance of the DCNNs and our fused solution for the test data sets. In particular, iChIMP outperforms the individual networks; reducing the error rate by 40% (3.8% down to 2.27%) over the best single DCNN architecture for AID, and similarly a 30% relative error rate reduction for R45.\nIn most NNs, accuracy is the prime objective, and sometimes the only objective. However, we can apply our XAI indices to \u201copen up\u201d the learned solutions and interpret what is going on. On R45, the Shapley values are 0.1411(\u00b5)\u00b10.0014(\u03c3), 0.1414\u00b1 0.0013, 0.1418\u00b10.0009, 0.1415\u00b10.0011, 0.1437\u00b1 0.0008, 0.1431\u00b10.0002, and 0.1474\u00b10.0037, with respect to CaffeNet, DenseNet, GoogLeNet, InceptionResNetV2, ResNet50, ResNet101, and Xception. On AID, the Shapley values are 0.1413 \u00b1 0.0010, 0.1419\u00b10.0003, 0.1420\u00b10.0011, 0.1439\u00b1 0.0007, 0.1422 \u00b1 0.0005, 0.1420 \u00b1 0.0007, and 0.1468\u00b10.0028. These Shapley values indicate that the deep nets have near equal overall importance, with perhaps the exception of Xception. Next, our XAI aggregation operator distance indices had a value of approximately 0 relative to the mean. As such, we know that we learned an additive measure, which is reinforced by our Interaction Index values near 0. At the moment, our XAI indices are evidence. That is, their results need to be analyzed by an expert to determine what is going on. In future work we will discover a way to automatically reason\nabout our various XAI information to deduce highlevel linguistic summaries for non-experts.\nIn prior work [8], we explored the \u201coffline\u201d fusion\u2014QP versus iChIMP\u2014of three DCNNs. Herein, we repeat those experiments using iChIMP. This is done because we are interested to see if fusion learned a mean in part due to adding more deep models. As we discovered in [8], we do not always learn equal Shapley values. For example, on RSD we fused CaffeNet, GoogLeNet, and ResNet50. The shared weight Shapley solution had Shapley values of 0.28, 0.4, and 0.32, respectively. Furthermore, if we trained a different iChIMP per class, versus a single shared weight solution across classes, then we obtain Shapley values of 0.06, 0.69, and 0.25 for the bridge class and 0.04, 0.32, and 0.64 for the mountain class. This informs us that while a single shared weight iChIMP has the best overall accuracy, using fewer deep models leads to nonuniform Shapley values and non-mean like aggregations. Furthermore, it informs us that different classes prefer different deep models. The question we need to address is why?\nIn [21], we created XAI indices that tell us which FM variables cannot be approximated from training data. To no surprise, based on the above we ran our XAI indices and found that the \u201cproblem classes\u201d that are bringing down the overall average per-class iChIMP solution are likely due to lack of training data variety (aka, they have a large percentage of un-approximated FM variables). This is probably a contributing factor to why the shared weight iChIMP performs better than N per-class iChIMPs and possibly why a mean aggregation might be a good general strategy for combining a larger number of deep models; e.g., seven versus three, which means we need to encounter 7! = 5,040 versus\n3! = 6 unique sorts. In [21] we created an XAI index called dominant walk. A dominant walk is when a large percentage of the input data is sorted in a specific permutation order. We ran this index on our iChIMP solutions and discovered that there is typically a very dominant walk order of h1 \u2265 h2 \u2265 ... \u2265 hN , the default order. This finding and pattern was too coincidental for our liking. Upon inspection, we discovered this is because a majority of our data has all the networks 100% certainty in a label (and 0s otherwise). As such, 1, 2, ..., N is the default sort order. This means that we have strong learners and there is not a lot of diverse information (variety) to help us learn fusion. As such, a better solution, to be addressed in future work, would be to learn these networks in parallel now that the ChI can be integrated into a homogeneous neural solution.\nIn summary, in this subsection we used iChIMP to fuse a set of heterogeneous DNNs. Furthermore, iChIMP has XAI tools that allow us to understand, explain, and pursue the design of new data collections and better architectures."}, {"heading": "C. Computational Complexity", "text": "In this subsection, a time complexity analysis is provided. This study consists of three steps, (i) assessing the complexity of o(A), (ii) g(A), and finally (iii) Cg, the dot the product of g and o, as in Eq. (8).\n1) o(A): For A \u2282 X , the computation of o(A)\u2014 see Eq. (9)\u2014has one minimum on x (1 \u2264 x \u2264 N \u2212 1) elements, one maximum on N \u2212 x elements, one maximum on two elements, and one subtraction. This gives N + 3 operations as O(n) is the worst case complexity for finding a maximum of n elements and O(1) for subtraction operations. Using a similar analysis, the number of operations for A = X is N . As a result, the total cost for computing o(A) for all A, where A \u2286 X and A 6= \u2205, is (2N \u2212 2)\u00d7 (N + 3) + N . The complexity of o(A) is the highest term without the constant, i.e., O(N2N). 2) g(A): Eq. (10) for g(A) has two parts: (a) \u2206g(A), a Relu on two elements (thus O(2) complexity); and (b) \u2228 B\u2282A g(B), a maximum\non |A|\u22121 elements with cost of |A|\u22121, where |A|. As a result, the cost of g(A) is |A| + 1. Let NCx be the combination of N elements taken x time. Thus, there are NC|A| sets with cardinality |A|. The total cost of computing g(A) for all A is NC1\u00d7(1+1)+\u00b7 \u00b7 \u00b7+NCN\u00d7(N+1) < 2N(N+1), because NC0 + \u00b7 \u00b7 \u00b7+ NCN = 2N . This results in a time complexity of O(N2N) for computation of g(A).\n3) Cg: Eq. (8) involves the dot product of g and o with 2N floating point operations (FLOPS)\nincluding N additions and N multiplications, resulting in a O(N) complexity.\nCombining all three components, the time complexity of iCHIMP is O(N2N)+O(N2N)+O(N) = O(N2N). This means that iChIMP has complexity of exponential order, O(N2N) w.r.t. the number of inputs, and complexity of polynomial order, O(Mlog(M)), with respect to the number of FM variables, M = 2N .\nNext, we discuss the cost of fusing deep models in practical applications. First, many pre-trained models, e.g., the ones used herein for large datasets like Imagenet, are publicly available. Common practice is to apply transfer learning, which is an offline procedure that is inexpensive relative to training an equivalent network from scratch. Next, the fusion of N models only adds 2N variables, where N is minuscule in relation to the number of parameters in the deep models. Translated, offline learning of N fusion variables is nominal. Online is a similar story. iChIMP is a tiny parallel network, relative to the complexity of a deep model, of common deep learning operations that have been accelerated in tool kits like TensorFlow and PyTorch. The only worthwhile cost of fusing N deep models is the storage and computational cost associated with evaluating N deep models, which can be performed in parallel. However, as evident in our experiments, fusion improves performance. Overall, the expense of fusing a set of deep models can be rationalized relative to the current dilemma of not knowing which deep model is correct."}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "In this article, we proposed a novel NN architecture with a gradient descent-based optimization solution that mimics the Choquet integral for information aggregation. This solution was demonstrated on synthetic data for validation and realworld benchmark data sets in remote-sensing fusion relative to the fusion of a set of heterogeneous architecture deep models. Adding iChIMP to the top of a set of deep neural networks produces a deep fuzzy neural network. Furthermore, we analyzed similarities and differences between multi-layer nets and the ChI, with respect to factors like representation and constrained learning algorithms. Last, our recently established XAI indices were used to \u201copen up\u201d our learned deep neural solutions enabling interpretability, helping us understand what was learned,\nidentifying flaws in our training data, and ultimately designing better deep model solutions.\nThe proposed NN architectures are not just limited to scalar-valued inputs and can be applied to higher-order FSs, such as Type-1 or Type-2, using the Zadehs extension principle. If one wishes to use a NN for higher-order FS-valued inputs, and can define an objective function and associated gradients, then using our proposed ChI-based NN is as straightforward as it is for real-valued inputs. Extensions of the fuzzy ChI for FS-valued inputs have been previously proposed [31]; we suggest application and extension of iChIMP for follow-on work.\nIn future work, now that we have the iChIMP foundation we will explore efficient representations\u2014e.g., k-additivity or our recently established data-compressed ChI [16]. We will also investigate advanced learning algorithms\u2014e.g., drop out [32], regularization [33], etc.\u2014with regard to the (2N \u2212 2) free variables. Once this is achieved we can push the ChI neuron back in the network for signal- and feature-level fusion, versus decision-level fusion. Furthermore, we will explore where and when a fusion neuron should be used, akin to the current revolution of what architecture should be employed. We will also, in the future, explore the possible benefit of enforcing g(X) = 1, focusing on what exact penalty functions to use to enforce a soft boundary. We will also continue to explore XAI and discover ways to linguistically summarize their contents for human consumption and to use them possibly in optimization directly to encourage certain factors, e.g., diversity, specificity, or efficiency. In this paper, we used iChIMP to fuse pre-trained DCNNs. Next, we will explore how to simultaneously learn iChIMP and the component networks to improve factors like accuracy and network diversity.\nAPPENDIX"}, {"heading": "A. Derivative of maximum", "text": "The derivative of f = max{f1(x), f2(x)} is\ndf(x)\ndx =  df1(x) dx if f1(x) > f2(x) df2(x) dx\nif f1(x) < f2(x) 1 2 ( df1(x) dx + df2(x) dx ) if f1(x) = f2(x).\n(11) Let Jf=fi be an indicator function that points to whether fi is equal to f (in other words, max of fis) or not, i.e.,\nJf=fi = { 1 if f(x) = fi(x) 0 else.\nAs such, we can write (11) as\ndf(x)\ndx =\n\u22112 i=1 Jf=fi\ndfi(x) dx\u22112\ni=1 Jf=fi = 2\u2211 i=1 If=fi dfi(x) dx ,\n(12)\nwhere If=fi = Jf=fi\u22112 i=1 Jf=fi\nis a normalized indicator variable. Respectively, (12) can be generalized for the case of f(x) = max{f1(x), . . . , fn(x)} as\ndf(x)\ndx = n\u2211 i=1 If=fi dfi(x) dx ."}, {"heading": "B. Derivative of min", "text": "The derivative for f(x) = min{f1(x), . . . , fn(x)} is\ndf(x)\ndx = n\u2211 i=1 If=fi dfi(x) dx ."}, {"heading": "C. Gradients of weights", "text": "Let yk, k = 1, . . . ,M , be the iChiMP output for observation ok. For a set of data, the error is\nE = 1\n2 M\u2211 k (lk \u2212 yk)2. (13)\nFor notational simplicity, we define 2N \u2212 1 \u2212 N (not defined on the densities nor empty set) max of subset auxiliary variables (see Fig. 3), gm. For example,\ngm123 = max (g12, g13, g23), g123 =g m 123 + max (\u2206wg123 , 0).\nWithout loss of generality, the following intermediate formulas are provided relative to N = 3,\n\u2202E \u2202yk = (yk \u2212 lk) = ek, (14) \u2202yk \u2202gi = oi, (15)\n\u2202g123 \u2202gm123 = 1, (16) \u2202gm123 \u2202g12 = I(gm123=g12) + 0 + 0 = I(gm123=g12). (17)\nThe same holds for \u2202g m 123 \u2202g13 and \u2202g m 123 \u2202g23 respectively. As such,\n\u2202g123 \u2202g12 = \u2202g123 \u2202gm123 \u2202gm123 \u2202g12 = Igm123=g12 , (18)\nand\n\u2202g123 \u2202\u2206wg123 = { 1 \u2206wg123 > 0 0 else. (19)\nFurthermore, we define the indicator variable\nIc>0 = { 1 if c > 0 0 else,\nand thus \u2202g123\n\u2202\u2206wg123 = I\u2206wg123>0.\nNext, the gradients are\n\u2202E \u2202g123 = \u2202E \u2202yk \u2202yk \u2202g123 = eko123, (20)\n\u2202E \u2202g12 = \u2202E \u2202yk ( \u2202yk \u2202g12 + \u2202yk \u2202g123 \u2202g123 \u2202g12 ) (21a)\n= ek ( o12 + o123Igm123=g12 ) , (21b)\nand similar for \u2202E \u2202g13 and \u2202E \u2202g23 . In a similar fashion, the error for each density is defined as\n\u2202E \u2202g1 = \u2202E \u2202yk ( \u2202yk \u2202g1 + \u2202yk \u2202g12 \u2202g12 \u2202g1 + (22a)\n\u2202yk \u2202g13 \u2202g13 \u2202g1 + \u2202yk \u2202g123 \u2202g123 \u2202g1\n) (22b)\n=ek ( o1 + o12Igm12=g1 + o13Igm13=g1+ (22c)\no123 ( Igm123=g12Igm12=g1 + Igm123=g13Igm13=g1 ) ) ,\n(22d)\nand similar for \u2202E \u2202g2 and \u2202E \u2202g3 . Last,\n\u2202E\n\u2202\u2206wg123 =\n\u2202E\n\u2202g123 \u2202g123 \u2202\u2206wg123\n(23a)\n= eko123I\u2206wg123>0, (23b)\n\u2202E\n\u2202\u2206wg12 =\n\u2202E\n\u2202g12 \u2202g12 \u2202\u2206wg12\n(24a)\n= ek(o12 + o123Igm123=g12)I\u2206wg12>0, (24b)\nand similar for \u2202E \u2202\u2206wg13 and \u2202E \u2202\u2206wg23 ."}, {"heading": "D. Gradients for inputs", "text": "Here we give the expressions for the gradients of the cost function with respect to inputs h1, h2, and h3. The gradients of ois w.r.t. h1 are\n\u2202o1 \u2202h1 = 0 + Io1=h1\u2212h2\u2228h3 = Io1=h1\u2212h2\u2228h3 , \u2202o2 \u2202h1 = 0 + Io2=h2\u2212h1\u2228h3(0\u2212 (Ihmax13 =h1 + 0)) = \u2212Io2=h2\u2212h1\u2228h3Ihmax13 =h1 , \u2202o3 \u2202h1 = \u2212Io3=h3\u2212h1\u2228h2Ihmax12 =h1 ,\n\u2202o12 \u2202h1\n= ( 0 + Io12=h1\u2227h2\u2212h3(Ihmin12 =h1 + 0) )\n= Io12=h1\u2227h2\u2212h3Ihmin12 =h1 ,\n\u2202o13 \u2202h1\n= ( 0 + Io13=h1\u2227h3\u2212h2(Ihmin13 =h1 + 0) )\n= Io13=h1\u2227h3\u2212h2Ihmin13 =h1 ,\n\u2202o23 \u2202h1 = 0\u2212 Io23=h2\u2227h3\u2212h1 = \u2212Io23=h2\u2227h3\u2212h1 ,\n\u2202o123 \u2202h1 = Io123=h1 + 0 + 0 = Io123=h1 .\nThe gradient of h1 w.r.t. the cost function, E, is\n\u2202E \u2202h1 = \u2202E \u2202yk \u2211 i \u2202yk \u2202oi \u2202oi \u2202h1\n= ek ( g1Io1=h1\u2212h2\u2228h3 \u2212 g2Io2=h2\u2212h1\u2228h3Ihmax13 =h1 \u2212 g3Io3=h3\u2212h1\u2228h2Ihmax12 =h1 + g12Io12=h1\u2227h2\u2212h3Ihmin12 =h1 + g13Io13=h1\u2227h3\u2212h2Ihmin13 =h1 \u2212 g23Io23=h2\u2227h3\u2212h1 + g123Io123=h1 ) .\nSimilarly,\n\u2202E \u2202h2 = \u2202E \u2202yk \u2211 i \u2202yk \u2202oi \u2202oi \u2202h2\n= ek ( \u2212g1Io1=h1\u2212h2\u2228h3Ihmax23 =h2 + g2Io2=h2\u2212h1\u2228h3 \u2212 g3Io3=h3\u2212h1\u2228h2Ihmax12 =h2 + g12Io12=h1\u2227h2\u2212h3Ihmin12 =h2 \u2212 g13Io13=h1\u2227h3\u2212h2 + g23Io23=h2\u2227h3\u2212h1Ihmin23 =h2 +g123Io123=h2) ,\n\u2202E \u2202h3 = \u2202E \u2202yk \u2211 i \u2202yk \u2202oi \u2202oi \u2202h3\n= ek ( \u2212g1Io1=h1\u2212h2\u2228h3Ihmax23 =h3 \u2212 g2Io2=h2\u2212h1\u2228h3Ihmax13 =h3 + g3Io3=h3\u2212h1\u2228h2 \u2212 g12Io12=h1\u2227h2\u2212h3 + g13Io13=h1\u2227h3\u2212h2Ihmin13 =h3 + g23Io23=h2\u2227h3\u2212h1Ihmin23 =h3 +g123Io123=h3) ."}], "title": "Enabling Explainable Fusion in Deep Learning with Fuzzy Integral Neural Networks", "year": 2019}