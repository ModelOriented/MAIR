{"abstractText": "To understand changes in physical systems and facilitate decisions, explaining how model predictions are made is crucial. We use model-based interpretability, where models of physical systems are constructed by composing basic constructs that explain locally how energy is exchanged and transformed. We use the port Hamiltonian (p-H) formalism to describe the basic constructs that contain physically interpretable processes commonly found in the behavior of physical systems. We describe how we can build models out of the p-H constructs and how we can train them. In addition we show how we can impose physical properties such as dissipativity that ensure numerical stability of the training process. We give examples on how to build and train models for describing the behavior of two physical systems: the inverted pendulum and swarm dynamics.", "authors": [{"affiliations": [], "name": "Ion Matei"}, {"affiliations": [], "name": "Johan de Kleer"}, {"affiliations": [], "name": "Christoforos Somarakis"}, {"affiliations": [], "name": "Rahul Rai"}, {"affiliations": [], "name": "John S. Baras"}], "id": "SP:a5dba56452b39fc8c6e1d12bb5f60bc396c2bd4e", "references": [{"authors": ["K. Alloula", "F. Monfreda", "R.T. H\u00e9treux", "J.P. Belaud"], "title": "Converting dae models to ode models: application to reactive rayleigh distillation", "venue": "Chemical engineering transactions,", "year": 2013}, {"authors": ["Y. Cao", "S. Li", "L. Petzold", "R. Serban"], "title": "Adjoint sensitivity analysis for differential-algebraic equations: The adjoint dae system and its numerical solution", "venue": "SIAM Journal on Scientific Computing,", "year": 2003}, {"authors": ["J.A. Carrillo", "M. Fornasier", "G. Toscani", "F. Vecil"], "title": "Particle, kinetic, and hydrodynamic models of swarming", "year": 2010}, {"authors": ["J.A. Carrillo", "S. Martin", "V. Panferov"], "title": "A new interaction potential for swarming models", "venue": "Physica D: Nonlinear Phenomena,", "year": 2013}, {"authors": ["J. Cervera", "A.J. van der Schaft", "A. Ba\u00f1os"], "title": "Interconnection of port-hamiltonian systems and composition of dirac", "venue": "structures. Automatica,", "year": 2007}, {"authors": ["R.T.Q. Chen", "Y. Rubanova", "J. Bettencourt", "D. Duvenaud"], "title": "Neural ordinary differential equations", "venue": "Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["J. de Kleer", "J. Kurien"], "title": "Fundamentals of model-based diagnosis", "venue": "IFAC Proceedings Volumes,", "year": 2003}, {"authors": ["J. de Kleer", "A. Mackworth", "R. Reiter"], "title": "Characterizing diagnoses and systems", "venue": "\u201dJournal of Artificial Inteligence\u201d,", "year": 1992}, {"authors": ["M. Abadi"], "title": "TensorFlow: Large-scale machine learning", "venue": "on heterogeneous systems,", "year": 2015}, {"authors": ["S. Chakraborty"], "title": "Interpretability of deep learning models: A survey of results", "year": 2017}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "D. Pedreschi", "F. Giannotti"], "title": "A survey of methods for explaining black box models, 2018", "year": 2018}, {"authors": ["H.K. Khalil"], "title": "Nonlinear Systems. Pearson Education", "year": 2002}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A method for stochastic optimization, 2014. cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations", "venue": "San Diego,", "year": 2015}, {"authors": ["I. Matei", "J. de Kleer", "R. Minhas"], "title": "Learning constitutive equations of physical components with constraints discovery", "venue": "Annual American Control Conference (ACC),", "year": 2018}, {"authors": ["I. Matei", "J. De Kleer", "M. Zhenirovskyy", "A. Feldman"], "title": "Learning constitutive equations of physical components with predefined feasibility conditions", "venue": "American Control Conference (ACC),", "year": 2019}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences, 2017", "year": 2017}, {"authors": ["C. Molnar"], "title": "Interpretable Machine Learning", "venue": "https://christophm.github.io/interpretable-ml-book/", "year": 2019}, {"authors": ["W.J. Murdoch", "C. Singh", "K. Kumbier", "R. Abbasi-Asl", "B. Yu"], "title": "Definitions, methods, and applications in interpretable machine learning", "venue": "Proceedings of the National Academy of Sciences,", "year": 2019}, {"authors": ["D.D. Nikoli\u0107. Dae"], "title": "tools: equation-based object-oriented modelling, simulation and optimisation software", "venue": "PeerJ Computer Science,", "year": 2016}, {"authors": ["A. Paszke"], "title": "Automatic differentiation in pytorch", "year": 2017}, {"authors": ["R.J. Patton", "P.M. Frank", "R.N. Clark"], "title": "Issues of Fault Diagnosis for Dynamic Systems", "year": 2000}, {"authors": ["L. Petzold", "S. Li", "Y. Cao", "R. Serban"], "title": "Sensitivity analysis of differential-algebraic equations and partial differential equations", "venue": "Computers and Chemical Engineering,", "year": 2006}, {"authors": ["C.W. Reynolds"], "title": "Flocks, herds and schools: A distributed behavioral model", "venue": "In ACM SIGGRAPH computer graphics,", "year": 1987}, {"authors": ["R. Serban", "A.C. Hindmarsh"], "title": "CVODES: The Sensitivity-Enabled ODE Solver in SUNDIALS. In 5th International Conference on Multibody Systems, Nonlinear Dynamics, and Control, Parts A, B, and C, volume 6 of International Design Engineering", "venue": "Technical Conferences and Computers and Information in Engineering Conference,", "year": 2005}, {"authors": ["A.J. van der Schaft"], "title": "Port-Hamiltonian systems: an introductory survey", "venue": "Proceedings of the International Congress of Mathematicians Vol. III, number suppl", "year": 2006}, {"authors": ["A.J. van der Schaft", "D. Jeltsema"], "title": "Port-hamiltonian systems theory: An introductory overview", "venue": "Foundations and Trends in Systems and Control,", "year": 2014}, {"authors": ["A.J. van der Schaft", "B.M. Maschke"], "title": "Port-hamiltonian systems on graphs", "venue": "SIAM Journal on Control and Optimization,", "year": 2013}], "sections": [{"text": "To understand changes in physical systems and facilitate decisions, explaining how model predictions are made is crucial. We use model-based interpretability, where models of physical systems are constructed by composing basic constructs that explain locally how energy is exchanged and transformed. We use the port Hamiltonian (p-H) formalism to describe the basic constructs that contain physically interpretable processes commonly found in the behavior of physical systems. We describe how we can build models out of the p-H constructs and how we can train them. In addition we show how we can impose physical properties such as dissipativity that ensure numerical stability of the training process. We give examples on how to build and train models for describing the behavior of two physical systems: the inverted pendulum and swarm dynamics.\nI. Introduction\nThe necessity for interpretability comes from the fact that it is not always enough to train and model and get an answer, but is also important to understand why a particular answer was given. A simple but meaningful definition of model interpretability given in [17] relates this notion to the degree to which a human can understand the cause of a decision. In our case, since we care about models that describe the behavior of physical systems, we change the definition to the degree to which a human can understand the physical processes that cause a prediction. Throughout this paper we focus on physically-interpretable models: models that embed physical laws that explain how energy is transformed and exchanged in the system. A physically-interpretable model facilitates learning and updating the model when something unexpected happens. This update is done by finding an explanation for an unexpected event. For example, an electrical motor unexpectedly overheats and we ask ourselves: \u201cWhy is the motor overheating?\u201d. We learn that the motor overheats every time every we subject it to a load above ar X iv :2\n00 3.\n10 02\n5v 1\n[ cs\n.A I]\n2 2\nM ar\n2 02\n2 some threshold. Consequently, we can update the model and decide that the motor should not be subjected to a high load or that we should use a different motor rated for higher loads. Such physical interpretability is important for any machine learning (ML) model giving predictions without explanations, where scientific findings stay completely hidden. Explainable predictions are crucial to facilitate failure analysis, failure progress or design feedback.\nIn diagnosis applications [9], [8], [22] we would like to explain/solve inconsistencies between our knowledge induced expectations and the observed behavior. For example, we may find out that there is a contradiction between the knowledge about the vehicles\u2019s past behaviour and the new observations about the current vehicle mileage. Consequently we may ask: \u201cWhy does my vehicle suddenly makes worse mileage, even though it has never done so before?\u201d. The explanation of the mechanics helps the vehicle\u2019s owner reconcile the contradiction: \u201cOne of the brake pads was stuck and consequently the engine had to generate more torque to cope with the additional friction-induced load.\u201d The more an algorithm/model prediction affects the physical world, the more important is for the algorithm/machine to explain its behaviour.\nML models such as classifiers are becoming more ubiquitous in fault detection for physical systems. Take for instance the fault detection and isolation for a wind-turbine. We would like the prediction model, e.g., the classifier, to predict faults with 100% accuracy, since failing to predict failures can lead to catastrophic events. An explanation might reveal that the most important feature learned for predicting generator bearing failures is the generator\u2019s electrical current at high frequencies, which is indicative of the presence of vibrations in the generator shaft due to bearing (incipient) failures. ML models tend to pick up biases from the training data. Such a phenomenon can turn the ML models (e.g., classifiers) to favor more common faults, discriminating against rare but possibly catastrophic faults. Interpretability is a useful debugging tool to detect bias in ML models. Interpretability enables changes in the loss function (e.g., adding new regularization terms) that capture biases in the training data that otherwise would be ignored by the loss function, which the ML model optimises. An additional reason for demanding interpretability from models is to ensure adoption by industry. Even today, classifiers based on decision trees are popular due to their ability to explain how predictions are made.\nInterpretability is one of the main traits behind fault diagnosis and prognostics. Having an interpretation for a faulty prediction helps with the understanding of the cause of the fault. In addition, it gives an avenue for repairing the system. Interpretability can also be used for\n3 diagnosing the ML model itself. From this perspective, we would like to understand why some predictions were incorrect (e.g., misclassification), and fix the model to improve the prediction accuracy. For example, this avenue can be used to generative adversarial networks more robust. The repair process may include adding/removing features (e.g., sensor measurement) that enables a better discrimination between classes.\nWe do not always need model interpretation. Such reasons may include: not having a significant impact, the problem is well understood, or interpretability may enable \u201cgaming\u201d the system [18]. An example of the first reason is feedback control design. For such an application a black-box, regression type of model is typically sufficient. The last reason has a significant impact on cyberphysical system security, where an attacker may use the model to understand the physical system and design attack schemes that leverage system weaknesses.\nThe way we look at the notion of interpretbility in this paper fits also with the view of [19], where interpretability translates to the extraction of insights for a particular audience into a chosen domain problem about domain relationships contained in data. In particular, the insights we produce will be in mathematical equations format, with physical meaning. Our interpretability method is not based on post hoc interpretations of deep learning models [11], [12]. Our models are not typical regression or statistical models, but they show how the energy of a system is transformed and exchanged. Namely, they are compositions of basic constructs that describe locally how energy is transformed as it passes through the system.\nAmong the many criteria used to classify interpretability (see for instance page 16 of [18]), our models are global, model-based, model specific, and intrinsically interpretable. Model-based interpretability is based on imposing constraints on the form of the ML models so that they provide meaningful information about the learned relationships. In ML applications there is often a trade-off between the choice of a simpler but easier to interpret model and a more complex (e.g., black box) model, but with low interpretability. The models we propose can be arbitrarily complex but they will still retain the physical interpretability.\nWe achieve physical interpretability by using a well defined mathematical formalism called the port-Hamilonian (p-H) formalism [5], [26], [28]. This is a general and powerful geometric framework to model complex dynamical networked systems. P-H systems are based on an energy function (Hamiltonian) and on the interconnection of atomic structure elements (e.g. inertias, springs and dampers for mechanical systems) that interact by exchanging energy. Such models\n4 give insights into the physical properties of the system, the framework being particularly suited for finding symmetries (e.g., discrete, or Lie groups of transformations) and conservation laws (under the form of Casimir functions). The models we learn are non-causal in nature since they deal with energy exchanges and transformations and not with changes in the outputs as a result of varying the inputs.\nPaper Structure: In Section II we describe the main constructs used to build physically interpretable models. In Section III we demonstrate how we can use these constructs to build models for predicting physical system behaviors. We discuss aspects of training p-H based and stable models in Section IV. In Section V we discussed how our approach fits the broader category of ML interpretable models. We end the paper with two modeling and learning examples (the inverted pendulum and the swarm dynamics) in Section VI and some conclusions.\nII. Interpretability constructs\nIn this section we briefly introduce the p-H formalism, its constructs and give some examples\nof simple physical systems represented in the p-H formalism."}, {"heading": "A. Port-Hamiltonian framework", "text": "Consider a finite-dimensional linear state space X along with a Hamiltonian H :X\u2192R+ defining energy-storage, and a set of pairs of effort and flow variables {(ei, fi) \u2208 Ei\u00d7Fi, i \u2208 {S ,R,P}}, describing ports (ensembles of elements) that interact by exchanging energy. The letters \u201cS\u201d, \u201cR\u201d and \u201cP\u201d refer to energy storing, resistive and external ports, respectively. Then, the dynamics of a p-H system \u03a3 = (X,H,S,R,P,D) are defined by a Dirac Structure D [26], [27] as\n( fS,eS, fR,eR, fP,eP) \u2208 D\u21d4 eTS fS+ e T R fR+ e T P fP = 0,\nwhere (i) S= ( fS,eS) \u2208FR\u00d7ER =X\u00d7X is an energy-storing port, consisting of the union of all the energy-storing elements of the system (e.g. inertias and springs in mechanical systems), satisfying fS = \u2212x\u0307,eS = \u2202H\u2202x (x), x \u2208 X such that d dt H = \u2212eTS fS = e T R fR + e T P fP, (ii) R = ( fR,eR) \u2208 FR\u00d7ER is an energy-dissipation (resistive) port, consisting of the union of all the resistive elements of the system (e.g. dampers in mechanical systems), satisfying \u3008eR, fR\u3009 \u2264 0 and, usually, an input-output relation fR =\u2212R(eR), (iii) P= ( fP,eP) \u2208FP\u00d7EP is an external port modeling the interaction of the system with the environment, consisting of a control port C and an interconnection port I, and (iv)\n5 D\u2282F \u00d7E = FR\u00d7ER\u00d7FR\u00d7ER\u00d7FP\u00d7EP is a central power-conserving interconnection (energyrouting) structure (e.g. transformers in electrical systems), satisfying \u3008e, f \u3009 = 0, \u2200( f ,e) \u2208D, and dimD = dimF , where E = F \u2217, and the duality product \u3008e, f \u3009 represents power.\nThe basic property of p-H systems is that the power-conserving interconnection of any number of p-H systems is again a p-H system. An important and useful special case is the class of input-state-output p-H systems x\u0307 = [J(x)\u2212R(x)]\u2202H\u2202x + g(x)u, y = gT (x) \u2202H \u2202x (x), where u,y are the input\u2013output pairs corresponding to the control port C, J(x) = \u2212JT (x) is skew-symmetric, while the matrix R(x) = RT (x) \u2265 0 specifies the resistive structure."}, {"heading": "B. Constructs", "text": "The Dirac structure operator that enforces the conservation of energy involves a set of constructs/elements with particular ways of transforming energy: energy-storing elements, resistive elements, source elements. With the exception of resistive elements, the energy-storing and source constructs can be defined for both flow and effort type of variables. Figure 1 shows examples of patterns of dependencies between the flow and effort variables. Here we are interested mainly in the energy storing and resistive constructs. Alternative mathematical definitions for flow store,\n6 effort store and resitive constructs are shown in equations (1), (2) and (3), respectively. x\u0307 = f ,e = \u2202H\u2202x , (1) x\u0307 = e,f = \u2202H\u2202x , (2) R(e, f ) = 0, p = eT f \u2265 0, (3)\nwhere e and f are the effort and flow variables, respectively, H(x) is the Hamiltonian function, and R is a resistive function such that (by convention) the instantaneous power p = eT f is positive. The previous construct types are one-port or two-terminal devices. They are not always enough and hence we also use two-port devices: transformers and gyrators. The constitutive relations for the transformers and gyrators are show in equations (4)-(5), respectively. e1 = g1(e2),f1 = g2( f2) (4) e1 = g1( f2),f1 = g2(e2) (5) This type of construct classification is domain agnostic, that is, it can be applied to multiples physical domains such as mechanical (translational and rotational), thermal, fluid and magnetics domains."}, {"heading": "C. Examples", "text": "In this section we present two examples of physical systems from the mechanical and electrical domain represented using the constructs described in the previous section. We consider the massspring damper example with a force as input acting on the mass and the RLC serial circuit.\n1) Port-Hamiltonian representation of the mass-spring-damper system: The mass-spring damper\nphysical system is depicted in Figure 2. It consists of three components and a force source.\nThe damper is a resistive construct, whose constitutive equation is fd = d \u00b7 ed, where d is the damping constant, fd is the flow variable (the force), and ed is the effort variable (the velocity). The spring\u2019s Hamiltonian is H(q) = 12kq 2, where q is the spring elongation and k is the stiffness\n7\nconstant. Similar to the damper, case the flow variable is the force fk = \u2202H\u2202q = kq, and the effort variable is the relative velocity ek = q\u0307. Hence, the spring is an effort storage construct. The mass\u2019s Hamiltonian in terms of the momentum p is H(p) = 12m p 2, where m is the mass constant. The mass velocity acting on the mass is expressed as em = \u2202H\u2202p = 1 m p, and the force fm = p\u0307. Hence it follows that the mass is a flow storage construct. The system has a flow source, as well, defined by the force fs = F(t). The Dirac structure conserves the system\u2019s energy and, assuming that the damper and the spring are grounded at the zero position, is given by fm + fk + fd = fs and em = ek = ed = es. It can be easily check that these effort and flow constraints do satisfy the definition of a Dirac structure.\n2) Port-Hamiltonian representation of the RLC circuit: Consider the RLC circuit in Figure 3. In this example the flow and effort variables are the current and the voltage, respectively. The resistor constitutive equation is eR = R \u00b7 fR, hence it is a resistive construct. The capacitor energy is given by H(q) = 12C q 2, where C is the capacitance, and q is the electric charge. Hence eC = \u2202H\u2202q = q C and fC = q\u0307, which shows that the capacitor is a flow storage construct. The inductor\u2019s energy is given by H(\u03c6) = 12L\u03c6 2, where L is the inductance and \u03c6 is the magnetic flux. The flow is given by fL = \u2202H\u2202\u03c6 = \u03c6 L , and the effort is eL = \u03c6\u0307. Therefore, the inductor is an effort storage element. The Dirac structure is given by fC = fR = fL and eC + eL + eR = 0, which results in the energy conservation.\nThe type of systems we can model using the p-H formalism are characterized by boundary conditions through which energy is transferred to the system (e.g., through flow or effort sources) and measurements of system variables (Figure 4). The measurements provide knowledge about the system behavior. The internal behavior of the system is modeled as compositions of pH constructs based on some given or assumed topology. In the case we know the structural decomposition of the system, we can use this information to construct a topology. When such information is not available, we choose a rich enough topology that has a good chance to capture the observed behavior Similar to neural networks, we can define a basic construct out of which we can build a network of p-H constructs. In the NN case the basic construct is the linear layer followed by a nonlinear map. Here we define the basic construct in terms of how the flow and the effort variables are manipulated. One possible example of such a construct, expressed in terms of mechanical components, is shown in Figure 5. In this design, we have a resistive (damper d1) and an effort storage element (spring k1) in parallel connected to a mass-spring-damper (m2, k2, d2), where the damper and the spring are grounded. This design serves two purposes: (i) to allow for a change in the effort variable (the relative velocity between the two ports), and (ii) to allow for a change in the flow variable through the grounded elements. Each component is characterized by a parametrized map: the resistive map for the damper (R(e, f ;w)), and the energy functions for the storage elements (H(x;w)), where w is a vector of parameters. In the case we choose to defined the resistive and energy maps as NN, the vector of parameters w\n9\nincludes the weights and the biases of the NN. In this way, we can define nonlinear masses, springs and dampers. The Dirac structure corresponding to this construct is fa + fb + fc = 0, and pb = pc. To ensure that the construct is dissipative, we enforce (by convention) a positive instantaneous power for the resistive element, that is, p = eT f \u2265 0. In the case if linear massessprings-dampers, where the damping coefficient is non-negative, such a condition is always satisfied. The dissipative conditions can be included as regularization functions or can be directly enforced as inequality constraints in the optimization problem designed for learning the model parameters. Alternatively, we can come up with sufficient conditions of the structure of the resistive that enforce dissipativity of the construct. We have investigated such conditions in [16],\n10\nwhere we showed how the gradients of the energy function should look like. For example, in the case of springs, an energy function of the form H(x) = \u2211n\ni=1 aix 2i, with ai \u2265 0 it is enough to\nensure that the resulting basic construct is dissipative. We recall that in the case of the spring, x denotes the spring\u2019s elongation. Similarly, we can derive sufficient conditions for the structure of the damper\u2019s resistive maps, and mass\u2019s energy function. Positive parameter type of constraints (i.e., box constraints) are much easier to deal with in the optimization formulation. Moreover, they can be removed altogether through variable transformations.\nWe can use the basic p-H construct to build a network of components that can model the observed behavior. An example of such a network is shown in Figure 6, where we depict three boundary conditions, two measured quantities, and three layers. As in the case of a typical NN, choosing the number of layers and the layer sizes, is more of an art than a formal process. We can start with a large enough network and impose sparsity constraints to achieve the simplest network structure that models the observed behavior. We will discuss how we can achieve sparsity in a subsequent section.\nThe resulting mathematical model is a differential algebraic equation (DAE) that takes the\nform:\n0 = F(z\u0307,z,u;\u03b2), (6)\ny = h(z,u;\u03b2), (7)\nwhere F is a map that depends on the state vector z and its time derivative z\u0307, on the exogenous inputs u through which the boundary conditions are set, and on the vector of parameters comprising the parameters of all energy functions and resistive maps of the basic p-H constructs.\n11\nThe DAE results from collecting the constitutive relations of the basic p-H constructs together with the Dirac structure (energy conservation) constraints.\nIV. Training sparse interpretable models\nIt should not be a surprise that learning the parameters of a DAE model is formulated as a (constrained) non-linear least square problem, to which we add regularization function depending on the additional objectives we would like to achieve (e.g., sparse models). The nonlinear least square formulation is given by:\nmin w\nJ(w) +\u03bbR(w) (8)\nsubject to: w \u2208W, (9)\nF(z\u0307,z,u;w) = 0,\u2200t \u2208 T , (10)\ny = h(z,u;w),\u2200t \u2208 T , (11)\nwhere J(w) is the quadratic loss function defined in terms of output measurements, e.g., J(w) = 1 T \u222b T 0 \u2016ym(t)\u2212y(t)\u2016 2, where y(t) is the simulated output and ym(t) is the measured/observed output. The map R(w) is the regularization function (e.g., function that encourages the sparsity of the model or dissipativity of the constructs), W is a feasibility set for the model parameters, and T is the time domain over which the output measurements are taken. Note that although the DAE appears as an equality constraint, we actually eliminate it by solving the DAE over the time horizon T , which computes the state vector z and the simulated output measurement vector y. We distinguish two case: (i) the DAE can be transformed into an ordinary differential equation (ODE), and (ii) the DAE cannot be represented as an ODE. The transformation process is based on index reduction methods, which consist of differentiating the algebraic constraints, and then solving the resulting differentiated system. Because the original constraints have been dropped, and replaced by some of their derivatives, such a process may introduce many additional solutions which no longer satisfy the constraints. Hence, when possible, modified index reduction techniques based on deflation [1] are used.\nIf the dynamical system can be expressed as an ODE, we can use TensorFlow [10] or Pytorch [21] ODE solvers capability to compute the latent variables needed for the evaluation of the loss function and its gradients. The advantage of such solvers is that they support automatic\n12\ndifferentiation throughout their computation steps. If the system admits a DAE then we can use DAE solvers that include sensitivity analysis (e.g., IDAS, CVODES). They compute the sensitivity of the latent variables with respect to the optimization variables [20]. Note that the stability of the optimization algorithm depends on the stability of the ODE/DAE solvers. ODE/DAE solvers can become numerically unstable when searching through the parameter space. We can actually learn and avoid regions of the parameter space that cause numerical instability, as we demonstrated in [15]. Depending on how the parameter feasibility set W is modeled there are several options to deal with constraint (9). For box constraints, we can use variable transformations to eliminate the constraints. Alternatively, we can use projected gradient descent algorithms, provided the projection operation can be done efficiently. If the constraint set is represented through a set of function equalities or inequalities, we can use nonlinear programming (e.g., penalty methods, barrier methods, primal-dual methods, augmented Lagragians methods, etc.) to solve the problem. A particular case of this approach is to include the constraints in the regularization function and test different weights until a satisfactory result is obtained. Unless we are lucky, the result will by suboptimal."}, {"heading": "A. Sparsity and dissipativity constraints", "text": "The sparsity constraints are meant to produce simple models capable of representing the observed behavior or enforce dissipativity for the model elements. Model sparsity can be also achieved after we trained a model, by using model reduction techniques. Sparsity during training can be achieved by cutting the flows on the links. For example, by defining the regularization\nfunction as R(w) = \u2211M\nj=1 | f j(w)|, where f j are the flows through the M links (connections) of the network, we encourage some of the flow to be very small, to the extent that they can be considered zero. We note that the flows f j are functions of the model parameters and are evaluated by solving the DAE. If we would like to avoid an arbitrary choice for the weight parameter \u03bb, we can formulate a constraints optimization problem that focuses on reducing the flows through the model, while maintaining a preset accuracy level \u03b5. In particular we have the\n13\nfollowing constrained optimization problem:\nmin w\nR(w) (12)\nsubject to: J(w) \u2264 \u03b5, (13)\nw \u2208W, (14)\nF(z\u0307,z,u;w) = 0,\u2200t \u2208 T , (15)\ny = h(z,u;w),\u2200t \u2208 T , (16)\nwhere \u03b5 can be chosen as a function of the measurement noise variance. This constrained optimization problem can be solved using a primal-dual method, where the primal problem is defined as\nmin w\nR(w) +\u03bbk[J(w)\u2212\u03b5] (17)\nw \u2208W, (18)\nF(z\u0307,z,u;w) = 0,\u2200t \u2208 T , (19)\ny = h(z,u;w),\u2200t \u2208 T , (20)\nand the dual problem is solved by a projected gradient decent algorithm based on the iteration:\n\u03bbk+1 = \u03bbk +\u03b1[J(w)\u2212\u03b5],\nwith \u03b1 the iteration step-size. Note that the primal optimization problem can be transformed into an unconstrained optimization problem by solving the DAE F(z\u0307,z,u;w) at each iteration of the algorithm. If a gradient based optimization algorithm is used, to compute the gradient of the loss function J(w), we need the partial derivatives \u2202y(t)\u2202w = \u2202h \u2202x \u2202x \u2202w , along the trajectory of the DAE. We mentioned earlier that in the case the DAE can put represented as an ODE, both TensorFlow and Pytorch can be used. In the case of DAEs, solvers endowed with sensitivity analysis can be used to compute the gradients of the loss function. If the size of the state variables and the number of parameters is large, sensitivity analysis based on backward methods [2] (e.g., based on solving the adjoint equations) are recommended. In the previous section we showed how we can define maps for the basic construct elements that ensure the constructs are dissipative, by imposing a specific structure on the maps and their parameters. Alternatively, we can impose the dissipative condition through a regularization function defined in terms of the construct\u2019s instantaneous\n14\npower. In particular we can define the function R(w) = \u222b T\n0 pi(t)dt, where pi(t) = e T i (t) fi(t) is the\ninstantaneous power of element i.\nV. How we fit in the larger interpretableML context\nWe will use the framework proposed in [19] to describe the main characteristics of the interpretability approach. Our approach fits the class of model-based interpretability, where we use constructs that provide insights about the learned relationships. In [19] the authors state that due to the imposed constraints, the space of possible models is smaller and consequently this may result in lower predictability. Our view is that with the right constructs we can attain high complexity and ensure accurate predictions. As neural networks can approximate arbitrarily close any function by composing layers of linear relations followed by non-linear activation functions, the same can be achieved by composing constructs capable of modeling energy transformations. As in the case of NN-based models we are often interested in finding the simplest model that is able to accurately describe the observed behavior. For this task we use constraints on the construct flows that effectively eliminate model elements. Here, by sparsity we understand the smallest number of constructs that when composed and simulated are able to accurately recover the observed behavior. Another relevant characteristics is the simulatability of the model. In our approach, simulatability translates to the ability to simulate the model under different boundary conditions. A sufficient condition that ensure simulatability is stability. Stability follows by imposing a dissipative constraint on the model constructs [13]. A component/construct is dissipative if it does not generate energy internally, and all energy is externally supplied. It is sufficient to impose such constraint locally since the composition of dissipative constructs results in a dissipative model. Our models have modularity embedded by default since they are created as compositions of constructs, where each construct can be independently interpreted. For example energy storage components stores flow or effort. In addition to the choice of model, the \u201cfeatures\u201d used as inputs are also important. In our context the \u201cfeatures\u201d are effort or flow sources (e.g., current of voltage sources) that supply energy to the system. They are seen as boundary conditions for the system dynamics. Since they have a particular physical meaning, they are easy to interpret. In the case of autonomous systems (there are no exogenous inputs) there will be no input features but only outputs used in the loss functions. The outputs are functions of measurements of effort or flow variables.\n15\nVI. Examples\nWe demonstrate learning interpretable models for two examples: the inverted pendulum and\nthe Cucker-Smale model for swarm dynamics.\nA. Inverted pendulum\nIn the first example we show how we can learn the closed-loop behavior of the inverted\npendulum using physically interpertable components.\n1) Physical model: We are using the inverted pendulum model [7], whose dynamics are given\nby\nx\u0307 = v (21) v\u0307 = (J + ml2)(bv\u2212F \u2212ml\u03c92 sin\u03b8)\u2212m2l2gsin\u03b8cos\u03b8\n(mlcos\u03b8)2\u2212 (m + M)(J + ml2) (22)\n\u03b8\u0307 = \u03c9 (23)\n\u03c9\u0307 = ml(F cos\u03b8+ ml\u03c92 sin\u03b8cos\u03b8\u2212bvcos\u03b8+ (m + M)gsin\u03b8)\n(mlcos\u03b8)2\u2212 (m + M)(J + ml2) (24)\nwhere x and v are the cart\u2019s position and velocity, respectively, while \u03b8 and \u03c9 are the pole\u2019s angle and angular velocity, respectively. The symbol F denotes the force acting on the cart and plays the role of the input signal. The state vector is given by zT = [x,v, \u03b8,\u03c9]. The parameters of the system and their values are listed in Table I. Note that in its original form, the dynamics of the\ninverted pendulum system are represented as a DAE. We explicitly solved for the accelerations to generate the ODE form.\nThe inverted pendulum system has two inertial elements (the cart with mass M and the pendulum with mass m) and at least one resistive element due to the cart friction. The translational motion of the cart is transform into a rotational motion that acts upon the pendulum. Hence our model will have two inertial elements (a translational one and a rotational one), a resistive\n16\nelement and a transformer. The inertial elements store kinetic energy, while the resistive element dissipates energy. We recall that the effort and flow variables for the translational and rotational cases are forces ( f ) and velocities (v), and torques (\u03c4) and angular velocities (\u03c9) respectively. We recall that the translational and rotational elements are given by\n\u03be\u0307 = f , v = \u2202H\u2202\u03be , \u03b7\u0307 = \u03c4, \u03c9 = \u2202H\u2202\u03b7 ,\nrespectively. We will consider resistive elements for both the translational and rotational elements, namely f = Rt(v) and \u03c4 = Rr(\u03c9). Finally the transformer element can be described by a map g :R4\u2192R2, such that g( f ,v, \u03c4,\u03c9) = 0. 2) Model training: By combining the five types of p-H elements we obtain the configuration shown in Figure 7, where we added a force source that generates the force F acting on the cart. Our objective is to learn the component maps, where we make the common sense assumption\nthat the inertial and resistive elements are linear, that is: v = \u03beM , \u03c9 = \u03b7 J , f = dtv and \u03c4 = dr\u03c9, respectively. The key component is the transformer element. If you model the map g as a nonlinear map, we end up with a DAE which will prevent us from using ODE solvers endowed with automatic differentiation feature. When solving the DAE, the map g corresponds to an algebraic loop for which we need to use a Newton-Rhapson algorithm to compute the variables f and \u03c4. Note that the velocities are states and hence considered known for the purpose of solving the DAE at each time instant. The solution of the algebraic loop is a map ( f , \u03c4) = h(v,\u03c9), Hence, we can replace the map g with two maps f = h1(v,\u03c9) and \u03c4 = h2(v,\u03c9). This change transforms the DAE into an ODE. In a sense, instead of learning the map g, we learn the solution of map g along the system trajectory, where the unknown variables are the force f and torque \u03c4. The resulting ODE model is given by\n17\nx\u0307 = v v\u0307 = 1 M (\u2212F \u2212d1v + f ) \u03b8\u0307 = \u03c9\n\u03c9\u0307 = 1 J (\u03c4\u2212d2\u03c9),\nwhere f = h1(v,\u03c9) and \u03c4 = h2(v,\u03c9). We parameterize the map h = h(v,\u03c9;\u03b2) as a neural network with one hidden layer of size 50, where the activation function is chosen as tanh. Hence, the total number of parameters is (50\u00d72+50+2\u00d750+2)+4 = 156 where the last 4 parameters correspond to the linear inertial and resistive components. We use Pytorch [21] and the torchdiffeq Python package [6] to solve a nonlinear least square optimization problem. We assume that the state vector is measured and that the input force F is generated using a pre-trained linear controller with gain K = [1.2501,2.7612,\u221216.3099,\u22123.7814]. Based on the diagram shown in Figure 7 we constructed an ODE and trained the components parameters using four time series with initial conditions belonging to \u03b80 \u2208 {\u22120.1,0.1}, x0 \u2208 {\u22120.2,0.2} and zero velocities. In particular, we solved the following optimization problem\nmin \u03b2\n1 N N\u2211 i=1 T\u2211 j=1 \u2016z(i)(t j)\u2212 z\u0302(i)(t j)\u20162 +\u03bb\u2016ODE(0)\u20162\nsubject to: \u02d9\u0302z = ODE(z\u0302;\u03b2), z\u0302(0) = z(0),\nwhere ODE(z) is the ODE generated by the block diagram shown in Figure 7. The loss function includes a regularization function that ensured the model has an equilibrium point at zero.\nWe used Adam [14] algorithm to learn the parameters of the p-H components. In this type of learning, the time complexity corresponding to one optimization iteration includes the time for solving the ODE (forward propagation) and the time for computing the sensitivities of the state vector with respect to the optimization parameters. The latter is typically larger. In addition, the choice of ODE solver and the solution sampling period makes a significant difference. We chose an ODE solver based on the midpoint method. Using a time horizon of 6 seconds and a sampling period of 0.05 seconds, the time for each optimization iteration is 1.2 seconds, where more than 80% is allocated for computing the state sensitivities. We have experimented with other ODE solvers but they typically have higher time complexity. For example, when\n18\nusing the fourth order Runge-Kutta method, the iteration time is more than 2.3 seconds. The iteration time increases with the length of the time horizon and the sampling frequency. A comparison between a simulated (\u201ctrue\u201d) and learned system trajectory is shown in Figure 8. We tested the generalizability to one hundred different initial conditions randomly drawn so that\nx0 \u2208 [\u22120.2,0,2], v0 \u2208 [\u22120.1,0.1], \u03b80 \u2208 [\u22120.2,0.2] and \u03c90 \u2208 [\u22120.1,0.1]. We computed the MSEs for each initial condition. The statistics of the resulting MSE values is: the mean is given by 1.57\u00d7 10\u22124, and the standard deviation is given by 1.4\u00d7 10\u22123. Note that the key aspect of learning this model is the velocity trajectories which are the inputs for the map h. The more diverse velocity trajectories we use, the more generalizable the model will be.\n19"}, {"heading": "B. Swarm dynamics", "text": "We demonstrate how the p-H formalism can be used to learn the interaction behavior between\nparticles.\n1) Mathematical model: We are interested in models describing the dynamics of swarms or particle ensembles (e.g. bird flocks), which have been studied intensively through the years [3], [4], [24]. We model the system of interacting particles as a graph topology based on port-Hamiltonian basic constructs, and learn the parameters of the constructs. To showcase our approach we use the Cucker-Smale (CS) model [3] to generate training and test data for the learning tasks. Let i denote a particle in an ensemble of N particles. The CS particle interaction model that include particle interactions based on potential energy as well [3] is given by\nx\u0307i = vi (25) v\u0307i = 1 N N\u2211 j=1 G(\u2016xi\u2212 x j\u2016)(v j\u2212 vi)\u2212 1 N \u2211 i, j \u2207U(\u2016xi\u2212 x j\u2016) (26)\nwhere a typical choice for the interaction function G is G(r) = 1(1+r2)\u03b3 , and the potential function takes the form U(r) = \u2212CAe\u2212r/lA +CRe\u2212r/lR , with CA,CR, lA, lR positive scalars. Note that in the case of the CS model it makes sense to use the p-H constructs. Indeed, consider the three particle example for the one dimensional case, where the grounded dampers and spring are omitted. This can be achieved by considering linear spring-dampers with zero coefficient. The fully connected topology of the mass-spring-damper network is shown in Figure 9. We denote by Hi and Hi j\nthe Hamiltonian functions of the masses and springs, respectively. We assume unitary masses,\n20\nand hence the momenta are equal to the mass velocities, that is, pi = vi, i = {1,2,3}. The forces through the links are the sum of the forces through the dampers and springs, and are given by fi j = \u2202Hi j \u2202qi j\n+ R(qi j)(vi \u2212 v j), for (i, j) \u2208 {(1,2), (2,3), (3,1)}. The forces through the masses can be expressed as: f1 = f31\u2212 f12, f2 = f12\u2212 f23 and f3 = f23\u2212 f31. We get the expressions for the mass momenta dynamics as:\np\u03071 = \u2202H31 \u2202q31 \u2212 \u2202H12 \u2202q12 + R(q31)(v3\u2212 v1) + R(q12)(v2\u2212 v1), (27) p\u03072 = \u2202H12 \u2202q12 \u2212 \u2202H23 \u2202q23 + R(q12)(v1\u2212 v2) + R(q23)(v3\u2212 v2), (28) p\u03073 = \u2202H23 \u2202q23 \u2212 \u2202H31 \u2202q31 + R(q23)(v2\u2212 v3) + R(q31)(v1\u2212 v3). (29)\nThe dynamics for the spring elongations are\nq\u0307i j = vi\u2212 v j = \u2202Hi \u2202pi \u2212 \u2202H j \u2202p j\n(30)\nfor (i, j) \u2208 {(1,2), (2,3), (3,1)}. To recover the CS model with potential, we replace the relative positions qi j with the absolute positions, namely qi j = qi \u2212 q j. Recalling that spring potentials are symmetric functions, we get that\n\u2202H31 \u2202q31 \u2212 \u2202H12 \u2202q12 = \u22121 3 (\u2207U(q1\u2212q3)\u2212\u2207U(q1\u2212q2)) (31) \u2202H12 \u2202q12 \u2212 \u2202H23 \u2202q23 = \u22121 3 (\u2207U(q2\u2212q1)\u2212\u2207U(q2\u2212q3)) (32) \u2202H23 \u2202q23 \u2212 \u2202H31 \u2202q31 = \u22121 3 (\u2207U(q3\u2212q2)\u2212\u2207U(q3\u2212q2)) (33)\nSubstituting (31)-(33) in (27)-(29), and recalling that under our assumptions pi = vi, we recover exactly the CS model with potential. Hence we showed that the CS dynamics can be modeled and trained using constructs from the p-H formalism. In particular, we can explain the swarm dynamics: each particle behaves like a flow store, and the interaction between particles can be understood as a combination of effort storage (the spring) and energy dissipation (damper). Tn the case of homogeneous particles, the training process is simplified since all energy functions and resistive maps have identical parameterizations.\n2) P-H formalism based model: We assume that we measure the particle trajectories and the objective is to learn and interpret how the particle interact. We model the interaction between the particles as a combination of resistive and potential maps. The \u201cforce\u201d that controls how particle interact is modeled as a combination of resistive and potential interaction. We consider\n21\nthe force expression as F(p,q) = \u2202H\u2202q (p,q) + R(p,q), where p and q are the relative momentum and distance, respectively. Since we cannot measure the resistive and potential effects separately, we model the overall effect of the two phenomena. In particular, we represent the force F as F(p,q) = f (p,q;\u03b2), where f is a function of p and q, and depends on a vector of parameters \u03b2. It follows that we have the following model:\nq\u0307i = pi (34) p\u0307i = 1 N N\u2211 j=1 F(p j\u2212 pi,q j\u2212qi;\u03b2), (35)\nwhere pi and qi are the particle momentum and position, respectively. We assume unitary mass, and hence the momentum can be interpreted as the particle velocity. In the original CS model, the potential function governing the behavior of the nonlinear springs depend on the relative position only. In addition, the resistive map describing the damping effect is zero for zero relative velocity. Hence, we can recover the force generate by the nonlinear force by evaluating the force F(p,q) at p = 0, that is, F(0,q). We generated is represented by trajectories generated by the CS model. We consider 100 particles in the model and assume we can measure the positions and velocities of the particles. The parameters for the CS model were chosen as: \u03b3 = 0.15, CA = 200, lA = 100, CR = 500, lR = 2.0. Figure 12 shows the velocities of the first 10 particles as generated by the CS model.\n3) P-H formalism based model training: To learn the force map F(p,q) we minimize solve\nthe following optimization function\nmin \u03b2\n1 n n\u2211 l,i \u2016p(l)i \u2212 p\u0302 (l) i \u2016 2 + \u2016q(l)i \u2212 q\u0302 (l) i \u2016 2 +\u03bb\u2016F(0,0;\u03b2)\u20162 (36)\nsubject to: \u02d9\u0302qi = p\u0302i, (37)\n\u02d9\u0302pi = 1 N N\u2211 j=1 F( p\u0302 j\u2212 p\u0302i, q\u0302 j\u2212 q\u0302i;\u03b2), (38)\nwhere pi and qi are the measured particle momenta and positions, and \u03b2 is the vector of parameters of the force map F(p,q). In addition to the quadratic loss function, we added a regularization function that enforces the zero behavior of the interaction function. As in the case of the inverted pendulum, the time complexity comes from the solving the ODE governing the particle interaction rather than from the number of optimization parameters. We modeled\n22\nthe interaction function by a one hidden layer neural network. The size of the hidden layer is 100 and we use tanh as the nonlinear activation function. We used Pytorch to train the model parameters and torchdiffeq to solve the ODE corresponding to the CS model. We experimented with different number of particles and ODE solvers. Figure 11 shows the time complexity per optimization iteration as a function of number of particles, when using CPUs and GPUs, and dopri5 as ODE solvers. Note that although the time complexity in the GPU case increases linearly, we still have large numbers for iteration. After experimenting with other ODE solvers, we chose one based on the midpoint method since it provides a good trade off between complexity and accuracy. The reduced complexity if beneficial in particular for the back propagation step. We generated 10 time series using the CS-model with 30 particles and trained the parameters of the interaction model using a stochastic version of the Adam algorithm, where at each iteration we chose one of the 30 time series at random. The time horizon of the time series is 40 second and the sampling period is 0.1 seconds. The initial conditions for the particle positions and velocities were chosen at random in the interval [-10,10]. Figure 12 shows a comparison between the velocity trajectories generated by the CS-model and the trajectories generated by the CS-model with learned interaction functions. The initial conditions\n23\nwere chosen at random. The two sets of trajectories match both qualitatively and quantitatively. We executed fifty more experiments for validation purposses, where the initial conditions where chose at random. The MSE statistics are: the mean is 0.06 and the standard deviation 0.04. The numbers do not appear to be very small. We have to recall though that we only used 10 time series and that the generalizability of the interaction function depends on what relative positions and velocities are hit during the particle evolution. We executed another validation experiment, meant at checking if we can recover the force component generated by the potential energy. We evaluated the interaction function by varying the relative position while setting the relative velocity to zero. We compared the learned potential function with the potential function defined by the CS model. A graphic comparison is shown in Figure 13. Except around the origin, the two function are almost identical, and demonstrate that indeed we learn a repulsive behavior when particle get too close to each others.\n24"}, {"heading": "C. Time complexity analysis", "text": "We distinguish between two situations: the ODE and the DAE cases. In the ODE case, and when using automatic differentiation, the time complexity is split in two parts: the state vector computation and evaluation of the state sensitivity with respect to the optimization parameters. Assume that loss function L depends on the state vector xk and optimization parameters \u03b2 and that the discrete representation of the state vector is given by xk+1 = f (xk;\u03b2). Then the derivative\nof the loss function with respect to the optimization parameters is dLd\u03b2 = \u2211T k=0 \u2202L \u2202x (xk;\u03b2) dxk d\u03b2 , where\n25\nwe assume that the loss function does not explicitly depends on \u03b2. To evaluate the gradient of L we require the state and the state sensitivity evolution for T time steps. The state evolution is determined using the iteration xk+1 = f (xk;\u03b2) that incurs O(c1 \u00d7T ) time complexity, where c1 = c1(n) is the time required to evaluate the map f at xk and \u03b2, and depends on the state vector size equal to n. The sensitivities evolve according to the dynamics dxk+1d\u03b2 = \u2202 f \u2202x (xk;\u03b2) dxk d\u03b2 + d f d\u03b2 . Hence, the time necessary to evaluate the sensitivities is O(c2\u00d7T ), where c2 = c2(n\u00d7m) is the time necessary to evaluate the current sensitivity, where m is the size of vector \u03b2. Note that c2 is a function of n\u00d7m since dxkd\u03b2 is a matrix. Hence, then time per optimization iteration is O((c1 +c2)\u00d7T ), while the total optimization time complexity is O((c1 +c2)\u00d7T \u00d7N), where N is the number of iterations. There are three parameters that determine the total time complexity: the number of optimization parameters (m), the number of states (n) and the state trajectory length (T ). Note that both c1 and c2 depend also on the type of ODE solvers, since some solvers employ more complex discretization schemes.\nIn the case the dynamics is represented as a DAE, the solvers need to invert the system Jacobian at every time iteration, an operation that takes O(n3). The Jacobian serves for computing the\n26\nsensitivities as well [23]. The inverted Jacobian is further used in a Newton-Rhapson algorithm that solves a nonlinear system of equations. In this case, the time complexity for the optimization algorithm is O((c1 +c2)\u00d7T \u00d7N), where c1 is O(n3) and c2 is O(n\u00d7m). We note that the size of the state vector has a more significant impact on the time complexity. There are several optimization steps that can be performed to the Jacobian to reduce the size of the matrix that needs to be inverted. In particular, after the system of equations is brought to the block lower triangular form (BLT), the blocks on the diagonal are further simplified through a \u201ctearing\u201d operation that is based on Pantelides\u2019s algorithm. These steps are case by case, and we cannot guarantee that they can always result in a reduction of the matrix size that needs to be inverted. A particular case is when the loss function depends on output measurements and not explicitly on the state vector. In this situation we can use backward methods [2], [25] to compute the sensitivity of the output measurements with respect to \u03b2, without having to explicitly compute the state vector sensitivities. Still the Jacobian inversion cannot be escaped, and therefore for large values of n and T it becomes the dominant component of the time complexity.\nVII. Conclusions\nWe showed how we can use the p-H formalism to learn physically interpretable system models. Basic p-H elements serve as blueprints to construct complex model architectures able to reproduce behaviors of physical systems. We discussed properties of models that ensure numerical stability and introduced algorithms and custom regularization functions that encourage model sparsity. We showcased our approach on two examples: the inverted pendulum and swarm dynamics based on the C-S model. We evaluated the time complexity of the learning algorithm. In the case the system dynamics is expressed as a DAE that cannot be reduced to an ODE, the learning procedure is numerically costly, especially for large number of states. We believe that to ensure the scalability of the learning process, we need to come up with efficient algorithms and hardware architectures that can keep the complexity in check.\nReferences\n[1] K. Alloula, F. Monfreda, R. T. He\u0301treux, and J. P. Belaud. Converting dae models to ode models: application to reactive\nrayleigh distillation. Chemical engineering transactions, 32, 2013.\n[2] Y. Cao, S. Li, L. Petzold, and R. Serban. Adjoint sensitivity analysis for differential-algebraic equations: The adjoint dae\nsystem and its numerical solution. SIAM Journal on Scientific Computing, 24(3):1076\u20131089, 2003.\n27\n[3] J. A. Carrillo, M. Fornasier, G. Toscani, and F. Vecil. Particle, kinetic, and hydrodynamic models of swarming. Birkha\u0308user\nBoston, Boston, 2010.\n[4] J. A. Carrillo, S. Martin, and V. Panferov. A new interaction potential for swarming models. Physica D: Nonlinear\nPhenomena, 260:112\u2013126, 2013.\n[5] J. Cervera, A. J. van der Schaft, and A. Ban\u0303os. Interconnection of port-hamiltonian systems and composition of dirac\nstructures. Automatica, 43(2):212\u2013225, 2007.\n[6] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations. Advances in Neural\nInformation Processing Systems, 2018.\n[7] MathWorks Corporations. Inverted pendulum: System modeling. http://ctms.engin.umich.edu/CTMS/index.php?example=\nInvertedPendulum&section=SystemModeling.\n[8] J. de Kleer and J. Kurien. Fundamentals of model-based diagnosis. IFAC Proceedings Volumes, 36(5):25 \u2013 36, 2003. 5th\nIFAC Symposium on Fault Detection, Supervision and Safety of Technical Processes 2003, Washington DC, 9-11 June 1997. [9] J. de Kleer, A. Mackworth, and R. Reiter. Characterizing diagnoses and systems. \u201dJournal of Artificial Inteligence\u201d,\n56(2\u20133):197\u2013222, 1992.\n[10] M. Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from\ntensorflow.org.\n[11] S. Chakraborty et al. Interpretability of deep learning models: A survey of results. In 2017 IEEE SmartWorld, Ubiquitous\nIntelligence Computing, Advanced Trusted Computed, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), pages 1\u20136, Aug 2017.\n[12] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, D. Pedreschi, and F. Giannotti. A survey of methods for explaining black\nbox models, 2018.\n[13] H. K. Khalil. Nonlinear Systems. Pearson Education. Prentice Hall, 2002. [14] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2014. cite arxiv:1412.6980Comment: Published as\na conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.\n[15] I. Matei, J. de Kleer, and R. Minhas. Learning constitutive equations of physical components with constraints discovery.\nIn 2018 Annual American Control Conference (ACC), pages 4819\u20134824, June 2018.\n[16] I. Matei, J. De Kleer, M. Zhenirovskyy, and A. Feldman. Learning constitutive equations of physical components with\npredefined feasibility conditions. In 2019 American Control Conference (ACC), pages 922\u2013927, July 2019.\n[17] T. Miller. Explanation in artificial intelligence: Insights from the social sciences, 2017. [18] C. Molnar. Interpretable Machine Learning. 2019. https://christophm.github.io/interpretable-ml-book/. [19] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu. Definitions, methods, and applications in interpretable\nmachine learning. Proceedings of the National Academy of Sciences, 116(44):22071\u201322080, Oct 2019.\n[20] D. D. Nikolic\u0301. Dae tools: equation-based object-oriented modelling, simulation and optimisation software. PeerJ Computer\nScience, 2:e54, April 2016.\n[21] A. Paszke and et al. Automatic differentiation in pytorch. 2017. [22] R. J. Patton, P. M. Frank, and R. N. Clark. Issues of Fault Diagnosis for Dynamic Systems. Springer-Verlag London, 2000. [23] L. Petzold, S. Li, Y. Cao, and R. Serban. Sensitivity analysis of differential-algebraic equations and partial differential\nequations. Computers and Chemical Engineering, 30(10):1553 \u2013 1559, 2006.\n28\n[24] C. W. Reynolds. Flocks, herds and schools: A distributed behavioral model. In ACM SIGGRAPH computer graphics,\nvolume 21, pages 25\u201334. ACM, 1987.\n[25] R. Serban, , and A. C. Hindmarsh. CVODES: The Sensitivity-Enabled ODE Solver in SUNDIALS. In 5th International\nConference on Multibody Systems, Nonlinear Dynamics, and Control, Parts A, B, and C, volume 6 of International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, pages 257\u2013269, September 2005.\n[26] A. J. van der Schaft. Port-Hamiltonian systems: an introductory survey. In M. Sanz-Sole, J. Soria, J. L. Varona, and\nJ. Verdera, editors, Proceedings of the International Congress of Mathematicians Vol. III, number suppl 2, pages 1339\u2013 1365. European Mathematical Society Publishing House (EMS Ph), 2006.\n[27] A. J. van der Schaft and D. Jeltsema. Port-hamiltonian systems theory: An introductory overview. Foundations and Trends\nin Systems and Control, 1(2-3):173\u2013378, 2014.\n[28] A. J. van der Schaft and B. M. Maschke. Port-hamiltonian systems on graphs. SIAM Journal on Control and Optimization,\n51(2):906\u2013937, 2013."}], "title": "Interpretable machine learning models: a physics-based view", "year": 2020}